====================================================================================================
STL + ARIMA + ES + LSTM
Epoch 34: reducing lr to 1.945464739390692e-06
Epoch 42: reducing lr to 4.789127033506297e-07
Epoch 45: reducing lr to 1.6329873249592577e-07
Epoch 48: reducing lr to 1.2089751319672992e-08
[I 2024-06-21 22:06:39,518] Trial 0 finished with value: 1.075411081314087 and parameters: {'hidden_size': 93, 'n_layers': 6, 'rnn_dropout': 9.149985387590931e-05, 'bidirectional': True, 'fc_dropout': 0.07387087581503825, 'learning_rate_model': 5.5595654267125665e-05}. Best is trial 0 with value: 1.075411081314087.
Epoch 4: reducing lr to 0.0011682718628332068
Epoch 10: reducing lr to 0.0031280717717906505
Epoch 13: reducing lr to 0.0032433886996518077
Epoch 16: reducing lr to 0.003145635061628007
Epoch 19: reducing lr to 0.002952473269805783
Epoch 24: reducing lr to 0.0024538831511928165
Epoch 28: reducing lr to 0.0019469793678262488
Epoch 31: reducing lr to 0.0015401609003945368
Epoch 34: reducing lr to 0.0011388126007422093
Epoch 37: reducing lr to 0.0007681519038442077
Epoch 40: reducing lr to 0.0004514690790738931
Epoch 43: reducing lr to 0.00020866231546405462
Epoch 46: reducing lr to 5.4988408524748314e-05
Epoch 49: reducing lr to 1.0304035796201983e-07
[I 2024-06-21 22:06:46,346] Trial 1 finished with value: 1.1078364849090576 and parameters: {'hidden_size': 79, 'n_layers': 3, 'rnn_dropout': 0.4310533872026856, 'bidirectional': False, 'fc_dropout': 0.16356179978521396, 'learning_rate_model': 0.032543911150884876}. Best is trial 0 with value: 1.075411081314087.
Epoch 16: reducing lr to 0.001542470497102604
Epoch 19: reducing lr to 0.0014477531000695687
Epoch 26: reducing lr to 0.0010829795113895852
Epoch 29: reducing lr to 0.0008886926899030038
Epoch 32: reducing lr to 0.0006887015078112981
Epoch 35: reducing lr to 0.0004955722198325696
Epoch 38: reducing lr to 0.0003214397701550405
Epoch 41: reducing lr to 0.0001772454425749137
Epoch 44: reducing lr to 7.20497069773493e-05
Epoch 47: reducing lr to 1.2462275725858007e-05
[I 2024-06-21 22:06:54,678] Trial 2 finished with value: 0.9998005032539368 and parameters: {'hidden_size': 21, 'n_layers': 5, 'rnn_dropout': 0.3338438418937016, 'bidirectional': True, 'fc_dropout': 0.15848119126790305, 'learning_rate_model': 0.015957993164212966}. Best is trial 2 with value: 0.9998005032539368.
[I 2024-06-21 22:07:15,916] Trial 3 finished with value: 1.1064445972442627 and parameters: {'hidden_size': 195, 'n_layers': 3, 'rnn_dropout': 0.5538580925354513, 'bidirectional': False, 'fc_dropout': 0.06803536909582233, 'learning_rate_model': 1.432910723846784e-05}. Best is trial 2 with value: 0.9998005032539368.
Epoch 28: reducing lr to 0.00035026767494734874
Epoch 31: reducing lr to 0.00027707976085453444
Epoch 34: reducing lr to 0.00020487594704614995
Epoch 37: reducing lr to 0.00013819292890930173
Epoch 43: reducing lr to 3.753900287516917e-05
Epoch 46: reducing lr to 9.89258659917008e-06
Epoch 49: reducing lr to 1.8537282523659743e-08
[I 2024-06-21 22:07:25,306] Trial 4 finished with value: 1.1075823307037354 and parameters: {'hidden_size': 47, 'n_layers': 7, 'rnn_dropout': 0.07867746706644008, 'bidirectional': False, 'fc_dropout': 0.42653222797841367, 'learning_rate_model': 0.005854751355295724}. Best is trial 2 with value: 0.9998005032539368.
Epoch 12: reducing lr to 0.0009829860871492667
Epoch 15: reducing lr to 0.000963566697187542
Epoch 18: reducing lr to 0.0009144936843024847
Epoch 21: reducing lr to 0.00083885049495203
Epoch 24: reducing lr to 0.0007413900456616357
Epoch 27: reducing lr to 0.000628236237061796
Epoch 30: reducing lr to 0.0005064987736920169
Epoch 33: reducing lr to 0.0003838269498813498
Epoch 36: reducing lr to 0.00026792873097625437
Epoch 39: reducing lr to 0.00016608637928437303
Epoch 42: reducing lr to 8.469897000136246e-05
Epoch 45: reducing lr to 2.8880491889576344e-05
Epoch 48: reducing lr to 2.1381547768199728e-06
[I 2024-06-21 22:07:34,647] Trial 5 finished with value: 1.0987299680709839 and parameters: {'hidden_size': 74, 'n_layers': 5, 'rnn_dropout': 0.6677005375178984, 'bidirectional': False, 'fc_dropout': 0.7910888711251958, 'learning_rate_model': 0.009832469717408681}. Best is trial 2 with value: 0.9998005032539368.
Epoch 17: reducing lr to 1.3451854401357507e-05
Epoch 20: reducing lr to 1.2483466976910361e-05
Epoch 25: reducing lr to 1.0154947628181115e-05
Epoch 28: reducing lr to 8.472136768203113e-06
Epoch 31: reducing lr to 6.701896285500768e-06
Epoch 34: reducing lr to 4.9554588334508234e-06
Epoch 37: reducing lr to 3.3425562158830798e-06
Epoch 40: reducing lr to 1.964534317998006e-06
Epoch 43: reducing lr to 9.079786381892356e-07
Epoch 46: reducing lr to 2.3927799409997105e-07
Epoch 49: reducing lr to 4.4837249933379374e-10
[I 2024-06-21 22:07:44,730] Trial 6 finished with value: 1.1069021224975586 and parameters: {'hidden_size': 67, 'n_layers': 6, 'rnn_dropout': 0.08258080526211363, 'bidirectional': False, 'fc_dropout': 0.2348913186989436, 'learning_rate_model': 0.00014161242322273667}. Best is trial 2 with value: 0.9998005032539368.
[I 2024-06-21 22:07:46,612] Trial 7 finished with value: 1.1112606525421143 and parameters: {'hidden_size': 40, 'n_layers': 1, 'rnn_dropout': 0.5430684263519128, 'bidirectional': False, 'fc_dropout': 0.39325852742427064, 'learning_rate_model': 1.634745612637677e-05}. Best is trial 2 with value: 0.9998005032539368.
Epoch 16: reducing lr to 0.0005792142290121482
Epoch 22: reducing lr to 0.0004927465115481645
Epoch 25: reducing lr to 0.0004297114881740523
Epoch 28: reducing lr to 0.00035850253807076895
Epoch 31: reducing lr to 0.00028359396147338995
Epoch 34: reducing lr to 0.0002096926215550373
Epoch 37: reducing lr to 0.00014144187231912027
Epoch 40: reducing lr to 8.313021359295028e-05
Epoch 43: reducing lr to 3.8421552343980914e-05
Epoch 46: reducing lr to 1.0125163289534128e-05
Epoch 49: reducing lr to 1.8973097744984236e-08
[I 2024-06-21 22:08:01,966] Trial 8 finished with value: 1.0240590572357178 and parameters: {'hidden_size': 122, 'n_layers': 2, 'rnn_dropout': 0.47144442952262744, 'bidirectional': True, 'fc_dropout': 0.3312447902556547, 'learning_rate_model': 0.005992397731141758}. Best is trial 2 with value: 0.9998005032539368.
Epoch 23: reducing lr to 0.00017514179104609657
Epoch 27: reducing lr to 0.0001418014029422349
Epoch 30: reducing lr to 0.00011432361341325304
Epoch 33: reducing lr to 8.663492611436326e-05
Epoch 38: reducing lr to 4.470349683037547e-05
Epoch 41: reducing lr to 2.4650002320883956e-05
Epoch 44: reducing lr to 1.0020147307652356e-05
Epoch 47: reducing lr to 1.7331623375086997e-06
[I 2024-06-21 22:08:06,604] Trial 9 finished with value: 0.9829268455505371 and parameters: {'hidden_size': 92, 'n_layers': 1, 'rnn_dropout': 0.4287171247324093, 'bidirectional': True, 'fc_dropout': 0.7556758047926507, 'learning_rate_model': 0.002219321201267233}. Best is trial 9 with value: 0.9829268455505371.
Epoch 7: reducing lr to 0.0036527207546016966
Epoch 10: reducing lr to 0.004929936043323959
Epoch 18: reducing lr to 0.004770373837948047
Epoch 21: reducing lr to 0.004375787961970578
Epoch 24: reducing lr to 0.0038673943169295275
Epoch 27: reducing lr to 0.003277137678229426
Epoch 30: reducing lr to 0.002642105178469411
Epoch 33: reducing lr to 0.0020021986717272426
Epoch 36: reducing lr to 0.0013976260641522195
Epoch 39: reducing lr to 0.000866374620380237
Epoch 42: reducing lr to 0.00044182453912060065
Epoch 45: reducing lr to 0.00015065248158841886
Epoch 48: reducing lr to 1.1153491581087498e-05
[I 2024-06-21 22:08:16,893] Trial 10 finished with value: 1.1720671653747559 and parameters: {'hidden_size': 183, 'n_layers': 1, 'rnn_dropout': 0.11142107780060684, 'bidirectional': True, 'fc_dropout': 0.13228335769354624, 'learning_rate_model': 0.051290191619112704}. Best is trial 9 with value: 0.9829268455505371.
Epoch 14: reducing lr to 2.461713439637327e-05
Epoch 24: reducing lr to 1.8748973750140736e-05
Epoch 27: reducing lr to 1.5887433054279965e-05
Epoch 30: reducing lr to 1.28088207658026e-05
Epoch 33: reducing lr to 9.706579485431035e-06
Epoch 36: reducing lr to 6.775635542151512e-06
Epoch 39: reducing lr to 4.200149683261068e-06
Epoch 42: reducing lr to 2.141947783777313e-06
Epoch 45: reducing lr to 7.30357235705244e-07
Epoch 48: reducing lr to 5.407168334524513e-08
[I 2024-06-21 22:08:44,377] Trial 11 finished with value: 1.0756374597549438 and parameters: {'hidden_size': 80, 'n_layers': 6, 'rnn_dropout': 0.5807983882803612, 'bidirectional': True, 'fc_dropout': 0.6007539472218698, 'learning_rate_model': 0.0002486528079375971}. Best is trial 9 with value: 0.9829268455505371.
[I 2024-06-21 22:09:10,261] Trial 12 finished with value: 1.101548194885254 and parameters: {'hidden_size': 65, 'n_layers': 7, 'rnn_dropout': 0.34247295189703597, 'bidirectional': True, 'fc_dropout': 0.49735657616729745, 'learning_rate_model': 2.877291692654975e-05}. Best is trial 9 with value: 0.9829268455505371.
Epoch 16: reducing lr to 0.00019052700386256702
Epoch 22: reducing lr to 0.00016208427177129056
Epoch 27: reducing lr to 0.0001259442406662781
Epoch 36: reducing lr to 5.37124071882607e-05
Epoch 39: reducing lr to 3.329579175790887e-05
Epoch 42: reducing lr to 1.6979834706650605e-05
Epoch 45: reducing lr to 5.789751380965813e-06
Epoch 48: reducing lr to 4.286417495638368e-07
[I 2024-06-21 22:10:22,125] Trial 13 finished with value: 0.9798255562782288 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.4627116915097054, 'bidirectional': True, 'fc_dropout': 0.7227036164498031, 'learning_rate_model': 0.001971142158600765}. Best is trial 13 with value: 0.9798255562782288.
Epoch 13: reducing lr to 0.004292100251866992
Epoch 22: reducing lr to 0.0035413066185079694
Epoch 27: reducing lr to 0.0027516992744600834
Epoch 30: reducing lr to 0.0022184844264979122
Epoch 33: reducing lr to 0.001681177043282919
Epoch 36: reducing lr to 0.0011735383143170234
Epoch 39: reducing lr to 0.0007274648331524961
Epoch 42: reducing lr to 0.0003709847992696101
Epoch 45: reducing lr to 0.0001264976833400662
Epoch 48: reducing lr to 9.365201497411185e-06
[I 2024-06-21 22:10:25,357] Trial 14 finished with value: 1.104465126991272 and parameters: {'hidden_size': 16, 'n_layers': 5, 'rnn_dropout': 0.26131592141767696, 'bidirectional': False, 'fc_dropout': 0.28581580800199985, 'learning_rate_model': 0.04306660168805714}. Best is trial 13 with value: 0.9798255562782288.
[I 2024-06-21 22:10:29,108] Trial 15 finished with value: 1.1016825437545776 and parameters: {'hidden_size': 131, 'n_layers': 1, 'rnn_dropout': 0.7435497869950091, 'bidirectional': False, 'fc_dropout': 0.13787240667626285, 'learning_rate_model': 3.5362503098265445e-05}. Best is trial 13 with value: 0.9798255562782288.
Epoch 12: reducing lr to 0.0007014291139487203
Epoch 15: reducing lr to 0.000687572025153313
Epoch 18: reducing lr to 0.0006525550087410218
Epoch 21: reducing lr to 0.000598578318759358
Epoch 27: reducing lr to 0.00044829036023357144
Epoch 30: reducing lr to 0.00036142219171913517
Epoch 33: reducing lr to 0.0002738872918798827
Epoch 36: reducing lr to 0.000191185831444572
Epoch 39: reducing lr to 0.00011851421234072723
Epoch 42: reducing lr to 6.0438620909395845e-05
Epoch 45: reducing lr to 2.060824471611525e-05
Epoch 48: reducing lr to 1.5257225205897624e-06
[I 2024-06-21 22:11:58,112] Trial 16 finished with value: 1.1469486951828003 and parameters: {'hidden_size': 188, 'n_layers': 5, 'rnn_dropout': 0.052800138177649995, 'bidirectional': True, 'fc_dropout': 0.7384196284371867, 'learning_rate_model': 0.007016152732955534}. Best is trial 13 with value: 0.9798255562782288.
Epoch 13: reducing lr to 0.0001425126812570336
Epoch 23: reducing lr to 0.00011284800401774833
Epoch 26: reducing lr to 9.704344894727592e-05
Epoch 29: reducing lr to 7.963382757976787e-05
Epoch 32: reducing lr to 6.171305081057547e-05
Epoch 35: reducing lr to 4.440715351419959e-05
Epoch 38: reducing lr to 2.8803521762512243e-05
Epoch 41: reducing lr to 1.58825803043917e-05
Epoch 44: reducing lr to 6.456218226835275e-06
Epoch 47: reducing lr to 1.1167175421605489e-06
[I 2024-06-21 22:12:00,208] Trial 17 finished with value: 1.092000961303711 and parameters: {'hidden_size': 38, 'n_layers': 1, 'rnn_dropout': 0.020968789502175424, 'bidirectional': False, 'fc_dropout': 0.6880223589463105, 'learning_rate_model': 0.0014299612122349629}. Best is trial 13 with value: 0.9798255562782288.
Epoch 5: reducing lr to 8.359503352487266e-05
Epoch 29: reducing lr to 9.769992045771066e-05
Epoch 32: reducing lr to 7.571355463677006e-05
Epoch 35: reducing lr to 5.448156264678812e-05
Epoch 38: reducing lr to 3.533801991723361e-05
Epoch 41: reducing lr to 1.9485774821609986e-05
Epoch 44: reducing lr to 7.920905303560782e-06
Epoch 47: reducing lr to 1.3700611707195483e-06
[I 2024-06-21 22:12:20,688] Trial 18 finished with value: 1.0937247276306152 and parameters: {'hidden_size': 118, 'n_layers': 6, 'rnn_dropout': 0.09933865209592892, 'bidirectional': False, 'fc_dropout': 0.7756765986557397, 'learning_rate_model': 0.0017543687266950109}. Best is trial 13 with value: 0.9798255562782288.
Epoch 23: reducing lr to 0.000768521807564351
Epoch 26: reducing lr to 0.000660889020115142
Epoch 29: reducing lr to 0.0005423253485745807
Epoch 32: reducing lr to 0.0004202805869016063
Epoch 35: reducing lr to 0.0003024233010107363
Epoch 38: reducing lr to 0.0001961588492576575
Epoch 41: reducing lr to 0.0001081641578915063
Epoch 44: reducing lr to 4.39683851292309e-05
Epoch 47: reducing lr to 7.60511266025641e-06
[I 2024-06-21 22:12:29,325] Trial 19 finished with value: 0.9979004263877869 and parameters: {'hidden_size': 19, 'n_layers': 6, 'rnn_dropout': 0.18637941907281635, 'bidirectional': True, 'fc_dropout': 0.6908334836475429, 'learning_rate_model': 0.009738376723091037}. Best is trial 13 with value: 0.9798255562782288.
[I 2024-06-21 22:12:35,418] Trial 20 finished with value: 1.0673646926879883 and parameters: {'hidden_size': 118, 'n_layers': 1, 'rnn_dropout': 0.04793415160976933, 'bidirectional': True, 'fc_dropout': 0.08599530328487433, 'learning_rate_model': 7.995347689531267e-05}. Best is trial 13 with value: 0.9798255562782288.
[I 2024-06-21 22:12:53,734] Trial 21 finished with value: 1.101003885269165 and parameters: {'hidden_size': 147, 'n_layers': 4, 'rnn_dropout': 0.010044784127292684, 'bidirectional': False, 'fc_dropout': 0.4544803695359537, 'learning_rate_model': 6.503886274016054e-05}. Best is trial 13 with value: 0.9798255562782288.
Epoch 17: reducing lr to 8.651110121155873e-06
Epoch 25: reducing lr to 6.530814829299657e-06
Epoch 28: reducing lr to 5.448571323803673e-06
Epoch 31: reducing lr to 4.3101003814449295e-06
Epoch 34: reducing lr to 3.1869375619105275e-06
Epoch 37: reducing lr to 2.1496532037129698e-06
Epoch 40: reducing lr to 1.2634245223525096e-06
Epoch 43: reducing lr to 5.8393608436912e-07
Epoch 46: reducing lr to 1.5388363676603882e-07
Epoch 49: reducing lr to 2.883557724683242e-10
[I 2024-06-21 22:13:03,688] Trial 22 finished with value: 1.106992483139038 and parameters: {'hidden_size': 62, 'n_layers': 6, 'rnn_dropout': 0.15634358488745503, 'bidirectional': False, 'fc_dropout': 0.6774630411920283, 'learning_rate_model': 9.107329229641331e-05}. Best is trial 13 with value: 0.9798255562782288.
[I 2024-06-21 22:13:40,099] Trial 23 finished with value: 1.0233967304229736 and parameters: {'hidden_size': 107, 'n_layers': 5, 'rnn_dropout': 0.663184719640143, 'bidirectional': True, 'fc_dropout': 0.056017714975377865, 'learning_rate_model': 0.0008818210135856344}. Best is trial 13 with value: 0.9798255562782288.
Epoch 22: reducing lr to 0.0001314667127817124
Epoch 25: reducing lr to 0.00011464871992149727
Epoch 28: reducing lr to 9.564989117017374e-05
Epoch 31: reducing lr to 7.566398747808456e-05
Epoch 34: reducing lr to 5.5946818504722695e-05
Epoch 37: reducing lr to 3.773724941260761e-05
Epoch 40: reducing lr to 2.2179468870452936e-05
Epoch 43: reducing lr to 1.0251021708430549e-05
Epoch 46: reducing lr to 2.7014335015195823e-06
Epoch 49: reducing lr to 5.0620973124186616e-09
[I 2024-06-21 22:14:17,070] Trial 24 finished with value: 0.9786233901977539 and parameters: {'hidden_size': 128, 'n_layers': 4, 'rnn_dropout': 0.2538899274577286, 'bidirectional': True, 'fc_dropout': 0.3041129380988403, 'learning_rate_model': 0.0015987953500037988}. Best is trial 24 with value: 0.9786233901977539.
Epoch 9: reducing lr to 6.238537953422173e-06
Epoch 12: reducing lr to 6.927567931610175e-06
Epoch 15: reducing lr to 6.7907103047230825e-06
Epoch 18: reducing lr to 6.444869570236275e-06
Epoch 21: reducing lr to 5.911776233881299e-06
[I 2024-06-21 22:14:42,201] Trial 25 finished with value: 1.1072602272033691 and parameters: {'hidden_size': 153, 'n_layers': 5, 'rnn_dropout': 0.2119356461302475, 'bidirectional': False, 'fc_dropout': 0.5037740056172516, 'learning_rate_model': 6.929406508731719e-05}. Best is trial 24 with value: 0.9786233901977539.
Epoch 23: reducing lr to 9.90491746636851e-05
Epoch 27: reducing lr to 8.01939493920349e-05
Epoch 33: reducing lr to 4.8995261938474235e-05
Epoch 38: reducing lr to 2.528148444287616e-05
Epoch 41: reducing lr to 1.3940489992470559e-05
Epoch 44: reducing lr to 5.666764710487039e-06
Epoch 47: reducing lr to 9.80167543469042e-07
[I 2024-06-21 22:14:50,337] Trial 26 finished with value: 0.9835362434387207 and parameters: {'hidden_size': 155, 'n_layers': 1, 'rnn_dropout': 0.20825207886283278, 'bidirectional': True, 'fc_dropout': 0.5115687047039521, 'learning_rate_model': 0.001255108400948586}. Best is trial 24 with value: 0.9786233901977539.
Epoch 5: reducing lr to 0.0025465246007663816
Epoch 8: reducing lr to 0.0043583447146142784
Epoch 11: reducing lr to 0.005314071493398399
Epoch 16: reducing lr to 0.005165672492335653
Epoch 19: reducing lr to 0.004848467687888337
Epoch 22: reducing lr to 0.004394517560005108
Epoch 25: reducing lr to 0.003832345102928683
Epoch 28: reducing lr to 0.0031972741804066505
Epoch 31: reducing lr to 0.00252920845586627
Epoch 34: reducing lr to 0.0018701256853789893
Epoch 37: reducing lr to 0.0012614372239256697
Epoch 40: reducing lr to 0.0007413896899105433
Epoch 43: reducing lr to 0.0003426593238129411
Epoch 46: reducing lr to 9.03004015877798e-05
Epoch 49: reducing lr to 1.6920994721225657e-07
[I 2024-06-21 22:15:03,029] Trial 27 finished with value: 1.1735916137695312 and parameters: {'hidden_size': 187, 'n_layers': 2, 'rnn_dropout': 0.05276887254721903, 'bidirectional': False, 'fc_dropout': 0.7262526820028192, 'learning_rate_model': 0.05344268592242116}. Best is trial 24 with value: 0.9786233901977539.
Epoch 8: reducing lr to 0.0037485102196038816
Epoch 11: reducing lr to 0.004570508439572225
Epoch 14: reducing lr to 0.00455060906722266
Epoch 17: reducing lr to 0.004366225912247097
Epoch 20: reducing lr to 0.004051905065502824
Epoch 23: reducing lr to 0.0036273964502096983
Epoch 26: reducing lr to 0.0031193734022277528
Epoch 29: reducing lr to 0.0025597569579877523
Epoch 32: reducing lr to 0.0019837098882731206
Epoch 35: reducing lr to 0.0014274275599592372
Epoch 38: reducing lr to 0.0009258630093133255
Epoch 41: reducing lr to 0.0005105310981597854
Epoch 44: reducing lr to 0.000207529262760537
Epoch 47: reducing lr to 3.589586970171796e-05
[I 2024-06-21 22:15:05,261] Trial 28 finished with value: 1.103877067565918 and parameters: {'hidden_size': 18, 'n_layers': 2, 'rnn_dropout': 0.4934226856013261, 'bidirectional': False, 'fc_dropout': 0.44532255055612513, 'learning_rate_model': 0.04596480257092418}. Best is trial 24 with value: 0.9786233901977539.
Epoch 10: reducing lr to 0.004547164923115793
Epoch 13: reducing lr to 0.004714796974956969
Epoch 16: reducing lr to 0.004572696043022684
Epoch 19: reducing lr to 0.004291903724834468
Epoch 22: reducing lr to 0.0038900633145914397
Epoch 25: reducing lr to 0.00339242360286295
Epoch 28: reducing lr to 0.002830253566190294
Epoch 31: reducing lr to 0.002238876257695146
Epoch 34: reducing lr to 0.0016554507344736888
Epoch 37: reducing lr to 0.0011166346706889963
Epoch 40: reducing lr to 0.0006562842894941065
Epoch 43: reducing lr to 0.0003033248694006542
Epoch 46: reducing lr to 7.993466284137163e-05
Epoch 49: reducing lr to 1.497860457090605e-07
[I 2024-06-21 22:15:34,853] Trial 29 finished with value: 1.1434566974639893 and parameters: {'hidden_size': 134, 'n_layers': 3, 'rnn_dropout': 0.3887925336775278, 'bidirectional': True, 'fc_dropout': 0.740945141365163, 'learning_rate_model': 0.047307907887800385}. Best is trial 24 with value: 0.9786233901977539.
[I 2024-06-21 22:15:50,671] Trial 30 finished with value: 1.1095914840698242 and parameters: {'hidden_size': 89, 'n_layers': 7, 'rnn_dropout': 0.13916453334437148, 'bidirectional': False, 'fc_dropout': 0.40452973254151736, 'learning_rate_model': 1.2192681272217677e-05}. Best is trial 24 with value: 0.9786233901977539.
Epoch 3: reducing lr to 0.0004351627280700692
Epoch 6: reducing lr to 0.001032167238589228
Epoch 9: reducing lr to 0.0015572184850538126
Epoch 12: reducing lr to 0.001729209138441118
Epoch 15: reducing lr to 0.0016950477326758023
Epoch 18: reducing lr to 0.0016087214830563666
Epoch 21: reducing lr to 0.0014756546004263426
Epoch 24: reducing lr to 0.0013042081254937467
Epoch 27: reducing lr to 0.001105154850540784
Epoch 30: reducing lr to 0.0008910017339283646
Epoch 33: reducing lr to 0.0006752049474470591
Epoch 36: reducing lr to 0.0004713238733609031
Epoch 39: reducing lr to 0.0002921690231262899
[I 2024-06-21 22:16:33,851] Trial 31 finished with value: 1.1053544282913208 and parameters: {'hidden_size': 191, 'n_layers': 6, 'rnn_dropout': 0.012015184593697903, 'bidirectional': False, 'fc_dropout': 0.10479747584873352, 'learning_rate_model': 0.017296680706943544}. Best is trial 24 with value: 0.9786233901977539.
Epoch 27: reducing lr to 4.4155156553781704e-05
Epoch 34: reducing lr to 2.4182632810605335e-05
Epoch 47: reducing lr to 5.396847475267223e-07
[I 2024-06-21 22:17:08,097] Trial 32 finished with value: 1.044336199760437 and parameters: {'hidden_size': 79, 'n_layers': 7, 'rnn_dropout': 0.46561134395766407, 'bidirectional': True, 'fc_dropout': 0.7243138549669195, 'learning_rate_model': 0.0006910684453876749}. Best is trial 24 with value: 0.9786233901977539.
Epoch 4: reducing lr to 8.491800923926532e-05
Epoch 22: reducing lr to 0.0001945129678049095
Epoch 25: reducing lr to 0.00016962972827952544
Epoch 29: reducing lr to 0.0001317343296778956
Epoch 32: reducing lr to 0.00010208886886374591
Epoch 36: reducing lr to 6.44588127889218e-05
Epoch 39: reducing lr to 3.9957382659458343e-05
Epoch 42: reducing lr to 2.0377042174010793e-05
Epoch 45: reducing lr to 6.948124649338791e-06
Epoch 48: reducing lr to 5.144014155204247e-07
[I 2024-06-21 22:17:28,312] Trial 33 finished with value: 1.0936726331710815 and parameters: {'hidden_size': 117, 'n_layers': 6, 'rnn_dropout': 0.22857508138731808, 'bidirectional': False, 'fc_dropout': 0.012426620440668579, 'learning_rate_model': 0.002365514599564762}. Best is trial 24 with value: 0.9786233901977539.
Epoch 5: reducing lr to 0.0006757351876047445
Epoch 12: reducing lr to 0.0014177566362140773
Epoch 18: reducing lr to 0.0013189703360458962
Epoch 24: reducing lr to 0.0010693036971745362
Epoch 27: reducing lr to 0.0009061024421897748
Epoch 30: reducing lr to 0.000730521018581931
Epoch 33: reducing lr to 0.0005535919708998475
Epoch 36: reducing lr to 0.0003864324646502543
Epoch 39: reducing lr to 0.00023954567566471761
Epoch 42: reducing lr to 0.00012216096277433281
Epoch 45: reducing lr to 4.1654210134671116e-05
Epoch 48: reducing lr to 3.0838515048372053e-06
[I 2024-06-21 22:18:02,544] Trial 34 finished with value: 1.1176776885986328 and parameters: {'hidden_size': 96, 'n_layers': 6, 'rnn_dropout': 0.2521958424762984, 'bidirectional': True, 'fc_dropout': 0.14720816130197206, 'learning_rate_model': 0.01418132908946585}. Best is trial 24 with value: 0.9786233901977539.
Epoch 5: reducing lr to 0.0038467697781144504
Epoch 8: reducing lr to 0.006583697925296865
Epoch 11: reducing lr to 0.008027414937751798
Epoch 14: reducing lr to 0.007992464664502607
Epoch 17: reducing lr to 0.00766862321183062
Epoch 20: reducing lr to 0.007116565624855041
Epoch 23: reducing lr to 0.0063709796917663115
Epoch 26: reducing lr to 0.005478713140241431
Epoch 29: reducing lr to 0.004495830499656224
Epoch 32: reducing lr to 0.0034840899212473624
Epoch 35: reducing lr to 0.0025070631569488666
Epoch 38: reducing lr to 0.0016261399906679185
Epoch 41: reducing lr to 0.0008966715667936206
Epoch 44: reducing lr to 0.0003644941314363867
Epoch 47: reducing lr to 6.30457299131762e-05
[I 2024-06-21 22:18:06,183] Trial 35 finished with value: 1.1909905672073364 and parameters: {'hidden_size': 129, 'n_layers': 1, 'rnn_dropout': 0.3361549440009058, 'bidirectional': False, 'fc_dropout': 0.0003216199130859465, 'learning_rate_model': 0.08073030553318121}. Best is trial 24 with value: 0.9786233901977539.
Epoch 28: reducing lr to 8.302267657750138e-06
Epoch 32: reducing lr to 5.989049269862941e-06
Epoch 35: reducing lr to 4.309568670446168e-06
Epoch 42: reducing lr to 1.1954203324272948e-06
Epoch 45: reducing lr to 4.0761212579970987e-07
Epoch 48: reducing lr to 3.017738815532097e-08
[I 2024-06-21 22:18:43,000] Trial 36 finished with value: 1.075834035873413 and parameters: {'hidden_size': 85, 'n_layers': 7, 'rnn_dropout': 0.48377288077924246, 'bidirectional': True, 'fc_dropout': 0.5024609586458804, 'learning_rate_model': 0.00013877304786559896}. Best is trial 24 with value: 0.9786233901977539.
Epoch 20: reducing lr to 1.7217014679186518e-05
Epoch 24: reducing lr to 1.4726790899432328e-05
Epoch 27: reducing lr to 1.247913126537682e-05
Epoch 30: reducing lr to 1.0060968008175147e-05
Epoch 33: reducing lr to 7.624244843245849e-06
Epoch 42: reducing lr to 1.6824396657419286e-06
Epoch 45: reducing lr to 5.736750413892836e-07
Epoch 48: reducing lr to 4.247178457966478e-08
[I 2024-06-21 22:19:37,367] Trial 37 finished with value: 1.0745429992675781 and parameters: {'hidden_size': 124, 'n_layers': 6, 'rnn_dropout': 0.6866510691432339, 'bidirectional': True, 'fc_dropout': 0.6915835440436799, 'learning_rate_model': 0.00019530977843660444}. Best is trial 24 with value: 0.9786233901977539.
Epoch 12: reducing lr to 0.000307299762906055
Epoch 16: reducing lr to 0.0002971091747113483
Epoch 22: reducing lr to 0.0002527553745315543
Epoch 25: reducing lr to 0.000220421424786338
Epoch 28: reducing lr to 0.00018389464188369565
Epoch 31: reducing lr to 0.00014547012767656575
Epoch 34: reducing lr to 0.00010756227767320528
Epoch 37: reducing lr to 7.255291021775008e-05
Epoch 40: reducing lr to 4.264182044751105e-05
Epoch 43: reducing lr to 1.9708417259565666e-05
Epoch 46: reducing lr to 5.193724114653997e-06
Epoch 49: reducing lr to 9.732290973465951e-09
[I 2024-06-21 22:20:20,006] Trial 38 finished with value: 1.0158584117889404 and parameters: {'hidden_size': 140, 'n_layers': 4, 'rnn_dropout': 0.3056822016252138, 'bidirectional': True, 'fc_dropout': 0.2539071567666215, 'learning_rate_model': 0.003073813202894127}. Best is trial 24 with value: 0.9786233901977539.
Epoch 5: reducing lr to 0.0007389495944000798
Epoch 8: reducing lr to 0.0012647029045589069
Epoch 13: reducing lr to 0.001545555018040695
Epoch 16: reducing lr to 0.0014989729892522126
Epoch 21: reducing lr to 0.0013230527610106212
Epoch 27: reducing lr to 0.000990867494283429
Epoch 33: reducing lr to 0.0006053799918421043
Epoch 40: reducing lr to 0.0002151361940453747
Epoch 43: reducing lr to 9.943275956288619e-05
Epoch 46: reducing lr to 2.6203338113196457e-05
Epoch 49: reducing lr to 4.9101281732362445e-08
[I 2024-06-21 22:20:38,422] Trial 39 finished with value: 1.10591459274292 and parameters: {'hidden_size': 95, 'n_layers': 7, 'rnn_dropout': 0.5422407131474489, 'bidirectional': False, 'fc_dropout': 0.2746769918195384, 'learning_rate_model': 0.015507979414037055}. Best is trial 24 with value: 0.9786233901977539.
Epoch 11: reducing lr to 0.00012835054035425725
Epoch 48: reducing lr to 2.8069527843860556e-07
[I 2024-06-21 22:22:34,141] Trial 40 finished with value: 1.0037192106246948 and parameters: {'hidden_size': 178, 'n_layers': 7, 'rnn_dropout': 0.5301758499002098, 'bidirectional': True, 'fc_dropout': 0.6839183541579219, 'learning_rate_model': 0.001290798895846048}. Best is trial 24 with value: 0.9786233901977539.
Epoch 6: reducing lr to 4.351378044650393e-05
Epoch 16: reducing lr to 7.048198158742115e-05
Epoch 19: reducing lr to 6.615394429514821e-05
Epoch 22: reducing lr to 5.996011288161107e-05
Epoch 25: reducing lr to 5.228966361727911e-05
Epoch 28: reducing lr to 4.362456587166771e-05
Epoch 31: reducing lr to 3.450927717187015e-05
Epoch 34: reducing lr to 2.5516554585799428e-05
Epoch 37: reducing lr to 1.7211427035362978e-05
Epoch 40: reducing lr to 1.0115742829401076e-05
Epoch 43: reducing lr to 4.675346373115088e-06
Epoch 46: reducing lr to 1.2320857064573409e-06
Epoch 49: reducing lr to 2.308751164824697e-09
[I 2024-06-21 22:23:01,043] Trial 41 finished with value: 1.092954397201538 and parameters: {'hidden_size': 164, 'n_layers': 5, 'rnn_dropout': 0.5865140202300089, 'bidirectional': False, 'fc_dropout': 0.4550863925637725, 'learning_rate_model': 0.000729188002289183}. Best is trial 24 with value: 0.9786233901977539.
Epoch 5: reducing lr to 0.0008423535105367442
Epoch 10: reducing lr to 0.001699189260287838
Epoch 16: reducing lr to 0.001708729755405658
Epoch 22: reducing lr to 0.0014536428560994953
Epoch 25: reducing lr to 0.0012676843373391118
Epoch 28: reducing lr to 0.0010576120604542754
Epoch 31: reducing lr to 0.0008366255802268694
Epoch 34: reducing lr to 0.0006186105312903061
Epoch 37: reducing lr to 0.00041726519098842733
Epoch 40: reducing lr to 0.0002452409875733955
Epoch 43: reducing lr to 0.00011334674883765536
Epoch 46: reducing lr to 2.987006693650311e-05
Epoch 49: reducing lr to 5.5972203453005795e-08
[I 2024-06-21 22:23:03,782] Trial 42 finished with value: 1.109168529510498 and parameters: {'hidden_size': 79, 'n_layers': 1, 'rnn_dropout': 0.3023393434624799, 'bidirectional': False, 'fc_dropout': 0.14529028106461103, 'learning_rate_model': 0.01767806762428918}. Best is trial 24 with value: 0.9786233901977539.
Epoch 17: reducing lr to 7.5481519613878205e-06
[I 2024-06-21 22:23:34,979] Trial 43 finished with value: 1.1062343120574951 and parameters: {'hidden_size': 177, 'n_layers': 5, 'rnn_dropout': 0.4555955301963006, 'bidirectional': False, 'fc_dropout': 0.276137640924172, 'learning_rate_model': 7.946206212265512e-05}. Best is trial 24 with value: 0.9786233901977539.
[I 2024-06-21 22:24:01,008] Trial 44 finished with value: 1.0668749809265137 and parameters: {'hidden_size': 125, 'n_layers': 3, 'rnn_dropout': 0.7330444427746806, 'bidirectional': True, 'fc_dropout': 0.08871304059522336, 'learning_rate_model': 5.91358615023068e-05}. Best is trial 24 with value: 0.9786233901977539.
Epoch 5: reducing lr to 0.00013957250330996637
Epoch 8: reducing lr to 0.00023887657787535424
Epoch 11: reducing lr to 0.0002912590205798642
Epoch 15: reducing lr to 0.0002870512435197667
Epoch 18: reducing lr to 0.0002724321523732661
Epoch 22: reducing lr to 0.00024085917587637998
Epoch 29: reducing lr to 0.0001631223996986613
Epoch 32: reducing lr to 0.0001264133754071128
Epoch 36: reducing lr to 7.981728262910042e-05
Epoch 39: reducing lr to 4.947794672068842e-05
Epoch 42: reducing lr to 2.5232238447737065e-05
Epoch 45: reducing lr to 8.603640136757357e-06
Epoch 48: reducing lr to 6.36966791520881e-07
[I 2024-06-21 22:24:18,868] Trial 45 finished with value: 1.093345046043396 and parameters: {'hidden_size': 108, 'n_layers': 6, 'rnn_dropout': 0.1665555507270332, 'bidirectional': False, 'fc_dropout': 0.3326789746140219, 'learning_rate_model': 0.0029291409380281584}. Best is trial 24 with value: 0.9786233901977539.
Epoch 38: reducing lr to 3.021077103497643e-05
Epoch 42: reducing lr to 1.2919805830576935e-05
Epoch 45: reducing lr to 4.405370543453788e-06
Epoch 48: reducing lr to 3.261497105784333e-07
[I 2024-06-21 22:24:22,183] Trial 46 finished with value: 0.9815720915794373 and parameters: {'hidden_size': 59, 'n_layers': 1, 'rnn_dropout': 0.41268561357482386, 'bidirectional': True, 'fc_dropout': 0.4974449853923324, 'learning_rate_model': 0.0014998246092237527}. Best is trial 24 with value: 0.9786233901977539.
[I 2024-06-21 22:24:30,353] Trial 47 finished with value: 1.1046767234802246 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.6012222537081952, 'bidirectional': False, 'fc_dropout': 0.6282368225772952, 'learning_rate_model': 1.2283488171661822e-05}. Best is trial 24 with value: 0.9786233901977539.
Epoch 8: reducing lr to 0.0016483950080788006
Epoch 11: reducing lr to 0.002009866014709446
Epoch 14: reducing lr to 0.0020011153313383894
Epoch 23: reducing lr to 0.0015951356273693953
Epoch 28: reducing lr to 0.001209259740462727
Epoch 40: reducing lr to 0.0002804053244782834
Epoch 43: reducing lr to 0.0001295991840551094
Epoch 46: reducing lr to 3.415304225608505e-05
Epoch 49: reducing lr to 6.399788235328385e-08
[I 2024-06-21 22:24:43,768] Trial 48 finished with value: 1.1028343439102173 and parameters: {'hidden_size': 76, 'n_layers': 7, 'rnn_dropout': 0.6757676860816557, 'bidirectional': False, 'fc_dropout': 0.7598447930829013, 'learning_rate_model': 0.020212870358199457}. Best is trial 24 with value: 0.9786233901977539.
[I 2024-06-21 22:24:53,333] Trial 49 finished with value: 1.0999655723571777 and parameters: {'hidden_size': 174, 'n_layers': 1, 'rnn_dropout': 0.5210434658727928, 'bidirectional': True, 'fc_dropout': 0.6396922093888224, 'learning_rate_model': 1.3749435862460687e-05}. Best is trial 24 with value: 0.9786233901977539.
Epoch 11: reducing lr to 0.0015271926603428868
Epoch 14: reducing lr to 0.0015205434711335285
Epoch 17: reducing lr to 0.0014589335639005596
Epoch 20: reducing lr to 0.001353906191894319
Epoch 23: reducing lr to 0.0012120606072948382
Epoch 26: reducing lr to 0.0010423094558812188
Epoch 29: reducing lr to 0.0008553188535117145
Epoch 32: reducing lr to 0.0006628381112679585
Epoch 35: reducing lr to 0.0004769615725608276
Epoch 38: reducing lr to 0.00030936846764440684
Epoch 41: reducing lr to 0.0001705891929300084
Epoch 44: reducing lr to 6.934396273074722e-05
Epoch 47: reducing lr to 1.199426923062831e-05
[I 2024-06-21 22:25:24,685] Trial 50 finished with value: 1.1269686222076416 and parameters: {'hidden_size': 158, 'n_layers': 6, 'rnn_dropout': 0.20775871463868248, 'bidirectional': False, 'fc_dropout': 0.27623796927286526, 'learning_rate_model': 0.015358709003280025}. Best is trial 24 with value: 0.9786233901977539.
Epoch 9: reducing lr to 0.00013201530926241513
Epoch 23: reducing lr to 0.00011571965765086799
Epoch 30: reducing lr to 7.553588053755758e-05
Epoch 37: reducing lr to 3.46110584892142e-05
Epoch 42: reducing lr to 1.2631444757588605e-05
Epoch 47: reducing lr to 1.1451347571128776e-06
[I 2024-06-21 22:26:02,970] Trial 51 finished with value: 1.0314812660217285 and parameters: {'hidden_size': 98, 'n_layers': 6, 'rnn_dropout': 0.7923774268990726, 'bidirectional': True, 'fc_dropout': 0.7210467490794191, 'learning_rate_model': 0.0014663495679358655}. Best is trial 24 with value: 0.9786233901977539.
[I 2024-06-21 22:27:38,167] Trial 52 finished with value: 1.0558876991271973 and parameters: {'hidden_size': 196, 'n_layers': 5, 'rnn_dropout': 0.7951304196883832, 'bidirectional': True, 'fc_dropout': 0.10834232245773592, 'learning_rate_model': 0.000264740682379336}. Best is trial 24 with value: 0.9786233901977539.
Epoch 9: reducing lr to 0.0005310038280021741
Epoch 12: reducing lr to 0.0005896517930795333
Epoch 15: reducing lr to 0.0005780029220923054
Epoch 22: reducing lr to 0.0004849911317653131
Epoch 25: reducing lr to 0.00042294822205295207
Epoch 28: reducing lr to 0.0003528600357481864
Epoch 31: reducing lr to 0.0002791304516893446
Epoch 34: reducing lr to 0.00020639225132469
Epoch 37: reducing lr to 0.0001392157065090654
Epoch 40: reducing lr to 8.182182000165466e-05
Epoch 43: reducing lr to 3.781683222259828e-05
Epoch 46: reducing lr to 9.965802472494505e-06
Epoch 49: reducing lr to 1.8674478525528006e-08
[I 2024-06-21 22:27:40,616] Trial 53 finished with value: 1.0930765867233276 and parameters: {'hidden_size': 20, 'n_layers': 2, 'rnn_dropout': 0.5965097541659968, 'bidirectional': False, 'fc_dropout': 0.6898770022704354, 'learning_rate_model': 0.005898082867158478}. Best is trial 24 with value: 0.9786233901977539.
Epoch 34: reducing lr to 3.4270001652533764e-06
Epoch 37: reducing lr to 2.3115802369047543e-06
Epoch 40: reducing lr to 1.358594563833119e-06
Epoch 43: reducing lr to 6.279222666762007e-07
Epoch 46: reducing lr to 1.6547523708335842e-07
Epoch 49: reducing lr to 3.100767620017051e-10
[I 2024-06-21 22:27:49,668] Trial 54 finished with value: 1.0911674499511719 and parameters: {'hidden_size': 143, 'n_layers': 2, 'rnn_dropout': 0.3535234245987387, 'bidirectional': False, 'fc_dropout': 0.1631249801864975, 'learning_rate_model': 9.793357468944974e-05}. Best is trial 24 with value: 0.9786233901977539.
Epoch 3: reducing lr to 0.00039364605302367306
Epoch 6: reducing lr to 0.0009336933825489996
Epoch 12: reducing lr to 0.0015642340400306698
Epoch 17: reducing lr to 0.001486270242790886
Epoch 20: reducing lr to 0.001379274926791669
Epoch 23: reducing lr to 0.00123477151918081
Epoch 28: reducing lr to 0.0009360705517422272
Epoch 31: reducing lr to 0.0007404799905063912
Epoch 34: reducing lr to 0.000547519381624435
Epoch 37: reducing lr to 0.00036931278694344054
Epoch 40: reducing lr to 0.00021705772383971597
Epoch 43: reducing lr to 0.00010032086214776959
Epoch 46: reducing lr to 2.6437378206352857e-05
Epoch 49: reducing lr to 4.953983915971516e-08
[I 2024-06-21 22:27:58,494] Trial 55 finished with value: 1.0998249053955078 and parameters: {'hidden_size': 64, 'n_layers': 6, 'rnn_dropout': 0.36558026194855425, 'bidirectional': False, 'fc_dropout': 0.16956813171494078, 'learning_rate_model': 0.015646491878786785}. Best is trial 24 with value: 0.9786233901977539.
Epoch 38: reducing lr to 2.231964318069365e-05
Epoch 41: reducing lr to 1.2307297979239078e-05
Epoch 44: reducing lr to 5.002877367141982e-06
Epoch 47: reducing lr to 8.65336443236055e-07
[I 2024-06-21 22:28:02,515] Trial 56 finished with value: 0.9839743971824646 and parameters: {'hidden_size': 71, 'n_layers': 1, 'rnn_dropout': 0.47474595957169685, 'bidirectional': True, 'fc_dropout': 0.5998866485809148, 'learning_rate_model': 0.001108066724703625}. Best is trial 24 with value: 0.9786233901977539.
Epoch 4: reducing lr to 0.00221447069315218
Epoch 9: reducing lr to 0.005553701677241191
Epoch 12: reducing lr to 0.006167093304270251
Epoch 15: reducing lr to 0.0060452592403179494
Epoch 18: reducing lr to 0.005737383215275239
Epoch 21: reducing lr to 0.0052628102659167016
Epoch 24: reducing lr to 0.0046513594100932535
Epoch 27: reducing lr to 0.003941450994814957
Epoch 30: reducing lr to 0.0031776901389478232
Epoch 33: reducing lr to 0.002408067259096721
Epoch 36: reducing lr to 0.0016809408641959497
Epoch 39: reducing lr to 0.0010419986722147883
Epoch 42: reducing lr to 0.0005313874302014111
Epoch 45: reducing lr to 0.00018119146393288861
Epoch 48: reducing lr to 1.3414431984343222e-05
[I 2024-06-21 22:28:25,392] Trial 57 finished with value: 1.107243537902832 and parameters: {'hidden_size': 116, 'n_layers': 7, 'rnn_dropout': 0.6431687131989174, 'bidirectional': False, 'fc_dropout': 0.3720011852508049, 'learning_rate_model': 0.061687300513606574}. Best is trial 24 with value: 0.9786233901977539.
Epoch 12: reducing lr to 0.0016283122981936714
Epoch 16: reducing lr to 0.0015743146643317119
Epoch 19: reducing lr to 0.001477641834999458
Epoch 22: reducing lr to 0.0013392938572168614
Epoch 25: reducing lr to 0.0011679635329711916
Epoch 28: reducing lr to 0.0009744163292526985
Epoch 31: reducing lr to 0.0007708134743597888
Epoch 34: reducing lr to 0.0005699483068281658
Epoch 37: reducing lr to 0.0003844415461310336
Epoch 40: reducing lr to 0.0002259494117256299
Epoch 43: reducing lr to 0.00010443046847222575
Epoch 46: reducing lr to 2.752037544494178e-05
Epoch 49: reducing lr to 5.156922000815189e-08
[I 2024-06-21 22:28:31,246] Trial 58 finished with value: 1.1552386283874512 and parameters: {'hidden_size': 56, 'n_layers': 2, 'rnn_dropout': 0.0651791719018096, 'bidirectional': True, 'fc_dropout': 0.5070294084895343, 'learning_rate_model': 0.01628744452416877}. Best is trial 24 with value: 0.9786233901977539.
Epoch 23: reducing lr to 0.0002530196549232075
Epoch 26: reducing lr to 0.00021758382152098945
Epoch 29: reducing lr to 0.00017854922424040496
Epoch 32: reducing lr to 0.0001383685511876166
Epoch 35: reducing lr to 9.956651653774524e-05
Epoch 38: reducing lr to 6.458117890838135e-05
Epoch 41: reducing lr to 3.561077595378016e-05
Epoch 44: reducing lr to 1.4475666823543119e-05
Epoch 47: reducing lr to 2.5038235246031652e-06
[I 2024-06-21 22:29:37,179] Trial 59 finished with value: 1.0296142101287842 and parameters: {'hidden_size': 144, 'n_layers': 6, 'rnn_dropout': 0.2739632958748159, 'bidirectional': True, 'fc_dropout': 0.6592078963298551, 'learning_rate_model': 0.003206155887492329}. Best is trial 24 with value: 0.9786233901977539.
[I 2024-06-21 22:29:39,195] Trial 60 finished with value: 1.112064242362976 and parameters: {'hidden_size': 42, 'n_layers': 1, 'rnn_dropout': 0.014666114330405834, 'bidirectional': False, 'fc_dropout': 0.09067353821407238, 'learning_rate_model': 1.2916159380130333e-05}. Best is trial 24 with value: 0.9786233901977539.
Epoch 32: reducing lr to 3.3967881782192526e-05
Epoch 37: reducing lr to 1.857774818901566e-05
Epoch 40: reducing lr to 1.0918776382884085e-05
Epoch 43: reducing lr to 5.046496576820777e-06
Epoch 46: reducing lr to 1.3298942589025842e-06
Epoch 49: reducing lr to 2.49203030539772e-09
[I 2024-06-21 22:30:14,784] Trial 61 finished with value: 0.9777580499649048 and parameters: {'hidden_size': 155, 'n_layers': 3, 'rnn_dropout': 0.5975507964860202, 'bidirectional': True, 'fc_dropout': 0.3824580053742648, 'learning_rate_model': 0.000787074253700552}. Best is trial 61 with value: 0.9777580499649048.
Epoch 10: reducing lr to 0.00028632629341359577
Epoch 16: reducing lr to 0.00028793393928816605
Epoch 22: reducing lr to 0.00024494985971345863
Epoch 25: reducing lr to 0.00021361443719770915
Epoch 28: reducing lr to 0.00017821566332645676
Epoch 31: reducing lr to 0.00014097776331329894
Epoch 34: reducing lr to 0.00010424057203666895
Epoch 37: reducing lr to 7.03123532489806e-05
Epoch 40: reducing lr to 4.132496868129044e-05
Epoch 43: reducing lr to 1.909978789512239e-05
Epoch 46: reducing lr to 5.033333101750036e-06
Epoch 49: reducing lr to 9.431741315344897e-09
[I 2024-06-21 22:30:30,895] Trial 62 finished with value: 1.0926458835601807 and parameters: {'hidden_size': 164, 'n_layers': 3, 'rnn_dropout': 0.7237489282748985, 'bidirectional': False, 'fc_dropout': 0.10051310491019105, 'learning_rate_model': 0.0029788886358192816}. Best is trial 61 with value: 0.9777580499649048.
Epoch 11: reducing lr to 0.0045565526813803065
Epoch 14: reducing lr to 0.004536714070504514
Epoch 17: reducing lr to 0.004352893917820605
Epoch 20: reducing lr to 0.004039532829884316
Epoch 23: reducing lr to 0.0036163204247751865
Epoch 26: reducing lr to 0.003109848593010713
Epoch 29: reducing lr to 0.002551940902157626
Epoch 32: reducing lr to 0.0019776527557046814
Epoch 35: reducing lr to 0.001423069000265792
Epoch 38: reducing lr to 0.0009230359452245804
Epoch 41: reducing lr to 0.000508972223769863
Epoch 44: reducing lr to 0.00020689558529398688
Epoch 47: reducing lr to 3.578626393591147e-05
[I 2024-06-21 22:30:36,202] Trial 63 finished with value: 1.1770046949386597 and parameters: {'hidden_size': 17, 'n_layers': 4, 'rnn_dropout': 0.0024143652956770904, 'bidirectional': True, 'fc_dropout': 0.6367735535538155, 'learning_rate_model': 0.04582445195599803}. Best is trial 61 with value: 0.9777580499649048.
Epoch 13: reducing lr to 0.0006966571339247921
Epoch 16: reducing lr to 0.0006756603384116015
Epoch 22: reducing lr to 0.0005747947099151568
Epoch 25: reducing lr to 0.000501263599850106
Epoch 28: reducing lr to 0.00041819750631373636
Epoch 33: reducing lr to 0.0002728743300169143
Epoch 36: reducing lr to 0.0001904787378271062
Epoch 39: reducing lr to 0.0001180758919772781
Epoch 42: reducing lr to 6.021509093978167e-05
Epoch 45: reducing lr to 2.0532025896991338e-05
Epoch 48: reducing lr to 1.5200796931470402e-06
[I 2024-06-21 22:30:39,096] Trial 64 finished with value: 1.0924561023712158 and parameters: {'hidden_size': 42, 'n_layers': 2, 'rnn_dropout': 0.1501053383219472, 'bidirectional': False, 'fc_dropout': 0.791964142687259, 'learning_rate_model': 0.006990203755569746}. Best is trial 61 with value: 0.9777580499649048.
Epoch 6: reducing lr to 0.0010622571277592726
Epoch 12: reducing lr to 0.0017796192942591213
Epoch 18: reducing lr to 0.0016556191652544543
Epoch 21: reducing lr to 0.0015186730975458446
Epoch 26: reducing lr to 0.001208048353685063
Epoch 29: reducing lr to 0.0009913241475748185
Epoch 32: reducing lr to 0.000768236807753019
Epoch 35: reducing lr to 0.0005528038139268396
Epoch 38: reducing lr to 0.0003585615249973197
Epoch 41: reducing lr to 0.00019771478855224456
Epoch 44: reducing lr to 8.037043081802788e-05
Epoch 47: reducing lr to 1.3901492609472502e-05
[I 2024-06-21 22:32:09,637] Trial 65 finished with value: 1.1306931972503662 and parameters: {'hidden_size': 151, 'n_layers': 7, 'rnn_dropout': 0.3206989858968967, 'bidirectional': True, 'fc_dropout': 0.0955656402669832, 'learning_rate_model': 0.017800916053720233}. Best is trial 61 with value: 0.9777580499649048.
Epoch 12: reducing lr to 0.00013630071479353638
Epoch 15: reducing lr to 0.00013360802486919203
Epoch 18: reducing lr to 0.00012680356769451988
Epoch 21: reducing lr to 0.0001163148935286116
Epoch 24: reducing lr to 0.0001028010411190578
Epoch 27: reducing lr to 8.7111149679702e-05
Epoch 30: reducing lr to 7.023104985797053e-05
Epoch 33: reducing lr to 5.322139174682608e-05
Epoch 36: reducing lr to 3.7150960754384695e-05
Epoch 39: reducing lr to 2.3029514364319688e-05
Epoch 42: reducing lr to 1.1744347457594241e-05
Epoch 45: reducing lr to 4.004565008192671e-06
Epoch 48: reducing lr to 2.9647624542169986e-07
[I 2024-06-21 22:32:12,674] Trial 66 finished with value: 1.0919243097305298 and parameters: {'hidden_size': 43, 'n_layers': 2, 'rnn_dropout': 0.6552713429068246, 'bidirectional': False, 'fc_dropout': 0.21331096172130817, 'learning_rate_model': 0.0013633688901489999}. Best is trial 61 with value: 0.9777580499649048.
[I 2024-06-21 22:32:25,505] Trial 67 finished with value: 1.107581377029419 and parameters: {'hidden_size': 74, 'n_layers': 7, 'rnn_dropout': 0.29324531508098756, 'bidirectional': False, 'fc_dropout': 0.7511091818957707, 'learning_rate_model': 1.3298296907035845e-05}. Best is trial 61 with value: 0.9777580499649048.
Epoch 9: reducing lr to 0.00015433093437824383
Epoch 35: reducing lr to 5.323470873488301e-05
Epoch 40: reducing lr to 2.3780690962047347e-05
Epoch 43: reducing lr to 1.0991082821562971e-05
Epoch 46: reducing lr to 2.8964604891752167e-06
Epoch 49: reducing lr to 5.427549798855437e-09
[I 2024-06-21 22:33:50,760] Trial 68 finished with value: 0.9733983278274536 and parameters: {'hidden_size': 148, 'n_layers': 7, 'rnn_dropout': 0.021829778813975055, 'bidirectional': True, 'fc_dropout': 0.6875914566924687, 'learning_rate_model': 0.0017142186024413223}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 22:34:35,311] Trial 69 finished with value: 1.0536847114562988 and parameters: {'hidden_size': 143, 'n_layers': 4, 'rnn_dropout': 0.5026472300170858, 'bidirectional': True, 'fc_dropout': 0.4614047479896726, 'learning_rate_model': 0.00017610709636270444}. Best is trial 68 with value: 0.9733983278274536.
Epoch 9: reducing lr to 0.0050333868147849495
Epoch 12: reducing lr to 0.005589311044644059
Epoch 16: reducing lr to 0.005403959885861986
Epoch 19: reducing lr to 0.0050721227356401504
Epoch 22: reducing lr to 0.004597232334651196
Epoch 25: reducing lr to 0.00400912741482035
Epoch 28: reducing lr to 0.003344761295001805
Epoch 31: reducing lr to 0.0026458783553860972
Epoch 34: reducing lr to 0.001956392744662517
Epoch 37: reducing lr to 0.0013196260829043064
Epoch 40: reducing lr to 0.0007755892674211569
Epoch 43: reducing lr to 0.0003584658615406092
Epoch 46: reducing lr to 9.44658703356839e-05
Epoch 49: reducing lr to 1.770154357212672e-07
[I 2024-06-21 22:34:57,708] Trial 70 finished with value: 1.1072494983673096 and parameters: {'hidden_size': 111, 'n_layers': 7, 'rnn_dropout': 0.3411798308011168, 'bidirectional': False, 'fc_dropout': 0.7454888933092325, 'learning_rate_model': 0.05590794448273345}. Best is trial 68 with value: 0.9733983278274536.
Epoch 25: reducing lr to 3.586244036316454e-06
Epoch 28: reducing lr to 2.9919553573579375e-06
Epoch 31: reducing lr to 2.3667906980819596e-06
Epoch 34: reducing lr to 1.7500321359961372e-06
Epoch 37: reducing lr to 1.1804317200019137e-06
Epoch 40: reducing lr to 6.937799917852718e-07
Epoch 43: reducing lr to 3.206548271379038e-07
Epoch 46: reducing lr to 8.45015957523439e-08
Epoch 49: reducing lr to 1.5834382023955275e-10
[I 2024-06-21 22:35:38,564] Trial 71 finished with value: 1.1068346500396729 and parameters: {'hidden_size': 172, 'n_layers': 7, 'rnn_dropout': 0.18232023196105357, 'bidirectional': False, 'fc_dropout': 0.25150129289124035, 'learning_rate_model': 5.001076586958479e-05}. Best is trial 68 with value: 0.9733983278274536.
Epoch 9: reducing lr to 0.00017834590306910675
Epoch 12: reducing lr to 0.00019804373525656927
Epoch 15: reducing lr to 0.00019413128056905963
Epoch 18: reducing lr to 0.00018424446436778952
Epoch 22: reducing lr to 0.00016289182264584117
Epoch 25: reducing lr to 0.00014205374544531152
Epoch 28: reducing lr to 0.00011851353684073618
Epoch 31: reducing lr to 9.375030810591445e-05
Epoch 34: reducing lr to 6.932005102007885e-05
Epoch 37: reducing lr to 4.675776254227178e-05
Epoch 40: reducing lr to 2.7481132167833003e-05
Epoch 43: reducing lr to 1.2701371889026333e-05
Epoch 46: reducing lr to 3.3471699223947816e-06
Epoch 49: reducing lr to 6.272114364035477e-09
[I 2024-06-21 22:35:49,338] Trial 72 finished with value: 1.0925847291946411 and parameters: {'hidden_size': 128, 'n_layers': 3, 'rnn_dropout': 0.6530812094617713, 'bidirectional': False, 'fc_dropout': 0.1922844998793111, 'learning_rate_model': 0.0019809629608084447}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 22:35:53,156] Trial 73 finished with value: 1.0925095081329346 and parameters: {'hidden_size': 80, 'n_layers': 1, 'rnn_dropout': 0.1830509367683007, 'bidirectional': True, 'fc_dropout': 0.41521278725894306, 'learning_rate_model': 4.9989708054979095e-05}. Best is trial 68 with value: 0.9733983278274536.
Epoch 3: reducing lr to 6.069049177927621e-05
Epoch 6: reducing lr to 0.00014395244185147938
Epoch 9: reducing lr to 0.00021717934365573193
Epoch 12: reducing lr to 0.0002411662264060254
Epoch 15: reducing lr to 0.00023640186497975315
Epoch 18: reducing lr to 0.00022436227104187233
Epoch 21: reducing lr to 0.00020580393866316042
Epoch 24: reducing lr to 0.00018189295041370912
Epoch 27: reducing lr to 0.0001541317466894196
Epoch 30: reducing lr to 0.000124264625438218
Epoch 33: reducing lr to 9.416826779742975e-05
Epoch 36: reducing lr to 6.573374927684665e-05
Epoch 39: reducing lr to 4.0747703220920387e-05
Epoch 42: reducing lr to 2.0780081470882443e-05
Epoch 45: reducing lr to 7.0855522136208275e-06
Epoch 48: reducing lr to 5.245758060453462e-07
[I 2024-06-21 22:36:17,222] Trial 74 finished with value: 1.107501745223999 and parameters: {'hidden_size': 121, 'n_layers': 7, 'rnn_dropout': 0.6534680870332976, 'bidirectional': False, 'fc_dropout': 0.7212500677354571, 'learning_rate_model': 0.002412302319431398}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 22:36:19,737] Trial 75 finished with value: 1.1027545928955078 and parameters: {'hidden_size': 21, 'n_layers': 1, 'rnn_dropout': 0.05229737203378111, 'bidirectional': True, 'fc_dropout': 0.7802800274162293, 'learning_rate_model': 4.6982180906249337e-05}. Best is trial 68 with value: 0.9733983278274536.
Epoch 16: reducing lr to 1.0908976821624078e-06
Epoch 19: reducing lr to 1.0239096982250258e-06
Epoch 22: reducing lr to 9.280435466136152e-07
Epoch 25: reducing lr to 8.093227737984329e-07
Epoch 28: reducing lr to 6.752071483080566e-07
Epoch 31: reducing lr to 5.341236104890089e-07
Epoch 34: reducing lr to 3.9493711197511234e-07
Epoch 37: reducing lr to 2.663929906155729e-07
Epoch 40: reducing lr to 1.565682484715224e-07
Epoch 43: reducing lr to 7.236352336960853e-08
Epoch 46: reducing lr to 1.9069830488983048e-08
Epoch 49: reducing lr to 3.5734115836061876e-11
[I 2024-06-21 22:37:02,384] Trial 76 finished with value: 1.1075395345687866 and parameters: {'hidden_size': 195, 'n_layers': 6, 'rnn_dropout': 0.6593902716995983, 'bidirectional': False, 'fc_dropout': 0.3815058668169547, 'learning_rate_model': 1.1286139856486e-05}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 22:37:21,162] Trial 77 finished with value: 1.1055104732513428 and parameters: {'hidden_size': 81, 'n_layers': 4, 'rnn_dropout': 0.5840729688720835, 'bidirectional': True, 'fc_dropout': 0.11013019275648253, 'learning_rate_model': 1.1054902973283413e-05}. Best is trial 68 with value: 0.9733983278274536.
Epoch 8: reducing lr to 0.000853098401870044
Epoch 11: reducing lr to 0.001040171486032295
Epoch 14: reducing lr to 0.001035642720801551
Epoch 17: reducing lr to 0.0009936801901890475
Epoch 20: reducing lr to 0.0009221460082546807
Epoch 23: reducing lr to 0.0008255349281000425
Epoch 28: reducing lr to 0.0006258315191313786
Epoch 31: reducing lr to 0.0004950649462078344
Epoch 34: reducing lr to 0.0003660566884815885
Epoch 37: reducing lr to 0.0002469125666407076
Epoch 40: reducing lr to 0.00014511893873488338
Epoch 43: reducing lr to 6.707182214166861e-05
Epoch 46: reducing lr to 1.7675317884894778e-05
Epoch 49: reducing lr to 3.312100005838786e-08
[I 2024-06-21 22:37:36,153] Trial 78 finished with value: 1.0947924852371216 and parameters: {'hidden_size': 156, 'n_layers': 3, 'rnn_dropout': 0.7875067601548086, 'bidirectional': False, 'fc_dropout': 0.4191169085329054, 'learning_rate_model': 0.010460822384971712}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 22:37:39,275] Trial 79 finished with value: 1.1032068729400635 and parameters: {'hidden_size': 101, 'n_layers': 1, 'rnn_dropout': 0.25000110279363963, 'bidirectional': False, 'fc_dropout': 0.6161199650496738, 'learning_rate_model': 3.321567816135707e-05}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 22:37:42,886] Trial 80 finished with value: 1.0934295654296875 and parameters: {'hidden_size': 20, 'n_layers': 4, 'rnn_dropout': 0.6479909685299331, 'bidirectional': False, 'fc_dropout': 0.5494465813255934, 'learning_rate_model': 0.0006265951288394312}. Best is trial 68 with value: 0.9733983278274536.
Epoch 14: reducing lr to 0.00039107577733250235
Epoch 17: reducing lr to 0.0003752300334784611
Epoch 20: reducing lr to 0.0003482175462143443
Epoch 23: reducing lr to 0.0003117356084654209
Epoch 29: reducing lr to 0.00021998350711728147
Epoch 32: reducing lr to 0.00017047847334248348
Epoch 36: reducing lr to 0.00010763994272863114
Epoch 42: reducing lr to 3.4027676863548864e-05
Epoch 45: reducing lr to 1.1602691811518369e-05
Epoch 48: reducing lr to 8.590002904251652e-07
[I 2024-06-21 22:38:15,272] Trial 81 finished with value: 1.0927377939224243 and parameters: {'hidden_size': 185, 'n_layers': 5, 'rnn_dropout': 0.004191870121172681, 'bidirectional': False, 'fc_dropout': 0.6643196560298716, 'learning_rate_model': 0.003950179114447677}. Best is trial 68 with value: 0.9733983278274536.
Epoch 7: reducing lr to 0.0018875376352692197
Epoch 10: reducing lr to 0.0025475366025506352
Epoch 13: reducing lr to 0.0026414519970979346
Epoch 16: reducing lr to 0.0025618403420381275
Epoch 19: reducing lr to 0.002404527220478999
Epoch 22: reducing lr to 0.0021793972393177657
Epoch 25: reducing lr to 0.0019005959638095115
Epoch 28: reducing lr to 0.001585641751790412
Epoch 31: reducing lr to 0.0012543242463156049
Epoch 34: reducing lr to 0.0009274617066014127
Epoch 37: reducing lr to 0.0006255914934591988
Epoch 40: reducing lr to 0.00036768146250115637
Epoch 43: reducing lr to 0.00016993692120860422
Epoch 46: reducing lr to 4.478317431719711e-05
Epoch 49: reducing lr to 8.391721884881687e-08
[I 2024-06-21 22:38:36,650] Trial 82 finished with value: 1.1485573053359985 and parameters: {'hidden_size': 140, 'n_layers': 5, 'rnn_dropout': 0.21929582013348173, 'bidirectional': False, 'fc_dropout': 0.2819437041731271, 'learning_rate_model': 0.026504124871653878}. Best is trial 68 with value: 0.9733983278274536.
Epoch 24: reducing lr to 1.956697020916107e-05
Epoch 27: reducing lr to 1.6580583738392798e-05
Epoch 33: reducing lr to 1.0130066538860952e-05
Epoch 42: reducing lr to 2.2353985361163887e-06
Epoch 45: reducing lr to 7.622218935040388e-07
Epoch 48: reducing lr to 5.643076955972899e-08
[I 2024-06-21 22:38:54,034] Trial 83 finished with value: 1.0746396780014038 and parameters: {'hidden_size': 52, 'n_layers': 6, 'rnn_dropout': 0.23168219694950196, 'bidirectional': True, 'fc_dropout': 0.6461587272897676, 'learning_rate_model': 0.00025950124791777955}. Best is trial 68 with value: 0.9733983278274536.
Epoch 15: reducing lr to 7.041540982280583e-06
Epoch 18: reducing lr to 6.682925815979482e-06
Epoch 21: reducing lr to 6.130141437486202e-06
Epoch 24: reducing lr to 5.417921152338449e-06
Epoch 27: reducing lr to 4.591017127030665e-06
Epoch 30: reducing lr to 3.701385573865501e-06
Epoch 33: reducing lr to 2.8049259128423764e-06
Epoch 36: reducing lr to 1.9579663193076534e-06
Epoch 39: reducing lr to 1.2137240211217944e-06
Epoch 42: reducing lr to 6.189621021174268e-07
Epoch 45: reducing lr to 2.1105250713048814e-07
Epoch 48: reducing lr to 1.5625181454881678e-08
[I 2024-06-21 22:39:03,248] Trial 84 finished with value: 1.1084508895874023 and parameters: {'hidden_size': 55, 'n_layers': 6, 'rnn_dropout': 0.2469135581045209, 'bidirectional': False, 'fc_dropout': 0.17711736072656087, 'learning_rate_model': 7.185360253135694e-05}. Best is trial 68 with value: 0.9733983278274536.
Epoch 9: reducing lr to 0.0014239440639673387
Epoch 14: reducing lr to 0.0015658504316223751
Epoch 17: reducing lr to 0.0015024047612654196
Epoch 22: reducing lr to 0.0013005560539032337
Epoch 25: reducing lr to 0.0011341812966278186
Epoch 28: reducing lr to 0.000946232262025945
Epoch 31: reducing lr to 0.0007485184264131837
Epoch 34: reducing lr to 0.000553463093153905
Epoch 37: reducing lr to 0.00037332193939248055
Epoch 40: reducing lr to 0.00021941403950459515
Epoch 43: reducing lr to 0.00010140991631645471
Epoch 46: reducing lr to 2.6724374712646173e-05
Epoch 49: reducing lr to 5.007762927840276e-08
[I 2024-06-21 22:39:05,425] Trial 85 finished with value: 1.0987077951431274 and parameters: {'hidden_size': 52, 'n_layers': 1, 'rnn_dropout': 0.3016660793658673, 'bidirectional': False, 'fc_dropout': 0.5396512190107284, 'learning_rate_model': 0.015816345654373305}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 22:39:10,066] Trial 86 finished with value: 1.059537649154663 and parameters: {'hidden_size': 30, 'n_layers': 2, 'rnn_dropout': 0.1661005294430515, 'bidirectional': True, 'fc_dropout': 0.44310768884113144, 'learning_rate_model': 0.00016522355094605626}. Best is trial 68 with value: 0.9733983278274536.
Epoch 7: reducing lr to 0.005711581162388893
Epoch 10: reducing lr to 0.0077087003711844276
Epoch 13: reducing lr to 0.007992882995324881
Epoch 16: reducing lr to 0.007751982670557931
Epoch 19: reducing lr to 0.007275962142593432
Epoch 22: reducing lr to 0.006594731667786999
Epoch 25: reducing lr to 0.00575109491931188
Epoch 28: reducing lr to 0.0047980614482061494
Epoch 31: reducing lr to 0.0037955135849580173
Epoch 34: reducing lr to 0.002806446193856226
Epoch 37: reducing lr to 0.0018930041566470068
Epoch 40: reducing lr to 0.0011125831219156988
Epoch 43: reducing lr to 0.0005142194252624743
Epoch 46: reducing lr to 0.00013551132970421458
Epoch 49: reducing lr to 2.5392871507434747e-07
[I 2024-06-21 22:40:08,467] Trial 87 finished with value: 1.1073018312454224 and parameters: {'hidden_size': 170, 'n_layers': 4, 'rnn_dropout': 0.7387649720950202, 'bidirectional': True, 'fc_dropout': 0.10096808299948697, 'learning_rate_model': 0.08019996926893054}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 22:40:14,252] Trial 88 finished with value: 1.0837233066558838 and parameters: {'hidden_size': 45, 'n_layers': 2, 'rnn_dropout': 0.3449454041282968, 'bidirectional': True, 'fc_dropout': 0.5834551354888987, 'learning_rate_model': 5.6882708517701854e-05}. Best is trial 68 with value: 0.9733983278274536.
Epoch 5: reducing lr to 0.00011656517258961006
Epoch 8: reducing lr to 0.00019949982172217588
Epoch 19: reducing lr to 0.0002219348176191838
Epoch 22: reducing lr to 0.0002011556054380569
Epoch 29: reducing lr to 0.00013623307043421112
Epoch 32: reducing lr to 0.00010557521411821713
Epoch 35: reducing lr to 7.59692589989207e-05
Epoch 38: reducing lr to 4.927544397002725e-05
Epoch 43: reducing lr to 1.568496263797091e-05
Epoch 46: reducing lr to 4.1334302809495e-06
Epoch 49: reducing lr to 7.745453036150234e-09
[I 2024-06-21 22:40:39,258] Trial 89 finished with value: 1.0930715799331665 and parameters: {'hidden_size': 135, 'n_layers': 6, 'rnn_dropout': 0.1685859136666566, 'bidirectional': False, 'fc_dropout': 0.5105749684962667, 'learning_rate_model': 0.002446297163720527}. Best is trial 68 with value: 0.9733983278274536.
Epoch 3: reducing lr to 0.0005517919142114715
Epoch 6: reducing lr to 0.0013088012819788468
Epoch 9: reducing lr to 0.001974572989107133
Epoch 12: reducing lr to 0.0021926593410333526
Epoch 15: reducing lr to 0.0021493422408694726
Epoch 18: reducing lr to 0.002039879450396903
Epoch 21: reducing lr to 0.0018711489384566633
Epoch 24: reducing lr to 0.0016537526117826721
Epoch 27: reducing lr to 0.0014013505090026929
Epoch 30: reducing lr to 0.0011298016135492842
Epoch 33: reducing lr to 0.0008561685236445106
Epoch 36: reducing lr to 0.0005976447097130559
Epoch 39: reducing lr to 0.0003704740644014725
Epoch 42: reducing lr to 0.0001889304336829236
Epoch 45: reducing lr to 6.442113590739116e-05
Epoch 48: reducing lr to 4.7693910476043214e-06
[I 2024-06-21 22:40:50,380] Trial 90 finished with value: 1.1073195934295654 and parameters: {'hidden_size': 70, 'n_layers': 6, 'rnn_dropout': 0.7562467519999598, 'bidirectional': False, 'fc_dropout': 0.04491283172953225, 'learning_rate_model': 0.021932412730099944}. Best is trial 68 with value: 0.9733983278274536.
Epoch 15: reducing lr to 0.00013001962762079975
Epoch 23: reducing lr to 0.00010470306100293806
Epoch 26: reducing lr to 9.003921906730479e-05
Epoch 29: reducing lr to 7.388615846205422e-05
Epoch 32: reducing lr to 5.72588357227909e-05
Epoch 35: reducing lr to 4.1202012776697475e-05
Epoch 38: reducing lr to 2.6724592273031743e-05
Epoch 41: reducing lr to 1.4736235602654012e-05
Epoch 44: reducing lr to 5.990232762524521e-06
Epoch 47: reducing lr to 1.0361170847248421e-06
[I 2024-06-21 22:40:54,845] Trial 91 finished with value: 1.0928115844726562 and parameters: {'hidden_size': 51, 'n_layers': 3, 'rnn_dropout': 0.24006483674028517, 'bidirectional': False, 'fc_dropout': 0.2961233420429906, 'learning_rate_model': 0.0013267520089493556}. Best is trial 68 with value: 0.9733983278274536.
Epoch 25: reducing lr to 1.922307096257968e-05
Epoch 28: reducing lr to 1.60375505874489e-05
Epoch 31: reducing lr to 1.2686528044961049e-05
Epoch 34: reducing lr to 9.380564065462307e-06
Epoch 37: reducing lr to 6.327378307301125e-06
Epoch 40: reducing lr to 3.7188160870961394e-06
Epoch 43: reducing lr to 1.7187816652033646e-06
Epoch 46: reducing lr to 4.52947472383107e-07
Epoch 49: reducing lr to 8.48758328246893e-10
[I 2024-06-21 22:40:58,009] Trial 92 finished with value: 1.0917065143585205 and parameters: {'hidden_size': 107, 'n_layers': 1, 'rnn_dropout': 0.16516324711154642, 'bidirectional': False, 'fc_dropout': 0.2092396414663897, 'learning_rate_model': 0.0002680689020235863}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 22:41:10,307] Trial 93 finished with value: 1.1067546606063843 and parameters: {'hidden_size': 35, 'n_layers': 6, 'rnn_dropout': 0.08526710143104088, 'bidirectional': True, 'fc_dropout': 0.4579240899477195, 'learning_rate_model': 1.5114066835104806e-05}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 22:41:33,483] Trial 94 finished with value: 1.0510598421096802 and parameters: {'hidden_size': 161, 'n_layers': 2, 'rnn_dropout': 0.4223231827265013, 'bidirectional': True, 'fc_dropout': 0.4408697393495774, 'learning_rate_model': 7.353072250353887e-05}. Best is trial 68 with value: 0.9733983278274536.
Epoch 9: reducing lr to 0.00015488238461154597
Epoch 23: reducing lr to 0.00013576407632975062
Epoch 30: reducing lr to 8.861985300609788e-05
Epoch 33: reducing lr to 6.715650588908868e-05
Epoch 36: reducing lr to 4.687830650042942e-05
Epoch 39: reducing lr to 2.905940010714807e-05
Epoch 42: reducing lr to 1.4819404628712402e-05
Epoch 45: reducing lr to 5.053092088144577e-06
Epoch 48: reducing lr to 3.741034961346132e-07
[I 2024-06-21 22:42:52,302] Trial 95 finished with value: 0.976422131061554 and parameters: {'hidden_size': 156, 'n_layers': 6, 'rnn_dropout': 0.14123922742166642, 'bidirectional': True, 'fc_dropout': 0.6881895979384922, 'learning_rate_model': 0.0017203437921322648}. Best is trial 68 with value: 0.9733983278274536.
Epoch 6: reducing lr to 0.003246367897033143
Epoch 11: reducing lr to 0.005409405638924722
Epoch 14: reducing lr to 0.005385853822223806
Epoch 17: reducing lr to 0.005167627930852102
Epoch 20: reducing lr to 0.004795614842310644
Epoch 23: reducing lr to 0.0042931894934245835
Epoch 26: reducing lr to 0.003691920996321782
Epoch 29: reducing lr to 0.0030295893566081506
Epoch 32: reducing lr to 0.002347811320663415
Epoch 35: reducing lr to 0.0016894257595381989
Epoch 38: reducing lr to 0.0010958011892261317
Epoch 41: reducing lr to 0.0006042368891217828
Epoch 44: reducing lr to 0.00024562036785645005
Epoch 47: reducing lr to 4.248440245671128e-05
[I 2024-06-21 22:43:26,342] Trial 96 finished with value: 1.1074423789978027 and parameters: {'hidden_size': 90, 'n_layers': 6, 'rnn_dropout': 0.573543201353976, 'bidirectional': True, 'fc_dropout': 0.0030349379274740686, 'learning_rate_model': 0.054401444720336284}. Best is trial 68 with value: 0.9733983278274536.
Epoch 3: reducing lr to 0.00030337446504804296
Epoch 6: reducing lr to 0.0007195772147946977
Epoch 13: reducing lr to 0.0012017644561310684
Epoch 16: reducing lr to 0.001165544052561457
Epoch 23: reducing lr to 0.0009516115967902518
Epoch 26: reducing lr to 0.0008183367726754511
Epoch 29: reducing lr to 0.000671526931125729
Epoch 32: reducing lr to 0.0005204066774226092
Epoch 35: reducing lr to 0.00037447150822367286
Epoch 38: reducing lr to 0.00024289100703363907
Epoch 41: reducing lr to 0.00013393278628334883
Epoch 44: reducing lr to 5.4443250366207306e-05
Epoch 47: reducing lr to 9.416926535022602e-06
[I 2024-06-21 22:44:02,686] Trial 97 finished with value: 1.1218054294586182 and parameters: {'hidden_size': 174, 'n_layers': 6, 'rnn_dropout': 0.41335102633723075, 'bidirectional': False, 'fc_dropout': 0.2999007548294875, 'learning_rate_model': 0.01205841152767771}. Best is trial 68 with value: 0.9733983278274536.
Epoch 7: reducing lr to 0.0027431437109580905
Epoch 11: reducing lr to 0.003830059545767548
Epoch 14: reducing lr to 0.0038133839872317972
Epoch 17: reducing lr to 0.0036588719734964082
Epoch 20: reducing lr to 0.003395472928199094
Epoch 26: reducing lr to 0.0026140168066583875
Epoch 32: reducing lr to 0.0016623373731971599
Epoch 35: reducing lr to 0.0011961760106552282
Epoch 38: reducing lr to 0.0007758678282247012
Epoch 41: reducing lr to 0.0004278220972065608
Epoch 44: reducing lr to 0.00017390831772241336
Epoch 47: reducing lr to 3.0080530475415368e-05
[I 2024-06-21 22:45:06,353] Trial 98 finished with value: 1.0786875486373901 and parameters: {'hidden_size': 154, 'n_layers': 5, 'rnn_dropout': 0.3214927391742699, 'bidirectional': True, 'fc_dropout': 0.6199173173796783, 'learning_rate_model': 0.03851823778112661}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 22:45:08,698] Trial 99 finished with value: 1.0944358110427856 and parameters: {'hidden_size': 60, 'n_layers': 1, 'rnn_dropout': 0.1762271902349821, 'bidirectional': False, 'fc_dropout': 0.4345331441904756, 'learning_rate_model': 0.00014023182899556565}. Best is trial 68 with value: 0.9733983278274536.
Epoch 6: reducing lr to 0.00122554353888497
Epoch 9: reducing lr to 0.0018489630184333368
Epoch 12: reducing lr to 0.002053176082098789
Epoch 15: reducing lr to 0.002012614544636959
Epoch 18: reducing lr to 0.0019101150915426391
Epoch 21: reducing lr to 0.001752118158342465
Epoch 24: reducing lr to 0.001548551224842974
Epoch 27: reducing lr to 0.0013122053635405682
Epoch 30: reducing lr to 0.0010579307086356579
Epoch 33: reducing lr to 0.0008017044426811406
Epoch 36: reducing lr to 0.000559626295162398
Epoch 39: reducing lr to 0.0003469068239795755
Epoch 42: reducing lr to 0.00017691186239423667
Epoch 45: reducing lr to 6.032306658468704e-05
Epoch 48: reducing lr to 4.465992250534539e-06
[I 2024-06-21 22:45:12,405] Trial 100 finished with value: 1.0959585905075073 and parameters: {'hidden_size': 41, 'n_layers': 3, 'rnn_dropout': 0.4910968717930021, 'bidirectional': False, 'fc_dropout': 0.35529368561329155, 'learning_rate_model': 0.020537209952065802}. Best is trial 68 with value: 0.9733983278274536.
Epoch 13: reducing lr to 0.0003205912481466736
Epoch 16: reducing lr to 0.00031092883524245016
Epoch 22: reducing lr to 0.0002645119736902008
Epoch 25: reducing lr to 0.0002306740508363024
Epoch 28: reducing lr to 0.00019244827045067017
Epoch 31: reducing lr to 0.0001522364881696717
Epoch 34: reducing lr to 0.00011256540207971337
Epoch 37: reducing lr to 7.592761781716004e-05
Epoch 40: reducing lr to 4.462525123043897e-05
Epoch 43: reducing lr to 2.0625129563711478e-05
Epoch 46: reducing lr to 5.4353036761954635e-06
Epoch 49: reducing lr to 1.0184976278722682e-08
[I 2024-06-21 22:45:20,542] Trial 101 finished with value: 1.0927467346191406 and parameters: {'hidden_size': 94, 'n_layers': 3, 'rnn_dropout': 0.5399772838472842, 'bidirectional': False, 'fc_dropout': 0.25181254985921314, 'learning_rate_model': 0.0032167877678542485}. Best is trial 68 with value: 0.9733983278274536.
Epoch 15: reducing lr to 5.286219493144185e-05
Epoch 18: reducing lr to 5.017000228865466e-05
Epoch 21: reducing lr to 4.6020144232797184e-05
Epoch 24: reducing lr to 4.0673370331693807e-05
Epoch 27: reducing lr to 3.446564365859608e-05
Epoch 30: reducing lr to 2.7787009436496144e-05
Epoch 33: reducing lr to 2.1057115302750636e-05
Epoch 36: reducing lr to 1.4698827643110166e-05
Epoch 39: reducing lr to 9.11165836554343e-06
Epoch 42: reducing lr to 4.646666884371294e-06
Epoch 45: reducing lr to 1.5844115373007474e-06
Epoch 48: reducing lr to 1.1730122568137658e-07
[I 2024-06-21 22:45:44,904] Trial 102 finished with value: 1.0928514003753662 and parameters: {'hidden_size': 178, 'n_layers': 4, 'rnn_dropout': 0.6275659350335385, 'bidirectional': False, 'fc_dropout': 0.10547278002200172, 'learning_rate_model': 0.0005394187370487681}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 22:47:09,521] Trial 103 finished with value: 1.0843591690063477 and parameters: {'hidden_size': 184, 'n_layers': 5, 'rnn_dropout': 0.6134196697896818, 'bidirectional': True, 'fc_dropout': 0.67431987215428, 'learning_rate_model': 1.8673586483652047e-05}. Best is trial 68 with value: 0.9733983278274536.
Epoch 22: reducing lr to 0.0005223084756877155
Epoch 25: reducing lr to 0.0004554917125004354
Epoch 28: reducing lr to 0.00038001063386852106
Epoch 31: reducing lr to 0.0003006079723751193
Epoch 34: reducing lr to 0.00022227297598365007
Epoch 37: reducing lr to 0.0001499275733019472
Epoch 40: reducing lr to 8.811754954673881e-05
Epoch 43: reducing lr to 4.0726625086170456e-05
Epoch 46: reducing lr to 1.073261500569517e-05
Epoch 49: reducing lr to 2.011137477386793e-08
[I 2024-06-21 22:47:34,025] Trial 104 finished with value: 1.107397198677063 and parameters: {'hidden_size': 122, 'n_layers': 7, 'rnn_dropout': 0.41428803853785756, 'bidirectional': False, 'fc_dropout': 0.20131131388468226, 'learning_rate_model': 0.006351907220679005}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 22:48:32,484] Trial 105 finished with value: 1.1020634174346924 and parameters: {'hidden_size': 115, 'n_layers': 7, 'rnn_dropout': 0.49946936192214075, 'bidirectional': True, 'fc_dropout': 0.7914720449333105, 'learning_rate_model': 2.0457769512926938e-05}. Best is trial 68 with value: 0.9733983278274536.
Epoch 17: reducing lr to 1.0015245025399623e-05
Epoch 20: reducing lr to 9.294256153086592e-06
Epoch 25: reducing lr to 7.560614743649844e-06
Epoch 28: reducing lr to 6.307719596912131e-06
Epoch 31: reducing lr to 4.989730889990318e-06
Epoch 34: reducing lr to 3.689464139998609e-06
Epoch 37: reducing lr to 2.48861744369336e-06
Epoch 40: reducing lr to 1.4626453698139067e-06
Epoch 43: reducing lr to 6.760130066807866e-07
Epoch 46: reducing lr to 1.7814850418359488e-07
Epoch 49: reducing lr to 3.3382463930187004e-10
[I 2024-06-21 22:48:45,810] Trial 106 finished with value: 1.1068196296691895 and parameters: {'hidden_size': 75, 'n_layers': 7, 'rnn_dropout': 0.007151281896996054, 'bidirectional': False, 'fc_dropout': 0.351864651366858, 'learning_rate_model': 0.00010543402232135162}. Best is trial 68 with value: 0.9733983278274536.
Epoch 6: reducing lr to 0.0015944000078580243
Epoch 16: reducing lr to 0.002582549041791246
Epoch 19: reducing lr to 0.0024239642757240124
Epoch 22: reducing lr to 0.0021970144507931304
Epoch 25: reducing lr to 0.0019159594782802093
Epoch 28: reducing lr to 0.0015984593261001805
Epoch 31: reducing lr to 0.0012644636073771694
Epoch 34: reducing lr to 0.0009349588662406598
Epoch 37: reducing lr to 0.0006306484777659747
Epoch 40: reducing lr to 0.0003706536246951775
Epoch 43: reducing lr to 0.00017131061051333212
Epoch 46: reducing lr to 4.5145180214171964e-05
Epoch 49: reducing lr to 8.459556576252691e-08
[I 2024-06-21 22:49:41,731] Trial 107 finished with value: 1.2152447700500488 and parameters: {'hidden_size': 164, 'n_layers': 4, 'rnn_dropout': 0.10747088692455692, 'bidirectional': True, 'fc_dropout': 0.3932361200135483, 'learning_rate_model': 0.026718371620438223}. Best is trial 68 with value: 0.9733983278274536.
Epoch 24: reducing lr to 1.1330442889558257e-05
[I 2024-06-21 22:50:11,686] Trial 108 finished with value: 1.070090413093567 and parameters: {'hidden_size': 93, 'n_layers': 5, 'rnn_dropout': 0.31839251152633796, 'bidirectional': True, 'fc_dropout': 0.7719911392887311, 'learning_rate_model': 0.0001502667013784738}. Best is trial 68 with value: 0.9733983278274536.
Epoch 26: reducing lr to 7.243720069372752e-05
Epoch 37: reducing lr to 2.5193996161390434e-05
Epoch 40: reducing lr to 1.4807371026813227e-05
Epoch 43: reducing lr to 6.843747374079851e-06
Epoch 46: reducing lr to 1.8035205619622687e-06
Epoch 49: reducing lr to 3.37953778410411e-09
[I 2024-06-21 22:50:16,332] Trial 109 finished with value: 0.9788951873779297 and parameters: {'hidden_size': 35, 'n_layers': 2, 'rnn_dropout': 0.011273835353954365, 'bidirectional': True, 'fc_dropout': 0.6356626809478018, 'learning_rate_model': 0.0010673815537119522}. Best is trial 68 with value: 0.9733983278274536.
Epoch 10: reducing lr to 1.8624594121977604e-05
[I 2024-06-21 22:50:42,619] Trial 110 finished with value: 1.0933998823165894 and parameters: {'hidden_size': 162, 'n_layers': 5, 'rnn_dropout': 0.6222787814873403, 'bidirectional': False, 'fc_dropout': 0.14383543815437952, 'learning_rate_model': 0.0001937670170464033}. Best is trial 68 with value: 0.9733983278274536.
Epoch 4: reducing lr to 0.003055188083347597
Epoch 7: reducing lr to 0.006061022979661935
Epoch 10: reducing lr to 0.008180328487800979
Epoch 13: reducing lr to 0.008481897767193922
Epoch 16: reducing lr to 0.008226258853431239
Epoch 19: reducing lr to 0.007721114782682027
Epoch 22: reducing lr to 0.0069982057589186575
Epoch 25: reducing lr to 0.006102954238609995
Epoch 28: reducing lr to 0.0050916129647090875
Epoch 31: reducing lr to 0.004027727945028059
Epoch 34: reducing lr to 0.002978148149965688
Epoch 37: reducing lr to 0.002008820564362639
Epoch 40: reducing lr to 0.0011806523757590462
Epoch 43: reducing lr to 0.0005456800252840746
Epoch 46: reducing lr to 0.00014380208562041473
Epoch 49: reducing lr to 2.6946439759950593e-07
[I 2024-06-21 22:50:47,241] Trial 111 finished with value: 1.1430079936981201 and parameters: {'hidden_size': 47, 'n_layers': 3, 'rnn_dropout': 0.19313499837247905, 'bidirectional': False, 'fc_dropout': 0.256255460657625, 'learning_rate_model': 0.08510670563663286}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 22:51:15,868] Trial 112 finished with value: 0.9816269874572754 and parameters: {'hidden_size': 133, 'n_layers': 3, 'rnn_dropout': 0.6859875992668774, 'bidirectional': True, 'fc_dropout': 0.6342845423815101, 'learning_rate_model': 0.0005392051542230778}. Best is trial 68 with value: 0.9733983278274536.
Epoch 10: reducing lr to 0.00014430869452913435
Epoch 13: reducing lr to 0.0001496286604796687
Epoch 16: reducing lr to 0.0001451189494123255
Epoch 19: reducing lr to 0.00013620773252077164
Epoch 22: reducing lr to 0.00012345493687959485
Epoch 25: reducing lr to 0.00010766185737629298
Epoch 28: reducing lr to 8.982084534630976e-05
Epoch 31: reducing lr to 7.105291218222938e-05
Epoch 34: reducing lr to 5.2537336645687166e-05
Epoch 37: reducing lr to 3.543748562404967e-05
Epoch 40: reducing lr to 2.0827819236426502e-05
Epoch 43: reducing lr to 9.626309285354654e-06
Epoch 46: reducing lr to 2.536804148805909e-06
Epoch 49: reducing lr to 4.753605615907819e-09
[I 2024-06-21 22:51:22,503] Trial 113 finished with value: 1.0924947261810303 and parameters: {'hidden_size': 82, 'n_layers': 3, 'rnn_dropout': 0.5578951004905116, 'bidirectional': False, 'fc_dropout': 0.23623103551987823, 'learning_rate_model': 0.001501362396927331}. Best is trial 68 with value: 0.9733983278274536.
Epoch 3: reducing lr to 1.5316438744007403e-05
Epoch 6: reducing lr to 3.632922873136697e-05
Epoch 9: reducing lr to 5.480947700447883e-05
Epoch 12: reducing lr to 6.086303843615618e-05
Epoch 15: reducing lr to 5.966065816536839e-05
Epoch 18: reducing lr to 5.662222994298838e-05
Epoch 24: reducing lr to 4.590426195771348e-05
Epoch 27: reducing lr to 3.8898176427060663e-05
Epoch 30: reducing lr to 3.136062120724817e-05
Epoch 33: reducing lr to 2.3765213677855266e-05
Epoch 36: reducing lr to 1.658920392133902e-05
Epoch 39: reducing lr to 1.028348398645425e-05
Epoch 42: reducing lr to 5.244262084772478e-06
Epoch 45: reducing lr to 1.7881783993789784e-06
Epoch 48: reducing lr to 1.3238701754308578e-07
[I 2024-06-21 22:51:39,868] Trial 114 finished with value: 1.1074886322021484 and parameters: {'hidden_size': 106, 'n_layers': 6, 'rnn_dropout': 0.7107091475163088, 'bidirectional': False, 'fc_dropout': 0.7185640368137097, 'learning_rate_model': 0.0006087919149176257}. Best is trial 68 with value: 0.9733983278274536.
Epoch 38: reducing lr to 9.354426858900427e-06
Epoch 42: reducing lr to 4.000473160165454e-06
Epoch 45: reducing lr to 1.3640736440451141e-06
Epoch 48: reducing lr to 1.0098860466437737e-07
[I 2024-06-21 22:52:10,186] Trial 115 finished with value: 0.978308379650116 and parameters: {'hidden_size': 186, 'n_layers': 2, 'rnn_dropout': 0.4870649393550366, 'bidirectional': True, 'fc_dropout': 0.01101401004617344, 'learning_rate_model': 0.00046440389064944195}. Best is trial 68 with value: 0.9733983278274536.
Epoch 8: reducing lr to 0.0006005049016818015
Epoch 14: reducing lr to 0.000728999760014961
Epoch 17: reducing lr to 0.0006994618951396501
Epoch 20: reducing lr to 0.0006491082351219762
Epoch 23: reducing lr to 0.0005811026837547948
Epoch 27: reducing lr to 0.00047048266046474386
Epoch 30: reducing lr to 0.00037931414412395675
Epoch 33: reducing lr to 0.00028744589039120065
Epoch 36: reducing lr to 0.0002006503520939869
Epoch 39: reducing lr to 0.00012438117539689464
Epoch 42: reducing lr to 6.343059249691889e-05
Epoch 45: reducing lr to 2.162844143357139e-05
Epoch 48: reducing lr to 1.601252345118532e-06
[I 2024-06-21 22:52:30,340] Trial 116 finished with value: 1.0943293571472168 and parameters: {'hidden_size': 189, 'n_layers': 3, 'rnn_dropout': 0.62379543638608, 'bidirectional': False, 'fc_dropout': 0.1156781766704774, 'learning_rate_model': 0.007363482458797472}. Best is trial 68 with value: 0.9733983278274536.
Epoch 37: reducing lr to 7.255475039697276e-06
[I 2024-06-21 22:53:27,809] Trial 117 finished with value: 1.0380979776382446 and parameters: {'hidden_size': 145, 'n_layers': 5, 'rnn_dropout': 0.20271682758903975, 'bidirectional': True, 'fc_dropout': 0.3397192927450718, 'learning_rate_model': 0.0003073891164855588}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 22:53:31,926] Trial 118 finished with value: 1.0934619903564453 and parameters: {'hidden_size': 81, 'n_layers': 1, 'rnn_dropout': 0.5053173014927518, 'bidirectional': True, 'fc_dropout': 0.5186502049304996, 'learning_rate_model': 4.7836881141741806e-05}. Best is trial 68 with value: 0.9733983278274536.
Epoch 18: reducing lr to 9.071410845468508e-05
Epoch 21: reducing lr to 8.321060722730456e-05
Epoch 24: reducing lr to 7.35429212511958e-05
Epoch 27: reducing lr to 6.231851692606834e-05
Epoch 30: reducing lr to 5.024264844858675e-05
Epoch 33: reducing lr to 3.807409516001743e-05
Epoch 36: reducing lr to 2.6577456331417155e-05
Epoch 39: reducing lr to 1.6475103198487598e-05
Epoch 42: reducing lr to 8.401798374981775e-06
Epoch 45: reducing lr to 2.8648290507265097e-06
Epoch 48: reducing lr to 2.1209638475009306e-07
[I 2024-06-21 22:53:36,966] Trial 119 finished with value: 1.0916831493377686 and parameters: {'hidden_size': 43, 'n_layers': 4, 'rnn_dropout': 0.7002661810836313, 'bidirectional': False, 'fc_dropout': 0.3431458672239224, 'learning_rate_model': 0.0009753415902513679}. Best is trial 68 with value: 0.9733983278274536.
Epoch 18: reducing lr to 0.00024921830395780874
Epoch 21: reducing lr to 0.00022860398186956105
Epoch 24: reducing lr to 0.00020204400852907363
Epoch 27: reducing lr to 0.00017120727258471588
Epoch 30: reducing lr to 0.00013803131448908077
Epoch 33: reducing lr to 0.00010460072399044443
Epoch 36: reducing lr to 7.301607989387038e-05
Epoch 39: reducing lr to 4.526194818646086e-05
Epoch 42: reducing lr to 2.3082208235055247e-05
Epoch 45: reducing lr to 7.870526969989142e-06
Epoch 48: reducing lr to 5.826910740064535e-07
[I 2024-06-21 22:53:45,368] Trial 120 finished with value: 0.9844418168067932 and parameters: {'hidden_size': 45, 'n_layers': 3, 'rnn_dropout': 0.20950432328281618, 'bidirectional': True, 'fc_dropout': 0.3413113266145149, 'learning_rate_model': 0.002679549863220907}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 22:53:54,892] Trial 121 finished with value: 1.0969467163085938 and parameters: {'hidden_size': 42, 'n_layers': 4, 'rnn_dropout': 0.23755766862952366, 'bidirectional': True, 'fc_dropout': 0.5067792100979368, 'learning_rate_model': 3.155042401877456e-05}. Best is trial 68 with value: 0.9733983278274536.
Epoch 11: reducing lr to 0.00018204869342689317
Epoch 14: reducing lr to 0.00018125607816665474
Epoch 23: reducing lr to 0.00014448344052589305
Epoch 34: reducing lr to 6.406649552800701e-05
Epoch 37: reducing lr to 4.321413416078384e-05
Epoch 49: reducing lr to 5.796769923598587e-09
[I 2024-06-21 22:54:45,776] Trial 122 finished with value: 1.0298917293548584 and parameters: {'hidden_size': 103, 'n_layers': 7, 'rnn_dropout': 0.7586393392581321, 'bidirectional': True, 'fc_dropout': 0.511301478975924, 'learning_rate_model': 0.0018308318127610833}. Best is trial 68 with value: 0.9733983278274536.
Epoch 6: reducing lr to 0.0029826619086342643
Epoch 9: reducing lr to 0.004499906686768614
Epoch 12: reducing lr to 0.004996909450778632
Epoch 15: reducing lr to 0.004898192963844913
Epoch 18: reducing lr to 0.004648735311219674
Epoch 21: reducing lr to 0.004264210878276274
Epoch 24: reducing lr to 0.003768780631090738
Epoch 27: reducing lr to 0.003193574793514853
Epoch 30: reducing lr to 0.002574734822961034
Epoch 33: reducing lr to 0.0019511451264664869
Epoch 36: reducing lr to 0.0013619883591974697
Epoch 39: reducing lr to 0.0008442831583695291
Epoch 42: reducing lr to 0.00043055856965222296
Epoch 45: reducing lr to 0.00014681103298692585
Epoch 48: reducing lr to 1.0869091588573687e-05
[I 2024-06-21 22:55:05,593] Trial 123 finished with value: 1.0541037321090698 and parameters: {'hidden_size': 102, 'n_layers': 3, 'rnn_dropout': 0.47941580452642407, 'bidirectional': True, 'fc_dropout': 0.46348843223778646, 'learning_rate_model': 0.0499823563097424}. Best is trial 68 with value: 0.9733983278274536.
Epoch 10: reducing lr to 0.0003002538657073338
Epoch 13: reducing lr to 0.0003113227784106973
Epoch 16: reducing lr to 0.0003019397111907275
Epoch 19: reducing lr to 0.00028339871247560605
Epoch 22: reducing lr to 0.00025686478669703194
Epoch 25: reducing lr to 0.00022400513684875276
Epoch 35: reducing lr to 9.700861910258009e-05
Epoch 38: reducing lr to 6.292206661205964e-05
Epoch 41: reducing lr to 3.469592309316128e-05
Epoch 44: reducing lr to 1.4103782054166815e-05
Epoch 47: reducing lr to 2.4394994526722327e-06
[I 2024-06-21 22:55:11,625] Trial 124 finished with value: 1.0949574708938599 and parameters: {'hidden_size': 27, 'n_layers': 7, 'rnn_dropout': 0.042395634134391624, 'bidirectional': False, 'fc_dropout': 0.08600710094813416, 'learning_rate_model': 0.0031237886599691338}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 22:55:14,997] Trial 125 finished with value: 1.095399022102356 and parameters: {'hidden_size': 24, 'n_layers': 2, 'rnn_dropout': 0.04882934489573687, 'bidirectional': True, 'fc_dropout': 0.3028644746004818, 'learning_rate_model': 5.659486653989453e-05}. Best is trial 68 with value: 0.9733983278274536.
Epoch 7: reducing lr to 0.0002688501172440289
Epoch 10: reducing lr to 0.00036285661354853196
Epoch 13: reducing lr to 0.0003762333877983654
Epoch 16: reducing lr to 0.0003648939567868256
Epoch 19: reducing lr to 0.0003424871711498527
Epoch 22: reducing lr to 0.00031042093803248735
Epoch 25: reducing lr to 0.00027071007123565763
Epoch 28: reducing lr to 0.0002258497859382159
Epoch 31: reducing lr to 0.00017865880625785773
Epoch 34: reducing lr to 0.00013210236654358012
Epoch 37: reducing lr to 8.91056915744007e-05
Epoch 40: reducing lr to 5.2370454742107836e-05
Epoch 43: reducing lr to 2.4204847806653722e-05
Epoch 46: reducing lr to 6.3786604519919935e-06
Epoch 49: reducing lr to 1.1952691011214364e-08
[I 2024-06-21 22:55:29,627] Trial 126 finished with value: 1.0925934314727783 and parameters: {'hidden_size': 154, 'n_layers': 3, 'rnn_dropout': 0.6362409339245143, 'bidirectional': False, 'fc_dropout': 0.022792679593253776, 'learning_rate_model': 0.0037750966900208034}. Best is trial 68 with value: 0.9733983278274536.
Epoch 20: reducing lr to 2.527819989688165e-05
Epoch 23: reducing lr to 2.2629862025718713e-05
Epoch 26: reducing lr to 1.9460511324873234e-05
Epoch 29: reducing lr to 1.5969290253699052e-05
Epoch 32: reducing lr to 1.2375565143445915e-05
Epoch 35: reducing lr to 8.905144275508809e-06
Epoch 38: reducing lr to 5.776085532163442e-06
Epoch 41: reducing lr to 3.1849974133725317e-06
Epoch 44: reducing lr to 1.2946912880996443e-06
Epoch 47: reducing lr to 2.2393983943940973e-07
[I 2024-06-21 22:55:33,860] Trial 127 finished with value: 1.092767357826233 and parameters: {'hidden_size': 80, 'n_layers': 2, 'rnn_dropout': 0.34710670096936047, 'bidirectional': False, 'fc_dropout': 0.7835866833438452, 'learning_rate_model': 0.00028675584665119325}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 22:56:27,883] Trial 128 finished with value: 1.0943549871444702 and parameters: {'hidden_size': 159, 'n_layers': 4, 'rnn_dropout': 0.7113048712859863, 'bidirectional': True, 'fc_dropout': 0.7288153917255746, 'learning_rate_model': 1.4932655812821317e-05}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 22:57:41,171] Trial 129 finished with value: 1.0945645570755005 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.30109057334967004, 'bidirectional': True, 'fc_dropout': 0.41336508781408976, 'learning_rate_model': 1.3316445065440956e-05}. Best is trial 68 with value: 0.9733983278274536.
Epoch 16: reducing lr to 0.0006592984754966636
Epoch 19: reducing lr to 0.0006188133993903824
Epoch 26: reducing lr to 0.0004628981508524633
Epoch 29: reducing lr to 0.0003798540955815152
Epoch 32: reducing lr to 0.00029437182430727476
Epoch 35: reducing lr to 0.0002118225338169154
Epoch 38: reducing lr to 0.0001373930657508836
Epoch 41: reducing lr to 7.576005524765565e-05
Epoch 44: reducing lr to 3.0796220776589665e-05
Epoch 47: reducing lr to 5.326753025559471e-06
[I 2024-06-21 22:57:56,519] Trial 130 finished with value: 1.0219025611877441 and parameters: {'hidden_size': 121, 'n_layers': 2, 'rnn_dropout': 0.5047673602716244, 'bidirectional': True, 'fc_dropout': 0.36146899953669653, 'learning_rate_model': 0.006820928234876919}. Best is trial 68 with value: 0.9733983278274536.
Epoch 8: reducing lr to 0.0006591865136568047
Epoch 11: reducing lr to 0.0008037373109360211
Epoch 14: reducing lr to 0.0008002379479585727
Epoch 17: reducing lr to 0.0007678136295000711
Epoch 22: reducing lr to 0.0006646575475936733
Epoch 27: reducing lr to 0.000516458439921393
Epoch 30: reducing lr to 0.00041638089471961973
Epoch 33: reducing lr to 0.00031553523346984175
Epoch 36: reducing lr to 0.0002202579957139656
Epoch 39: reducing lr to 0.00013653576039893847
Epoch 42: reducing lr to 6.962905883053945e-05
Epoch 45: reducing lr to 2.374198256250206e-05
Epoch 48: reducing lr to 1.7577274521946365e-06
[I 2024-06-21 22:58:17,901] Trial 131 finished with value: 1.0940637588500977 and parameters: {'hidden_size': 159, 'n_layers': 4, 'rnn_dropout': 0.4228267921420521, 'bidirectional': False, 'fc_dropout': 0.11377700245822933, 'learning_rate_model': 0.008083045312026036}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 22:58:43,239] Trial 132 finished with value: 1.0964757204055786 and parameters: {'hidden_size': 104, 'n_layers': 4, 'rnn_dropout': 0.7087983769322811, 'bidirectional': True, 'fc_dropout': 0.2151136177701167, 'learning_rate_model': 1.941101032854524e-05}. Best is trial 68 with value: 0.9733983278274536.
Epoch 10: reducing lr to 0.00047311986192689095
Epoch 15: reducing lr to 0.0004823734330784113
Epoch 22: reducing lr to 0.00040475026734355777
Epoch 27: reducing lr to 0.000314502847950523
Epoch 30: reducing lr to 0.0002535595647181966
Epoch 33: reducing lr to 0.0001921485290667389
Epoch 36: reducing lr to 0.0001341284439972744
Epoch 39: reducing lr to 8.31449002926395e-05
Epoch 42: reducing lr to 4.240135431933731e-05
Epoch 45: reducing lr to 1.4457932245303274e-05
Epoch 48: reducing lr to 1.070386786050214e-06
[I 2024-06-21 22:58:46,248] Trial 133 finished with value: 1.0929749011993408 and parameters: {'hidden_size': 93, 'n_layers': 1, 'rnn_dropout': 0.23292315244194245, 'bidirectional': False, 'fc_dropout': 0.08750633135486768, 'learning_rate_model': 0.004922256224784005}. Best is trial 68 with value: 0.9733983278274536.
Epoch 28: reducing lr to 3.496186241426402e-06
Epoch 31: reducing lr to 2.7656632825822624e-06
Epoch 34: reducing lr to 2.0449630910692042e-06
Epoch 37: reducing lr to 1.3793685551706802e-06
Epoch 40: reducing lr to 8.107019564618489e-07
Epoch 43: reducing lr to 3.746944258808949e-07
Epoch 46: reducing lr to 9.874255500550155e-08
Epoch 49: reducing lr to 1.8502932685025993e-10
[I 2024-06-21 22:59:04,272] Trial 134 finished with value: 1.1068123579025269 and parameters: {'hidden_size': 108, 'n_layers': 6, 'rnn_dropout': 0.11488559829936743, 'bidirectional': False, 'fc_dropout': 0.6375698015839202, 'learning_rate_model': 5.8439024207513246e-05}. Best is trial 68 with value: 0.9733983278274536.
Epoch 9: reducing lr to 0.00039063083923929133
Epoch 16: reducing lr to 0.0004193902561251737
Epoch 19: reducing lr to 0.00039363705470199463
Epoch 22: reducing lr to 0.00035678178354737257
Epoch 27: reducing lr to 0.00027722992688543736
Epoch 33: reducing lr to 0.00016937628072829594
Epoch 36: reducing lr to 0.00011823237520720842
Epoch 39: reducing lr to 7.329108394163618e-05
Epoch 42: reducing lr to 3.7376209577735364e-05
Epoch 45: reducing lr to 1.2744468056170335e-05
Epoch 48: reducing lr to 9.435312028796376e-07
[I 2024-06-21 22:59:19,939] Trial 135 finished with value: 1.0923867225646973 and parameters: {'hidden_size': 158, 'n_layers': 3, 'rnn_dropout': 0.17351319008943628, 'bidirectional': False, 'fc_dropout': 0.3048651930347619, 'learning_rate_model': 0.004338901037624098}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 22:59:22,688] Trial 136 finished with value: 1.0957562923431396 and parameters: {'hidden_size': 29, 'n_layers': 2, 'rnn_dropout': 0.013260706320747318, 'bidirectional': False, 'fc_dropout': 0.3221254817399354, 'learning_rate_model': 0.00019235578705946124}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 22:59:47,367] Trial 137 finished with value: 1.1069517135620117 and parameters: {'hidden_size': 119, 'n_layers': 7, 'rnn_dropout': 0.6675892112144237, 'bidirectional': False, 'fc_dropout': 0.03178296534646785, 'learning_rate_model': 1.9113215939952844e-05}. Best is trial 68 with value: 0.9733983278274536.
Epoch 13: reducing lr to 6.096343605583295e-05
Epoch 16: reducing lr to 5.912603751598827e-05
Epoch 19: reducing lr to 5.54953266654977e-05
Epoch 22: reducing lr to 5.02994354564761e-05
Epoch 25: reducing lr to 4.386483670154672e-05
Epoch 28: reducing lr to 3.659584563676996e-05
Epoch 31: reducing lr to 2.8949197663843484e-05
Epoch 34: reducing lr to 2.1405368148559465e-05
Epoch 37: reducing lr to 1.443834945722048e-05
Epoch 40: reducing lr to 8.48591053433152e-06
Epoch 43: reducing lr to 3.922062048073544e-06
Epoch 46: reducing lr to 1.033574029307802e-06
Epoch 49: reducing lr to 1.936768872167448e-09
[I 2024-06-21 22:59:56,415] Trial 138 finished with value: 1.09215247631073 and parameters: {'hidden_size': 103, 'n_layers': 3, 'rnn_dropout': 0.7498016497027592, 'bidirectional': False, 'fc_dropout': 0.7162082925849766, 'learning_rate_model': 0.0006117023983794077}. Best is trial 68 with value: 0.9733983278274536.
Epoch 6: reducing lr to 0.00028940363792888315
Epoch 9: reducing lr to 0.00043661984005678045
Epoch 12: reducing lr to 0.00048484334388362545
Epoch 15: reducing lr to 0.00047526501710127195
Epoch 18: reducing lr to 0.00045106047954709754
Epoch 21: reducing lr to 0.0004137505955658733
Epoch 27: reducing lr to 0.00030986822896879334
Epoch 33: reducing lr to 0.00018931696418289962
Epoch 36: reducing lr to 0.00013215188246026277
Epoch 39: reducing lr to 8.191964927936127e-05
Epoch 42: reducing lr to 4.1776513804028716e-05
Epoch 45: reducing lr to 1.4244875328148829e-05
Epoch 48: reducing lr to 1.054613208962673e-06
[I 2024-06-21 23:00:08,248] Trial 139 finished with value: 1.092684030532837 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.21904776332648687, 'bidirectional': False, 'fc_dropout': 0.176102939221585, 'learning_rate_model': 0.004849720213485595}. Best is trial 68 with value: 0.9733983278274536.
Epoch 14: reducing lr to 0.00012556020282467685
Epoch 24: reducing lr to 9.562952815373323e-05
Epoch 27: reducing lr to 8.103418068647163e-05
Epoch 30: reducing lr to 6.533165507420097e-05
Epoch 33: reducing lr to 4.950860930036285e-05
Epoch 36: reducing lr to 3.455926913507732e-05
Epoch 39: reducing lr to 2.142295027653409e-05
Epoch 42: reducing lr to 1.0925048945200082e-05
Epoch 45: reducing lr to 3.7252021771929517e-06
Epoch 48: reducing lr to 2.757937385637144e-07
[I 2024-06-21 23:00:10,240] Trial 140 finished with value: 1.0923454761505127 and parameters: {'hidden_size': 42, 'n_layers': 1, 'rnn_dropout': 0.6961533969643021, 'bidirectional': False, 'fc_dropout': 0.45723367418026584, 'learning_rate_model': 0.0012682587865373047}. Best is trial 68 with value: 0.9733983278274536.
Epoch 27: reducing lr to 1.4558856376018923e-05
Epoch 30: reducing lr to 1.1737691119664662e-05
Epoch 33: reducing lr to 8.894872831123109e-06
Epoch 36: reducing lr to 6.20902724671803e-06
Epoch 39: reducing lr to 3.848914786136961e-06
Epoch 42: reducing lr to 1.9628287365494242e-06
Epoch 45: reducing lr to 6.692815674810582e-07
Epoch 48: reducing lr to 4.954997255651424e-08
[I 2024-06-21 23:00:24,493] Trial 141 finished with value: 1.078850269317627 and parameters: {'hidden_size': 30, 'n_layers': 7, 'rnn_dropout': 0.7609084947349537, 'bidirectional': True, 'fc_dropout': 0.4222774368943676, 'learning_rate_model': 0.0002278593719884827}. Best is trial 68 with value: 0.9733983278274536.
Epoch 19: reducing lr to 1.8237113262106364e-05
Epoch 23: reducing lr to 1.586388273946441e-05
Epoch 26: reducing lr to 1.364211895578287e-05
Epoch 32: reducing lr to 8.675462274012425e-06
Epoch 42: reducing lr to 1.7316311034117998e-06
Epoch 45: reducing lr to 5.904482431960879e-07
[I 2024-06-21 23:01:24,105] Trial 142 finished with value: 1.073012113571167 and parameters: {'hidden_size': 118, 'n_layers': 7, 'rnn_dropout': 0.24936233417518514, 'bidirectional': True, 'fc_dropout': 0.6195818128120637, 'learning_rate_model': 0.00020102027670166017}. Best is trial 68 with value: 0.9733983278274536.
Epoch 19: reducing lr to 1.7580508348272806e-05
Epoch 23: reducing lr to 1.5292723082258405e-05
[I 2024-06-21 23:03:04,988] Trial 143 finished with value: 1.06727135181427 and parameters: {'hidden_size': 180, 'n_layers': 6, 'rnn_dropout': 0.6101056763560886, 'bidirectional': True, 'fc_dropout': 0.0331800636611991, 'learning_rate_model': 0.0001937827879848057}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 23:03:09,757] Trial 144 finished with value: 1.1090408563613892 and parameters: {'hidden_size': 22, 'n_layers': 5, 'rnn_dropout': 0.5362800255489347, 'bidirectional': False, 'fc_dropout': 0.428806795791651, 'learning_rate_model': 1.4435761775167338e-05}. Best is trial 68 with value: 0.9733983278274536.
Epoch 10: reducing lr to 0.00028356394625716065
Epoch 13: reducing lr to 0.00029401758208145735
Epoch 16: reducing lr to 0.0002851560822881981
Epoch 22: reducing lr to 0.00024258669375904434
Epoch 25: reducing lr to 0.00021155358129051443
Epoch 28: reducing lr to 0.0001764963188507787
Epoch 31: reducing lr to 0.00013961767333006242
Epoch 34: reducing lr to 0.00010323490593343502
Epoch 37: reducing lr to 6.963401132395515e-05
Epoch 40: reducing lr to 4.0926283990598643e-05
Epoch 43: reducing lr to 1.8915521741456975e-05
Epoch 46: reducing lr to 4.984773770312992e-06
Epoch 49: reducing lr to 9.340748121896073e-09
[I 2024-06-21 23:03:12,458] Trial 145 finished with value: 1.0923900604248047 and parameters: {'hidden_size': 40, 'n_layers': 2, 'rnn_dropout': 0.2685310409317198, 'bidirectional': False, 'fc_dropout': 0.40964882201913505, 'learning_rate_model': 0.0029501496595471795}. Best is trial 68 with value: 0.9733983278274536.
Epoch 8: reducing lr to 0.0015975169055687326
Epoch 17: reducing lr to 0.0018607711596646214
Epoch 22: reducing lr to 0.0016107757769564424
Epoch 25: reducing lr to 0.0014047158934843331
Epoch 28: reducing lr to 0.0011719356520403365
Epoch 31: reducing lr to 0.0009270614259595902
Epoch 34: reducing lr to 0.0006854798308893407
Epoch 37: reducing lr to 0.0004623698726210771
Epoch 40: reducing lr to 0.00027175054769647164
Epoch 43: reducing lr to 0.0001255990745308383
Epoch 46: reducing lr to 3.3098900514318476e-05
Epoch 49: reducing lr to 6.202257255040477e-08
[I 2024-06-21 23:03:28,495] Trial 146 finished with value: 1.125978946685791 and parameters: {'hidden_size': 95, 'n_layers': 6, 'rnn_dropout': 0.36324724484732107, 'bidirectional': False, 'fc_dropout': 0.5249612381952792, 'learning_rate_model': 0.01958899532517217}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 23:03:34,031] Trial 147 finished with value: 1.1014584302902222 and parameters: {'hidden_size': 192, 'n_layers': 1, 'rnn_dropout': 0.18805749127281166, 'bidirectional': False, 'fc_dropout': 0.6910633164011616, 'learning_rate_model': 2.1207815222486866e-05}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 23:03:39,505] Trial 148 finished with value: 1.066739797592163 and parameters: {'hidden_size': 103, 'n_layers': 1, 'rnn_dropout': 0.6183084817336942, 'bidirectional': True, 'fc_dropout': 0.38915259453631584, 'learning_rate_model': 9.070770449331516e-05}. Best is trial 68 with value: 0.9733983278274536.
Epoch 12: reducing lr to 5.47349400987404e-05
Epoch 37: reducing lr to 1.2922818934851809e-05
Epoch 40: reducing lr to 7.595181544638185e-06
Epoch 43: reducing lr to 3.5103803138080757e-06
Epoch 46: reducing lr to 9.25084274770087e-07
Epoch 49: reducing lr to 1.7334746972054448e-09
[I 2024-06-21 23:05:48,636] Trial 149 finished with value: 1.034926176071167 and parameters: {'hidden_size': 192, 'n_layers': 7, 'rnn_dropout': 0.49114720190776073, 'bidirectional': True, 'fc_dropout': 0.7182805741341455, 'learning_rate_model': 0.0005474946675652392}. Best is trial 68 with value: 0.9733983278274536.
Epoch 6: reducing lr to 0.00015685370938765133
Epoch 9: reducing lr to 0.00023664333314973176
Epoch 12: reducing lr to 0.00026277996193934284
Epoch 15: reducing lr to 0.00025758861017786836
Epoch 18: reducing lr to 0.000244470006947601
Epoch 21: reducing lr to 0.00022424844463014314
Epoch 24: reducing lr to 0.00019819451213818495
Epoch 27: reducing lr to 0.00016794530118201568
Epoch 30: reducing lr to 0.00013540143671728397
Epoch 33: reducing lr to 0.00010260779130010132
Epoch 36: reducing lr to 7.162492190767431e-05
Epoch 39: reducing lr to 4.439958306384803e-05
Epoch 42: reducing lr to 2.264242841707666e-05
Epoch 45: reducing lr to 7.720571693483127e-06
Epoch 48: reducing lr to 5.715891997033415e-07
[I 2024-06-21 23:05:59,147] Trial 150 finished with value: 1.10738205909729 and parameters: {'hidden_size': 59, 'n_layers': 7, 'rnn_dropout': 0.547374440786582, 'bidirectional': False, 'fc_dropout': 0.561104235706677, 'learning_rate_model': 0.002628497037637164}. Best is trial 68 with value: 0.9733983278274536.
Epoch 12: reducing lr to 0.0023424373990653525
Epoch 18: reducing lr to 0.002179221288402548
Epoch 21: reducing lr to 0.0019989649876923825
Epoch 24: reducing lr to 0.0017667185659656977
Epoch 27: reducing lr to 0.0014970751635045004
Epoch 30: reducing lr to 0.0012069770728064815
Epoch 33: reducing lr to 0.0009146524187119337
Epoch 36: reducing lr to 0.0006384691379946212
Epoch 39: reducing lr to 0.0003957807250755029
Epoch 42: reducing lr to 0.0002018360560614711
Epoch 45: reducing lr to 6.882167020465097e-05
Epoch 48: reducing lr to 5.0951827087787616e-06
[I 2024-06-21 23:06:20,920] Trial 151 finished with value: 1.1058332920074463 and parameters: {'hidden_size': 151, 'n_layers': 2, 'rnn_dropout': 0.4803911642196355, 'bidirectional': True, 'fc_dropout': 0.10440853707141279, 'learning_rate_model': 0.023430590821514066}. Best is trial 68 with value: 0.9733983278274536.
[I 2024-06-21 23:06:36,203] Trial 152 finished with value: 1.1064062118530273 and parameters: {'hidden_size': 130, 'n_layers': 4, 'rnn_dropout': 0.19863258319767887, 'bidirectional': False, 'fc_dropout': 0.3373327478797674, 'learning_rate_model': 1.8101769159892324e-05}. Best is trial 68 with value: 0.9733983278274536.
Epoch 22: reducing lr to 0.00010570432404762543
Epoch 27: reducing lr to 8.213536502854218e-05
Epoch 32: reducing lr to 5.547822850994804e-05
Epoch 37: reducing lr to 3.0342208732332632e-05
Epoch 42: reducing lr to 1.1073510899561749e-05
Epoch 45: reducing lr to 3.7758244488543925e-06
Epoch 48: reducing lr to 2.7954153664071657e-07
[I 2024-06-21 23:07:04,656] Trial 153 finished with value: 0.9688200950622559 and parameters: {'hidden_size': 181, 'n_layers': 2, 'rnn_dropout': 0.6609820283939132, 'bidirectional': True, 'fc_dropout': 0.4787717544837091, 'learning_rate_model': 0.0012854933251677597}. Best is trial 153 with value: 0.9688200950622559.
Epoch 24: reducing lr to 6.100828180268109e-06
Epoch 28: reducing lr to 4.840567321985588e-06
Epoch 34: reducing lr to 2.8313084114356693e-06
Epoch 37: reducing lr to 1.9097742202685293e-06
Epoch 40: reducing lr to 1.1224394604098543e-06
Epoch 43: reducing lr to 5.187748787974256e-07
Epoch 46: reducing lr to 1.3671182026446825e-07
Epoch 49: reducing lr to 2.5617826148572427e-10
[I 2024-06-21 23:08:04,106] Trial 154 finished with value: 1.0750843286514282 and parameters: {'hidden_size': 131, 'n_layers': 6, 'rnn_dropout': 0.22571155872388848, 'bidirectional': True, 'fc_dropout': 0.09208243277352271, 'learning_rate_model': 8.091045824612664e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 0.004266341842232598
Epoch 9: reducing lr to 0.006436579395179877
Epoch 12: reducing lr to 0.007147482525589351
Epoch 15: reducing lr to 0.007006280374080205
Epoch 18: reducing lr to 0.00664946097789611
Epoch 21: reducing lr to 0.0060994446743793725
Epoch 24: reducing lr to 0.0053907908415877365
Epoch 27: reducing lr to 0.004568027548958931
Epoch 30: reducing lr to 0.0036828508373856553
Epoch 33: reducing lr to 0.002790880209793469
Epoch 36: reducing lr to 0.0019481617774568887
Epoch 39: reducing lr to 0.001207646282274593
Epoch 42: reducing lr to 0.000615862641327726
Epoch 45: reducing lr to 0.0002099956589516061
Epoch 48: reducing lr to 1.5546938155193423e-05
[I 2024-06-21 23:08:11,782] Trial 155 finished with value: 1.1914589405059814 and parameters: {'hidden_size': 126, 'n_layers': 2, 'rnn_dropout': 0.10513252842996845, 'bidirectional': False, 'fc_dropout': 0.48174692311416867, 'learning_rate_model': 0.07149379468062945}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.005587593129092662
Epoch 12: reducing lr to 0.006204727977130286
Epoch 15: reducing lr to 0.006082150421079872
Epoch 18: reducing lr to 0.0057723955832947125
Epoch 21: reducing lr to 0.0052949265535923015
Epoch 24: reducing lr to 0.004679744320312157
Epoch 27: reducing lr to 0.003965503690544566
Epoch 30: reducing lr to 0.0031970819857919525
Epoch 33: reducing lr to 0.002422762484067343
Epoch 36: reducing lr to 0.0016911988019958004
Epoch 39: reducing lr to 0.0010483574667415785
Epoch 42: reducing lr to 0.0005346302207853836
Epoch 45: reducing lr to 0.00018229718442935375
Epoch 48: reducing lr to 1.3496293525012277e-05
[I 2024-06-21 23:08:36,455] Trial 156 finished with value: 1.0412204265594482 and parameters: {'hidden_size': 79, 'n_layers': 5, 'rnn_dropout': 0.4791882311787358, 'bidirectional': True, 'fc_dropout': 0.6002537476597487, 'learning_rate_model': 0.06206374712466089}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.00020316837802482603
Epoch 15: reducing lr to 0.0002211507902248546
Epoch 22: reducing lr to 0.00018556337337134962
Epoch 25: reducing lr to 0.0001618250184490768
Epoch 28: reducing lr to 0.00013500844504730697
Epoch 31: reducing lr to 0.00010679862957000944
Epoch 34: reducing lr to 7.896812928127861e-05
Epoch 37: reducing lr to 5.326558453155109e-05
Epoch 40: reducing lr to 3.1306001162589313e-05
Epoch 43: reducing lr to 1.4469169635949976e-05
Epoch 46: reducing lr to 3.8130345155331332e-06
Epoch 49: reducing lr to 7.14507751619145e-09
[I 2024-06-21 23:08:43,081] Trial 157 finished with value: 1.0925008058547974 and parameters: {'hidden_size': 114, 'n_layers': 2, 'rnn_dropout': 0.6023407319839538, 'bidirectional': False, 'fc_dropout': 0.7340850781155146, 'learning_rate_model': 0.002256676630910648}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 0.0006058749562107815
Epoch 26: reducing lr to 0.0005210211371769301
Epoch 29: reducing lr to 0.000427549802211836
Epoch 32: reducing lr to 0.00033133410097010237
Epoch 35: reducing lr to 0.00023841965504883518
Epoch 38: reducing lr to 0.00015464458267098555
Epoch 41: reducing lr to 8.527273238190446e-05
Epoch 44: reducing lr to 3.4663093685342006e-05
Epoch 47: reducing lr to 5.995597333293724e-06
[I 2024-06-21 23:10:02,824] Trial 158 finished with value: 1.032740831375122 and parameters: {'hidden_size': 174, 'n_layers': 5, 'rnn_dropout': 0.6580625220190969, 'bidirectional': True, 'fc_dropout': 0.0983638172417428, 'learning_rate_model': 0.007677386005956413}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:10:05,780] Trial 159 finished with value: 1.1041940450668335 and parameters: {'hidden_size': 97, 'n_layers': 1, 'rnn_dropout': 0.4718589184879114, 'bidirectional': False, 'fc_dropout': 0.6306409517343752, 'learning_rate_model': 1.1178590317786991e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 33: reducing lr to 4.16821912117427e-05
Epoch 38: reducing lr to 2.1507950503213046e-05
Epoch 41: reducing lr to 1.185972166413196e-05
Epoch 44: reducing lr to 4.820939022860769e-06
Epoch 47: reducing lr to 8.338669771318797e-07
[I 2024-06-21 23:10:09,368] Trial 160 finished with value: 0.9850569367408752 and parameters: {'hidden_size': 72, 'n_layers': 1, 'rnn_dropout': 0.7979879941826636, 'bidirectional': True, 'fc_dropout': 0.7431595695144926, 'learning_rate_model': 0.001067769949377941}. Best is trial 153 with value: 0.9688200950622559.
Epoch 12: reducing lr to 0.00021780799538225897
Epoch 16: reducing lr to 0.00021058510797920642
Epoch 22: reducing lr to 0.00017914801146670583
Epoch 25: reducing lr to 0.00015623034726092747
Epoch 28: reducing lr to 0.00013034088582252186
Epoch 31: reducing lr to 0.00010310634996135804
Epoch 34: reducing lr to 7.623801547127435e-05
Epoch 37: reducing lr to 5.142406809636295e-05
Epoch 40: reducing lr to 3.0223679131057328e-05
Epoch 43: reducing lr to 1.3968936438051894e-05
Epoch 46: reducing lr to 3.6812089514273815e-06
Epoch 49: reducing lr to 6.89805539503055e-09
[I 2024-06-21 23:10:23,035] Trial 161 finished with value: 1.0929797887802124 and parameters: {'hidden_size': 100, 'n_layers': 5, 'rnn_dropout': 0.2113101153684557, 'bidirectional': False, 'fc_dropout': 0.1955583994972887, 'learning_rate_model': 0.0021786580164286193}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:10:27,048] Trial 162 finished with value: 1.1017955541610718 and parameters: {'hidden_size': 144, 'n_layers': 1, 'rnn_dropout': 0.7768666557003711, 'bidirectional': False, 'fc_dropout': 0.6013123232650325, 'learning_rate_model': 2.0904698538829833e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.0005533257698086839
Epoch 14: reducing lr to 0.0006084687014100183
Epoch 17: reducing lr to 0.0005838145557313748
Epoch 20: reducing lr to 0.0005417862481753245
Epoch 23: reducing lr to 0.00048502457032756786
Epoch 27: reducing lr to 0.00039269419436165266
Epoch 30: reducing lr to 0.00031659926019292497
Epoch 33: reducing lr to 0.00023992028152161653
Epoch 36: reducing lr to 0.0001674753077745671
Epoch 39: reducing lr to 0.00010381629243889876
Epoch 42: reducing lr to 5.294313162116293e-05
Epoch 45: reducing lr to 1.805244719468458e-05
Epoch 48: reducing lr to 1.3365051519962333e-06
[I 2024-06-21 23:10:38,682] Trial 163 finished with value: 1.0941293239593506 and parameters: {'hidden_size': 105, 'n_layers': 4, 'rnn_dropout': 0.537978120419944, 'bidirectional': False, 'fc_dropout': 0.5453162700820984, 'learning_rate_model': 0.006146022063804241}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:10:46,396] Trial 164 finished with value: 1.0968871116638184 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.6086419650446049, 'bidirectional': False, 'fc_dropout': 0.18701495408328475, 'learning_rate_model': 5.0899740979609974e-05}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:10:53,219] Trial 165 finished with value: 1.1069084405899048 and parameters: {'hidden_size': 88, 'n_layers': 3, 'rnn_dropout': 0.652381543204501, 'bidirectional': False, 'fc_dropout': 0.3421320797575714, 'learning_rate_model': 2.4989769577785342e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 37: reducing lr to 2.1747894168454957e-05
Epoch 40: reducing lr to 1.2781979323220142e-05
Epoch 43: reducing lr to 5.9076413544597355e-06
Epoch 46: reducing lr to 1.5568302091073602e-06
Epoch 49: reducing lr to 2.917275592016072e-09
[I 2024-06-21 23:11:00,534] Trial 166 finished with value: 0.9798585772514343 and parameters: {'hidden_size': 37, 'n_layers': 3, 'rnn_dropout': 0.15507821400760574, 'bidirectional': True, 'fc_dropout': 0.551076207214932, 'learning_rate_model': 0.0009213822578516836}. Best is trial 153 with value: 0.9688200950622559.
Epoch 15: reducing lr to 0.0007778326211249299
Epoch 22: reducing lr to 0.0006526643877124125
Epoch 25: reducing lr to 0.0005691717318118316
Epoch 28: reducing lr to 0.0004748523510966211
Epoch 31: reducing lr to 0.00037563265266440227
Epoch 34: reducing lr to 0.0002777470834345045
Epoch 37: reducing lr to 0.0001873459696427145
Epoch 40: reducing lr to 0.00011010961758933045
Epoch 43: reducing lr to 5.089103291012059e-05
Epoch 46: reducing lr to 1.3411223304431348e-05
Epoch 49: reducing lr to 2.513070094366937e-08
[I 2024-06-21 23:11:04,433] Trial 167 finished with value: 0.9995325803756714 and parameters: {'hidden_size': 32, 'n_layers': 2, 'rnn_dropout': 0.36928166471329765, 'bidirectional': True, 'fc_dropout': 0.5835181586049222, 'learning_rate_model': 0.00793719388055494}. Best is trial 153 with value: 0.9688200950622559.
Epoch 34: reducing lr to 2.222759476360744e-06
Epoch 42: reducing lr to 5.471740135755692e-07
Epoch 45: reducing lr to 1.8657434276948082e-07
Epoch 48: reducing lr to 1.3812951100344786e-08
[I 2024-06-21 23:11:32,541] Trial 168 finished with value: 1.0758436918258667 and parameters: {'hidden_size': 77, 'n_layers': 6, 'rnn_dropout': 0.3427539807608588, 'bidirectional': True, 'fc_dropout': 0.46482852593447216, 'learning_rate_model': 6.351992141755928e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 5: reducing lr to 0.0002572509958261362
Epoch 8: reducing lr to 0.00044028183260066565
Epoch 11: reducing lr to 0.0005368297573707338
Epoch 14: reducing lr to 0.0005344924735933415
Epoch 17: reducing lr to 0.0005128357223461988
Epoch 20: reducing lr to 0.0004759171199357229
Epoch 23: reducing lr to 0.00042605639656926
Epoch 31: reducing lr to 0.0002555017115199557
Epoch 35: reducing lr to 0.0001676587191137396
Epoch 42: reducing lr to 4.650642722361618e-05
Epoch 45: reducing lr to 1.5857672108919586e-05
Epoch 48: reducing lr to 1.1740159239174465e-06
[I 2024-06-21 23:11:54,437] Trial 169 finished with value: 1.0943306684494019 and parameters: {'hidden_size': 113, 'n_layers': 7, 'rnn_dropout': 0.24399518066817283, 'bidirectional': False, 'fc_dropout': 0.2638756501455059, 'learning_rate_model': 0.005398802811105274}. Best is trial 153 with value: 0.9688200950622559.
Epoch 20: reducing lr to 2.1017964017664224e-05
Epoch 23: reducing lr to 1.8815961093809398e-05
Epoch 26: reducing lr to 1.6180753711105434e-05
Epoch 29: reducing lr to 1.327792205572707e-05
Epoch 32: reducing lr to 1.0289861775928644e-05
Epoch 35: reducing lr to 7.404324782550752e-06
Epoch 38: reducing lr to 4.802618792999514e-06
Epoch 41: reducing lr to 2.648217092344286e-06
Epoch 44: reducing lr to 1.0764917999805262e-06
Epoch 47: reducing lr to 1.8619836486219747e-07
[I 2024-06-21 23:12:00,036] Trial 170 finished with value: 1.0921897888183594 and parameters: {'hidden_size': 182, 'n_layers': 1, 'rnn_dropout': 0.7028702905638164, 'bidirectional': False, 'fc_dropout': 0.5336950309593632, 'learning_rate_model': 0.0002384277397661184}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 0.00022003890338525198
[I 2024-06-21 23:12:12,773] Trial 171 finished with value: 1.0060293674468994 and parameters: {'hidden_size': 44, 'n_layers': 5, 'rnn_dropout': 0.6782304349717934, 'bidirectional': True, 'fc_dropout': 0.7650412078041857, 'learning_rate_model': 0.0022892462334419333}. Best is trial 153 with value: 0.9688200950622559.
Epoch 37: reducing lr to 2.8066894917747e-05
Epoch 40: reducing lr to 1.6495871633677416e-05
Epoch 43: reducing lr to 7.624147323094023e-06
Epoch 46: reducing lr to 2.009177971224905e-06
Epoch 49: reducing lr to 3.7649101495900936e-09
[I 2024-06-21 23:12:24,708] Trial 172 finished with value: 0.979374885559082 and parameters: {'hidden_size': 52, 'n_layers': 4, 'rnn_dropout': 0.2693967989609338, 'bidirectional': True, 'fc_dropout': 0.7941606989318544, 'learning_rate_model': 0.0011890962320255246}. Best is trial 153 with value: 0.9688200950622559.
Epoch 5: reducing lr to 0.004382502064575645
Epoch 8: reducing lr to 0.007500596972116863
Epoch 11: reducing lr to 0.009145377697946534
Epoch 14: reducing lr to 0.009105559966835227
Epoch 17: reducing lr to 0.008736617733014877
Epoch 20: reducing lr to 0.008107676139356253
Epoch 23: reducing lr to 0.00725825387611867
Epoch 26: reducing lr to 0.0062417230646162735
Epoch 29: reducing lr to 0.005121956234246816
Epoch 32: reducing lr to 0.003969312476120704
Epoch 35: reducing lr to 0.002856217058754028
Epoch 38: reducing lr to 0.0018526094041126512
Epoch 41: reducing lr to 0.0010215493048418138
Epoch 44: reducing lr to 0.00041525653358144424
Epoch 47: reducing lr to 7.182598841218024e-05
[I 2024-06-21 23:12:26,950] Trial 173 finished with value: 1.157962679862976 and parameters: {'hidden_size': 47, 'n_layers': 1, 'rnn_dropout': 0.2962445550191101, 'bidirectional': False, 'fc_dropout': 0.3311358765835271, 'learning_rate_model': 0.0919734611324751}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 8.542453036356333e-05
Epoch 13: reducing lr to 8.857372102291613e-05
Epoch 16: reducing lr to 8.590416634874935e-05
Epoch 19: reducing lr to 8.062910984288403e-05
Epoch 22: reducing lr to 7.308000421189872e-05
Epoch 25: reducing lr to 6.373118150952436e-05
Epoch 28: reducing lr to 5.317007097599162e-05
Epoch 31: reducing lr to 4.206026306270171e-05
Epoch 34: reducing lr to 3.109983999338431e-05
Epoch 37: reducing lr to 2.0977464847683216e-05
Epoch 40: reducing lr to 1.2329171728524799e-05
Epoch 43: reducing lr to 5.6983603969185135e-06
Epoch 46: reducing lr to 1.5016787709373498e-06
Epoch 49: reducing lr to 2.813929739979397e-09
[I 2024-06-21 23:12:39,808] Trial 174 finished with value: 1.0924731492996216 and parameters: {'hidden_size': 188, 'n_layers': 2, 'rnn_dropout': 0.3159058100294404, 'bidirectional': False, 'fc_dropout': 0.6242908186314149, 'learning_rate_model': 0.0008887418605061119}. Best is trial 153 with value: 0.9688200950622559.
Epoch 7: reducing lr to 0.0003941030059492007
Epoch 10: reducing lr to 0.0005319055970439603
Epoch 13: reducing lr to 0.0005515143924419481
Epoch 16: reducing lr to 0.0005348921052984201
Epoch 19: reducing lr to 0.000502046363352267
Epoch 22: reducing lr to 0.00045504099474552454
Epoch 27: reducing lr to 0.00035358022051713974
Epoch 30: reducing lr to 0.00028506465805166277
Epoch 33: reducing lr to 0.00021602322434342397
Epoch 36: reducing lr to 0.0001507940710719349
Epoch 39: reducing lr to 9.347575823851354e-05
Epoch 42: reducing lr to 4.766977567343114e-05
Epoch 45: reducing lr to 1.6254348425869934e-05
Epoch 48: reducing lr to 1.2033836841751112e-06
[I 2024-06-21 23:12:45,365] Trial 175 finished with value: 1.0928702354431152 and parameters: {'hidden_size': 93, 'n_layers': 2, 'rnn_dropout': 0.7286449112694068, 'bidirectional': False, 'fc_dropout': 0.1507747197331348, 'learning_rate_model': 0.005533852722614425}. Best is trial 153 with value: 0.9688200950622559.
Epoch 14: reducing lr to 1.7085577683815538e-05
Epoch 17: reducing lr to 1.6393298327055785e-05
Epoch 20: reducing lr to 1.5213158885200896e-05
Epoch 24: reducing lr to 1.30127675440177e-05
Epoch 27: reducing lr to 1.1026708766123079e-05
Epoch 30: reducing lr to 8.889990959485177e-06
Epoch 33: reducing lr to 6.736873397697135e-06
Epoch 36: reducing lr to 4.702645139302098e-06
Epoch 39: reducing lr to 2.915123366576029e-06
Epoch 42: reducing lr to 1.4866236932839493e-06
Epoch 45: reducing lr to 5.069060877133265e-07
Epoch 48: reducing lr to 3.7528573854883855e-08
[I 2024-06-21 23:12:59,651] Trial 176 finished with value: 1.106658697128296 and parameters: {'hidden_size': 81, 'n_layers': 7, 'rnn_dropout': 0.09821770355953596, 'bidirectional': False, 'fc_dropout': 0.46672505168658235, 'learning_rate_model': 0.0001725780425093903}. Best is trial 153 with value: 0.9688200950622559.
Epoch 14: reducing lr to 0.000626328075025841
Epoch 17: reducing lr to 0.0006009502970587438
Epoch 22: reducing lr to 0.0005202123735792803
Epoch 25: reducing lr to 0.0004536637560658978
Epoch 28: reducing lr to 0.0003784855943029065
Epoch 31: reducing lr to 0.0002994015875775564
Epoch 34: reducing lr to 0.00022138096125424322
Epoch 37: reducing lr to 0.00014932589150442894
Epoch 40: reducing lr to 8.77639206282115e-05
Epoch 43: reducing lr to 4.056318304246206e-05
Epoch 46: reducing lr to 1.0689543414883156e-05
Epoch 49: reducing lr to 2.003066481596691e-08
[I 2024-06-21 23:13:01,696] Trial 177 finished with value: 1.0933747291564941 and parameters: {'hidden_size': 16, 'n_layers': 2, 'rnn_dropout': 0.11632290733698857, 'bidirectional': False, 'fc_dropout': 0.6984755026157125, 'learning_rate_model': 0.006326416066049896}. Best is trial 153 with value: 0.9688200950622559.
Epoch 48: reducing lr to 7.679509097920307e-08
[I 2024-06-21 23:13:43,833] Trial 178 finished with value: 1.0480525493621826 and parameters: {'hidden_size': 103, 'n_layers': 6, 'rnn_dropout': 0.3966611176681699, 'bidirectional': True, 'fc_dropout': 0.7397926508306973, 'learning_rate_model': 0.00035314815123986564}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:13:57,016] Trial 179 finished with value: 1.063969612121582 and parameters: {'hidden_size': 107, 'n_layers': 2, 'rnn_dropout': 0.7079732150704507, 'bidirectional': True, 'fc_dropout': 0.5740759697558036, 'learning_rate_model': 5.800691551121672e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.0003219645564088714
Epoch 23: reducing lr to 0.00028222202751719376
Epoch 41: reducing lr to 3.972080901851922e-05
[I 2024-06-21 23:14:10,793] Trial 180 finished with value: 1.0040438175201416 and parameters: {'hidden_size': 37, 'n_layers': 6, 'rnn_dropout': 0.5953087370956281, 'bidirectional': True, 'fc_dropout': 0.49859771615006726, 'learning_rate_model': 0.003576195751981789}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.00020889859111599486
Epoch 15: reducing lr to 0.0002273881838861553
Epoch 18: reducing lr to 0.0002158076432652259
Epoch 22: reducing lr to 0.0001907970503917175
Epoch 25: reducing lr to 0.00016638917281310987
Epoch 28: reducing lr to 0.00013881625789076971
Epoch 31: reducing lr to 0.00010981080553573072
Epoch 34: reducing lr to 8.119536667221452e-05
Epoch 37: reducing lr to 5.4767900752012115e-05
Epoch 40: reducing lr to 3.2188963656251114e-05
Epoch 43: reducing lr to 1.4877261810885461e-05
Epoch 46: reducing lr to 3.920578319890899e-06
Epoch 49: reducing lr to 7.346599116770172e-09
[I 2024-06-21 23:14:13,387] Trial 181 finished with value: 1.0924965143203735 and parameters: {'hidden_size': 76, 'n_layers': 1, 'rnn_dropout': 0.7044308780514614, 'bidirectional': False, 'fc_dropout': 0.20311089675419175, 'learning_rate_model': 0.00232032451794255}. Best is trial 153 with value: 0.9688200950622559.
Epoch 11: reducing lr to 0.0009212093071593503
Epoch 14: reducing lr to 0.0009171984870815815
Epoch 18: reducing lr to 0.0008616627009047125
Epoch 24: reducing lr to 0.0006985593888009506
Epoch 27: reducing lr to 0.0005919425602657598
Epoch 30: reducing lr to 0.0004772379611098887
Epoch 33: reducing lr to 0.0003616529802138985
Epoch 36: reducing lr to 0.0002524502880072478
Epoch 39: reducing lr to 0.00015649144506319092
Epoch 42: reducing lr to 7.980584722232058e-05
Epoch 45: reducing lr to 2.7212044295319392e-05
Epoch 48: reducing lr to 2.014631977860157e-06
[I 2024-06-21 23:14:31,819] Trial 182 finished with value: 1.1396574974060059 and parameters: {'hidden_size': 95, 'n_layers': 3, 'rnn_dropout': 0.10137070707753822, 'bidirectional': True, 'fc_dropout': 0.24228664377406287, 'learning_rate_model': 0.009264440595593887}. Best is trial 153 with value: 0.9688200950622559.
Epoch 29: reducing lr to 2.788427576169771e-06
Epoch 34: reducing lr to 1.752138991015177e-06
Epoch 37: reducing lr to 1.1818528359018817e-06
Epoch 42: reducing lr to 4.3132193755201684e-07
Epoch 45: reducing lr to 1.470713247052116e-07
Epoch 48: reducing lr to 1.088836217381635e-08
[I 2024-06-21 23:15:18,210] Trial 183 finished with value: 1.0741013288497925 and parameters: {'hidden_size': 129, 'n_layers': 5, 'rnn_dropout': 0.006706463620025272, 'bidirectional': True, 'fc_dropout': 0.18472676038734337, 'learning_rate_model': 5.007097358286679e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 11: reducing lr to 0.00015253077572545184
Epoch 16: reducing lr to 0.00014827125178470276
Epoch 40: reducing lr to 2.1280245223909947e-05
Epoch 47: reducing lr to 1.197946557441481e-06
[I 2024-06-21 23:15:34,662] Trial 184 finished with value: 1.0463180541992188 and parameters: {'hidden_size': 50, 'n_layers': 6, 'rnn_dropout': 0.7860718314440683, 'bidirectional': True, 'fc_dropout': 0.624179663753091, 'learning_rate_model': 0.001533975286317709}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:16:32,696] Trial 185 finished with value: 1.0233174562454224 and parameters: {'hidden_size': 167, 'n_layers': 4, 'rnn_dropout': 0.471961038639523, 'bidirectional': True, 'fc_dropout': 0.4296056934724326, 'learning_rate_model': 0.00029102706982436836}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 3.616954079156924e-05
Epoch 13: reducing lr to 3.750293741112472e-05
Epoch 16: reducing lr to 3.637262312936444e-05
Epoch 19: reducing lr to 3.413911513517698e-05
Epoch 22: reducing lr to 3.094275358776539e-05
Epoch 25: reducing lr to 2.6984375091008578e-05
Epoch 28: reducing lr to 2.251270264960155e-05
Epoch 31: reducing lr to 1.7808706633515332e-05
Epoch 34: reducing lr to 1.316796154997402e-05
Epoch 37: reducing lr to 8.882053752977029e-06
Epoch 40: reducing lr to 5.220285998216621e-06
Epoch 43: reducing lr to 2.412738799314714e-06
Epoch 46: reducing lr to 6.35824760523583e-07
Epoch 49: reducing lr to 1.1914440276332342e-09
[I 2024-06-21 23:16:59,824] Trial 186 finished with value: 1.10734224319458 and parameters: {'hidden_size': 127, 'n_layers': 7, 'rnn_dropout': 0.31425835599757, 'bidirectional': False, 'fc_dropout': 0.5160627192683411, 'learning_rate_model': 0.00037630157098835073}. Best is trial 153 with value: 0.9688200950622559.
Epoch 7: reducing lr to 0.003594854831272372
Epoch 10: reducing lr to 0.004851836642831306
Epoch 16: reducing lr to 0.004879078413295152
Epoch 19: reducing lr to 0.00457947228916152
Epoch 22: reducing lr to 0.004150707540146974
Epoch 25: reducing lr to 0.0036197246905876348
Epoch 28: reducing lr to 0.0030198878187019224
Epoch 31: reducing lr to 0.002388886712823924
Epoch 34: reducing lr to 0.0017663701822404538
Epoch 37: reducing lr to 0.0011914520593619513
Epoch 40: reducing lr to 0.0007002570211814878
Epoch 43: reducing lr to 0.00032364841410495654
Epoch 46: reducing lr to 8.52904903964613e-05
Epoch 49: reducing lr to 1.598220951838647e-07
[I 2024-06-21 23:17:32,358] Trial 187 finished with value: 1.2004380226135254 and parameters: {'hidden_size': 143, 'n_layers': 3, 'rnn_dropout': 0.10889052339849768, 'bidirectional': True, 'fc_dropout': 0.17437514404893434, 'learning_rate_model': 0.05047765912753394}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 0.0016836587812636855
Epoch 9: reducing lr to 0.00254011605744289
Epoch 12: reducing lr to 0.0028206651419755533
Epoch 15: reducing lr to 0.0027649414678976017
Epoch 18: reducing lr to 0.0026241271281361687
Epoch 21: reducing lr to 0.0024070700301588657
Epoch 24: reducing lr to 0.0021274086029743453
Epoch 27: reducing lr to 0.0018027152957425456
Epoch 30: reducing lr to 0.0014533913084667021
Epoch 33: reducing lr to 0.0011013861866762318
Epoch 36: reducing lr to 0.0007688178315831108
Epoch 39: reducing lr to 0.000476582595347785
Epoch 42: reducing lr to 0.00024304253678393908
Epoch 45: reducing lr to 8.287217674899337e-05
Epoch 48: reducing lr to 6.1354059085565875e-06
[I 2024-06-21 23:17:55,285] Trial 188 finished with value: 1.1023972034454346 and parameters: {'hidden_size': 143, 'n_layers': 5, 'rnn_dropout': 0.597325977698404, 'bidirectional': False, 'fc_dropout': 0.7444478553998761, 'learning_rate_model': 0.02821413746745477}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:18:01,268] Trial 189 finished with value: 1.101931095123291 and parameters: {'hidden_size': 46, 'n_layers': 2, 'rnn_dropout': 0.05728822355344363, 'bidirectional': True, 'fc_dropout': 0.528478684742879, 'learning_rate_model': 2.283088758720842e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 13: reducing lr to 0.00016677467315124418
Epoch 16: reducing lr to 0.00016174819234969409
Epoch 22: reducing lr to 0.00013760169128694116
Epoch 25: reducing lr to 0.00011999887599893912
Epoch 28: reducing lr to 0.00010011345471367566
Epoch 31: reducing lr to 7.919489600219669e-05
Epoch 34: reducing lr to 5.8557612687523276e-05
Epoch 37: reducing lr to 3.9498282369879615e-05
Epoch 40: reducing lr to 2.321448801635305e-05
Epoch 43: reducing lr to 1.072939221383963e-05
Epoch 46: reducing lr to 2.827497629194595e-06
Epoch 49: reducing lr to 5.298323331524683e-09
[I 2024-06-21 23:18:07,536] Trial 190 finished with value: 1.0926918983459473 and parameters: {'hidden_size': 54, 'n_layers': 4, 'rnn_dropout': 0.08396293466394243, 'bidirectional': False, 'fc_dropout': 0.27136334790808, 'learning_rate_model': 0.0016734041608502312}. Best is trial 153 with value: 0.9688200950622559.
Epoch 19: reducing lr to 2.454911751894959e-05
Epoch 23: reducing lr to 2.135449377764995e-05
Epoch 26: reducing lr to 1.8363760571080796e-05
Epoch 29: reducing lr to 1.5069296886058761e-05
Epoch 32: reducing lr to 1.1678106059607061e-05
Epoch 35: reducing lr to 8.403270325038068e-06
Epoch 38: reducing lr to 5.450558311649305e-06
Epoch 41: reducing lr to 3.0054981054854575e-06
Epoch 44: reducing lr to 1.221725392062939e-06
Epoch 47: reducing lr to 2.1131909255310122e-07
[I 2024-06-21 23:18:17,619] Trial 191 finished with value: 1.0923364162445068 and parameters: {'hidden_size': 115, 'n_layers': 3, 'rnn_dropout': 0.5570160664808824, 'bidirectional': False, 'fc_dropout': 0.4491930070394994, 'learning_rate_model': 0.0002705949305417018}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:19:26,105] Trial 192 finished with value: 1.1059753894805908 and parameters: {'hidden_size': 129, 'n_layers': 7, 'rnn_dropout': 0.4775108199567785, 'bidirectional': True, 'fc_dropout': 0.5916895700060374, 'learning_rate_model': 1.413889647537952e-05}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:19:46,893] Trial 193 finished with value: 1.106532096862793 and parameters: {'hidden_size': 157, 'n_layers': 4, 'rnn_dropout': 0.00425322636888259, 'bidirectional': False, 'fc_dropout': 0.6143703026250493, 'learning_rate_model': 1.6306861470410482e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 0.002836203043909326
Epoch 12: reducing lr to 0.004751544167111782
Epoch 16: reducing lr to 0.004593974797587733
Epoch 19: reducing lr to 0.004311875829937961
Epoch 22: reducing lr to 0.003908165480520444
Epoch 25: reducing lr to 0.003408210033569655
Epoch 28: reducing lr to 0.002843423973850281
Epoch 33: reducing lr to 0.001855337251189491
Epoch 36: reducing lr to 0.0012951100890592425
Epoch 40: reducing lr to 0.0006593382673202406
Epoch 48: reducing lr to 1.0335382149349176e-05
[I 2024-06-21 23:20:36,668] Trial 194 finished with value: 1.0287219285964966 and parameters: {'hidden_size': 117, 'n_layers': 6, 'rnn_dropout': 0.5196664439027832, 'bidirectional': True, 'fc_dropout': 0.2214451700804693, 'learning_rate_model': 0.04752805227340119}. Best is trial 153 with value: 0.9688200950622559.
Epoch 34: reducing lr to 1.2664261019259893e-06
Epoch 37: reducing lr to 8.542297658442047e-07
Epoch 40: reducing lr to 5.020599750819696e-07
Epoch 43: reducing lr to 2.3204467760522484e-07
[I 2024-06-21 23:21:02,711] Trial 195 finished with value: 1.1068612337112427 and parameters: {'hidden_size': 157, 'n_layers': 5, 'rnn_dropout': 0.6663745150696564, 'bidirectional': False, 'fc_dropout': 0.7988164056718448, 'learning_rate_model': 3.619072928538003e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0005119294104797645
Epoch 22: reducing lr to 0.0004526003808320566
Epoch 25: reducing lr to 0.0003947010859283887
Epoch 28: reducing lr to 0.0003292938284845242
Epoch 31: reducing lr to 0.00026048836867712986
Epoch 34: reducing lr to 0.00019260808174023032
Epoch 37: reducing lr to 0.00012991800809730448
Epoch 40: reducing lr to 7.635724545792506e-05
Epoch 43: reducing lr to 3.529118687904658e-05
Epoch 46: reducing lr to 9.300223651369233e-06
Epoch 49: reducing lr to 1.7427279673607604e-08
[I 2024-06-21 23:21:05,404] Trial 196 finished with value: 0.9836214780807495 and parameters: {'hidden_size': 29, 'n_layers': 1, 'rnn_dropout': 0.011795032719957544, 'bidirectional': True, 'fc_dropout': 0.6733894609486798, 'learning_rate_model': 0.005504171884830901}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:21:14,389] Trial 197 finished with value: 1.1080824136734009 and parameters: {'hidden_size': 106, 'n_layers': 3, 'rnn_dropout': 0.2829182296536041, 'bidirectional': False, 'fc_dropout': 0.5805140363958952, 'learning_rate_model': 1.269705997902816e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 17: reducing lr to 0.0014585292860055403
Epoch 20: reducing lr to 0.0013535310176171235
Epoch 23: reducing lr to 0.0012117247391490377
Epoch 26: reducing lr to 0.0010420206266410059
Epoch 29: reducing lr to 0.0008550818403164434
Epoch 32: reducing lr to 0.0006626544354632524
Epoch 35: reducing lr to 0.0004768294040883686
Epoch 38: reducing lr to 0.00030928274007189816
Epoch 41: reducing lr to 0.00017054192179886355
Epoch 44: reducing lr to 6.932474716673624e-05
Epoch 47: reducing lr to 1.1990945557750591e-05
[I 2024-06-21 23:21:26,945] Trial 198 finished with value: 1.1676826477050781 and parameters: {'hidden_size': 101, 'n_layers': 2, 'rnn_dropout': 0.023172948942318784, 'bidirectional': True, 'fc_dropout': 0.2637880633954088, 'learning_rate_model': 0.015354453027065824}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.003209022466312228
Epoch 12: reducing lr to 0.003563450490390371
Epoch 15: reducing lr to 0.0034930527140771017
Epoch 18: reducing lr to 0.0033151567559183
Epoch 22: reducing lr to 0.00293095332975775
Epoch 25: reducing lr to 0.0025560085918046884
Epoch 29: reducing lr to 0.001984994504839836
Epoch 32: reducing lr to 0.0015382918347505985
Epoch 35: reducing lr to 0.001106915972523974
Epoch 38: reducing lr to 0.000717971673047492
Epoch 41: reducing lr to 0.00039589751723681786
Epoch 44: reducing lr to 0.00016093107780707495
Epoch 47: reducing lr to 2.783588648211724e-05
[I 2024-06-21 23:21:40,337] Trial 199 finished with value: 1.017484188079834 and parameters: {'hidden_size': 76, 'n_layers': 3, 'rnn_dropout': 0.742487262800581, 'bidirectional': True, 'fc_dropout': 0.76154813405434, 'learning_rate_model': 0.03564396230455292}. Best is trial 153 with value: 0.9688200950622559.
Epoch 8: reducing lr to 0.0010164787407363769
Epoch 14: reducing lr to 0.0012339828633901476
Epoch 17: reducing lr to 0.0011839839181552144
Epoch 20: reducing lr to 0.0010987499345809153
Epoch 23: reducing lr to 0.0009836364741858418
Epoch 26: reducing lr to 0.0008458765114739586
Epoch 29: reducing lr to 0.0006941260332275484
Epoch 33: reducing lr to 0.0004865616181922816
Epoch 36: reducing lr to 0.00033964221882884835
Epoch 39: reducing lr to 0.00021054086350445755
Epoch 42: reducing lr to 0.00010736939632775087
Epoch 45: reducing lr to 3.661061025632822e-05
Epoch 48: reducing lr to 2.7104507603665715e-06
[I 2024-06-21 23:21:50,309] Trial 200 finished with value: 1.093712568283081 and parameters: {'hidden_size': 111, 'n_layers': 3, 'rnn_dropout': 0.5682836626556875, 'bidirectional': False, 'fc_dropout': 0.7923249575119574, 'learning_rate_model': 0.012464216955083157}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.004866269249132282
Epoch 12: reducing lr to 0.005403735786904527
Epoch 15: reducing lr to 0.005296982238845314
Epoch 18: reducing lr to 0.005027214844001257
Epoch 21: reducing lr to 0.0046113841270945
Epoch 24: reducing lr to 0.004075618133533366
Epoch 27: reducing lr to 0.003453581572742603
Epoch 30: reducing lr to 0.002784358380249584
Epoch 33: reducing lr to 0.002109998760071268
Epoch 36: reducing lr to 0.0014728754464013588
Epoch 39: reducing lr to 0.0009130209706824481
Epoch 42: reducing lr to 0.00046561275006206103
Epoch 45: reducing lr to 0.00015876374000338352
Epoch 48: reducing lr to 1.1754005103928322e-05
[I 2024-06-21 23:21:54,507] Trial 201 finished with value: 1.1246203184127808 and parameters: {'hidden_size': 77, 'n_layers': 1, 'rnn_dropout': 0.1479244722227267, 'bidirectional': True, 'fc_dropout': 0.6405277265033917, 'learning_rate_model': 0.05405169938844537}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:22:11,088] Trial 202 finished with value: 1.1070237159729004 and parameters: {'hidden_size': 48, 'n_layers': 7, 'rnn_dropout': 0.7659159410613351, 'bidirectional': True, 'fc_dropout': 0.04606680448653897, 'learning_rate_model': 1.0548472419055645e-05}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:22:13,553] Trial 203 finished with value: 1.0950431823730469 and parameters: {'hidden_size': 73, 'n_layers': 1, 'rnn_dropout': 0.23508998837643535, 'bidirectional': False, 'fc_dropout': 0.38156073798942236, 'learning_rate_model': 0.00012970411945941392}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 0.00217678046546955
Epoch 10: reducing lr to 0.003506180687153268
Epoch 13: reducing lr to 0.003635436668110886
Epoch 16: reducing lr to 0.003525866957841157
Epoch 19: reducing lr to 0.003309356534362187
Epoch 22: reducing lr to 0.0029995096056639357
Epoch 25: reducing lr to 0.002615794746861899
Epoch 28: reducing lr to 0.002182322515525433
Epoch 31: reducing lr to 0.0017263294444745624
Epoch 34: reducing lr to 0.0012764677533992172
Epoch 37: reducing lr to 0.0008610030608462736
Epoch 40: reducing lr to 0.0005060408716228447
Epoch 43: reducing lr to 0.00023388458897090624
Epoch 46: reducing lr to 6.163518936024918e-05
Epoch 49: reducing lr to 1.1549546795677768e-07
[I 2024-06-21 23:22:29,226] Trial 204 finished with value: 1.1138315200805664 and parameters: {'hidden_size': 109, 'n_layers': 5, 'rnn_dropout': 0.08899653805394801, 'bidirectional': False, 'fc_dropout': 0.6753998876555427, 'learning_rate_model': 0.03647769011909392}. Best is trial 153 with value: 0.9688200950622559.
Epoch 7: reducing lr to 0.0030350560767036473
Epoch 10: reducing lr to 0.004096297897177274
Epoch 13: reducing lr to 0.004247308655103776
Epoch 16: reducing lr to 0.004119297518821855
Epoch 19: reducing lr to 0.0038663467237691485
Epoch 22: reducing lr to 0.0035043501709035307
Epoch 28: reducing lr to 0.002549624200505065
Epoch 34: reducing lr to 0.0014913071061118538
Epoch 37: reducing lr to 0.00100591650639413
Epoch 40: reducing lr to 0.0005912114472336083
Epoch 43: reducing lr to 0.00027324916639180977
Epoch 46: reducing lr to 7.200886636949122e-05
Epoch 49: reducing lr to 1.3493424462010517e-07
[I 2024-06-21 23:22:38,779] Trial 205 finished with value: 1.1402387619018555 and parameters: {'hidden_size': 86, 'n_layers': 4, 'rnn_dropout': 0.08126518075693295, 'bidirectional': False, 'fc_dropout': 0.22769917517498836, 'learning_rate_model': 0.042617166273324125}. Best is trial 153 with value: 0.9688200950622559.
Epoch 12: reducing lr to 2.400758154178815e-05
Epoch 19: reducing lr to 2.1786119826436155e-05
Epoch 23: reducing lr to 1.8951050273544552e-05
[I 2024-06-21 23:24:37,298] Trial 206 finished with value: 1.055902123451233 and parameters: {'hidden_size': 180, 'n_layers': 7, 'rnn_dropout': 0.1249196515526358, 'bidirectional': True, 'fc_dropout': 0.16850254273504195, 'learning_rate_model': 0.0002401395315598264}. Best is trial 153 with value: 0.9688200950622559.
Epoch 13: reducing lr to 0.00019667118309843077
Epoch 16: reducing lr to 0.00019074364082007837
Epoch 22: reducing lr to 0.00016226856818484412
Epoch 25: reducing lr to 0.00014151022134992075
Epoch 28: reducing lr to 0.00011806008196912408
Epoch 31: reducing lr to 9.339160196095407e-05
Epoch 34: reducing lr to 6.905481959020687e-05
Epoch 37: reducing lr to 4.6578858631582095e-05
Epoch 40: reducing lr to 2.7375984236287708e-05
Epoch 43: reducing lr to 1.265277407385037e-05
Epoch 46: reducing lr to 3.334363026677358e-06
Epoch 49: reducing lr to 6.248116085974531e-09
[I 2024-06-21 23:24:42,844] Trial 207 finished with value: 1.0925538539886475 and parameters: {'hidden_size': 45, 'n_layers': 4, 'rnn_dropout': 0.06701996307636841, 'bidirectional': False, 'fc_dropout': 0.5528218732453634, 'learning_rate_model': 0.0019733834274571717}. Best is trial 153 with value: 0.9688200950622559.
Epoch 5: reducing lr to 0.00411685693231406
Epoch 8: reducing lr to 0.007045948680949027
Epoch 11: reducing lr to 0.008591031109546603
Epoch 14: reducing lr to 0.00855362692811344
Epoch 17: reducing lr to 0.008207048108401277
Epoch 20: reducing lr to 0.007616229776379596
Epoch 23: reducing lr to 0.00681829519897509
Epoch 26: reducing lr to 0.005863381349174154
Epoch 29: reducing lr to 0.0048114891263627965
Epoch 32: reducing lr to 0.003728712809042454
Epoch 35: reducing lr to 0.0026830876118854215
Epoch 38: reducing lr to 0.0017403135824717295
Epoch 41: reducing lr to 0.0009596281474304011
Epoch 44: reducing lr to 0.00039008578062792317
Epoch 47: reducing lr to 6.747225990037936e-05
[I 2024-06-21 23:24:54,578] Trial 208 finished with value: 1.1274638175964355 and parameters: {'hidden_size': 177, 'n_layers': 2, 'rnn_dropout': 0.5036863653751306, 'bidirectional': False, 'fc_dropout': 0.6905811853710797, 'learning_rate_model': 0.08639849462086026}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:24:57,860] Trial 209 finished with value: 1.11143159866333 and parameters: {'hidden_size': 17, 'n_layers': 2, 'rnn_dropout': 0.17907973626202028, 'bidirectional': True, 'fc_dropout': 0.39215624979538544, 'learning_rate_model': 1.5202975046828169e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 0.0006207248031327303
Epoch 13: reducing lr to 0.0006436079345205732
Epoch 18: reducing lr to 0.0006006344373249404
Epoch 22: reducing lr to 0.0005310251169577248
Epoch 25: reducing lr to 0.00046309326990212385
Epoch 28: reducing lr to 0.00038635251137655754
Epoch 31: reducing lr to 0.00030562472393108186
Epoch 34: reducing lr to 0.00022598241951338766
Epoch 37: reducing lr to 0.0001524296672441054
Epoch 40: reducing lr to 8.958811551444557e-05
Epoch 43: reducing lr to 4.140629887577699e-05
Epoch 46: reducing lr to 1.0911728229486082e-05
Epoch 49: reducing lr to 2.0447007158686663e-08
[I 2024-06-21 23:25:06,599] Trial 210 finished with value: 1.0981199741363525 and parameters: {'hidden_size': 67, 'n_layers': 5, 'rnn_dropout': 0.2357536332487234, 'bidirectional': False, 'fc_dropout': 0.30442726176771207, 'learning_rate_model': 0.006457912195134263}. Best is trial 153 with value: 0.9688200950622559.
Epoch 12: reducing lr to 0.001656639978109472
Epoch 16: reducing lr to 0.0016017029497038802
Epoch 19: reducing lr to 0.001503348307264327
Epoch 23: reducing lr to 0.001307714709025121
Epoch 26: reducing lr to 0.0011245670378265786
Epoch 29: reducing lr to 0.000922819402686593
Epoch 32: reducing lr to 0.0007151483536307075
Epoch 35: reducing lr to 0.0005146027024751102
Epoch 38: reducing lr to 0.0003337833877384162
Epoch 41: reducing lr to 0.00018405184976119822
Epoch 44: reducing lr to 7.481648978550567e-05
Epoch 47: reducing lr to 1.2940839923761965e-05
[I 2024-06-21 23:25:42,179] Trial 211 finished with value: 1.0867081880569458 and parameters: {'hidden_size': 107, 'n_layers': 5, 'rnn_dropout': 0.28271547760130356, 'bidirectional': True, 'fc_dropout': 0.7496643078391715, 'learning_rate_model': 0.0165707965050135}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00014936346763035898
Epoch 22: reducing lr to 0.00013205328888711934
Epoch 27: reducing lr to 0.00010260928475429509
Epoch 31: reducing lr to 7.600158386393281e-05
Epoch 37: reducing lr to 3.790562487679049e-05
Epoch 40: reducing lr to 2.2278428874812585e-05
Epoch 43: reducing lr to 1.029675955539534e-05
Epoch 46: reducing lr to 2.713486714905748e-06
Epoch 49: reducing lr to 5.084683298364756e-09
[I 2024-06-21 23:26:12,592] Trial 212 finished with value: 0.9694513082504272 and parameters: {'hidden_size': 189, 'n_layers': 2, 'rnn_dropout': 0.7465246102585805, 'bidirectional': True, 'fc_dropout': 0.2197073987390904, 'learning_rate_model': 0.001605928829877941}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0003534148410599202
Epoch 22: reducing lr to 0.00031245653869644863
Epoch 25: reducing lr to 0.00027248526592529756
Epoch 28: reducing lr to 0.00022733080708687068
Epoch 31: reducing lr to 0.0001798303702217646
Epoch 34: reducing lr to 0.00013296863434997022
Epoch 37: reducing lr to 8.96900065567638e-05
Epoch 40: reducing lr to 5.2713876590906696e-05
Epoch 43: reducing lr to 2.4363572294050115e-05
Epoch 46: reducing lr to 6.420488833587491e-06
Epoch 49: reducing lr to 1.2031071374048186e-08
[I 2024-06-21 23:26:23,783] Trial 213 finished with value: 1.0080052614212036 and parameters: {'hidden_size': 50, 'n_layers': 4, 'rnn_dropout': 0.14825411675014752, 'bidirectional': True, 'fc_dropout': 0.7711342139391983, 'learning_rate_model': 0.003799852073396136}. Best is trial 153 with value: 0.9688200950622559.
Epoch 26: reducing lr to 1.1723866569655842e-06
Epoch 29: reducing lr to 9.620601690315182e-07
Epoch 32: reducing lr to 7.455583876688751e-07
Epoch 35: reducing lr to 5.364849953154038e-07
Epoch 38: reducing lr to 3.479767563324542e-07
Epoch 41: reducing lr to 1.918782301025782e-07
Epoch 44: reducing lr to 7.799788842739967e-08
Epoch 47: reducing lr to 1.349111928966709e-08
[I 2024-06-21 23:26:43,621] Trial 214 finished with value: 1.1078904867172241 and parameters: {'hidden_size': 105, 'n_layers': 7, 'rnn_dropout': 0.11455182536701028, 'bidirectional': False, 'fc_dropout': 0.3470718617221808, 'learning_rate_model': 1.727543140097415e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.0005013724227798435
Epoch 12: reducing lr to 0.0005567476777051674
Epoch 15: reducing lr to 0.0005457488442476196
Epoch 18: reducing lr to 0.0005179546706383215
Epoch 21: reducing lr to 0.00047511157188479577
Epoch 27: reducing lr to 0.00035582300767729517
Epoch 30: reducing lr to 0.00028687284560796123
Epoch 33: reducing lr to 0.00021739347665319453
Epoch 36: reducing lr to 0.00015175056972995676
Epoch 39: reducing lr to 9.406868232814869e-05
Epoch 42: reducing lr to 4.7972148811417996e-05
Epoch 45: reducing lr to 1.635745103690661e-05
Epoch 48: reducing lr to 1.211016841571981e-06
[I 2024-06-21 23:26:55,698] Trial 215 finished with value: 1.0937811136245728 and parameters: {'hidden_size': 107, 'n_layers': 4, 'rnn_dropout': 0.6453974486198909, 'bidirectional': False, 'fc_dropout': 0.1921377657771018, 'learning_rate_model': 0.005568954385864473}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 0.003283974407439604
Epoch 9: reducing lr to 0.004954493284148655
Epoch 12: reducing lr to 0.005501703932700836
Epoch 18: reducing lr to 0.005118356775473931
Epoch 21: reducing lr to 0.004694987169563897
Epoch 24: reducing lr to 0.004149507895590886
Epoch 27: reducing lr to 0.003516193994293234
Epoch 30: reducing lr to 0.0028348379814925836
Epoch 33: reducing lr to 0.0021482524190783647
Epoch 36: reducing lr to 0.0014995782464943125
Epoch 39: reducing lr to 0.0009295737732431641
Epoch 42: reducing lr to 0.000474054172733622
Epoch 45: reducing lr to 0.00016164208007054816
Epoch 48: reducing lr to 1.1967101770960636e-05
[I 2024-06-21 23:27:29,093] Trial 216 finished with value: 1.0875859260559082 and parameters: {'hidden_size': 199, 'n_layers': 2, 'rnn_dropout': 0.6571573921810379, 'bidirectional': True, 'fc_dropout': 0.11105321443653891, 'learning_rate_model': 0.055031640853951194}. Best is trial 153 with value: 0.9688200950622559.
Epoch 16: reducing lr to 0.00031963231307090723
Epoch 22: reducing lr to 0.00027191615701909323
Epoch 27: reducing lr to 0.0002112868420014586
Epoch 30: reducing lr to 0.00017034440240426804
Epoch 33: reducing lr to 0.0001290877210375011
Epoch 36: reducing lr to 9.010912155304847e-05
Epoch 39: reducing lr to 5.585775622013326e-05
Epoch 42: reducing lr to 2.8485746024554513e-05
Epoch 45: reducing lr to 9.713014892830975e-06
Epoch 48: reducing lr to 7.190988737253764e-07
[I 2024-06-21 23:28:14,145] Trial 217 finished with value: 0.9883724451065063 and parameters: {'hidden_size': 180, 'n_layers': 3, 'rnn_dropout': 0.7582112528812855, 'bidirectional': True, 'fc_dropout': 0.3744396438931783, 'learning_rate_model': 0.003306831655210467}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:28:16,324] Trial 218 finished with value: 1.0933399200439453 and parameters: {'hidden_size': 45, 'n_layers': 1, 'rnn_dropout': 0.2124096139779087, 'bidirectional': False, 'fc_dropout': 0.4411750686502212, 'learning_rate_model': 0.00019346062318522451}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 0.0018017395956177514
Epoch 9: reducing lr to 0.002718263183187439
Epoch 12: reducing lr to 0.003018488145479039
Epoch 18: reducing lr to 0.0028081662408751215
Epoch 21: reducing lr to 0.0025758861777841558
Epoch 24: reducing lr to 0.002276611127321061
Epoch 27: reducing lr to 0.001929145955291057
Epoch 30: reducing lr to 0.0015553226684243656
Epoch 33: reducing lr to 0.0011786302098050973
Epoch 36: reducing lr to 0.0008227376855663056
Epoch 39: reducing lr to 0.0005100069813290129
Epoch 42: reducing lr to 0.0002600879506085774
Epoch 45: reducing lr to 8.868428917148258e-05
Epoch 48: reducing lr to 6.565703148197696e-06
[I 2024-06-21 23:28:38,699] Trial 219 finished with value: 1.1372100114822388 and parameters: {'hidden_size': 160, 'n_layers': 2, 'rnn_dropout': 0.1959045110095069, 'bidirectional': True, 'fc_dropout': 0.07035094701648266, 'learning_rate_model': 0.030192892524910118}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 0.0026584814415672444
Epoch 16: reducing lr to 0.002673408106775681
Epoch 19: reducing lr to 0.002509244022239571
Epoch 25: reducing lr to 0.0019833666345152223
Epoch 28: reducing lr to 0.0016546962135454534
Epoch 31: reducing lr to 0.001308949880130948
Epoch 34: reducing lr to 0.00096785252557137
Epoch 39: reducing lr to 0.00046719487423061806
Epoch 42: reducing lr to 0.0002382550863457386
Epoch 45: reducing lr to 8.123976110627727e-05
Epoch 48: reducing lr to 6.014550719608478e-06
[I 2024-06-21 23:28:43,087] Trial 220 finished with value: 1.103420615196228 and parameters: {'hidden_size': 32, 'n_layers': 5, 'rnn_dropout': 0.09632070148617872, 'bidirectional': False, 'fc_dropout': 0.7256872441762966, 'learning_rate_model': 0.02765837555610633}. Best is trial 153 with value: 0.9688200950622559.
Epoch 8: reducing lr to 0.001388509160839823
Epoch 11: reducing lr to 0.0016929906726284405
Epoch 16: reducing lr to 0.001645712775645173
Epoch 22: reducing lr to 0.0014000333358391963
Epoch 25: reducing lr to 0.0012209328612932011
Epoch 28: reducing lr to 0.0010186079302825796
Epoch 31: reducing lr to 0.0008057713055298477
Epoch 34: reducing lr to 0.0005957965273750473
Epoch 37: reducing lr to 0.0004018766884987342
Epoch 40: reducing lr to 0.00023619663968780185
Epoch 43: reducing lr to 0.00010916658532448276
Epoch 46: reducing lr to 2.876847588758093e-05
Epoch 49: reducing lr to 5.39079804819792e-08
[I 2024-06-21 23:28:45,775] Trial 221 finished with value: 1.1047585010528564 and parameters: {'hidden_size': 78, 'n_layers': 1, 'rnn_dropout': 0.6406874841040265, 'bidirectional': False, 'fc_dropout': 0.5357172799004265, 'learning_rate_model': 0.017026110563109636}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 0.00011589414798076417
Epoch 13: reducing lr to 0.00012016660656779255
Epoch 16: reducing lr to 0.00011654486275329462
Epoch 19: reducing lr to 0.0001093882746316418
Epoch 22: reducing lr to 9.914651899779396e-05
Epoch 25: reducing lr to 8.646311486195988e-05
Epoch 28: reducing lr to 7.21350184497785e-05
Epoch 31: reducing lr to 5.706251273203122e-05
Epoch 34: reducing lr to 4.219267513712584e-05
Epoch 37: reducing lr to 2.845980428539363e-05
Epoch 40: reducing lr to 1.6726797872983983e-05
Epoch 43: reducing lr to 7.730878007494311e-06
Epoch 46: reducing lr to 2.0373045184784263e-06
Epoch 49: reducing lr to 3.817615248273912e-09
[I 2024-06-21 23:29:11,582] Trial 222 finished with value: 1.0925559997558594 and parameters: {'hidden_size': 182, 'n_layers': 4, 'rnn_dropout': 0.4462602429719284, 'bidirectional': False, 'fc_dropout': 0.5463254941038999, 'learning_rate_model': 0.0012057424285486997}. Best is trial 153 with value: 0.9688200950622559.
Epoch 12: reducing lr to 0.0009282027597215904
Epoch 18: reducing lr to 0.0008635275438935436
Epoch 23: reducing lr to 0.0007327025895094168
Epoch 26: reducing lr to 0.0006300863445259606
Epoch 29: reducing lr to 0.000517048681437605
Epoch 32: reducing lr to 0.00040069217465577185
Epoch 35: reducing lr to 0.00028832797403735116
Epoch 38: reducing lr to 0.00018701628944242855
Epoch 41: reducing lr to 0.0001031228493442277
Epoch 44: reducing lr to 4.1919109287002695e-05
Epoch 47: reducing lr to 7.250654028076106e-06
[I 2024-06-21 23:30:09,184] Trial 223 finished with value: 1.057558536529541 and parameters: {'hidden_size': 169, 'n_layers': 4, 'rnn_dropout': 0.6598868499081856, 'bidirectional': True, 'fc_dropout': 0.7758964562757713, 'learning_rate_model': 0.00928449104813407}. Best is trial 153 with value: 0.9688200950622559.
Epoch 15: reducing lr to 7.7480618577978e-06
Epoch 18: reducing lr to 7.3534646384338255e-06
Epoch 21: reducing lr to 6.745216022205072e-06
Epoch 24: reducing lr to 5.96153431963605e-06
Epoch 27: reducing lr to 5.0516619558070085e-06
Epoch 30: reducing lr to 4.07276822758504e-06
Epoch 33: reducing lr to 3.086361285680395e-06
Epoch 36: reducing lr to 2.1544210557966594e-06
Epoch 39: reducing lr to 1.3355043757625084e-06
Epoch 42: reducing lr to 6.810663556324509e-07
Epoch 45: reducing lr to 2.3222869604895865e-07
Epoch 48: reducing lr to 1.7192951479852772e-08
[I 2024-06-21 23:30:16,850] Trial 224 finished with value: 1.107986569404602 and parameters: {'hidden_size': 64, 'n_layers': 5, 'rnn_dropout': 0.7642098028305495, 'bidirectional': False, 'fc_dropout': 0.4953078651359258, 'learning_rate_model': 7.906311395751618e-05}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:31:09,211] Trial 225 finished with value: 0.9959896802902222 and parameters: {'hidden_size': 157, 'n_layers': 4, 'rnn_dropout': 0.5329504721787962, 'bidirectional': True, 'fc_dropout': 0.5000968546443917, 'learning_rate_model': 0.0004354367145964692}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 0.00418868278154667
Epoch 9: reducing lr to 0.006319416090328879
Epoch 12: reducing lr to 0.007017378844325056
Epoch 15: reducing lr to 0.006878746957191935
Epoch 18: reducing lr to 0.0065284226474699395
Epoch 21: reducing lr to 0.005988418141196038
Epoch 32: reducing lr to 0.003029304492004959
Epoch 35: reducing lr to 0.002179811042410272
Epoch 41: reducing lr to 0.0007796271814272115
Epoch 44: reducing lr to 0.0003169159621673551
Epoch 47: reducing lr to 5.481624101117965e-05
[I 2024-06-21 23:31:12,307] Trial 226 finished with value: 1.0945454835891724 and parameters: {'hidden_size': 32, 'n_layers': 3, 'rnn_dropout': 0.5377155277254789, 'bidirectional': False, 'fc_dropout': 0.7598771111040699, 'learning_rate_model': 0.07019241257270516}. Best is trial 153 with value: 0.9688200950622559.
Epoch 8: reducing lr to 0.0001953537866322838
Epoch 13: reducing lr to 0.0002387359308928624
Epoch 16: reducing lr to 0.00023154058431775677
Epoch 19: reducing lr to 0.0002173225350939426
Epoch 22: reducing lr to 0.00019697515960369368
Epoch 25: reducing lr to 0.00017177694206436028
Epoch 28: reducing lr to 0.00014331120160130544
Epoch 31: reducing lr to 0.00011336653738725556
Epoch 34: reducing lr to 8.38245155074691e-05
Epoch 37: reducing lr to 5.654131428991496e-05
Epoch 40: reducing lr to 3.323125929174634e-05
Epoch 43: reducing lr to 1.5358995401913755e-05
Epoch 46: reducing lr to 4.047528715532024e-06
Epoch 49: reducing lr to 7.584485874371703e-09
[I 2024-06-21 23:31:27,019] Trial 227 finished with value: 1.092555284500122 and parameters: {'hidden_size': 151, 'n_layers': 3, 'rnn_dropout': 0.5949993570809757, 'bidirectional': False, 'fc_dropout': 0.4847170889742864, 'learning_rate_model': 0.002395457850714959}. Best is trial 153 with value: 0.9688200950622559.
Epoch 12: reducing lr to 0.0004246646096270094
Epoch 16: reducing lr to 0.0004105820014380219
Epoch 22: reducing lr to 0.00034928846492271774
Epoch 25: reducing lr to 0.0003046054361549755
Epoch 28: reducing lr to 0.0002541282348203841
Epoch 31: reducing lr to 0.00020102851495217597
Epoch 34: reducing lr to 0.00014864278522937866
Epoch 37: reducing lr to 0.00010026253519871236
Epoch 40: reducing lr to 5.8927712351223866e-05
Epoch 43: reducing lr to 2.723551506435204e-05
Epoch 46: reducing lr to 7.177326799090965e-06
Epoch 49: reducing lr to 1.3449276719049163e-08
[I 2024-06-21 23:31:54,169] Trial 228 finished with value: 0.9945976734161377 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.6129506921622574, 'bidirectional': True, 'fc_dropout': 0.3808712944016839, 'learning_rate_model': 0.004247773156507247}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:32:05,520] Trial 229 finished with value: 1.1010972261428833 and parameters: {'hidden_size': 172, 'n_layers': 2, 'rnn_dropout': 0.6849273903898975, 'bidirectional': False, 'fc_dropout': 0.022169392097891996, 'learning_rate_model': 2.195637079696368e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 1.5879705202143495e-05
Epoch 12: reducing lr to 1.7633576543595023e-05
Epoch 34: reducing lr to 6.172174161858149e-06
Epoch 38: reducing lr to 3.552850947966728e-06
Epoch 41: reducing lr to 1.959081287207639e-06
Epoch 44: reducing lr to 7.963602935994281e-07
Epoch 47: reducing lr to 1.3774464841448463e-07
[I 2024-06-21 23:32:42,061] Trial 230 finished with value: 1.0920851230621338 and parameters: {'hidden_size': 197, 'n_layers': 5, 'rnn_dropout': 0.3341058774513466, 'bidirectional': False, 'fc_dropout': 0.029580054687486257, 'learning_rate_model': 0.00017638256496317842}. Best is trial 153 with value: 0.9688200950622559.
Epoch 14: reducing lr to 0.00020339477979618043
Epoch 17: reducing lr to 0.00019515355963193798
Epoch 20: reducing lr to 0.0001811046227831572
Epoch 23: reducing lr to 0.00016213071510318508
Epoch 26: reducing lr to 0.00013942403244834295
Epoch 29: reducing lr to 0.0001144113227725401
Epoch 32: reducing lr to 8.86642271275277e-05
Epoch 35: reducing lr to 6.380053965174025e-05
Epoch 38: reducing lr to 4.1382527068106504e-05
Epoch 41: reducing lr to 2.28187828827685e-05
Epoch 44: reducing lr to 9.275762447817767e-06
Epoch 47: reducing lr to 1.6044077629435213e-06
[I 2024-06-21 23:32:51,855] Trial 231 finished with value: 1.0926802158355713 and parameters: {'hidden_size': 75, 'n_layers': 5, 'rnn_dropout': 0.5206650053405577, 'bidirectional': False, 'fc_dropout': 0.5283661820888798, 'learning_rate_model': 0.00205445046128604}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0004974885924452333
Epoch 23: reducing lr to 0.0004221187644953159
Epoch 26: reducing lr to 0.00036300031293017574
Epoch 29: reducing lr to 0.0002978779571920278
Epoch 32: reducing lr to 0.0002308435757295245
Epoch 35: reducing lr to 0.00016610920981127497
Epoch 38: reducing lr to 0.00010774233115893976
Epoch 41: reducing lr to 5.941031242371842e-05
Epoch 44: reducing lr to 2.415010247585076e-05
Epoch 47: reducing lr to 4.177188894833369e-06
[I 2024-06-21 23:33:29,718] Trial 232 finished with value: 1.0275318622589111 and parameters: {'hidden_size': 89, 'n_layers': 7, 'rnn_dropout': 0.12326333746588594, 'bidirectional': True, 'fc_dropout': 0.43879127173288324, 'learning_rate_model': 0.0053489068365791615}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.0036087404049765506
Epoch 12: reducing lr to 0.004007316215702068
Epoch 15: reducing lr to 0.003928149646296854
Epoch 18: reducing lr to 0.0037280948511593678
Epoch 21: reducing lr to 0.0034197220437978846
Epoch 24: reducing lr to 0.003022407327000982
Epoch 27: reducing lr to 0.0025611158621486104
Epoch 30: reducing lr to 0.0020648316141844035
Epoch 33: reducing lr to 0.0015647382810306597
Epoch 36: reducing lr to 0.0010922587433636617
Epoch 39: reducing lr to 0.0006770804283137797
Epoch 42: reducing lr to 0.0003452902949257944
Epoch 45: reducing lr to 0.00011773642066713955
Epoch 48: reducing lr to 8.71656518931993e-06
[I 2024-06-21 23:33:36,599] Trial 233 finished with value: 1.1084567308425903 and parameters: {'hidden_size': 59, 'n_layers': 2, 'rnn_dropout': 0.4067856107436928, 'bidirectional': True, 'fc_dropout': 0.09517866973841188, 'learning_rate_model': 0.040083797577684495}. Best is trial 153 with value: 0.9688200950622559.
Epoch 4: reducing lr to 0.00023921237259807217
Epoch 7: reducing lr to 0.0004745605336833244
Epoch 10: reducing lr to 0.0006404960129506494
Epoch 13: reducing lr to 0.0006641080135405552
Epoch 16: reducing lr to 0.000644092227467391
Epoch 19: reducing lr to 0.00060454091070022
Epoch 22: reducing lr to 0.0005479392292228845
Epoch 25: reducing lr to 0.0004778436297367755
Epoch 28: reducing lr to 0.00039865853898743176
Epoch 31: reducing lr to 0.0003153594252220385
Epoch 34: reducing lr to 0.000233180369085905
Epoch 37: reducing lr to 0.0001572848283692109
Epoch 40: reducing lr to 9.24416593394848e-05
Epoch 43: reducing lr to 4.2725164528840694e-05
Epoch 46: reducing lr to 1.1259286547137503e-05
Epoch 49: reducing lr to 2.1098281389499152e-08
[I 2024-06-21 23:33:55,330] Trial 234 finished with value: 1.1073919534683228 and parameters: {'hidden_size': 104, 'n_layers': 7, 'rnn_dropout': 0.4427469649986126, 'bidirectional': False, 'fc_dropout': 0.6833903783331062, 'learning_rate_model': 0.006663608401168412}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 2.1995718699195997e-05
Epoch 38: reducing lr to 1.0266129350484487e-05
Epoch 41: reducing lr to 5.660857209362343e-06
Epoch 44: reducing lr to 2.3011203969477945e-06
Epoch 47: reducing lr to 3.9801961823627804e-07
[I 2024-06-21 23:34:43,190] Trial 235 finished with value: 0.9760733246803284 and parameters: {'hidden_size': 181, 'n_layers': 3, 'rnn_dropout': 0.42240111731010094, 'bidirectional': True, 'fc_dropout': 0.18517034842739824, 'learning_rate_model': 0.000509665689217419}. Best is trial 153 with value: 0.9688200950622559.
Epoch 13: reducing lr to 0.00031642498408107
Epoch 29: reducing lr to 0.000176813266033468
Epoch 32: reducing lr to 0.00013702325258417545
Epoch 35: reducing lr to 9.859847362266036e-05
Epoch 38: reducing lr to 6.395328355898057e-05
Epoch 41: reducing lr to 3.526454751713835e-05
Epoch 44: reducing lr to 1.4334926068548977e-05
Epoch 47: reducing lr to 2.479479912835883e-06
[I 2024-06-21 23:34:53,118] Trial 236 finished with value: 1.0944006443023682 and parameters: {'hidden_size': 61, 'n_layers': 6, 'rnn_dropout': 0.7893419559744661, 'bidirectional': False, 'fc_dropout': 0.26409244175925944, 'learning_rate_model': 0.0031749837967185394}. Best is trial 153 with value: 0.9688200950622559.
Epoch 16: reducing lr to 0.00027506713090608826
Epoch 22: reducing lr to 0.00023400386662927536
Epoch 25: reducing lr to 0.00020406871973951968
Epoch 28: reducing lr to 0.0001702517991276917
Epoch 33: reducing lr to 0.00011108948503937355
Epoch 36: reducing lr to 7.754553128852923e-05
Epoch 39: reducing lr to 4.80697104579513e-05
Epoch 42: reducing lr to 2.4514081055864626e-05
Epoch 45: reducing lr to 8.358764210508437e-06
Epoch 48: reducing lr to 6.188375078009303e-07
[I 2024-06-21 23:35:02,061] Trial 237 finished with value: 1.0937907695770264 and parameters: {'hidden_size': 63, 'n_layers': 5, 'rnn_dropout': 0.29691774757018363, 'bidirectional': False, 'fc_dropout': 0.753999099380966, 'learning_rate_model': 0.00284577202801892}. Best is trial 153 with value: 0.9688200950622559.
Epoch 34: reducing lr to 3.1942075727915504e-06
Epoch 37: reducing lr to 2.154556971633685e-06
Epoch 40: reducing lr to 1.2663066340495283e-06
Epoch 43: reducing lr to 5.85268153669104e-07
Epoch 46: reducing lr to 1.54234674617258e-07
Epoch 49: reducing lr to 2.8901356684393404e-10
[I 2024-06-21 23:35:25,407] Trial 238 finished with value: 1.0778582096099854 and parameters: {'hidden_size': 55, 'n_layers': 7, 'rnn_dropout': 0.6003628373786594, 'bidirectional': True, 'fc_dropout': 0.6641252209298395, 'learning_rate_model': 9.128104780247618e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 3: reducing lr to 5.628193226430451e-05
Epoch 16: reducing lr to 0.00021623135423391184
Epoch 19: reducing lr to 0.00020295338809554108
Epoch 27: reducing lr to 0.00014293561104270074
Epoch 30: reducing lr to 0.00011523803855797926
Epoch 33: reducing lr to 8.732788142329062e-05
Epoch 36: reducing lr to 6.0958847355090216e-05
Epoch 39: reducing lr to 3.7787788587156075e-05
Epoch 42: reducing lr to 1.9270615602266307e-05
Epoch 45: reducing lr to 6.570857444895047e-06
Epoch 48: reducing lr to 4.864706005466585e-07
[I 2024-06-21 23:35:45,209] Trial 239 finished with value: 1.0929067134857178 and parameters: {'hidden_size': 132, 'n_layers': 5, 'rnn_dropout': 0.5939067437656814, 'bidirectional': False, 'fc_dropout': 0.722348721697368, 'learning_rate_model': 0.0022370725918161565}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:35:47,600] Trial 240 finished with value: 1.099199652671814 and parameters: {'hidden_size': 32, 'n_layers': 1, 'rnn_dropout': 0.08421002133022953, 'bidirectional': True, 'fc_dropout': 0.6226407120541021, 'learning_rate_model': 1.4376264309585954e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 4: reducing lr to 0.0022354514644863177
Epoch 7: reducing lr to 0.004434791681081907
Epoch 10: reducing lr to 0.005985466949712933
Epoch 13: reducing lr to 0.006206122264172125
Epoch 16: reducing lr to 0.006019073752407729
Epoch 19: reducing lr to 0.005649464739172903
Epoch 22: reducing lr to 0.005120519223618423
Epoch 25: reducing lr to 0.004465472376235865
Epoch 28: reducing lr to 0.0037254837830098613
Epoch 31: reducing lr to 0.0029470494410281634
Epoch 34: reducing lr to 0.002179082093041986
Epoch 37: reducing lr to 0.0014698345077250625
Epoch 40: reducing lr to 0.0008638718829866332
Epoch 43: reducing lr to 0.00039926877769358925
Epoch 46: reducing lr to 0.00010521859019039861
Epoch 49: reducing lr to 1.9716448408646503e-07
[I 2024-06-21 23:35:58,342] Trial 241 finished with value: 1.1072300672531128 and parameters: {'hidden_size': 60, 'n_layers': 7, 'rnn_dropout': 0.7788150886690349, 'bidirectional': False, 'fc_dropout': 0.03476123337198143, 'learning_rate_model': 0.06227175040056983}. Best is trial 153 with value: 0.9688200950622559.
Epoch 24: reducing lr to 8.108790327600094e-06
Epoch 28: reducing lr to 6.43374052191204e-06
Epoch 34: reducing lr to 3.76317535631587e-06
Epoch 37: reducing lr to 2.5383371351613628e-06
Epoch 42: reducing lr to 9.263763287943995e-07
Epoch 45: reducing lr to 3.158740188931671e-07
Epoch 48: reducing lr to 2.3385596926536636e-08
[I 2024-06-21 23:36:23,309] Trial 242 finished with value: 1.0752534866333008 and parameters: {'hidden_size': 73, 'n_layers': 6, 'rnn_dropout': 0.6485032827575212, 'bidirectional': True, 'fc_dropout': 0.7445155137323587, 'learning_rate_model': 0.00010754047185755203}. Best is trial 153 with value: 0.9688200950622559.
Epoch 7: reducing lr to 6.641686011352699e-05
Epoch 16: reducing lr to 9.014357565700565e-05
Epoch 19: reducing lr to 8.460819273621704e-05
Epoch 22: reducing lr to 7.668653534154799e-05
Epoch 25: reducing lr to 6.687634402726273e-05
Epoch 28: reducing lr to 5.5794037931228215e-05
Epoch 31: reducing lr to 4.413595599256298e-05
Epoch 34: reducing lr to 3.2634631107216643e-05
Epoch 37: reducing lr to 2.201271218804906e-05
Epoch 40: reducing lr to 1.2937621907492862e-05
Epoch 43: reducing lr to 5.97957704955934e-06
Epoch 46: reducing lr to 1.5757872947739843e-06
Epoch 49: reducing lr to 2.952798440296594e-09
[I 2024-06-21 23:36:56,767] Trial 243 finished with value: 1.0926655530929565 and parameters: {'hidden_size': 188, 'n_layers': 5, 'rnn_dropout': 0.7883559319558645, 'bidirectional': False, 'fc_dropout': 0.45244338830430486, 'learning_rate_model': 0.0009326016716911777}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:37:02,483] Trial 244 finished with value: 1.106616735458374 and parameters: {'hidden_size': 63, 'n_layers': 3, 'rnn_dropout': 0.1995697653611102, 'bidirectional': False, 'fc_dropout': 0.4019160047186067, 'learning_rate_model': 2.15159018655157e-05}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:37:56,416] Trial 245 finished with value: 1.096444845199585 and parameters: {'hidden_size': 159, 'n_layers': 4, 'rnn_dropout': 0.5915102298771057, 'bidirectional': True, 'fc_dropout': 0.7889974259490361, 'learning_rate_model': 1.2970911036527296e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.0014696690865700838
Epoch 12: reducing lr to 0.0016319901409939772
Epoch 17: reducing lr to 0.0015506492769073689
Epoch 20: reducing lr to 0.0014390193696334976
Epoch 23: reducing lr to 0.001288256676503298
Epoch 26: reducing lr to 0.001107834135883989
Epoch 29: reducing lr to 0.000909088387943606
Epoch 32: reducing lr to 0.0007045073630332644
Epoch 35: reducing lr to 0.0005069457142562935
Epoch 38: reducing lr to 0.0003288168855120245
Epoch 41: reducing lr to 0.00018131326553205482
Epoch 44: reducing lr to 7.370326403269585e-05
Epoch 47: reducing lr to 1.274828776971909e-05
[I 2024-06-21 23:38:43,455] Trial 246 finished with value: 1.107457160949707 and parameters: {'hidden_size': 188, 'n_layers': 7, 'rnn_dropout': 0.3127349692011665, 'bidirectional': False, 'fc_dropout': 0.14237786837613556, 'learning_rate_model': 0.016324232713169764}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:38:54,732] Trial 247 finished with value: 1.0541256666183472 and parameters: {'hidden_size': 62, 'n_layers': 3, 'rnn_dropout': 0.7327326320079657, 'bidirectional': True, 'fc_dropout': 0.6902711844628926, 'learning_rate_model': 0.00025711669394827373}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 0.0005177586437311348
Epoch 9: reducing lr to 0.0007811363320507244
Epoch 12: reducing lr to 0.0008674107690828048
Epoch 15: reducing lr to 0.000850274628295025
Epoch 18: reducing lr to 0.0008069714112868538
Epoch 21: reducing lr to 0.0007402220260126035
Epoch 28: reducing lr to 0.0005190768493866626
Epoch 31: reducing lr to 0.0004106165072606291
Epoch 34: reducing lr to 0.0003036145460005978
Epoch 37: reducing lr to 0.00020479409113769372
Epoch 40: reducing lr to 0.00012036447382738164
Epoch 43: reducing lr to 5.563067543840217e-05
Epoch 46: reducing lr to 1.4660252862197e-05
Epoch 49: reducing lr to 2.7471202445651655e-08
[I 2024-06-21 23:39:23,320] Trial 248 finished with value: 1.0936086177825928 and parameters: {'hidden_size': 200, 'n_layers': 4, 'rnn_dropout': 0.2358496094387963, 'bidirectional': False, 'fc_dropout': 0.37191010472945796, 'learning_rate_model': 0.008676409799751065}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 0.004280069848206587
Epoch 9: reducing lr to 0.006457290675160865
Epoch 12: reducing lr to 0.007170481311537295
Epoch 15: reducing lr to 0.007028824807317762
Epoch 18: reducing lr to 0.0066708572568170196
Epoch 21: reducing lr to 0.006119071140336463
Epoch 24: reducing lr to 0.005408137039247113
Epoch 27: reducing lr to 0.004582726303020475
Epoch 30: reducing lr to 0.003694701317297147
Epoch 33: reducing lr to 0.002799860554456306
Epoch 36: reducing lr to 0.001954430467943544
Epoch 39: reducing lr to 0.0012115321817150499
Epoch 42: reducing lr to 0.0006178443311059837
Epoch 45: reducing lr to 0.00021067137172081364
Epoch 48: reducing lr to 1.559696425900009e-05
[I 2024-06-21 23:39:47,991] Trial 249 finished with value: 1.1089105606079102 and parameters: {'hidden_size': 134, 'n_layers': 6, 'rnn_dropout': 0.04088872306089302, 'bidirectional': False, 'fc_dropout': 0.4415960217233577, 'learning_rate_model': 0.07172384357890647}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:40:07,271] Trial 250 finished with value: 1.102278232574463 and parameters: {'hidden_size': 67, 'n_layers': 5, 'rnn_dropout': 0.31717015204209914, 'bidirectional': True, 'fc_dropout': 0.6554867077248162, 'learning_rate_model': 1.901708352739404e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 0.00037412687538332303
Epoch 15: reducing lr to 0.0003814442804209297
Epoch 23: reducing lr to 0.00030717195928767075
Epoch 29: reducing lr to 0.0002167630615726855
Epoch 32: reducing lr to 0.00016798275606294842
Epoch 35: reducing lr to 0.00012087615080192036
Epoch 38: reducing lr to 7.840310771278173e-05
Epoch 41: reducing lr to 4.323234028912435e-05
Epoch 44: reducing lr to 1.75738084123655e-05
Epoch 47: reducing lr to 3.039702105341784e-06
[I 2024-06-21 23:40:09,229] Trial 251 finished with value: 1.0928254127502441 and parameters: {'hidden_size': 23, 'n_layers': 1, 'rnn_dropout': 0.12036402269413077, 'bidirectional': False, 'fc_dropout': 0.08863635175676494, 'learning_rate_model': 0.0038923505213127522}. Best is trial 153 with value: 0.9688200950622559.
Epoch 16: reducing lr to 0.0003986569943507328
Epoch 19: reducing lr to 0.00037417694569836417
Epoch 22: reducing lr to 0.00033914367678021924
Epoch 25: reducing lr to 0.00029575842880381865
Epoch 28: reducing lr to 0.0002467472951038369
Epoch 31: reducing lr to 0.00019518981170372446
Epoch 34: reducing lr to 0.00014432558120892357
Epoch 37: reducing lr to 9.735049463520341e-05
Epoch 40: reducing lr to 5.721620676898968e-05
Epoch 43: reducing lr to 2.64444825567632e-05
Epoch 46: reducing lr to 6.968867410595686e-06
Epoch 49: reducing lr to 1.3058653848107422e-08
[I 2024-06-21 23:40:31,226] Trial 252 finished with value: 1.0237370729446411 and parameters: {'hidden_size': 113, 'n_layers': 3, 'rnn_dropout': 0.1585593834561535, 'bidirectional': True, 'fc_dropout': 0.5705311785278516, 'learning_rate_model': 0.004124400176641758}. Best is trial 153 with value: 0.9688200950622559.
Epoch 19: reducing lr to 2.9719459277184942e-05
[I 2024-06-21 23:41:08,645] Trial 253 finished with value: 1.0522112846374512 and parameters: {'hidden_size': 98, 'n_layers': 6, 'rnn_dropout': 0.08798077795625635, 'bidirectional': True, 'fc_dropout': 0.5272285277327067, 'learning_rate_model': 0.0003275855033338442}. Best is trial 153 with value: 0.9688200950622559.
Epoch 8: reducing lr to 0.0025885034565835455
Epoch 11: reducing lr to 0.003156127688355928
Epoch 16: reducing lr to 0.003067990711508639
Epoch 19: reducing lr to 0.0028795967714874324
Epoch 22: reducing lr to 0.002609987194438115
Epoch 25: reducing lr to 0.002276102326759131
Epoch 28: reducing lr to 0.001898921680030624
Epoch 31: reducing lr to 0.0015021447955865924
Epoch 34: reducing lr to 0.0011107030576578904
Epoch 37: reducing lr to 0.0007491914541421778
Epoch 40: reducing lr to 0.0004403253759561066
Epoch 43: reducing lr to 0.0002035118610848284
Epoch 46: reducing lr to 5.363111845124793e-05
Epoch 49: reducing lr to 1.0049699184569438e-07
[I 2024-06-21 23:41:32,520] Trial 254 finished with value: 1.0357180833816528 and parameters: {'hidden_size': 93, 'n_layers': 4, 'rnn_dropout': 0.7482270160054405, 'bidirectional': True, 'fc_dropout': 0.4436147986225336, 'learning_rate_model': 0.03174062317178117}. Best is trial 153 with value: 0.9688200950622559.
Epoch 3: reducing lr to 4.0636139927154175e-05
Epoch 15: reducing lr to 0.00015828606726890529
Epoch 18: reducing lr to 0.00015022479424931667
Epoch 22: reducing lr to 0.00013281479378951538
Epoch 25: reducing lr to 0.00011582434650122106
Epoch 28: reducing lr to 9.663070067667621e-05
Epoch 33: reducing lr to 6.30516378221333e-05
Epoch 36: reducing lr to 4.401292122108861e-05
Epoch 39: reducing lr to 2.7283176017382066e-05
Epoch 42: reducing lr to 1.3913584708120365e-05
Epoch 45: reducing lr to 4.744227353784122e-06
Epoch 48: reducing lr to 3.512368285692093e-07
[I 2024-06-21 23:42:07,590] Trial 255 finished with value: 1.0930744409561157 and parameters: {'hidden_size': 191, 'n_layers': 5, 'rnn_dropout': 0.598816256071803, 'bidirectional': False, 'fc_dropout': 0.669401639883065, 'learning_rate_model': 0.001615189656981585}. Best is trial 153 with value: 0.9688200950622559.
Epoch 8: reducing lr to 0.0009880083667888847
Epoch 20: reducing lr to 0.0010679752412610733
Epoch 23: reducing lr to 0.000956085973495413
Epoch 26: reducing lr to 0.0008221845053060603
Epoch 29: reducing lr to 0.0006746843794666825
Epoch 32: reducing lr to 0.0005228535743735544
Epoch 35: reducing lr to 0.0003762322335015006
Epoch 38: reducing lr to 0.00024403305476343782
Epoch 41: reducing lr to 0.00013456252402616834
Epoch 44: reducing lr to 5.469923674974175e-05
Epoch 47: reducing lr to 9.461203923890813e-06
[I 2024-06-21 23:42:42,549] Trial 256 finished with value: 1.1118240356445312 and parameters: {'hidden_size': 176, 'n_layers': 6, 'rnn_dropout': 0.6467290852849135, 'bidirectional': False, 'fc_dropout': 0.7206264917655328, 'learning_rate_model': 0.012115108898561665}. Best is trial 153 with value: 0.9688200950622559.
Epoch 3: reducing lr to 0.001004729005908657
Epoch 6: reducing lr to 0.002383127728237458
Epoch 9: reducing lr to 0.003595396571323028
Epoch 12: reducing lr to 0.003992498590996879
Epoch 15: reducing lr to 0.003913624751301901
Epoch 18: reducing lr to 0.003714309687374845
Epoch 21: reducing lr to 0.0034070771325621528
Epoch 24: reducing lr to 0.0030112315437417943
Epoch 27: reducing lr to 0.0025516457700398005
Epoch 30: reducing lr to 0.0020571966040450716
Epoch 33: reducing lr to 0.001558952437497947
Epoch 36: reducing lr to 0.0010882199604803178
Epoch 39: reducing lr to 0.0006745768266157977
Epoch 42: reducing lr to 0.00034401353468798
Epoch 45: reducing lr to 0.00011730107341684157
Epoch 48: reducing lr to 8.684334443169461e-06
[I 2024-06-21 23:43:00,533] Trial 257 finished with value: 1.1072638034820557 and parameters: {'hidden_size': 93, 'n_layers': 7, 'rnn_dropout': 0.20061852533840183, 'bidirectional': False, 'fc_dropout': 0.6469722785482318, 'learning_rate_model': 0.03993558200464396}. Best is trial 153 with value: 0.9688200950622559.
Epoch 22: reducing lr to 5.783897237074996e-05
Epoch 25: reducing lr to 5.043987183958673e-05
Epoch 28: reducing lr to 4.208130937177039e-05
Epoch 31: reducing lr to 3.328848184157587e-05
Epoch 34: reducing lr to 2.4613884543526424e-05
Epoch 37: reducing lr to 1.660255801594465e-05
Epoch 40: reducing lr to 9.757889735374032e-06
Epoch 43: reducing lr to 4.509951978112824e-06
Epoch 46: reducing lr to 1.1884996159845025e-06
Epoch 49: reducing lr to 2.2270771086996786e-09
[I 2024-06-21 23:43:03,345] Trial 258 finished with value: 1.0938295125961304 and parameters: {'hidden_size': 18, 'n_layers': 3, 'rnn_dropout': 0.18841756528789544, 'bidirectional': False, 'fc_dropout': 0.5792678758912593, 'learning_rate_model': 0.0007033923501905388}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.0010996657822756255
Epoch 14: reducing lr to 0.0012092554640237194
Epoch 18: reducing lr to 0.0011360358132838698
Epoch 21: reducing lr to 0.0010420675622087694
Epoch 24: reducing lr to 0.0009209966760199015
Epoch 27: reducing lr to 0.0007804306106818634
Epoch 30: reducing lr to 0.0006292014435696959
Epoch 33: reducing lr to 0.0004768115610347924
Epoch 36: reducing lr to 0.00033283623388704177
Epoch 39: reducing lr to 0.00020632190052751292
Epoch 42: reducing lr to 0.0001052178543400166
Epoch 45: reducing lr to 3.587698160741025e-05
Epoch 48: reducing lr to 2.6561368793533503e-06
[I 2024-06-21 23:43:14,271] Trial 259 finished with value: 1.1004985570907593 and parameters: {'hidden_size': 83, 'n_layers': 5, 'rnn_dropout': 0.4269662037074211, 'bidirectional': False, 'fc_dropout': 0.3165186178140391, 'learning_rate_model': 0.012214450382481491}. Best is trial 153 with value: 0.9688200950622559.
Epoch 7: reducing lr to 0.00010659384234930892
Epoch 10: reducing lr to 0.0001438655897065835
Epoch 18: reducing lr to 0.00013920923908264905
Epoch 21: reducing lr to 0.00012769441835505343
Epoch 24: reducing lr to 0.00011285845478389196
Epoch 27: reducing lr to 9.563356207563664e-05
Epoch 30: reducing lr to 7.710201840895202e-05
Epoch 33: reducing lr to 5.842824127664847e-05
Epoch 36: reducing lr to 4.0785579395261454e-05
Epoch 39: reducing lr to 2.5282578632355243e-05
Epoch 42: reducing lr to 1.2893341274376421e-05
Epoch 45: reducing lr to 4.3963467099798025e-06
Epoch 48: reducing lr to 3.254816349541629e-07
[I 2024-06-21 23:43:22,910] Trial 260 finished with value: 1.092515468597412 and parameters: {'hidden_size': 62, 'n_layers': 5, 'rnn_dropout': 0.7683177185760145, 'bidirectional': False, 'fc_dropout': 0.08719602714371498, 'learning_rate_model': 0.0014967524119181418}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:43:40,256] Trial 261 finished with value: 1.0866827964782715 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.7310751001972866, 'bidirectional': True, 'fc_dropout': 0.616105750079846, 'learning_rate_model': 2.6425810722624015e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 11: reducing lr to 0.00012424575927421944
Epoch 14: reducing lr to 0.00012370481013052127
Epoch 17: reducing lr to 0.00011869249576983292
Epoch 20: reducing lr to 0.00011014792512177766
Epoch 23: reducing lr to 9.860798467032135e-05
Epoch 26: reducing lr to 8.479776855107759e-05
Epoch 29: reducing lr to 6.958502560656496e-05
Epoch 32: reducing lr to 5.392562873625039e-05
Epoch 35: reducing lr to 3.880352117076084e-05
Epoch 38: reducing lr to 2.516887433793131e-05
Epoch 41: reducing lr to 1.3878395535771174e-05
Epoch 44: reducing lr to 5.641523511925781e-06
Epoch 47: reducing lr to 9.75801630138961e-07
[I 2024-06-21 23:43:43,830] Trial 262 finished with value: 1.0930674076080322 and parameters: {'hidden_size': 118, 'n_layers': 1, 'rnn_dropout': 0.46478725814001565, 'bidirectional': False, 'fc_dropout': 0.572661562286544, 'learning_rate_model': 0.0012495178317291505}. Best is trial 153 with value: 0.9688200950622559.
Epoch 12: reducing lr to 0.0031540983637519545
Epoch 15: reducing lr to 0.00309178754684002
Epoch 18: reducing lr to 0.002934327424394122
Epoch 21: reducing lr to 0.0026916118225373694
Epoch 24: reducing lr to 0.0023788913805534487
Epoch 27: reducing lr to 0.0020158158017402348
Epoch 30: reducing lr to 0.001625197929278295
Epoch 33: reducing lr to 0.0012315819831139017
Epoch 36: reducing lr to 0.0008597004403440925
Epoch 39: reducing lr to 0.0005329198286636381
Epoch 42: reducing lr to 0.00027177280145187463
Epoch 45: reducing lr to 9.266856713856173e-05
Epoch 48: reducing lr to 6.860677451268893e-06
[I 2024-06-21 23:44:48,325] Trial 263 finished with value: 1.10736882686615 and parameters: {'hidden_size': 137, 'n_layers': 6, 'rnn_dropout': 0.6460098173144257, 'bidirectional': True, 'fc_dropout': 0.326359991237894, 'learning_rate_model': 0.03154935461727455}. Best is trial 153 with value: 0.9688200950622559.
Epoch 12: reducing lr to 0.0019085107861859223
Epoch 18: reducing lr to 0.0017755297691463342
Epoch 21: reducing lr to 0.0016286651851362814
Epoch 24: reducing lr to 0.0014394414299591656
Epoch 27: reducing lr to 0.0012197483264310186
Epoch 30: reducing lr to 0.000983389679079323
Epoch 33: reducing lr to 0.0007452169297754907
Epoch 36: reducing lr to 0.0005201954327555399
Epoch 39: reducing lr to 0.0003224640210545103
Epoch 42: reducing lr to 0.0001644467810274223
Epoch 45: reducing lr to 5.607274711431562e-05
Epoch 48: reducing lr to 4.1513216793638565e-06
[I 2024-06-21 23:45:08,380] Trial 264 finished with value: 1.125157117843628 and parameters: {'hidden_size': 146, 'n_layers': 2, 'rnn_dropout': 0.4525857173034107, 'bidirectional': True, 'fc_dropout': 0.6764141404245652, 'learning_rate_model': 0.019090173051118056}. Best is trial 153 with value: 0.9688200950622559.
Epoch 15: reducing lr to 0.00029334433313405795
Epoch 22: reducing lr to 0.0002461395862993713
Epoch 27: reducing lr to 0.00019125768932181517
Epoch 38: reducing lr to 6.029480196233063e-05
Epoch 41: reducing lr to 3.324722032256744e-05
Epoch 47: reducing lr to 2.3376399458229813e-06
[I 2024-06-21 23:45:13,902] Trial 265 finished with value: 0.9782182574272156 and parameters: {'hidden_size': 108, 'n_layers': 1, 'rnn_dropout': 0.11800200431754836, 'bidirectional': True, 'fc_dropout': 0.2979990131242876, 'learning_rate_model': 0.0029933571601558667}. Best is trial 153 with value: 0.9688200950622559.
Epoch 24: reducing lr to 5.5286626392216036e-05
Epoch 27: reducing lr to 4.684856821012547e-05
Epoch 33: reducing lr to 2.8622581732151926e-05
Epoch 36: reducing lr to 1.997986854005483e-05
Epoch 39: reducing lr to 1.2385323560874687e-05
Epoch 42: reducing lr to 6.316135936370519e-06
Epoch 45: reducing lr to 2.1536638837624137e-06
Epoch 48: reducing lr to 1.5944557794714005e-07
[I 2024-06-21 23:45:23,413] Trial 266 finished with value: 1.1079814434051514 and parameters: {'hidden_size': 52, 'n_layers': 7, 'rnn_dropout': 0.12478018643473794, 'bidirectional': False, 'fc_dropout': 0.18479195893576314, 'learning_rate_model': 0.0007332227927258254}. Best is trial 153 with value: 0.9688200950622559.
Epoch 3: reducing lr to 3.188124608749892e-05
Epoch 6: reducing lr to 7.561947661018155e-05
Epoch 9: reducing lr to 0.00011408620851831988
Epoch 12: reducing lr to 0.00012668672779925311
Epoch 15: reducing lr to 0.0001241839670763183
Epoch 25: reducing lr to 9.087045423971632e-05
Epoch 28: reducing lr to 7.581200265091985e-05
Epoch 31: reducing lr to 5.997119650729335e-05
Epoch 34: reducing lr to 4.434338921770949e-05
Epoch 37: reducing lr to 2.9910504000648047e-05
Epoch 40: reducing lr to 1.757942358566009e-05
Epoch 43: reducing lr to 8.124948971991222e-06
Epoch 46: reducing lr to 2.1411533382104803e-06
Epoch 49: reducing lr to 4.012212979800749e-09
[I 2024-06-21 23:45:40,854] Trial 267 finished with value: 1.0936362743377686 and parameters: {'hidden_size': 106, 'n_layers': 6, 'rnn_dropout': 0.5074246022190988, 'bidirectional': False, 'fc_dropout': 0.3627019412177587, 'learning_rate_model': 0.0012672035046764618}. Best is trial 153 with value: 0.9688200950622559.
Epoch 11: reducing lr to 7.287943763802778e-05
Epoch 14: reducing lr to 7.256213047508079e-05
Epoch 17: reducing lr to 6.962203292965276e-05
Epoch 20: reducing lr to 6.461000268148734e-05
Epoch 23: reducing lr to 5.784096383950753e-05
Epoch 26: reducing lr to 4.974023838771375e-05
Epoch 29: reducing lr to 4.0816825973443894e-05
Epoch 32: reducing lr to 3.1631417599541965e-05
Epoch 35: reducing lr to 2.2761169619148115e-05
Epoch 38: reducing lr to 1.4763428695237821e-05
Epoch 41: reducing lr to 8.140717782832122e-06
Epoch 44: reducing lr to 3.3091758090787044e-06
Epoch 47: reducing lr to 5.723806950532666e-07
[I 2024-06-21 23:45:49,120] Trial 268 finished with value: 1.092707633972168 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.6304396409378458, 'bidirectional': False, 'fc_dropout': 0.1779065548249406, 'learning_rate_model': 0.0007329357350066418}. Best is trial 153 with value: 0.9688200950622559.
Epoch 14: reducing lr to 6.25732037907932e-06
Epoch 17: reducing lr to 6.003784103793055e-06
Epoch 29: reducing lr to 3.519796832049675e-06
Epoch 34: reducing lr to 2.211702868882563e-06
[I 2024-06-21 23:46:21,237] Trial 269 finished with value: 1.1069889068603516 and parameters: {'hidden_size': 181, 'n_layers': 5, 'rnn_dropout': 0.580767076679052, 'bidirectional': False, 'fc_dropout': 0.7568440238780506, 'learning_rate_model': 6.320395612953416e-05}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:48:06,018] Trial 270 finished with value: 1.0840476751327515 and parameters: {'hidden_size': 165, 'n_layers': 7, 'rnn_dropout': 0.011654985410665387, 'bidirectional': True, 'fc_dropout': 0.6139388141931627, 'learning_rate_model': 2.1128387034312285e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 8: reducing lr to 0.00019611572728538742
Epoch 11: reducing lr to 0.00023912128663888991
Epoch 14: reducing lr to 0.00023808018506727146
Epoch 17: reducing lr to 0.00022843356963373309
Epoch 20: reducing lr to 0.00021198883349887314
Epoch 23: reducing lr to 0.00018977925930811057
Epoch 26: reducing lr to 0.0001632003509696275
Epoch 29: reducing lr to 0.00013392216322746157
Epoch 32: reducing lr to 0.00010378435289500384
Epoch 35: reducing lr to 7.468060047017668e-05
Epoch 38: reducing lr to 4.8439589810510905e-05
Epoch 41: reducing lr to 2.6710125290253057e-05
Epoch 44: reducing lr to 1.08575807227181e-05
Epoch 47: reducing lr to 1.8780113113411359e-06
[I 2024-06-21 23:48:27,181] Trial 271 finished with value: 1.0925073623657227 and parameters: {'hidden_size': 200, 'n_layers': 3, 'rnn_dropout': 0.7241622127464323, 'bidirectional': False, 'fc_dropout': 0.6758494552049629, 'learning_rate_model': 0.0024048008829167943}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:48:53,683] Trial 272 finished with value: 1.0741480588912964 and parameters: {'hidden_size': 101, 'n_layers': 4, 'rnn_dropout': 0.45910842151387793, 'bidirectional': True, 'fc_dropout': 0.0911726091673491, 'learning_rate_model': 4.026351717100039e-05}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:49:00,294] Trial 273 finished with value: 0.9955556392669678 and parameters: {'hidden_size': 25, 'n_layers': 4, 'rnn_dropout': 0.6781768112288086, 'bidirectional': True, 'fc_dropout': 0.340579529588093, 'learning_rate_model': 0.0025587706106692735}. Best is trial 153 with value: 0.9688200950622559.
Epoch 25: reducing lr to 0.0003858055805149859
Epoch 33: reducing lr to 0.0002100221107842151
Epoch 36: reducing lr to 0.0001466050198839966
Epoch 39: reducing lr to 9.087900669974992e-05
Epoch 42: reducing lr to 4.634551186787159e-05
Epoch 45: reducing lr to 1.5802803500406146e-05
Epoch 48: reducing lr to 1.169953750120717e-06
[I 2024-06-21 23:49:37,375] Trial 274 finished with value: 0.991070032119751 and parameters: {'hidden_size': 94, 'n_layers': 6, 'rnn_dropout': 0.7535089969493642, 'bidirectional': True, 'fc_dropout': 0.37623913982574025, 'learning_rate_model': 0.005380122591470972}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:49:46,820] Trial 275 finished with value: 1.0794379711151123 and parameters: {'hidden_size': 18, 'n_layers': 7, 'rnn_dropout': 0.13766186546311676, 'bidirectional': True, 'fc_dropout': 0.6401117475095902, 'learning_rate_model': 0.00011905406569348063}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 0.00028865270146367623
Epoch 13: reducing lr to 0.00029929393516291456
Epoch 16: reducing lr to 0.0002902734094998094
Epoch 19: reducing lr to 0.00027244879513773815
Epoch 22: reducing lr to 0.00024694008324029456
Epoch 25: reducing lr to 0.00021535005965971025
Epoch 28: reducing lr to 0.00017966366989571097
Epoch 31: reducing lr to 0.00014212321104547994
Epoch 34: reducing lr to 0.00010508752920235542
Epoch 37: reducing lr to 7.088364281749482e-05
Epoch 40: reducing lr to 4.166073505001374e-05
Epoch 43: reducing lr to 1.9254974133117892e-05
Epoch 46: reducing lr to 5.074229054779881e-06
Epoch 49: reducing lr to 9.508374441361683e-09
[I 2024-06-21 23:49:52,396] Trial 276 finished with value: 1.0927715301513672 and parameters: {'hidden_size': 56, 'n_layers': 4, 'rnn_dropout': 0.679496880584524, 'bidirectional': False, 'fc_dropout': 0.28214346180139044, 'learning_rate_model': 0.003003092177939155}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:51:30,364] Trial 277 finished with value: 1.0618200302124023 and parameters: {'hidden_size': 197, 'n_layers': 5, 'rnn_dropout': 0.036398572832483024, 'bidirectional': True, 'fc_dropout': 0.4257452388173397, 'learning_rate_model': 0.000137716137762918}. Best is trial 153 with value: 0.9688200950622559.
Epoch 17: reducing lr to 0.001926441197884076
Epoch 21: reducing lr to 0.0017302023646076797
Epoch 24: reducing lr to 0.001529181681145354
Epoch 27: reducing lr to 0.0012957920743179737
Epoch 30: reducing lr to 0.0010446979303063194
Epoch 33: reducing lr to 0.0007916765863300127
Epoch 36: reducing lr to 0.0005526263937031586
Epoch 39: reducing lr to 0.00034256765406496284
Epoch 42: reducing lr to 0.00017469901854748546
Epoch 45: reducing lr to 5.95685353457831e-05
Epoch 48: reducing lr to 4.4101308552754044e-06
[I 2024-06-21 23:51:42,544] Trial 278 finished with value: 1.0098536014556885 and parameters: {'hidden_size': 38, 'n_layers': 5, 'rnn_dropout': 0.545087803913045, 'bidirectional': True, 'fc_dropout': 0.6388962466224248, 'learning_rate_model': 0.020280327015801246}. Best is trial 153 with value: 0.9688200950622559.
Epoch 11: reducing lr to 7.368493153586999e-05
Epoch 14: reducing lr to 7.336411736200499e-05
Epoch 17: reducing lr to 7.039152463400342e-05
Epoch 20: reducing lr to 6.532409934010853e-05
Epoch 23: reducing lr to 5.848024626165571e-05
Epoch 26: reducing lr to 5.0289988218352045e-05
Epoch 29: reducing lr to 4.126795053362767e-05
Epoch 32: reducing lr to 3.1981021200808434e-05
Epoch 35: reducing lr to 2.3012735545425372e-05
Epoch 38: reducing lr to 1.4926600257898693e-05
Epoch 41: reducing lr to 8.230692386240778e-06
Epoch 44: reducing lr to 3.34575019833701e-06
Epoch 47: reducing lr to 5.787068848819702e-07
[I 2024-06-21 23:51:50,980] Trial 279 finished with value: 1.092626690864563 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.20913657152467036, 'bidirectional': False, 'fc_dropout': 0.3819503644737736, 'learning_rate_model': 0.0007410364460054091}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.00024019511735354367
Epoch 32: reducing lr to 0.00011514104112724769
Epoch 36: reducing lr to 7.269994169735068e-05
Epoch 39: reducing lr to 4.506597723470019e-05
Epoch 42: reducing lr to 2.2982269047773116e-05
Epoch 45: reducing lr to 7.836449898122704e-06
Epoch 48: reducing lr to 5.801681926694957e-07
[I 2024-06-21 23:53:41,335] Trial 280 finished with value: 0.9873771071434021 and parameters: {'hidden_size': 171, 'n_layers': 7, 'rnn_dropout': 0.6534031715014135, 'bidirectional': True, 'fc_dropout': 0.7712289066712654, 'learning_rate_model': 0.002667948198731734}. Best is trial 153 with value: 0.9688200950622559.
Epoch 7: reducing lr to 0.0002974317316583477
Epoch 10: reducing lr to 0.0004014321139888614
Epoch 13: reducing lr to 0.0004162309809929602
Epoch 16: reducing lr to 0.0004036860483875511
Epoch 19: reducing lr to 0.00037889718416379847
Epoch 22: reducing lr to 0.00034342197090509905
Epoch 25: reducing lr to 0.00029948941845501316
Epoch 28: reducing lr to 0.0002498600097886424
Epoch 31: reducing lr to 0.00019765212924588408
Epoch 34: reducing lr to 0.0001461462469869742
Epoch 37: reducing lr to 9.857857016120487e-05
Epoch 40: reducing lr to 5.7937988650909584e-05
Epoch 43: reducing lr to 2.677807944240227e-05
Epoch 46: reducing lr to 7.05677960398483e-06
Epoch 49: reducing lr to 1.3223388637025046e-08
[I 2024-06-21 23:53:53,143] Trial 281 finished with value: 1.0926827192306519 and parameters: {'hidden_size': 127, 'n_layers': 3, 'rnn_dropout': 0.5449633472936152, 'bidirectional': False, 'fc_dropout': 0.10419373405821207, 'learning_rate_model': 0.00417642944403634}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:53:57,049] Trial 282 finished with value: 1.105799674987793 and parameters: {'hidden_size': 48, 'n_layers': 3, 'rnn_dropout': 0.03544932466648367, 'bidirectional': False, 'fc_dropout': 0.4073115220130942, 'learning_rate_model': 4.020666708053822e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 21: reducing lr to 7.815544618816395e-06
Epoch 24: reducing lr to 6.90750856878995e-06
Epoch 27: reducing lr to 5.853257966062904e-06
Epoch 30: reducing lr to 4.719033712189818e-06
Epoch 34: reducing lr to 3.2056774154258297e-06
Epoch 42: reducing lr to 7.8913773455101e-07
Epoch 45: reducing lr to 2.690786669789778e-07
Epoch 48: reducing lr to 1.99211232045909e-08
[I 2024-06-21 23:54:49,239] Trial 283 finished with value: 1.0762220621109009 and parameters: {'hidden_size': 112, 'n_layers': 7, 'rnn_dropout': 0.6925488112050673, 'bidirectional': True, 'fc_dropout': 0.6093838901541935, 'learning_rate_model': 9.160882213458435e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.0013814725199420535
Epoch 14: reducing lr to 0.0015191462897768235
Epoch 17: reducing lr to 0.0014575929940221903
Epoch 22: reducing lr to 0.0012617647663109358
Epoch 25: reducing lr to 0.001100352418028351
Epoch 28: reducing lr to 0.0009180092817897622
Epoch 31: reducing lr to 0.0007261925962730794
Epoch 34: reducing lr to 0.0005369551187733919
Epoch 37: reducing lr to 0.000362186980101779
Epoch 40: reducing lr to 0.00021286964406491677
Epoch 43: reducing lr to 9.83851937627931e-05
Epoch 46: reducing lr to 2.5927274962818903e-05
Epoch 49: reducing lr to 4.858397914826035e-08
[I 2024-06-21 23:54:51,724] Trial 284 finished with value: 1.106597900390625 and parameters: {'hidden_size': 63, 'n_layers': 1, 'rnn_dropout': 0.6913152758112215, 'bidirectional': False, 'fc_dropout': 0.5893687661548159, 'learning_rate_model': 0.015344596350606936}. Best is trial 153 with value: 0.9688200950622559.
Epoch 4: reducing lr to 0.0011392764635273368
Epoch 7: reducing lr to 0.0022601491748622895
Epoch 10: reducing lr to 0.00305043599799008
Epoch 13: reducing lr to 0.003162890869101919
Epoch 18: reducing lr to 0.002951705651202621
Epoch 21: reducing lr to 0.002707552593271962
Epoch 24: reducing lr to 0.0023929800993584332
Epoch 28: reducing lr to 0.0018986571869996853
Epoch 31: reducing lr to 0.0015019355679843842
Epoch 34: reducing lr to 0.0011105483523736847
Epoch 37: reducing lr to 0.0007490871023300188
Epoch 40: reducing lr to 0.0004402640448361819
Epoch 43: reducing lr to 0.00020348351475041362
Epoch 46: reducing lr to 5.362364839220299e-05
Epoch 49: reducing lr to 1.0048299403098149e-07
[I 2024-06-21 23:54:57,332] Trial 285 finished with value: 1.1017963886260986 and parameters: {'hidden_size': 98, 'n_layers': 2, 'rnn_dropout': 0.6629862812226415, 'bidirectional': False, 'fc_dropout': 0.4615822771820013, 'learning_rate_model': 0.03173620215025359}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 0.00010529323672416062
Epoch 13: reducing lr to 0.00010917489081313858
Epoch 16: reducing lr to 0.00010588443020353568
Epoch 19: reducing lr to 9.938245973859403e-05
Epoch 22: reducing lr to 9.007752399149544e-05
Epoch 25: reducing lr to 7.855427888023896e-05
Epoch 28: reducing lr to 6.553678253879459e-05
Epoch 31: reducing lr to 5.184296848333046e-05
Epoch 34: reducing lr to 3.833328436890851e-05
Epoch 37: reducing lr to 2.5856567928197754e-05
Epoch 40: reducing lr to 1.5196787057527802e-05
Epoch 43: reducing lr to 7.023729690508722e-06
Epoch 46: reducing lr to 1.8509509813986053e-06
Epoch 49: reducing lr to 3.468415559035494e-09
[I 2024-06-21 23:55:04,610] Trial 286 finished with value: 1.0924744606018066 and parameters: {'hidden_size': 128, 'n_layers': 2, 'rnn_dropout': 0.23690393879672353, 'bidirectional': False, 'fc_dropout': 0.14759648359214214, 'learning_rate_model': 0.0010954524035037083}. Best is trial 153 with value: 0.9688200950622559.
Epoch 43: reducing lr to 3.9954650349551493e-07
Epoch 46: reducing lr to 1.0529177877656161e-07
Epoch 49: reducing lr to 1.973016289558735e-10
[I 2024-06-21 23:55:34,296] Trial 287 finished with value: 1.0758111476898193 and parameters: {'hidden_size': 79, 'n_layers': 6, 'rnn_dropout': 0.6422931891459392, 'bidirectional': True, 'fc_dropout': 0.05367156826308426, 'learning_rate_model': 6.231506576301107e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 0.0008926144360079515
Epoch 16: reducing lr to 0.0008976262283184719
Epoch 22: reducing lr to 0.000763624528755813
Epoch 25: reducing lr to 0.0006659372009086163
Epoch 28: reducing lr to 0.000555582485671833
Epoch 31: reducing lr to 0.0004394943446838458
Epoch 34: reducing lr to 0.00032496714957034147
Epoch 37: reducing lr to 0.00021919684982989591
Epoch 40: reducing lr to 0.00012882946645494767
Epoch 43: reducing lr to 5.9543069540069804e-05
Epoch 46: reducing lr to 1.5691279114799645e-05
Epoch 49: reducing lr to 2.9403197151015025e-08
[I 2024-06-21 23:55:42,736] Trial 288 finished with value: 1.0994915962219238 and parameters: {'hidden_size': 66, 'n_layers': 5, 'rnn_dropout': 0.6984667978195485, 'bidirectional': False, 'fc_dropout': 0.5465366750833173, 'learning_rate_model': 0.009286604341821391}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:56:37,785] Trial 289 finished with value: 1.0710340738296509 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.4252667727167214, 'bidirectional': True, 'fc_dropout': 0.23928987572866847, 'learning_rate_model': 2.639603471883599e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.0002925731649805756
Epoch 33: reducing lr to 0.00012685878724224284
Epoch 39: reducing lr to 5.489327067831956e-05
Epoch 42: reducing lr to 2.7993887918400248e-05
Epoch 45: reducing lr to 9.545302061784955e-06
Epoch 48: reducing lr to 7.06682326520906e-07
[I 2024-06-21 23:57:01,301] Trial 290 finished with value: 0.9825058579444885 and parameters: {'hidden_size': 68, 'n_layers': 6, 'rnn_dropout': 0.5126261635614769, 'bidirectional': True, 'fc_dropout': 0.18568001747706805, 'learning_rate_model': 0.0032497332048521473}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 0.0037121107001282906
Epoch 9: reducing lr to 0.0056004174369133596
Epoch 13: reducing lr to 0.006199588598578381
Epoch 16: reducing lr to 0.006012737007269892
Epoch 19: reducing lr to 0.0056435171100708325
Epoch 22: reducing lr to 0.0051151284564292885
Epoch 25: reducing lr to 0.00446077122759868
Epoch 28: reducing lr to 0.0037215616776795136
Epoch 31: reducing lr to 0.002943946853822129
Epoch 34: reducing lr to 0.00217678800454498
Epoch 37: reducing lr to 0.0014682870990948677
Epoch 40: reducing lr to 0.0008629624181454625
Epoch 43: reducing lr to 0.0003988484365265242
Epoch 46: reducing lr to 0.00010510781843095091
Epoch 49: reducing lr to 1.9695691376303559e-07
[I 2024-06-21 23:57:03,674] Trial 291 finished with value: 1.1642935276031494 and parameters: {'hidden_size': 72, 'n_layers': 1, 'rnn_dropout': 0.7226649414535005, 'bidirectional': False, 'fc_dropout': 0.5930013882654873, 'learning_rate_model': 0.06220619210575453}. Best is trial 153 with value: 0.9688200950622559.
Epoch 8: reducing lr to 0.0021248239844132216
Epoch 15: reducing lr to 0.0025533410282891104
Epoch 18: reducing lr to 0.0024233031829101626
Epoch 22: reducing lr to 0.00214245933326784
Epoch 25: reducing lr to 0.0018683833713167175
Epoch 28: reducing lr to 0.0015587672174008902
Epoch 31: reducing lr to 0.0012330651062512385
Epoch 34: reducing lr to 0.000911742455073834
Epoch 37: reducing lr to 0.000614988543526922
Epoch 40: reducing lr to 0.0003614497471107033
Epoch 43: reducing lr to 0.00016705671473831314
Epoch 46: reducing lr to 4.4024158633546725e-05
Epoch 49: reducing lr to 8.249493277379116e-08
[I 2024-06-21 23:57:06,441] Trial 292 finished with value: 1.0979777574539185 and parameters: {'hidden_size': 29, 'n_layers': 2, 'rnn_dropout': 0.25788599130616857, 'bidirectional': False, 'fc_dropout': 0.6595676886381469, 'learning_rate_model': 0.026054914944806804}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 0.0009548389182301994
Epoch 18: reducing lr to 0.0009239346220623253
Epoch 22: reducing lr to 0.0008168570768720473
Epoch 25: reducing lr to 0.0007123599292978113
Epoch 28: reducing lr to 0.0005943123460775093
Epoch 31: reducing lr to 0.00047013165787795463
Epoch 34: reducing lr to 0.00034762072966668025
Epoch 37: reducing lr to 0.00023447714324125146
Epoch 40: reducing lr to 0.00013781021617369424
Epoch 43: reducing lr to 6.369383892334841e-05
Epoch 46: reducing lr to 1.6785123980999488e-05
Epoch 49: reducing lr to 3.1452904891184076e-08
[I 2024-06-21 23:57:23,198] Trial 293 finished with value: 1.061301589012146 and parameters: {'hidden_size': 126, 'n_layers': 2, 'rnn_dropout': 0.14919415061814822, 'bidirectional': True, 'fc_dropout': 0.4315927127412398, 'learning_rate_model': 0.009933976962588156}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:57:26,746] Trial 294 finished with value: 1.0965840816497803 and parameters: {'hidden_size': 58, 'n_layers': 2, 'rnn_dropout': 0.031788549260681, 'bidirectional': False, 'fc_dropout': 0.08370872939596685, 'learning_rate_model': 7.844769404538512e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 42: reducing lr to 4.295021839368691e-06
Epoch 45: reducing lr to 1.4645082861745005e-06
Epoch 48: reducing lr to 1.0842424013237771e-07
[I 2024-06-21 23:57:35,793] Trial 295 finished with value: 1.0696755647659302 and parameters: {'hidden_size': 22, 'n_layers': 5, 'rnn_dropout': 0.18536251317177568, 'bidirectional': True, 'fc_dropout': 0.3017738300892052, 'learning_rate_model': 0.0004985972340693427}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:57:56,272] Trial 296 finished with value: 1.0542758703231812 and parameters: {'hidden_size': 103, 'n_layers': 3, 'rnn_dropout': 0.08220525109933147, 'bidirectional': True, 'fc_dropout': 0.3394338994606496, 'learning_rate_model': 0.00012241212858594047}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.0005920734338768532
Epoch 15: reducing lr to 0.0006444777924889938
Epoch 18: reducing lr to 0.0006116555009888237
Epoch 22: reducing lr to 0.000540768916609269
Epoch 27: reducing lr to 0.00042019333420816073
Epoch 30: reducing lr to 0.00033876971103317407
Epoch 33: reducing lr to 0.0002567211445552585
Epoch 36: reducing lr to 0.00017920307705523138
Epoch 39: reducing lr to 0.00011108622101210826
Epoch 42: reducing lr to 5.6650572681576655e-05
Epoch 45: reducing lr to 1.9316603316944878e-05
Epoch 48: reducing lr to 1.4300964059746173e-06
[I 2024-06-21 23:58:06,145] Trial 297 finished with value: 1.0958025455474854 and parameters: {'hidden_size': 71, 'n_layers': 5, 'rnn_dropout': 0.41422937905073165, 'bidirectional': False, 'fc_dropout': 0.27163269503704285, 'learning_rate_model': 0.006576408666557594}. Best is trial 153 with value: 0.9688200950622559.
Epoch 12: reducing lr to 7.711893163508988e-05
Epoch 42: reducing lr to 6.644950689017968e-06
[I 2024-06-21 23:58:24,750] Trial 298 finished with value: 1.043468713760376 and parameters: {'hidden_size': 57, 'n_layers': 6, 'rnn_dropout': 0.4209837319133043, 'bidirectional': True, 'fc_dropout': 0.14892453578933118, 'learning_rate_model': 0.0007713939900614159}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:58:35,970] Trial 299 finished with value: 1.0333423614501953 and parameters: {'hidden_size': 197, 'n_layers': 1, 'rnn_dropout': 0.2267699459424457, 'bidirectional': True, 'fc_dropout': 0.22300672695474966, 'learning_rate_model': 0.00018138042998014145}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.004542227928014973
Epoch 12: reducing lr to 0.005043904960924304
Epoch 15: reducing lr to 0.004944260053792273
Epoch 18: reducing lr to 0.004692456273073191
Epoch 21: reducing lr to 0.004304315420407228
Epoch 24: reducing lr to 0.0038042256937099216
Epoch 27: reducing lr to 0.003223610093951647
Epoch 30: reducing lr to 0.002598949985890597
Epoch 33: reducing lr to 0.00196949548111865
Epoch 36: reducing lr to 0.0013747977443551254
Epoch 39: reducing lr to 0.0008522235699630974
Epoch 42: reducing lr to 0.0004346079365314337
Epoch 45: reducing lr to 0.0001481917783172533
Epoch 48: reducing lr to 1.0971314474350623e-05
[I 2024-06-21 23:59:05,971] Trial 300 finished with value: 1.0721491575241089 and parameters: {'hidden_size': 93, 'n_layers': 5, 'rnn_dropout': 0.5754710137768604, 'bidirectional': True, 'fc_dropout': 0.18958565157735868, 'learning_rate_model': 0.05045243613732324}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-21 23:59:43,057] Trial 301 finished with value: 1.045783519744873 and parameters: {'hidden_size': 160, 'n_layers': 3, 'rnn_dropout': 0.24397251331365055, 'bidirectional': True, 'fc_dropout': 0.5067686673885805, 'learning_rate_model': 0.00012907761430379762}. Best is trial 153 with value: 0.9688200950622559.
Epoch 15: reducing lr to 3.7946524970568724e-05
Epoch 18: reducing lr to 3.601396512363845e-05
Epoch 21: reducing lr to 3.303503675062743e-05
Epoch 24: reducing lr to 2.9196915961028382e-05
Epoch 27: reducing lr to 2.474077002314832e-05
Epoch 30: reducing lr to 1.9946588460939443e-05
Epoch 33: reducing lr to 1.5115610554580072e-05
Epoch 36: reducing lr to 1.0551386126148358e-05
Epoch 39: reducing lr to 6.540700251660061e-06
Epoch 42: reducing lr to 3.3355569360369827e-06
Epoch 45: reducing lr to 1.1373517887748434e-06
Epoch 48: reducing lr to 8.420334976951536e-08
[I 2024-06-21 23:59:59,907] Trial 302 finished with value: 1.0929774045944214 and parameters: {'hidden_size': 169, 'n_layers': 3, 'rnn_dropout': 0.5866928755750672, 'bidirectional': False, 'fc_dropout': 0.6428757011448218, 'learning_rate_model': 0.00038721560089512944}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 0.002956306704731854
Epoch 13: reducing lr to 0.0030652914825363173
Epoch 16: reducing lr to 0.0029729055794672646
Epoch 19: reducing lr to 0.002790350465031649
Epoch 22: reducing lr to 0.0025290968005791947
Epoch 25: reducing lr to 0.0022055599064487556
Epoch 28: reducing lr to 0.0018400690837679866
Epoch 31: reducing lr to 0.0014555893625150958
Epoch 34: reducing lr to 0.0010762794375015487
Epoch 37: reducing lr to 0.0007259720330161122
Epoch 40: reducing lr to 0.00042667853003936493
Epoch 43: reducing lr to 0.00019720449121221235
Epoch 46: reducing lr to 5.196894849736483e-05
Epoch 49: reducing lr to 9.738232474368336e-08
[I 2024-06-22 00:00:06,576] Trial 303 finished with value: 1.104495644569397 and parameters: {'hidden_size': 45, 'n_layers': 5, 'rnn_dropout': 0.602660644715514, 'bidirectional': False, 'fc_dropout': 0.7203591299088523, 'learning_rate_model': 0.03075689746034307}. Best is trial 153 with value: 0.9688200950622559.
Epoch 26: reducing lr to 0.00011947600725389324
Epoch 33: reducing lr to 6.872449894998265e-05
Epoch 36: reducing lr to 4.797283722870363e-05
Epoch 39: reducing lr to 2.9737888916512308e-05
Epoch 42: reducing lr to 1.5165413154867385e-05
Epoch 45: reducing lr to 5.171073409914861e-06
Epoch 48: reducing lr to 3.8283819247160196e-07
[I 2024-06-22 00:01:04,651] Trial 304 finished with value: 0.98003089427948 and parameters: {'hidden_size': 146, 'n_layers': 5, 'rnn_dropout': 0.6539058163177005, 'bidirectional': True, 'fc_dropout': 0.3177822129870094, 'learning_rate_model': 0.0017605109671915277}. Best is trial 153 with value: 0.9688200950622559.
Epoch 7: reducing lr to 0.0003109159806814982
Epoch 10: reducing lr to 0.00041963128379745907
Epoch 13: reducing lr to 0.00043510106646624204
Epoch 16: reducing lr to 0.00042198740168727906
Epoch 20: reducing lr to 0.00038485289382807274
Epoch 23: reducing lr to 0.0003445327564089021
Epoch 27: reducing lr to 0.0002789467204059773
Epoch 30: reducing lr to 0.00022489338162316147
Epoch 33: reducing lr to 0.00017042517218296077
Epoch 36: reducing lr to 0.00011896454931970118
Epoch 39: reducing lr to 7.374495145672718e-05
Epoch 42: reducing lr to 3.760766811883266e-05
Epoch 45: reducing lr to 1.2823390344349553e-05
Epoch 48: reducing lr to 9.493741804893728e-07
[I 2024-06-22 00:01:09,231] Trial 305 finished with value: 1.0930380821228027 and parameters: {'hidden_size': 151, 'n_layers': 1, 'rnn_dropout': 0.31980399014469163, 'bidirectional': False, 'fc_dropout': 0.3983829061386307, 'learning_rate_model': 0.004365770420996029}. Best is trial 153 with value: 0.9688200950622559.
Epoch 16: reducing lr to 1.8452924654007867e-05
[I 2024-06-22 00:01:44,842] Trial 306 finished with value: 1.0668919086456299 and parameters: {'hidden_size': 107, 'n_layers': 5, 'rnn_dropout': 0.4659518895113781, 'bidirectional': True, 'fc_dropout': 0.1556131310876815, 'learning_rate_model': 0.00019090909423650804}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:01:57,335] Trial 307 finished with value: 1.0789061784744263 and parameters: {'hidden_size': 43, 'n_layers': 5, 'rnn_dropout': 0.48919958132768193, 'bidirectional': True, 'fc_dropout': 0.5226064416195636, 'learning_rate_model': 5.769443580257137e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 33: reducing lr to 1.2703026219016772e-05
Epoch 36: reducing lr to 8.867292136394687e-06
Epoch 42: reducing lr to 2.8031727240195612e-06
Epoch 45: reducing lr to 9.558204440954406e-07
Epoch 48: reducing lr to 7.076375486050738e-08
[I 2024-06-22 00:02:05,096] Trial 308 finished with value: 1.0774266719818115 and parameters: {'hidden_size': 17, 'n_layers': 6, 'rnn_dropout': 0.3548030735231481, 'bidirectional': True, 'fc_dropout': 0.050095841587132386, 'learning_rate_model': 0.0003254125867309244}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:02:09,136] Trial 309 finished with value: 1.1005264520645142 and parameters: {'hidden_size': 144, 'n_layers': 1, 'rnn_dropout': 0.15666738693999394, 'bidirectional': False, 'fc_dropout': 0.6000993984551881, 'learning_rate_model': 2.7475150823638172e-05}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:02:20,982] Trial 310 finished with value: 1.106311321258545 and parameters: {'hidden_size': 131, 'n_layers': 3, 'rnn_dropout': 0.5360722949982456, 'bidirectional': False, 'fc_dropout': 0.13773004396474617, 'learning_rate_model': 1.1877472237951753e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 5: reducing lr to 0.004323825142874373
Epoch 8: reducing lr to 0.0074001721611845015
Epoch 11: reducing lr to 0.009022931067413587
Epoch 14: reducing lr to 0.008983646452284045
Epoch 17: reducing lr to 0.008619643952489413
Epoch 20: reducing lr to 0.00799912320064718
Epoch 23: reducing lr to 0.007161073774865664
Epoch 26: reducing lr to 0.0061581532019791465
Epoch 29: reducing lr to 0.005053378827896332
Epoch 32: reducing lr to 0.003916167712253544
Epoch 35: reducing lr to 0.002817975428231358
Epoch 38: reducing lr to 0.0018278049852335748
Epoch 41: reducing lr to 0.0010078718740748783
Epoch 44: reducing lr to 0.00040969670160695415
Epoch 47: reducing lr to 7.086431678349076e-05
[I 2024-06-22 00:02:27,035] Trial 311 finished with value: 1.1062208414077759 and parameters: {'hidden_size': 106, 'n_layers': 2, 'rnn_dropout': 0.51067466400271, 'bidirectional': False, 'fc_dropout': 0.3143679221073626, 'learning_rate_model': 0.09074203682326881}. Best is trial 153 with value: 0.9688200950622559.
Epoch 44: reducing lr to 2.5775520496941313e-07
Epoch 47: reducing lr to 4.458333793243307e-08
[I 2024-06-22 00:02:57,600] Trial 312 finished with value: 1.0720748901367188 and parameters: {'hidden_size': 111, 'n_layers': 4, 'rnn_dropout': 0.5995539766017357, 'bidirectional': True, 'fc_dropout': 0.39139582836811376, 'learning_rate_model': 5.7089139866110815e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 11: reducing lr to 0.00016515289169375947
Epoch 15: reducing lr to 0.00016276695168855642
Epoch 18: reducing lr to 0.0001544774739172845
Epoch 22: reducing lr to 0.0001365746176985874
Epoch 25: reducing lr to 0.00011910319168708254
Epoch 28: reducing lr to 9.936619729108447e-05
Epoch 31: reducing lr to 7.860377691597366e-05
Epoch 34: reducing lr to 5.812053246833477e-05
Epoch 37: reducing lr to 3.920346300953579e-05
Epoch 40: reducing lr to 2.3041212620638383e-05
Epoch 43: reducing lr to 1.064930689469233e-05
Epoch 46: reducing lr to 2.806392887610997e-06
Epoch 49: reducing lr to 5.258776085382763e-09
[I 2024-06-22 00:03:00,254] Trial 313 finished with value: 1.092691421508789 and parameters: {'hidden_size': 88, 'n_layers': 1, 'rnn_dropout': 0.3700232482737623, 'bidirectional': False, 'fc_dropout': 0.023417697122490023, 'learning_rate_model': 0.0016609136950705152}. Best is trial 153 with value: 0.9688200950622559.
Epoch 5: reducing lr to 0.0047038769631615115
Epoch 8: reducing lr to 0.008050626054985289
Epoch 11: reducing lr to 0.00981602081160622
Epoch 14: reducing lr to 0.009773283191557212
Epoch 17: reducing lr to 0.00937728591675092
Epoch 20: reducing lr to 0.008702223171772761
Epoch 23: reducing lr to 0.007790511606743146
Epoch 26: reducing lr to 0.006699437193973187
Epoch 29: reducing lr to 0.005497556323211454
Epoch 32: reducing lr to 0.004260387614403024
Epoch 35: reducing lr to 0.0030656673805270498
Epoch 38: reducing lr to 0.001988463797469013
Epoch 41: reducing lr to 0.0010964609191220901
Epoch 44: reducing lr to 0.00044570786581139973
Epoch 47: reducing lr to 7.7093086841723e-05
[I 2024-06-22 00:03:09,788] Trial 314 finished with value: 1.1073243618011475 and parameters: {'hidden_size': 70, 'n_layers': 5, 'rnn_dropout': 0.5862158826090934, 'bidirectional': False, 'fc_dropout': 0.19112796389616965, 'learning_rate_model': 0.09871800142213323}. Best is trial 153 with value: 0.9688200950622559.
Epoch 5: reducing lr to 0.00031650549014067373
Epoch 8: reducing lr to 0.0005416951517711087
Epoch 11: reducing lr to 0.0006604816627942496
Epoch 14: reducing lr to 0.0006576060154320861
Epoch 17: reducing lr to 0.0006309609070377761
Epoch 20: reducing lr to 0.000585538574215657
Epoch 23: reducing lr to 0.0005241930675162908
Epoch 26: reducing lr to 0.0004507789360459804
Epoch 29: reducing lr to 0.0003699090712365337
Epoch 32: reducing lr to 0.00028666482576950535
Epoch 35: reducing lr to 0.00020627677222021619
Epoch 38: reducing lr to 0.00013379595465054812
Epoch 41: reducing lr to 7.377656842316405e-05
Epoch 44: reducing lr to 2.9989939709940213e-05
Epoch 47: reducing lr to 5.1872924033491766e-06
[I 2024-06-22 00:03:20,524] Trial 315 finished with value: 1.1074204444885254 and parameters: {'hidden_size': 60, 'n_layers': 7, 'rnn_dropout': 0.17377580031281437, 'bidirectional': False, 'fc_dropout': 0.5732704374357874, 'learning_rate_model': 0.006642348358707951}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 3.292145920590197e-05
Epoch 24: reducing lr to 2.7572517929598372e-05
Epoch 27: reducing lr to 2.336429388521278e-05
Epoch 33: reducing lr to 1.4274639266328374e-05
Epoch 38: reducing lr to 7.365693258105851e-06
Epoch 42: reducing lr to 3.1499800714169818e-06
Epoch 45: reducing lr to 1.074074146396604e-06
Epoch 48: reducing lr to 7.951861677278214e-08
[I 2024-06-22 00:03:35,987] Trial 316 finished with value: 1.1081361770629883 and parameters: {'hidden_size': 97, 'n_layers': 6, 'rnn_dropout': 0.7359628616118871, 'bidirectional': False, 'fc_dropout': 0.052828467213309874, 'learning_rate_model': 0.00036567249474403436}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:04:23,797] Trial 317 finished with value: 1.106220006942749 and parameters: {'hidden_size': 102, 'n_layers': 7, 'rnn_dropout': 0.08844595220435299, 'bidirectional': True, 'fc_dropout': 0.4852283444132348, 'learning_rate_model': 1.3156239670034818e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 12: reducing lr to 0.00014877876302378705
Epoch 15: reducing lr to 0.0001458395629120532
Epoch 18: reducing lr to 0.00013841217177159188
Epoch 23: reducing lr to 0.00011744264255822058
Epoch 26: reducing lr to 0.00010099460053843192
Epoch 29: reducing lr to 8.287614149137038e-05
Epoch 32: reducing lr to 6.422571520524032e-05
Epoch 35: reducing lr to 4.621520338433215e-05
Epoch 38: reducing lr to 2.9976265333329444e-05
Epoch 41: reducing lr to 1.652924407327171e-05
Epoch 44: reducing lr to 6.7190849859677595e-06
Epoch 47: reducing lr to 1.1621850141171073e-06
[I 2024-06-22 00:04:26,835] Trial 318 finished with value: 1.0919275283813477 and parameters: {'hidden_size': 43, 'n_layers': 2, 'rnn_dropout': 0.003897638362884859, 'bidirectional': False, 'fc_dropout': 0.4375746474169927, 'learning_rate_model': 0.0014881824892021818}. Best is trial 153 with value: 0.9688200950622559.
Epoch 19: reducing lr to 1.85273135850492e-05
Epoch 22: reducing lr to 1.6792646693842422e-05
Epoch 25: reducing lr to 1.4644432851528699e-05
Epoch 28: reducing lr to 1.221765414787672e-05
Epoch 31: reducing lr to 9.66479333271606e-06
Epoch 34: reducing lr to 7.146258827923029e-06
Epoch 37: reducing lr to 4.820294682772989e-06
Epoch 40: reducing lr to 2.8330516274261186e-06
Epoch 43: reducing lr to 1.3093944631171235e-06
Epoch 46: reducing lr to 3.4506239182575827e-07
Epoch 49: reducing lr to 6.465972252495947e-10
[I 2024-06-22 00:04:38,284] Trial 319 finished with value: 1.0925689935684204 and parameters: {'hidden_size': 129, 'n_layers': 3, 'rnn_dropout': 0.29338558532875336, 'bidirectional': False, 'fc_dropout': 0.1612773075265369, 'learning_rate_model': 0.00020421903674544914}. Best is trial 153 with value: 0.9688200950622559.
Epoch 7: reducing lr to 0.00024721908520567576
Epoch 10: reducing lr to 0.0003336620455362283
Epoch 13: reducing lr to 0.000345962556791148
Epoch 16: reducing lr to 0.0003355354690510998
Epoch 22: reducing lr to 0.0002854452180112772
Epoch 29: reducing lr to 0.00019331839351806592
Epoch 33: reducing lr to 0.00013551042011652553
Epoch 36: reducing lr to 9.45924586772846e-05
Epoch 39: reducing lr to 5.863693270994979e-05
Epoch 42: reducing lr to 2.9903040971639975e-05
Epoch 45: reducing lr to 1.0196281398005495e-05
Epoch 48: reducing lr to 7.548773012697272e-07
[I 2024-06-22 00:04:52,861] Trial 320 finished with value: 1.092519760131836 and parameters: {'hidden_size': 153, 'n_layers': 3, 'rnn_dropout': 0.03808874296524403, 'bidirectional': False, 'fc_dropout': 0.39608989004894257, 'learning_rate_model': 0.0034713615148726346}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 1.1697037250509943e-05
Epoch 26: reducing lr to 1.0058847271022343e-05
Epoch 29: reducing lr to 8.254287310697412e-06
Epoch 32: reducing lr to 6.396744545524992e-06
Epoch 35: reducing lr to 4.602935899185347e-06
Epoch 38: reducing lr to 2.985572229961555e-06
Epoch 41: reducing lr to 1.646277531195558e-06
Epoch 44: reducing lr to 6.692065646534252e-07
Epoch 47: reducing lr to 1.1575115397606127e-07
[I 2024-06-22 00:06:50,690] Trial 321 finished with value: 1.0756374597549438 and parameters: {'hidden_size': 178, 'n_layers': 7, 'rnn_dropout': 0.245728614788076, 'bidirectional': True, 'fc_dropout': 0.29486149581222076, 'learning_rate_model': 0.0001482198086876757}. Best is trial 153 with value: 0.9688200950622559.
Epoch 7: reducing lr to 0.00350739663552296
Epoch 10: reducing lr to 0.004733797695844755
Epoch 13: reducing lr to 0.004908309998383309
Epoch 16: reducing lr to 0.0047603767090609195
Epoch 19: reducing lr to 0.004468059616691311
Epoch 22: reducing lr to 0.004049726162710865
Epoch 25: reducing lr to 0.0035316614431389628
Epoch 28: reducing lr to 0.0029464178310707905
Epoch 31: reducing lr to 0.0023307681707521216
Epoch 34: reducing lr to 0.001723396666920607
Epoch 37: reducing lr to 0.0011624655627370418
Epoch 40: reducing lr to 0.0006832206682526786
Epoch 43: reducing lr to 0.0003157744643397145
Epoch 46: reducing lr to 8.321548243236596e-05
Epoch 49: reducing lr to 1.5593382910877863e-07
[I 2024-06-22 00:06:56,074] Trial 322 finished with value: 1.1180570125579834 and parameters: {'hidden_size': 92, 'n_layers': 2, 'rnn_dropout': 0.2551888051761512, 'bidirectional': False, 'fc_dropout': 0.4031864075190263, 'learning_rate_model': 0.049249602585571886}. Best is trial 153 with value: 0.9688200950622559.
Epoch 12: reducing lr to 0.0007065347649118204
Epoch 18: reducing lr to 0.0006573049086847216
Epoch 21: reducing lr to 0.0006029353263441445
Epoch 24: reducing lr to 0.0005328842884629411
Epoch 27: reducing lr to 0.00045155343281490495
Epoch 30: reducing lr to 0.00036405295728694634
Epoch 33: reducing lr to 0.0002758808973458652
Epoch 36: reducing lr to 0.00019257745905887358
Epoch 39: reducing lr to 0.00011937686858117351
Epoch 42: reducing lr to 6.087854918855919e-05
Epoch 45: reducing lr to 2.0758250614630584e-05
Epoch 48: reducing lr to 1.5368281426715244e-06
[I 2024-06-22 00:07:45,691] Trial 323 finished with value: 1.034686803817749 and parameters: {'hidden_size': 154, 'n_layers': 4, 'rnn_dropout': 0.6036977802465054, 'bidirectional': True, 'fc_dropout': 0.4173274507306175, 'learning_rate_model': 0.007067222792988559}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:07:48,087] Trial 324 finished with value: 1.1043113470077515 and parameters: {'hidden_size': 72, 'n_layers': 1, 'rnn_dropout': 0.2146102633087491, 'bidirectional': False, 'fc_dropout': 0.5002649239226041, 'learning_rate_model': 3.398051356678928e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 0.0015589067629339325
Epoch 10: reducing lr to 0.0025109600494749714
Epoch 13: reducing lr to 0.002603527042821738
Epoch 16: reducing lr to 0.0025250584213591014
Epoch 19: reducing lr to 0.002370003941239881
Epoch 22: reducing lr to 0.002148106289968088
Epoch 25: reducing lr to 0.0018733079362003816
Epoch 28: reducing lr to 0.00156287571591274
Epoch 31: reducing lr to 0.0012363151400583993
Epoch 34: reducing lr to 0.0009141455672756077
Epoch 37: reducing lr to 0.0006166094908291741
Epoch 40: reducing lr to 0.00036240243313818423
Epoch 43: reducing lr to 0.00016749703209695092
Epoch 46: reducing lr to 4.414019468319714e-05
Epoch 49: reducing lr to 8.271236761892009e-08
[I 2024-06-22 00:08:08,224] Trial 325 finished with value: 1.0964393615722656 and parameters: {'hidden_size': 156, 'n_layers': 4, 'rnn_dropout': 0.6030952572969704, 'bidirectional': False, 'fc_dropout': 0.5236300276982652, 'learning_rate_model': 0.02612358881610851}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:09:24,727] Trial 326 finished with value: 1.0888476371765137 and parameters: {'hidden_size': 153, 'n_layers': 6, 'rnn_dropout': 0.6870326050135719, 'bidirectional': True, 'fc_dropout': 0.31660020925214455, 'learning_rate_model': 2.1681290038658853e-05}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:10:22,562] Trial 327 finished with value: 1.0957907438278198 and parameters: {'hidden_size': 169, 'n_layers': 4, 'rnn_dropout': 0.4336609237912984, 'bidirectional': True, 'fc_dropout': 0.7023683349801254, 'learning_rate_model': 1.375738861706401e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 8: reducing lr to 0.0028492185864670396
Epoch 11: reducing lr to 0.0034740141636881465
Epoch 14: reducing lr to 0.0034588887783388172
Epoch 21: reducing lr to 0.0029806735668585745
Epoch 26: reducing lr to 0.002371015724934435
Epoch 30: reducing lr to 0.0017997337016251158
Epoch 33: reducing lr to 0.0013638459423269624
Epoch 36: reducing lr to 0.0009520267211245454
Epoch 39: reducing lr to 0.0005901519800336861
Epoch 42: reducing lr to 0.0003009594469365426
Epoch 45: reducing lr to 0.00010262057338126241
Epoch 48: reducing lr to 7.5974699466366254e-06
[I 2024-06-22 00:11:16,181] Trial 328 finished with value: 1.0124757289886475 and parameters: {'hidden_size': 144, 'n_layers': 5, 'rnn_dropout': 0.5133993446668773, 'bidirectional': True, 'fc_dropout': 0.4075268651510099, 'learning_rate_model': 0.03493755175681622}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0003825094981229683
Epoch 23: reducing lr to 0.00032455907373024997
Epoch 26: reducing lr to 0.00027910402293835024
Epoch 29: reducing lr to 0.0002290326846438413
Epoch 32: reducing lr to 0.00017749122620722697
Epoch 35: reducing lr to 0.0001277182058913408
Epoch 38: reducing lr to 8.284102518942012e-05
Epoch 41: reducing lr to 4.567945704408694e-05
Epoch 44: reducing lr to 1.856855356672906e-05
Epoch 47: reducing lr to 3.2117609368167812e-06
[I 2024-06-22 00:11:56,087] Trial 329 finished with value: 1.0390115976333618 and parameters: {'hidden_size': 120, 'n_layers': 5, 'rnn_dropout': 0.3531033847628007, 'bidirectional': True, 'fc_dropout': 0.28392200868061873, 'learning_rate_model': 0.004112672532871488}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.00234761005858216
Epoch 12: reducing lr to 0.0026068973658865055
Epoch 15: reducing lr to 0.0025553967829177707
Epoch 26: reducing lr to 0.0017696245940040773
Epoch 29: reducing lr to 0.001452153456297713
Epoch 32: reducing lr to 0.001125361203359036
Epoch 35: reducing lr to 0.0008097815139601766
Epoch 38: reducing lr to 0.0005252432911011469
Epoch 41: reducing lr to 0.0002896249569423949
Epoch 44: reducing lr to 0.00011773162106668705
Epoch 47: reducing lr to 2.0363773635423062e-05
[I 2024-06-22 00:12:19,181] Trial 330 finished with value: 1.1076328754425049 and parameters: {'hidden_size': 116, 'n_layers': 7, 'rnn_dropout': 0.22004324085232937, 'bidirectional': False, 'fc_dropout': 0.3096369860840687, 'learning_rate_model': 0.026075892366703723}. Best is trial 153 with value: 0.9688200950622559.
Epoch 24: reducing lr to 1.1489106912718192e-05
Epoch 27: reducing lr to 9.735595097726631e-06
Epoch 34: reducing lr to 5.331932662367352e-06
Epoch 42: reducing lr to 1.3125554186181843e-06
Epoch 45: reducing lr to 4.475526222032244e-07
Epoch 48: reducing lr to 3.313436560225104e-08
[I 2024-06-22 00:12:37,699] Trial 331 finished with value: 1.0743324756622314 and parameters: {'hidden_size': 66, 'n_layers': 5, 'rnn_dropout': 0.49884144083644666, 'bidirectional': True, 'fc_dropout': 0.5958670984259942, 'learning_rate_model': 0.000152370936810405}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 0.005104382792260462
Epoch 9: reducing lr to 0.0077009218484427305
Epoch 12: reducing lr to 0.008551468250961492
Epoch 15: reducing lr to 0.008382529647575632
Epoch 18: reducing lr to 0.007955619931200503
Epoch 21: reducing lr to 0.007297563484025524
Epoch 24: reducing lr to 0.006449708210459791
Epoch 27: reducing lr to 0.005465328864335874
Epoch 30: reducing lr to 0.004406276181323513
Epoch 36: reducing lr to 0.0023308407579892077
Epoch 39: reducing lr to 0.0014448652101337362
Epoch 42: reducing lr to 0.0007368370339363772
Epoch 45: reducing lr to 0.0002512452746733132
Epoch 48: reducing lr to 1.8600835686946592e-05
[I 2024-06-22 00:12:44,993] Trial 332 finished with value: 1.1054787635803223 and parameters: {'hidden_size': 89, 'n_layers': 3, 'rnn_dropout': 0.5308821053368749, 'bidirectional': False, 'fc_dropout': 0.012337457874604675, 'learning_rate_model': 0.08553737811366681}. Best is trial 153 with value: 0.9688200950622559.
Epoch 5: reducing lr to 0.0005546616993361103
Epoch 8: reducing lr to 0.0009492964980479621
Epoch 11: reducing lr to 0.0011574645397240137
Epoch 14: reducing lr to 0.0011524250964844033
Epoch 17: reducing lr to 0.0011057307371087914
Epoch 20: reducing lr to 0.0010261301327093883
Epoch 23: reducing lr to 0.000918624879080511
Epoch 31: reducing lr to 0.0005508900482186008
Epoch 40: reducing lr to 0.00016148301302579062
Epoch 43: reducing lr to 7.463505468678755e-05
Epoch 46: reducing lr to 1.9668443093122898e-05
Epoch 49: reducing lr to 3.6855829642183645e-08
[I 2024-06-22 00:12:56,569] Trial 333 finished with value: 1.0908122062683105 and parameters: {'hidden_size': 126, 'n_layers': 3, 'rnn_dropout': 0.11739834458198617, 'bidirectional': False, 'fc_dropout': 0.25448700187569345, 'learning_rate_model': 0.011640418074851954}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.00445502112006284
Epoch 15: reducing lr to 0.0048493345802560085
Epoch 18: reducing lr to 0.0046023652121412245
Epoch 21: reducing lr to 0.004221676324751608
Epoch 24: reducing lr to 0.0037311878839091893
Epoch 27: reducing lr to 0.00316171959641806
Epoch 30: reducing lr to 0.002549052416704586
Epoch 33: reducing lr to 0.001931682888508493
Epoch 36: reducing lr to 0.0013484028287399155
Epoch 39: reducing lr to 0.0008358616219552327
Epoch 42: reducing lr to 0.00042626384384030906
Epoch 45: reducing lr to 0.00014534662563962312
Epoch 48: reducing lr to 1.0760674821407023e-05
[I 2024-06-22 00:13:09,203] Trial 334 finished with value: 1.1072473526000977 and parameters: {'hidden_size': 69, 'n_layers': 7, 'rnn_dropout': 0.561852603835846, 'bidirectional': False, 'fc_dropout': 0.21523390980153484, 'learning_rate_model': 0.04948379344068349}. Best is trial 153 with value: 0.9688200950622559.
Epoch 21: reducing lr to 0.0002403623614513854
Epoch 27: reducing lr to 0.00018001341883709668
Epoch 30: reducing lr to 0.00014513103592292203
Epoch 33: reducing lr to 0.00010998092343909216
Epoch 36: reducing lr to 7.677170469072461e-05
Epoch 39: reducing lr to 4.759002297779562e-05
Epoch 42: reducing lr to 2.426945512286039e-05
Epoch 45: reducing lr to 8.27535213036158e-06
Epoch 48: reducing lr to 6.126621303768377e-07
[I 2024-06-22 00:14:17,449] Trial 335 finished with value: 0.9950758218765259 and parameters: {'hidden_size': 141, 'n_layers': 6, 'rnn_dropout': 0.4573082691479611, 'bidirectional': True, 'fc_dropout': 0.1172869614215272, 'learning_rate_model': 0.0028173740784533195}. Best is trial 153 with value: 0.9688200950622559.
Epoch 11: reducing lr to 0.00016900142900295139
Epoch 23: reducing lr to 0.00013412844364051738
Epoch 37: reducing lr to 4.011701643566344e-05
Epoch 40: reducing lr to 2.357813913467107e-05
Epoch 43: reducing lr to 1.0897466369715362e-05
Epoch 46: reducing lr to 2.871789912279785e-06
Epoch 49: reducing lr to 5.3813206909005335e-09
[I 2024-06-22 00:14:44,211] Trial 336 finished with value: 1.0254802703857422 and parameters: {'hidden_size': 67, 'n_layers': 7, 'rnn_dropout': 0.15858583216812303, 'bidirectional': True, 'fc_dropout': 0.35806331524051055, 'learning_rate_model': 0.0016996177604808832}. Best is trial 153 with value: 0.9688200950622559.
Epoch 3: reducing lr to 0.00010391079361019191
Epoch 9: reducing lr to 0.0003718420677341368
Epoch 13: reducing lr to 0.0004116242886471158
Epoch 16: reducing lr to 0.000399218198770027
Epoch 27: reducing lr to 0.000263895573251749
Epoch 30: reducing lr to 0.00021275879414388998
Epoch 33: reducing lr to 0.00016122952958290727
Epoch 36: reducing lr to 0.00011254557104549423
Epoch 39: reducing lr to 6.976589009819534e-05
Epoch 42: reducing lr to 3.557846861377981e-05
Epoch 45: reducing lr to 1.2131477799875133e-05
Epoch 48: reducing lr to 8.981487332995059e-07
[I 2024-06-22 00:15:08,712] Trial 337 finished with value: 1.1074230670928955 and parameters: {'hidden_size': 122, 'n_layers': 7, 'rnn_dropout': 0.27793114624048404, 'bidirectional': False, 'fc_dropout': 0.4614634425856199, 'learning_rate_model': 0.004130206249628986}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:15:24,078] Trial 338 finished with value: 0.9926725029945374 and parameters: {'hidden_size': 67, 'n_layers': 4, 'rnn_dropout': 0.4967619100643778, 'bidirectional': True, 'fc_dropout': 0.09072880090466091, 'learning_rate_model': 0.0008125391685209514}. Best is trial 153 with value: 0.9688200950622559.
Epoch 20: reducing lr to 1.0652261948652606e-05
[I 2024-06-22 00:15:28,363] Trial 339 finished with value: 1.1102083921432495 and parameters: {'hidden_size': 20, 'n_layers': 5, 'rnn_dropout': 0.07806609438086759, 'bidirectional': False, 'fc_dropout': 0.09266139736499923, 'learning_rate_model': 0.00012083923722009125}. Best is trial 153 with value: 0.9688200950622559.
Epoch 20: reducing lr to 5.694667452219021e-05
Epoch 23: reducing lr to 5.098050464501828e-05
Epoch 27: reducing lr to 4.12757402912709e-05
Epoch 32: reducing lr to 2.7879646617515485e-05
Epoch 35: reducing lr to 2.0061489928050775e-05
Epoch 38: reducing lr to 1.301235310086381e-05
Epoch 41: reducing lr to 7.175155343071629e-06
Epoch 44: reducing lr to 2.916677757549527e-06
Epoch 47: reducing lr to 5.044911900819668e-07
[I 2024-06-22 00:15:38,527] Trial 340 finished with value: 1.0772879123687744 and parameters: {'hidden_size': 19, 'n_layers': 7, 'rnn_dropout': 0.4892588864971285, 'bidirectional': True, 'fc_dropout': 0.3118709986999874, 'learning_rate_model': 0.0006460029564286756}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0013017306911381646
Epoch 22: reducing lr to 0.001150869230188911
Epoch 25: reducing lr to 0.0010036432892125365
Epoch 28: reducing lr to 0.0008373261511562162
Epoch 31: reducing lr to 0.000662368086791014
Epoch 34: reducing lr to 0.0004897625458313391
Epoch 37: reducing lr to 0.0003303546446243543
Epoch 40: reducing lr to 0.00019416069455786868
Epoch 43: reducing lr to 8.973819465479468e-05
Epoch 46: reducing lr to 2.3648546681642214e-05
Epoch 49: reducing lr to 4.4313970539236466e-08
[I 2024-06-22 00:16:08,448] Trial 341 finished with value: 1.116574764251709 and parameters: {'hidden_size': 188, 'n_layers': 2, 'rnn_dropout': 0.4918255128666393, 'bidirectional': True, 'fc_dropout': 0.4046106791891455, 'learning_rate_model': 0.013995971563871303}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.004541540763619197
Epoch 12: reducing lr to 0.005043141901042731
Epoch 15: reducing lr to 0.004943512068546646
Epoch 18: reducing lr to 0.004691746381599074
Epoch 21: reducing lr to 0.004303664248261759
Epoch 26: reducing lr to 0.003423405944523185
Epoch 29: reducing lr to 0.002809245978776234
Epoch 33: reducing lr to 0.0019691975288375855
Epoch 36: reducing lr to 0.0013745897600627721
Epoch 39: reducing lr to 0.0008520946425504266
Epoch 42: reducing lr to 0.0004345421874970744
Epoch 45: reducing lr to 0.00014816935933797265
Epoch 48: reducing lr to 1.096965469487704e-05
[I 2024-06-22 00:16:13,230] Trial 342 finished with value: 1.1103856563568115 and parameters: {'hidden_size': 33, 'n_layers': 5, 'rnn_dropout': 0.020847946150714593, 'bidirectional': False, 'fc_dropout': 0.38512036731922916, 'learning_rate_model': 0.05044480351334593}. Best is trial 153 with value: 0.9688200950622559.
Epoch 7: reducing lr to 0.004033009311338331
Epoch 10: reducing lr to 0.0054431968121242316
Epoch 13: reducing lr to 0.005643861240536145
Epoch 16: reducing lr to 0.005473758912430003
Epoch 19: reducing lr to 0.005137635662652693
Epoch 22: reducing lr to 0.00465661144712484
Epoch 25: reducing lr to 0.004060910403009014
Epoch 28: reducing lr to 0.0033879631483509183
Epoch 31: reducing lr to 0.0026800532451935694
Epoch 34: reducing lr to 0.0019816620494031807
Epoch 37: reducing lr to 0.0013366707349679644
Epoch 40: reducing lr to 0.000785606990910228
Epoch 43: reducing lr to 0.0003630959048277288
Epoch 46: reducing lr to 9.568601740053904e-05
Epoch 49: reducing lr to 1.7930181559072667e-07
[I 2024-06-22 00:16:17,913] Trial 343 finished with value: 1.1478122472763062 and parameters: {'hidden_size': 91, 'n_layers': 1, 'rnn_dropout': 0.689081979586423, 'bidirectional': True, 'fc_dropout': 0.7704258710277405, 'learning_rate_model': 0.0566300668124204}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:16:24,723] Trial 344 finished with value: 1.0835965871810913 and parameters: {'hidden_size': 64, 'n_layers': 2, 'rnn_dropout': 0.4711269626448618, 'bidirectional': True, 'fc_dropout': 0.4691435371714093, 'learning_rate_model': 4.456357235021669e-05}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:16:32,783] Trial 345 finished with value: 1.1106394529342651 and parameters: {'hidden_size': 51, 'n_layers': 6, 'rnn_dropout': 0.12007876908163997, 'bidirectional': False, 'fc_dropout': 0.012071446106383289, 'learning_rate_model': 1.345171370168811e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00033237674164450357
Epoch 21: reducing lr to 0.0003048838926117855
Epoch 24: reducing lr to 0.0002694614647367824
Epoch 27: reducing lr to 0.00022833521657054516
Epoch 30: reducing lr to 0.00018408920141979373
Epoch 33: reducing lr to 0.0001395035888675569
Epoch 36: reducing lr to 9.737987273554342e-05
Epoch 39: reducing lr to 6.036482320835104e-05
Epoch 42: reducing lr to 3.0784212239990314e-05
Epoch 45: reducing lr to 1.0496741482331293e-05
Epoch 48: reducing lr to 7.77121734190072e-07
[I 2024-06-22 00:16:45,161] Trial 346 finished with value: 0.996293306350708 and parameters: {'hidden_size': 69, 'n_layers': 3, 'rnn_dropout': 0.28977288442400895, 'bidirectional': True, 'fc_dropout': 0.28671619570026013, 'learning_rate_model': 0.003573654255997655}. Best is trial 153 with value: 0.9688200950622559.
Epoch 3: reducing lr to 1.0335117906028553e-05
Epoch 6: reducing lr to 2.4513979303489207e-05
Epoch 9: reducing lr to 3.698395016469995e-05
Epoch 12: reducing lr to 4.1068729413547914e-05
Epoch 15: reducing lr to 4.025739578213596e-05
Epoch 18: reducing lr to 3.820714673585629e-05
Epoch 21: reducing lr to 3.5046807321062665e-05
Epoch 24: reducing lr to 3.097495231440862e-05
Epoch 27: reducing lr to 2.624743560969502e-05
Epoch 30: reducing lr to 2.1161297557503075e-05
Epoch 33: reducing lr to 1.6036122334161695e-05
Epoch 36: reducing lr to 1.1193945365483264e-05
Epoch 39: reducing lr to 6.93901639023914e-06
Epoch 42: reducing lr to 3.538685975383452e-06
Epoch 45: reducing lr to 1.2066143379332146e-06
Epoch 48: reducing lr to 8.933117276172095e-08
[I 2024-06-22 00:16:52,805] Trial 347 finished with value: 1.1074364185333252 and parameters: {'hidden_size': 42, 'n_layers': 7, 'rnn_dropout': 0.5164515266187463, 'bidirectional': False, 'fc_dropout': 0.5963986645985317, 'learning_rate_model': 0.0004107962905784676}. Best is trial 153 with value: 0.9688200950622559.
Epoch 24: reducing lr to 5.893019211035702e-06
Epoch 28: reducing lr to 4.675685886882183e-06
Epoch 34: reducing lr to 2.7348671963785956e-06
Epoch 42: reducing lr to 6.732389520114557e-07
Epoch 45: reducing lr to 2.29559723016714e-07
Epoch 48: reducing lr to 1.699535521106489e-08
[I 2024-06-22 00:17:41,017] Trial 348 finished with value: 1.0757604837417603 and parameters: {'hidden_size': 105, 'n_layers': 7, 'rnn_dropout': 0.10995483969938472, 'bidirectional': True, 'fc_dropout': 0.07955220580390786, 'learning_rate_model': 7.815445226932655e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 12: reducing lr to 0.0009142380547031105
Epoch 18: reducing lr to 0.0008505358701460719
Epoch 21: reducing lr to 0.0007801830104389158
Epoch 24: reducing lr to 0.0006895387452406727
Epoch 27: reducing lr to 0.000584298682121797
Epoch 30: reducing lr to 0.0004710752874566216
Epoch 33: reducing lr to 0.00035698287960495603
Epoch 36: reducing lr to 0.00024919034461329866
Epoch 39: reducing lr to 0.00015447063828744802
Epoch 42: reducing lr to 7.877529761786371e-05
Epoch 45: reducing lr to 2.6860649473247136e-05
Epoch 48: reducing lr to 1.9886166135709565e-06
[I 2024-06-22 00:18:36,428] Trial 349 finished with value: 1.0734050273895264 and parameters: {'hidden_size': 165, 'n_layers': 4, 'rnn_dropout': 0.5226009138090556, 'bidirectional': True, 'fc_dropout': 0.41667867244303136, 'learning_rate_model': 0.009144806935610207}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 0.0006342369309615313
Epoch 18: reducing lr to 0.0009885128475692747
Epoch 21: reducing lr to 0.0009067470947952984
Epoch 24: reducing lr to 0.0008013981920011679
Epoch 27: reducing lr to 0.0006790857086321318
Epoch 30: reducing lr to 0.0005474948090587715
Epoch 33: reducing lr to 0.0004148939218650124
Epoch 36: reducing lr to 0.0002896148954871899
Epoch 39: reducing lr to 0.00017952941889816404
Epoch 42: reducing lr to 9.15545087510277e-05
Epoch 45: reducing lr to 3.1218080307182824e-05
Epoch 48: reducing lr to 2.311217128404992e-06
[I 2024-06-22 00:19:23,587] Trial 350 finished with value: 1.0217291116714478 and parameters: {'hidden_size': 101, 'n_layers': 7, 'rnn_dropout': 0.5283295477365123, 'bidirectional': True, 'fc_dropout': 0.2992373734936557, 'learning_rate_model': 0.01062831029435455}. Best is trial 153 with value: 0.9688200950622559.
Epoch 7: reducing lr to 2.5685139709271938e-05
Epoch 10: reducing lr to 3.4666240465008904e-05
Epoch 13: reducing lr to 3.594421764794053e-05
Epoch 24: reducing lr to 2.7194677615593137e-05
Epoch 27: reducing lr to 2.3044121017408685e-05
Epoch 30: reducing lr to 1.857871028057212e-05
Epoch 33: reducing lr to 1.407902658429214e-05
Epoch 36: reducing lr to 9.827803199531492e-06
Epoch 39: reducing lr to 6.092158328007845e-06
Epoch 42: reducing lr to 3.1068142835722397e-06
Epoch 45: reducing lr to 1.0593555590780452e-06
Epoch 48: reducing lr to 7.842893250066454e-08
[I 2024-06-22 00:20:11,754] Trial 351 finished with value: 1.1077278852462769 and parameters: {'hidden_size': 189, 'n_layers': 7, 'rnn_dropout': 0.20928863414633742, 'bidirectional': False, 'fc_dropout': 0.4374675646526467, 'learning_rate_model': 0.00036066149754058905}. Best is trial 153 with value: 0.9688200950622559.
Epoch 37: reducing lr to 2.0829347283740942e-05
[I 2024-06-22 00:20:21,770] Trial 352 finished with value: 1.033921480178833 and parameters: {'hidden_size': 34, 'n_layers': 5, 'rnn_dropout': 0.07050813114034708, 'bidirectional': True, 'fc_dropout': 0.5667606332122982, 'learning_rate_model': 0.0008824666370552557}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:20:53,842] Trial 353 finished with value: 1.0222136974334717 and parameters: {'hidden_size': 145, 'n_layers': 3, 'rnn_dropout': 0.637996787434679, 'bidirectional': True, 'fc_dropout': 0.2542540402804782, 'learning_rate_model': 0.0002376472029630957}. Best is trial 153 with value: 0.9688200950622559.
Epoch 42: reducing lr to 5.480070397676916e-06
[I 2024-06-22 00:21:13,851] Trial 354 finished with value: 0.9832900762557983 and parameters: {'hidden_size': 83, 'n_layers': 4, 'rnn_dropout': 0.03334995401623022, 'bidirectional': True, 'fc_dropout': 0.3331700449384865, 'learning_rate_model': 0.0006361662512963186}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:21:19,976] Trial 355 finished with value: 1.1033947467803955 and parameters: {'hidden_size': 80, 'n_layers': 3, 'rnn_dropout': 0.1442448486782075, 'bidirectional': False, 'fc_dropout': 0.4766935564933209, 'learning_rate_model': 3.8194352935715024e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 29: reducing lr to 1.3165995597968708e-05
Epoch 32: reducing lr to 1.0203123220410001e-05
Epoch 35: reducing lr to 7.341909907578208e-06
Epoch 38: reducing lr to 4.76213504055631e-06
Epoch 41: reducing lr to 2.625893903725069e-06
Epoch 44: reducing lr to 1.067417495019852e-06
Epoch 47: reducing lr to 1.8462880274758482e-07
[I 2024-06-22 00:21:24,795] Trial 356 finished with value: 1.0912530422210693 and parameters: {'hidden_size': 58, 'n_layers': 3, 'rnn_dropout': 0.6646010935235407, 'bidirectional': False, 'fc_dropout': 0.45451177415375393, 'learning_rate_model': 0.00023641790929480275}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.000237350375929934
Epoch 23: reducing lr to 0.00020805241755197254
Epoch 40: reducing lr to 3.657306918055706e-05
Epoch 47: reducing lr to 2.0588382257311078e-06
[I 2024-06-22 00:21:55,732] Trial 357 finished with value: 0.9848207235336304 and parameters: {'hidden_size': 80, 'n_layers': 7, 'rnn_dropout': 0.4426751595676166, 'bidirectional': True, 'fc_dropout': 0.5173199853374325, 'learning_rate_model': 0.0026363504591914843}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.0022100250227947366
Epoch 14: reducing lr to 0.002430270067068283
Epoch 17: reducing lr to 0.0023317995424002058
Epoch 22: reducing lr to 0.0020185212996816524
Epoch 27: reducing lr to 0.001568450347334342
Epoch 30: reducing lr to 0.00126452141830768
Epoch 33: reducing lr to 0.0009582597713134902
Epoch 36: reducing lr to 0.0006689090605883335
Epoch 39: reducing lr to 0.00041465013303658633
Epoch 42: reducing lr to 0.00021145887658248936
Epoch 45: reducing lr to 7.210284103834168e-05
Epoch 48: reducing lr to 5.3381027780921575e-06
[I 2024-06-22 00:21:59,596] Trial 358 finished with value: 1.1051201820373535 and parameters: {'hidden_size': 42, 'n_layers': 3, 'rnn_dropout': 0.3683471947671468, 'bidirectional': False, 'fc_dropout': 0.1825133074343378, 'learning_rate_model': 0.02454767750352977}. Best is trial 153 with value: 0.9688200950622559.
Epoch 12: reducing lr to 0.0005985666408442157
Epoch 22: reducing lr to 0.0004923236323320088
Epoch 25: reducing lr to 0.0004293427061471209
Epoch 28: reducing lr to 0.000358194868165988
Epoch 31: reducing lr to 0.0002833505787414503
Epoch 34: reducing lr to 0.00020951266157691749
Epoch 37: reducing lr to 0.00014132048571019237
Epoch 40: reducing lr to 8.305887054182934e-05
Epoch 43: reducing lr to 3.8388578643390635e-05
Epoch 46: reducing lr to 1.011647378891872e-05
Epoch 49: reducing lr to 1.895681487227782e-08
[I 2024-06-22 00:22:09,291] Trial 359 finished with value: 1.019996166229248 and parameters: {'hidden_size': 79, 'n_layers': 2, 'rnn_dropout': 0.3265451222818258, 'bidirectional': True, 'fc_dropout': 0.14458337088239182, 'learning_rate_model': 0.005987255004818895}. Best is trial 153 with value: 0.9688200950622559.
Epoch 29: reducing lr to 2.232194170050184e-06
Epoch 34: reducing lr to 1.4026236414696698e-06
Epoch 37: reducing lr to 9.460977336127298e-07
Epoch 40: reducing lr to 5.560539137772743e-07
Epoch 43: reducing lr to 2.5699987562741923e-07
Epoch 46: reducing lr to 6.772671970202744e-08
Epoch 49: reducing lr to 1.269101184947876e-10
[I 2024-06-22 00:22:34,363] Trial 360 finished with value: 1.1065819263458252 and parameters: {'hidden_size': 155, 'n_layers': 5, 'rnn_dropout': 0.7033588484292764, 'bidirectional': False, 'fc_dropout': 0.7338081272640069, 'learning_rate_model': 4.0082853962425134e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 16: reducing lr to 0.0007082585357546315
Epoch 19: reducing lr to 0.0006647670037874919
Epoch 23: reducing lr to 0.000578259598741552
Epoch 26: reducing lr to 0.0004972733575325115
Epoch 29: reducing lr to 0.00040806238075144046
Epoch 32: reducing lr to 0.0003162321239924438
Epoch 35: reducing lr to 0.0002275526536414138
Epoch 38: reducing lr to 0.00014759599056899856
Epoch 41: reducing lr to 8.138606077918364e-05
Epoch 44: reducing lr to 3.3083174077677865e-05
Epoch 47: reducing lr to 5.722322193096527e-06
[I 2024-06-22 00:22:48,959] Trial 361 finished with value: 1.0449212789535522 and parameters: {'hidden_size': 61, 'n_layers': 4, 'rnn_dropout': 0.27529546958079804, 'bidirectional': True, 'fc_dropout': 0.12861131385876787, 'learning_rate_model': 0.007327456112320097}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 8.829746481283583e-05
Epoch 9: reducing lr to 0.0001332133391269715
Epoch 12: reducing lr to 0.00014792639927637034
Epoch 15: reducing lr to 0.00014500403804386042
Epoch 18: reducing lr to 0.00013761919893715276
Epoch 21: reducing lr to 0.00012623589984810444
Epoch 24: reducing lr to 0.00011156939182336155
Epoch 27: reducing lr to 9.454124087656167e-05
Epoch 30: reducing lr to 7.622136346552371e-05
Epoch 33: reducing lr to 5.7760877171558746e-05
Epoch 36: reducing lr to 4.031972878776485e-05
Epoch 39: reducing lr to 2.499380243278673e-05
Epoch 42: reducing lr to 1.2746074251218046e-05
Epoch 45: reducing lr to 4.3461318836618626e-06
Epoch 48: reducing lr to 3.2176400191767825e-07
[I 2024-06-22 00:23:39,333] Trial 362 finished with value: 1.1072924137115479 and parameters: {'hidden_size': 195, 'n_layers': 7, 'rnn_dropout': 0.7208535947382511, 'bidirectional': False, 'fc_dropout': 0.057811805451898124, 'learning_rate_model': 0.0014796565895539}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:24:04,117] Trial 363 finished with value: 1.1065016984939575 and parameters: {'hidden_size': 179, 'n_layers': 4, 'rnn_dropout': 0.625372228316752, 'bidirectional': False, 'fc_dropout': 0.1499398165802812, 'learning_rate_model': 1.655985402893686e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 24: reducing lr to 2.748797075335893e-05
Epoch 27: reducing lr to 2.3292650625142483e-05
Epoch 30: reducing lr to 1.8779080673295814e-05
Epoch 33: reducing lr to 1.4230868129978543e-05
Epoch 36: reducing lr to 9.933795529298345e-06
Epoch 39: reducing lr to 6.157861928434534e-06
Epoch 42: reducing lr to 3.1403211087887348e-06
Epoch 45: reducing lr to 1.0707806518967053e-06
Epoch 48: reducing lr to 7.927478432615486e-08
[I 2024-06-22 00:24:14,740] Trial 364 finished with value: 1.091588020324707 and parameters: {'hidden_size': 93, 'n_layers': 4, 'rnn_dropout': 0.7765936076171157, 'bidirectional': False, 'fc_dropout': 0.08408565526563959, 'learning_rate_model': 0.0003645512124245168}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:24:31,186] Trial 365 finished with value: 1.08984375 and parameters: {'hidden_size': 69, 'n_layers': 4, 'rnn_dropout': 0.2786053454005976, 'bidirectional': True, 'fc_dropout': 0.6888090948911805, 'learning_rate_model': 3.2961258390009616e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 15: reducing lr to 3.9773589613378615e-05
Epoch 18: reducing lr to 3.77479800927515e-05
Epoch 21: reducing lr to 3.462562107074132e-05
Epoch 24: reducing lr to 3.060270088791854e-05
Epoch 27: reducing lr to 2.5931998631835007e-05
Epoch 30: reducing lr to 2.090698487536548e-05
Epoch 33: reducing lr to 1.58434030894743e-05
Epoch 36: reducing lr to 1.105941853593219e-05
Epoch 39: reducing lr to 6.855624534668703e-06
Epoch 42: reducing lr to 3.496158681431038e-06
Epoch 45: reducing lr to 1.1921134630340482e-06
Epoch 48: reducing lr to 8.825760673479674e-08
[I 2024-06-22 00:24:38,404] Trial 366 finished with value: 1.092056393623352 and parameters: {'hidden_size': 122, 'n_layers': 2, 'rnn_dropout': 0.6945908571527113, 'bidirectional': False, 'fc_dropout': 0.3008456604378525, 'learning_rate_model': 0.0004058594143691851}. Best is trial 153 with value: 0.9688200950622559.
Epoch 14: reducing lr to 0.0009655292012079213
Epoch 22: reducing lr to 0.0008019443124911114
Epoch 27: reducing lr to 0.0006231342892779267
Epoch 30: reducing lr to 0.0005023854638516674
Epoch 33: reducing lr to 0.0003807098659870889
Epoch 36: reducing lr to 0.0002657528641373208
Epoch 39: reducing lr to 0.00016473758087904108
Epoch 42: reducing lr to 8.401112409754212e-05
Epoch 45: reducing lr to 2.8645951516225418e-05
Epoch 48: reducing lr to 2.120790681306786e-06
[I 2024-06-22 00:24:40,832] Trial 367 finished with value: 1.0940930843353271 and parameters: {'hidden_size': 67, 'n_layers': 1, 'rnn_dropout': 0.09498432409007487, 'bidirectional': False, 'fc_dropout': 0.16592525185545917, 'learning_rate_model': 0.009752619584408855}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 0.0007908000559231169
Epoch 13: reducing lr to 0.000819953042061054
Epoch 16: reducing lr to 0.0007952401876076994
Epoch 22: reducing lr to 0.0006765231388650557
Epoch 27: reducing lr to 0.0005256783529110959
Epoch 30: reducing lr to 0.0004238142045915116
Epoch 33: reducing lr to 0.00032116822767208636
Epoch 36: reducing lr to 0.0002241900827877642
Epoch 39: reducing lr to 0.00013897322241631337
Epoch 42: reducing lr to 7.087208985559183e-05
Epoch 45: reducing lr to 2.416582889070342e-05
Epoch 48: reducing lr to 1.7891067325317978e-06
[I 2024-06-22 00:24:43,329] Trial 368 finished with value: 1.093526005744934 and parameters: {'hidden_size': 63, 'n_layers': 1, 'rnn_dropout': 0.012841767333649657, 'bidirectional': False, 'fc_dropout': 0.6047208780302441, 'learning_rate_model': 0.008227345353826203}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:25:02,303] Trial 369 finished with value: 1.1043027639389038 and parameters: {'hidden_size': 81, 'n_layers': 4, 'rnn_dropout': 0.6512590027005323, 'bidirectional': True, 'fc_dropout': 0.0034120147256857082, 'learning_rate_model': 1.5407187680771952e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.00011561854102722243
Epoch 12: reducing lr to 0.00012838830237145137
Epoch 15: reducing lr to 0.00012585192617765845
Epoch 18: reducing lr to 0.00011944247552629014
Epoch 22: reducing lr to 0.00010559992999827734
Epoch 25: reducing lr to 9.209096768247744e-05
Epoch 28: reducing lr to 7.683026066593973e-05
Epoch 31: reducing lr to 6.077669101183857e-05
Epoch 34: reducing lr to 4.493898107526782e-05
Epoch 37: reducing lr to 3.0312242635256898e-05
Epoch 40: reducing lr to 1.7815539086367165e-05
Epoch 43: reducing lr to 8.23407805608167e-06
Epoch 46: reducing lr to 2.1699119314646077e-06
Epoch 49: reducing lr to 4.066102441638991e-09
[I 2024-06-22 00:25:11,515] Trial 370 finished with value: 1.0923725366592407 and parameters: {'hidden_size': 152, 'n_layers': 2, 'rnn_dropout': 0.38518864974932177, 'bidirectional': False, 'fc_dropout': 0.4027020877092425, 'learning_rate_model': 0.0012842237663788143}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:25:19,110] Trial 371 finished with value: 1.1068572998046875 and parameters: {'hidden_size': 69, 'n_layers': 4, 'rnn_dropout': 0.0015709163060160236, 'bidirectional': False, 'fc_dropout': 0.22716070903122879, 'learning_rate_model': 2.2070334240387426e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 17: reducing lr to 3.752483915154613e-06
Epoch 20: reducing lr to 3.4823458267205596e-06
Epoch 23: reducing lr to 3.117508600533107e-06
Epoch 26: reducing lr to 2.680896213910409e-06
Epoch 29: reducing lr to 2.199942697562037e-06
Epoch 32: reducing lr to 1.7048681395981723e-06
Epoch 35: reducing lr to 1.2267800765349638e-06
Epoch 38: reducing lr to 7.95718343464469e-07
Epoch 41: reducing lr to 4.3876789074454093e-07
Epoch 44: reducing lr to 1.783577478775085e-07
Epoch 47: reducing lr to 3.085013839947175e-08
[I 2024-06-22 00:25:36,778] Trial 372 finished with value: 1.1079497337341309 and parameters: {'hidden_size': 112, 'n_layers': 6, 'rnn_dropout': 0.7540787484865374, 'bidirectional': False, 'fc_dropout': 0.2326879502483765, 'learning_rate_model': 3.950372375988253e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 17: reducing lr to 3.0116783600032303e-06
Epoch 20: reducing lr to 2.7948702260992226e-06
Epoch 23: reducing lr to 2.502058210411453e-06
Epoch 26: reducing lr to 2.151640698642648e-06
Epoch 29: reducing lr to 1.7656357669481782e-06
Epoch 32: reducing lr to 1.368297532722462e-06
Epoch 35: reducing lr to 9.845923640238246e-07
Epoch 38: reducing lr to 6.386297103077169e-07
Epoch 41: reducing lr to 3.521474819073666e-07
Epoch 44: reducing lr to 1.4314682801231135e-07
Epoch 47: reducing lr to 2.475978480429147e-08
[I 2024-06-22 00:25:42,744] Trial 373 finished with value: 1.1075806617736816 and parameters: {'hidden_size': 40, 'n_layers': 6, 'rnn_dropout': 0.5875755482799475, 'bidirectional': False, 'fc_dropout': 0.47531546231323163, 'learning_rate_model': 3.170500198727212e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 0.0017117973454145272
Epoch 9: reducing lr to 0.002582568375827315
Epoch 12: reducing lr to 0.002867806206381698
Epoch 15: reducing lr to 0.002811151236607009
Epoch 18: reducing lr to 0.00266798350233546
Epoch 21: reducing lr to 0.0024472987838784033
Epoch 24: reducing lr to 0.0021629634458653233
Epoch 27: reducing lr to 0.001832843621362587
Epoch 30: reducing lr to 0.0014776814704785504
Epoch 33: reducing lr to 0.0011197933759556293
Epoch 36: reducing lr to 0.0007816668899048185
Epoch 39: reducing lr to 0.00048454759994468014
Epoch 42: reducing lr to 0.000247104445342124
Epoch 45: reducing lr to 8.425719851689676e-05
Epoch 48: reducing lr to 6.237945398547551e-06
[I 2024-06-22 00:26:08,898] Trial 374 finished with value: 1.114366888999939 and parameters: {'hidden_size': 161, 'n_layers': 5, 'rnn_dropout': 0.49217735320676365, 'bidirectional': False, 'fc_dropout': 0.44991556476574257, 'learning_rate_model': 0.028685673223941464}. Best is trial 153 with value: 0.9688200950622559.
Epoch 24: reducing lr to 1.917155829890956e-05
Epoch 42: reducing lr to 2.19022530817713e-06
Epoch 45: reducing lr to 7.468188131229565e-07
Epoch 48: reducing lr to 5.5290409138570403e-08
[I 2024-06-22 00:26:38,884] Trial 375 finished with value: 1.068861484527588 and parameters: {'hidden_size': 93, 'n_layers': 5, 'rnn_dropout': 0.7402089502870131, 'bidirectional': True, 'fc_dropout': 0.4959565550893039, 'learning_rate_model': 0.0002542572125328951}. Best is trial 153 with value: 0.9688200950622559.
Epoch 29: reducing lr to 3.1741226643031335e-06
Epoch 34: reducing lr to 1.9944947216559486e-06
[I 2024-06-22 00:26:48,726] Trial 376 finished with value: 1.1063706874847412 and parameters: {'hidden_size': 75, 'n_layers': 5, 'rnn_dropout': 0.12825858559070966, 'bidirectional': False, 'fc_dropout': 0.30713139358283037, 'learning_rate_model': 5.699678680247873e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 0.00596060650003706
Epoch 9: reducing lr to 0.00899269641291489
Epoch 12: reducing lr to 0.00998591589150171
Epoch 15: reducing lr to 0.009788639045616294
Epoch 18: reducing lr to 0.009290118277500528
Epoch 21: reducing lr to 0.008521677567612969
Epoch 24: reducing lr to 0.0075316006356145795
Epoch 27: reducing lr to 0.006382098694281915
Epoch 30: reducing lr to 0.005145397497848007
Epoch 33: reducing lr to 0.0038992043616023022
Epoch 36: reducing lr to 0.0027218226253891906
Epoch 39: reducing lr to 0.0016872310157182865
Epoch 42: reducing lr to 0.0008604361766536353
Epoch 45: reducing lr to 0.0002933898726388717
Epoch 48: reducing lr to 2.1720992843610178e-05
[I 2024-06-22 00:27:30,068] Trial 377 finished with value: 1.1070852279663086 and parameters: {'hidden_size': 102, 'n_layers': 6, 'rnn_dropout': 0.6741341780760262, 'bidirectional': True, 'fc_dropout': 0.21363224794263616, 'learning_rate_model': 0.09988566154433381}. Best is trial 153 with value: 0.9688200950622559.
Epoch 29: reducing lr to 3.333751786741907e-06
[I 2024-06-22 00:27:35,108] Trial 378 finished with value: 1.1102783679962158 and parameters: {'hidden_size': 38, 'n_layers': 4, 'rnn_dropout': 0.6320209204821483, 'bidirectional': False, 'fc_dropout': 0.39046435641705934, 'learning_rate_model': 5.986319998853214e-05}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:28:02,268] Trial 379 finished with value: 1.0696731805801392 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.5112422874358202, 'bidirectional': True, 'fc_dropout': 0.5030732548777529, 'learning_rate_model': 2.9913111272232188e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 8: reducing lr to 0.0033459858777376084
Epoch 11: reducing lr to 0.004079715886303564
Epoch 14: reducing lr to 0.004061953358003849
Epoch 17: reducing lr to 0.003897369724374023
Epoch 23: reducing lr to 0.0032378776058505088
Epoch 26: reducing lr to 0.0027844073351218795
Epoch 29: reducing lr to 0.00228488389522723
Epoch 32: reducing lr to 0.0017706941912490539
Epoch 35: reducing lr to 0.0012741468416275982
Epoch 38: reducing lr to 0.0008264415387426629
Epoch 41: reducing lr to 0.000455709000246244
Epoch 44: reducing lr to 0.0001852442548462445
Epoch 47: reducing lr to 3.2041282017299806e-05
[I 2024-06-22 00:28:10,913] Trial 380 finished with value: 1.1322827339172363 and parameters: {'hidden_size': 72, 'n_layers': 5, 'rnn_dropout': 0.31637097304600914, 'bidirectional': False, 'fc_dropout': 0.15217767144974684, 'learning_rate_model': 0.04102898785522371}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 1.6126689330966475e-05
Epoch 28: reducing lr to 1.2225516012734323e-05
Epoch 31: reducing lr to 9.67101247250663e-06
Epoch 34: reducing lr to 7.150857331077775e-06
Epoch 37: reducing lr to 4.823396465235573e-06
Epoch 40: reducing lr to 2.8348746507954066e-06
Epoch 43: reducing lr to 1.3102370374926716e-06
Epoch 46: reducing lr to 3.4528443395096116e-07
Epoch 49: reducing lr to 6.470133002121642e-10
[I 2024-06-22 00:28:48,786] Trial 381 finished with value: 1.0765491724014282 and parameters: {'hidden_size': 86, 'n_layers': 7, 'rnn_dropout': 0.49390585041997215, 'bidirectional': True, 'fc_dropout': 0.42491575551464705, 'learning_rate_model': 0.00020435044842635042}. Best is trial 153 with value: 0.9688200950622559.
Epoch 8: reducing lr to 0.0025915199728256543
Epoch 14: reducing lr to 0.0031460483219584905
Epoch 18: reducing lr to 0.002955557093101019
Epoch 21: reducing lr to 0.0027110854595981124
Epoch 24: reducing lr to 0.0023961025054875626
Epoch 27: reducing lr to 0.002030400098396873
Epoch 30: reducing lr to 0.0016369561309482529
Epoch 33: reducing lr to 0.0012404923989282825
Epoch 36: reducing lr to 0.000865920317302591
Epoch 39: reducing lr to 0.0005367754690791593
Epoch 42: reducing lr to 0.00027373906005355756
Epoch 45: reducing lr to 9.333901821485975e-05
Epoch 48: reducing lr to 6.910314007906731e-06
[I 2024-06-22 00:28:53,061] Trial 382 finished with value: 1.1136828660964966 and parameters: {'hidden_size': 39, 'n_layers': 3, 'rnn_dropout': 0.1719811846333421, 'bidirectional': False, 'fc_dropout': 0.14446084213348556, 'learning_rate_model': 0.03177761215284233}. Best is trial 153 with value: 0.9688200950622559.
Epoch 7: reducing lr to 0.004949941219375337
Epoch 11: reducing lr to 0.00691125643272811
Epoch 18: reducing lr to 0.006464515542980125
Epoch 21: reducing lr to 0.005929797171852633
Epoch 24: reducing lr to 0.0052408535519258
Epoch 27: reducing lr to 0.004440974266811884
Epoch 30: reducing lr to 0.003580417504501206
Epoch 33: reducing lr to 0.0027132557894209803
Epoch 36: reducing lr to 0.001893976388834237
Epoch 39: reducing lr to 0.0011740572939878146
Epoch 42: reducing lr to 0.0005987332853652688
Epoch 45: reducing lr to 0.00020415492410040954
Epoch 48: reducing lr to 1.5114521866373132e-05
[I 2024-06-22 00:28:58,020] Trial 383 finished with value: 1.0373868942260742 and parameters: {'hidden_size': 31, 'n_layers': 2, 'rnn_dropout': 0.5632245564290674, 'bidirectional': True, 'fc_dropout': 0.01704090516910526, 'learning_rate_model': 0.0695052950120162}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 0.000771786747249446
Epoch 18: reducing lr to 0.0007468071137635739
Epoch 21: reducing lr to 0.0006850342739021679
Epoch 24: reducing lr to 0.0006054447063742353
Epoch 27: reducing lr to 0.0005130393998507064
Epoch 30: reducing lr to 0.0004136243845076241
Epoch 33: reducing lr to 0.00031344633817148815
Epoch 36: reducing lr to 0.00021879985144712043
Epoch 39: reducing lr to 0.0001356318711412536
Epoch 42: reducing lr to 6.916810297461027e-05
Epoch 45: reducing lr to 2.3584806721303406e-05
Epoch 48: reducing lr to 1.746091006494567e-06
[I 2024-06-22 00:29:29,791] Trial 384 finished with value: 1.0896806716918945 and parameters: {'hidden_size': 142, 'n_layers': 3, 'rnn_dropout': 0.37789875377219184, 'bidirectional': True, 'fc_dropout': 0.6995902372969417, 'learning_rate_model': 0.008029534218627702}. Best is trial 153 with value: 0.9688200950622559.
Epoch 8: reducing lr to 0.0014085918658690753
Epoch 11: reducing lr to 0.0017174772466133814
Epoch 14: reducing lr to 0.0017099995841860776
Epoch 17: reducing lr to 0.0016407132285177442
Epoch 22: reducing lr to 0.0014202827207966374
Epoch 27: reducing lr to 0.0011036013972692675
Epoch 33: reducing lr to 0.0006742558502829444
Epoch 38: reducing lr to 0.00034791504555792
Epoch 41: reducing lr to 0.0001918442020993269
Epoch 44: reducing lr to 7.798405615263016e-05
Epoch 47: reducing lr to 1.3488726752218516e-05
[I 2024-06-22 00:29:49,388] Trial 385 finished with value: 1.094494342803955 and parameters: {'hidden_size': 192, 'n_layers': 3, 'rnn_dropout': 0.7497392709774249, 'bidirectional': False, 'fc_dropout': 0.58931776380391, 'learning_rate_model': 0.017272367747345682}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:29:58,918] Trial 386 finished with value: 1.1055500507354736 and parameters: {'hidden_size': 89, 'n_layers': 4, 'rnn_dropout': 0.6538216208902006, 'bidirectional': False, 'fc_dropout': 0.783678297134273, 'learning_rate_model': 5.1958110052086076e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 3: reducing lr to 5.73638774600844e-06
Epoch 6: reducing lr to 1.3606200893016835e-05
Epoch 9: reducing lr to 2.0527514098317196e-05
Epoch 12: reducing lr to 2.2794723610709213e-05
Epoch 15: reducing lr to 2.2344402255551644e-05
Epoch 18: reducing lr to 2.12064352180896e-05
Epoch 21: reducing lr to 1.945232535141117e-05
Epoch 24: reducing lr to 1.7192289290277444e-05
Epoch 27: reducing lr to 1.4568335781420947e-05
Epoch 30: reducing lr to 1.1745333638399189e-05
Epoch 33: reducing lr to 8.900664364701566e-06
Epoch 36: reducing lr to 6.213070001513031e-06
Epoch 39: reducing lr to 3.851420849983826e-06
Epoch 42: reducing lr to 1.964106752407811e-06
Epoch 45: reducing lr to 6.697173428704456e-07
Epoch 48: reducing lr to 4.9582234999760156e-08
[I 2024-06-22 00:30:07,661] Trial 387 finished with value: 1.1074979305267334 and parameters: {'hidden_size': 58, 'n_layers': 6, 'rnn_dropout': 0.5979061877233948, 'bidirectional': False, 'fc_dropout': 0.20159236968859082, 'learning_rate_model': 0.00022800773332305063}. Best is trial 153 with value: 0.9688200950622559.
Epoch 5: reducing lr to 0.0044666819516999325
Epoch 9: reducing lr to 0.008439412454756655
Epoch 14: reducing lr to 0.00928046119880666
Epoch 17: reducing lr to 0.008904432256265868
Epoch 20: reducing lr to 0.008263409839465163
Epoch 28: reducing lr to 0.005608116596227119
Epoch 31: reducing lr to 0.004436308904498633
Epoch 34: reducing lr to 0.0032802575886283873
Epoch 37: reducing lr to 0.00221259943046125
Epoch 40: reducing lr to 0.0013004201671969764
Epoch 43: reducing lr to 0.000601034922967697
Epoch 46: reducing lr to 0.0001583896632618445
Epoch 49: reducing lr to 2.9679941714738645e-07
[I 2024-06-22 00:30:28,391] Trial 388 finished with value: 1.0421106815338135 and parameters: {'hidden_size': 107, 'n_layers': 3, 'rnn_dropout': 0.43635127147344355, 'bidirectional': True, 'fc_dropout': 0.38826996317470447, 'learning_rate_model': 0.09374010390011783}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:30:32,387] Trial 389 finished with value: 1.0094420909881592 and parameters: {'hidden_size': 75, 'n_layers': 1, 'rnn_dropout': 0.6538813834272106, 'bidirectional': True, 'fc_dropout': 0.712244021779393, 'learning_rate_model': 0.0004150090046603508}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:30:52,215] Trial 390 finished with value: 1.093412160873413 and parameters: {'hidden_size': 187, 'n_layers': 3, 'rnn_dropout': 0.06146582781974468, 'bidirectional': False, 'fc_dropout': 0.6005826338841801, 'learning_rate_model': 5.482676347013862e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 0.007930798712354473
Epoch 13: reducing lr to 0.008223169031744686
Epoch 16: reducing lr to 0.007975328034757803
Epoch 19: reducing lr to 0.007485592695666514
Epoch 22: reducing lr to 0.006784735026764618
Epoch 25: reducing lr to 0.005916791934371034
Epoch 28: reducing lr to 0.0049363002481550965
Epoch 31: reducing lr to 0.0039048675915372135
Epoch 34: reducing lr to 0.0028873037981507733
Epoch 37: reducing lr to 0.0019475442299116143
Epoch 40: reducing lr to 0.0011446381835853614
Epoch 43: reducing lr to 0.0005290347995602143
Epoch 46: reducing lr to 0.0001394155989179413
Epoch 49: reducing lr to 2.6124475327599093e-07
[I 2024-06-22 00:31:36,430] Trial 391 finished with value: 1.107248306274414 and parameters: {'hidden_size': 95, 'n_layers': 7, 'rnn_dropout': 0.42700887309822655, 'bidirectional': True, 'fc_dropout': 0.5646478409288389, 'learning_rate_model': 0.08251064153258493}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0005624814919304436
Epoch 21: reducing lr to 0.000515955316046929
Epoch 24: reducing lr to 0.00045600990596694004
Epoch 27: reducing lr to 0.0003864119151099606
Epoch 30: reducing lr to 0.0003115343394684303
Epoch 33: reducing lr to 0.00023608206280510742
Epoch 36: reducing lr to 0.000164796055913171
Epoch 39: reducing lr to 0.00010215545062014801
Epoch 42: reducing lr to 5.209615312726417e-05
Epoch 45: reducing lr to 1.776364609682836e-05
Epoch 48: reducing lr to 1.3151238871170547e-06
[I 2024-06-22 00:32:58,611] Trial 392 finished with value: 1.037175178527832 and parameters: {'hidden_size': 161, 'n_layers': 6, 'rnn_dropout': 0.5403580125424159, 'bidirectional': True, 'fc_dropout': 0.2117453053734832, 'learning_rate_model': 0.006047698667517102}. Best is trial 153 with value: 0.9688200950622559.
Epoch 24: reducing lr to 4.460507597352893e-06
Epoch 28: reducing lr to 3.5390911983143244e-06
Epoch 34: reducing lr to 2.070058737353746e-06
Epoch 37: reducing lr to 1.3962960711281842e-06
Epoch 42: reducing lr to 5.095838572284615e-07
Epoch 45: reducing lr to 1.7375692355537338e-07
Epoch 48: reducing lr to 1.2864018989910166e-08
[I 2024-06-22 00:34:15,172] Trial 393 finished with value: 1.0735247135162354 and parameters: {'hidden_size': 176, 'n_layers': 5, 'rnn_dropout': 0.1410209023867453, 'bidirectional': True, 'fc_dropout': 0.06934301291208511, 'learning_rate_model': 5.915618388982257e-05}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:34:37,343] Trial 394 finished with value: 1.0867359638214111 and parameters: {'hidden_size': 66, 'n_layers': 6, 'rnn_dropout': 0.6874986626406856, 'bidirectional': True, 'fc_dropout': 0.33207191009709, 'learning_rate_model': 3.559891823841408e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 43: reducing lr to 1.947625527380636e-07
Epoch 46: reducing lr to 5.132542879851456e-08
Epoch 49: reducing lr to 9.617646150986733e-11
[I 2024-06-22 00:36:21,436] Trial 395 finished with value: 1.0754618644714355 and parameters: {'hidden_size': 185, 'n_layers': 6, 'rnn_dropout': 0.40817106704486206, 'bidirectional': True, 'fc_dropout': 0.25494028504026184, 'learning_rate_model': 3.0376041777025878e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 15: reducing lr to 3.48369151779344e-05
Epoch 18: reducing lr to 3.306272311381237e-05
Epoch 21: reducing lr to 3.0327909448207514e-05
Epoch 24: reducing lr to 2.680431174081184e-05
Epoch 27: reducing lr to 2.271333428823015e-05
Epoch 30: reducing lr to 1.8312022269281803e-05
Epoch 33: reducing lr to 1.3876929261928773e-05
Epoch 36: reducing lr to 9.686730043695884e-06
Epoch 39: reducing lr to 6.004708469302579e-06
Epoch 42: reducing lr to 3.062217532225043e-06
Epoch 45: reducing lr to 1.0441490445765898e-06
Epoch 48: reducing lr to 7.730312474972935e-08
[I 2024-06-22 00:36:41,509] Trial 396 finished with value: 1.0925695896148682 and parameters: {'hidden_size': 160, 'n_layers': 4, 'rnn_dropout': 0.1818219051321271, 'bidirectional': False, 'fc_dropout': 0.2470982018530048, 'learning_rate_model': 0.0003554843837326049}. Best is trial 153 with value: 0.9688200950622559.
Epoch 7: reducing lr to 0.0011574769502301723
Epoch 10: reducing lr to 0.0015622019091023147
Epoch 13: reducing lr to 0.001619792763148924
Epoch 18: reducing lr to 0.00151163971525877
Epoch 21: reducing lr to 0.0013866030406772486
Epoch 24: reducing lr to 0.0012255028730728177
Epoch 27: reducing lr to 0.0010384618973412346
Epoch 30: reducing lr to 0.0008372323124644637
Epoch 33: reducing lr to 0.0006344582485223287
Epoch 36: reducing lr to 0.00044288081760947837
Epoch 39: reducing lr to 0.0002745374532370705
Epoch 42: reducing lr to 0.0001400056983370276
Epoch 45: reducing lr to 4.773887374606746e-05
Epoch 48: reducing lr to 3.5343269543481466e-06
[I 2024-06-22 00:36:51,367] Trial 397 finished with value: 1.1028263568878174 and parameters: {'hidden_size': 71, 'n_layers': 5, 'rnn_dropout': 0.028058895444063127, 'bidirectional': False, 'fc_dropout': 0.7542463880602278, 'learning_rate_model': 0.016252875201921996}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:36:58,366] Trial 398 finished with value: 1.0576817989349365 and parameters: {'hidden_size': 60, 'n_layers': 2, 'rnn_dropout': 0.1522694357737513, 'bidirectional': True, 'fc_dropout': 0.606544498730502, 'learning_rate_model': 0.0001107130781541933}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:37:03,868] Trial 399 finished with value: 1.108062982559204 and parameters: {'hidden_size': 22, 'n_layers': 6, 'rnn_dropout': 0.5809614478361608, 'bidirectional': False, 'fc_dropout': 0.7586922321512182, 'learning_rate_model': 4.485346455877421e-05}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:37:08,259] Trial 400 finished with value: 1.1124049425125122 and parameters: {'hidden_size': 32, 'n_layers': 5, 'rnn_dropout': 0.7447918922767776, 'bidirectional': False, 'fc_dropout': 0.5471993712935661, 'learning_rate_model': 4.8676465808827524e-05}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:37:13,187] Trial 401 finished with value: 1.111203908920288 and parameters: {'hidden_size': 18, 'n_layers': 7, 'rnn_dropout': 0.028185016522687612, 'bidirectional': False, 'fc_dropout': 0.2286263466846098, 'learning_rate_model': 1.093529205055685e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 0.0021811426385383343
Epoch 9: reducing lr to 0.003290664059356618
Epoch 12: reducing lr to 0.003654109180949372
Epoch 18: reducing lr to 0.003399498539619202
Epoch 21: reducing lr to 0.0031183058795242933
Epoch 24: reducing lr to 0.002756010698354143
Epoch 27: reducing lr to 0.002335377714561692
Epoch 30: reducing lr to 0.0018828362306276998
Epoch 33: reducing lr to 0.0014268213963482656
Epoch 36: reducing lr to 0.0009959864625752103
Epoch 39: reducing lr to 0.0006174021904355901
Epoch 42: reducing lr to 0.0003148562201897534
Epoch 45: reducing lr to 0.00010735906839748552
Epoch 48: reducing lr to 7.948282384064031e-06
[I 2024-06-22 00:37:37,801] Trial 402 finished with value: 1.202674388885498 and parameters: {'hidden_size': 122, 'n_layers': 3, 'rnn_dropout': 0.12121124948169851, 'bidirectional': True, 'fc_dropout': 0.5817883529146217, 'learning_rate_model': 0.036550789818385214}. Best is trial 153 with value: 0.9688200950622559.
Epoch 13: reducing lr to 0.0002401009782713845
Epoch 16: reducing lr to 0.00023286449005101767
Epoch 22: reducing lr to 0.00019810142670661953
Epoch 25: reducing lr to 0.00017275913047470366
Epoch 29: reducing lr to 0.00013416462125859666
Epoch 32: reducing lr to 0.00010397224823106418
Epoch 35: reducing lr to 7.481580521088291e-05
Epoch 38: reducing lr to 4.852728677784931e-05
Epoch 41: reducing lr to 2.675848236747746e-05
Epoch 44: reducing lr to 1.087723772034625e-05
Epoch 47: reducing lr to 1.8814113379985765e-06
[I 2024-06-22 00:37:49,606] Trial 403 finished with value: 1.0929296016693115 and parameters: {'hidden_size': 174, 'n_layers': 2, 'rnn_dropout': 0.18944682695539142, 'bidirectional': False, 'fc_dropout': 0.7096969413258452, 'learning_rate_model': 0.002409154630446646}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:37:52,475] Trial 404 finished with value: 1.1043978929519653 and parameters: {'hidden_size': 36, 'n_layers': 2, 'rnn_dropout': 0.4615682967460868, 'bidirectional': False, 'fc_dropout': 0.1367898791544147, 'learning_rate_model': 5.7328011282134295e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 11: reducing lr to 0.0014878738298455204
Epoch 16: reducing lr to 0.001446323957900671
Epoch 19: reducing lr to 0.0013575105635335201
Epoch 22: reducing lr to 0.0012304101818070886
Epoch 25: reducing lr to 0.001073008895847149
Epoch 28: reducing lr to 0.0008951969475340281
Epoch 31: reducing lr to 0.0007081468656156255
Epoch 34: reducing lr to 0.0005236118989467853
Epoch 37: reducing lr to 0.0003531867111316785
Epoch 40: reducing lr to 0.0002075798789507333
Epoch 43: reducing lr to 9.594034274608403e-05
Epoch 46: reducing lr to 2.5282987726812924e-05
Epoch 49: reducing lr to 4.737667766031156e-08
[I 2024-06-22 00:38:19,028] Trial 405 finished with value: 1.110463261604309 and parameters: {'hidden_size': 101, 'n_layers': 4, 'rnn_dropout': 0.40926391173178606, 'bidirectional': True, 'fc_dropout': 0.6049359107101884, 'learning_rate_model': 0.014963286414081123}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:38:53,602] Trial 406 finished with value: 1.103359341621399 and parameters: {'hidden_size': 124, 'n_layers': 4, 'rnn_dropout': 0.7147413098802236, 'bidirectional': True, 'fc_dropout': 0.7703870024702832, 'learning_rate_model': 1.0699403522342655e-05}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:39:36,049] Trial 407 finished with value: 1.1041367053985596 and parameters: {'hidden_size': 122, 'n_layers': 5, 'rnn_dropout': 0.6675037318324135, 'bidirectional': True, 'fc_dropout': 0.4653498154951399, 'learning_rate_model': 1.2571394105090453e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 12: reducing lr to 0.005160759610682308
Epoch 15: reducing lr to 0.005058806180528275
Epoch 18: reducing lr to 0.004801168736639164
Epoch 21: reducing lr to 0.004404035632186851
Epoch 24: reducing lr to 0.0038923600785729954
Epoch 27: reducing lr to 0.003298293069028175
Epoch 30: reducing lr to 0.002659161150195359
Epoch 33: reducing lr to 0.0020151237604833583
Epoch 36: reducing lr to 0.0014066483660756565
Epoch 39: reducing lr to 0.0008719674564072438
Epoch 42: reducing lr to 0.00044467671431350426
Epoch 45: reducing lr to 0.0001516250108001084
Epoch 48: reducing lr to 1.1225492362360035e-05
[I 2024-06-22 00:39:50,543] Trial 408 finished with value: 1.107248306274414 and parameters: {'hidden_size': 82, 'n_layers': 7, 'rnn_dropout': 0.6950230956440161, 'bidirectional': False, 'fc_dropout': 0.6487975355372214, 'learning_rate_model': 0.05162129276724368}. Best is trial 153 with value: 0.9688200950622559.
Epoch 43: reducing lr to 5.768173672105644e-07
Epoch 46: reducing lr to 1.5200765390628606e-07
Epoch 49: reducing lr to 2.8484045077302195e-10
[I 2024-06-22 00:40:03,186] Trial 409 finished with value: 1.0727694034576416 and parameters: {'hidden_size': 57, 'n_layers': 4, 'rnn_dropout': 0.6619552731288607, 'bidirectional': True, 'fc_dropout': 0.5215806566165936, 'learning_rate_model': 8.996302522110986e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 3: reducing lr to 1.630363803190697e-06
Epoch 6: reducing lr to 3.867077752955465e-06
Epoch 9: reducing lr to 5.834214393661009e-06
Epoch 12: reducing lr to 6.47858790656129e-06
Epoch 15: reducing lr to 6.350600108357871e-06
Epoch 18: reducing lr to 6.027173528905805e-06
Epoch 21: reducing lr to 5.5286303062231774e-06
Epoch 24: reducing lr to 4.88629559122034e-06
Epoch 27: reducing lr to 4.140530309737821e-06
Epoch 30: reducing lr to 3.3381925470028984e-06
Epoch 33: reducing lr to 2.5296966744718724e-06
Epoch 36: reducing lr to 1.7658437479588585e-06
Epoch 39: reducing lr to 1.0946291329481434e-06
Epoch 42: reducing lr to 5.582273543060824e-07
Epoch 45: reducing lr to 1.9034328963288625e-07
Epoch 48: reducing lr to 1.4091983457907868e-08
[I 2024-06-22 00:40:08,162] Trial 410 finished with value: 1.1073170900344849 and parameters: {'hidden_size': 20, 'n_layers': 6, 'rnn_dropout': 0.2614334602647021, 'bidirectional': False, 'fc_dropout': 0.5257320338908137, 'learning_rate_model': 6.480307324345786e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00027822652269822673
Epoch 23: reducing lr to 0.00023607503326648137
Epoch 27: reducing lr to 0.0001911352551374284
Epoch 30: reducing lr to 0.00015409772092929373
Epoch 33: reducing lr to 0.000116775915915491
Epoch 36: reducing lr to 8.151491959983281e-05
Epoch 39: reducing lr to 5.0530295144766976e-05
Epoch 42: reducing lr to 2.576890393461242e-05
Epoch 45: reducing lr to 8.786631302303612e-06
Epoch 48: reducing lr to 6.505144636389202e-07
[I 2024-06-22 00:40:47,462] Trial 411 finished with value: 0.9947588443756104 and parameters: {'hidden_size': 168, 'n_layers': 3, 'rnn_dropout': 0.5053955268011132, 'bidirectional': True, 'fc_dropout': 0.2871289134310855, 'learning_rate_model': 0.0029914409535772893}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 0.00014989386270200484
Epoch 26: reducing lr to 0.00012890097205746072
Epoch 29: reducing lr to 0.00010577610230305448
Epoch 32: reducing lr to 8.197227452660464e-05
Epoch 35: reducing lr to 5.898517948795441e-05
Epoch 38: reducing lr to 3.82591714756885e-05
Epoch 41: reducing lr to 2.1096530082407357e-05
Epoch 44: reducing lr to 8.575672178616663e-06
Epoch 47: reducing lr to 1.4833147240708461e-06
[I 2024-06-22 00:41:15,880] Trial 412 finished with value: 0.9843642711639404 and parameters: {'hidden_size': 108, 'n_layers': 4, 'rnn_dropout': 0.5858165878050149, 'bidirectional': True, 'fc_dropout': 0.07058412080007663, 'learning_rate_model': 0.0018993903479429634}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:41:18,282] Trial 413 finished with value: 1.092036247253418 and parameters: {'hidden_size': 61, 'n_layers': 1, 'rnn_dropout': 0.6305121653149466, 'bidirectional': False, 'fc_dropout': 0.3905671544705639, 'learning_rate_model': 0.00018695778940416125}. Best is trial 153 with value: 0.9688200950622559.
Epoch 27: reducing lr to 8.365349169429462e-05
Epoch 33: reducing lr to 5.110890246337001e-05
Epoch 38: reducing lr to 2.63721199029965e-05
Epoch 41: reducing lr to 1.454187844145958e-05
Epoch 44: reducing lr to 5.91122719651629e-06
Epoch 47: reducing lr to 1.0224516697109879e-06
[I 2024-06-22 00:41:22,180] Trial 414 finished with value: 0.9832320213317871 and parameters: {'hidden_size': 74, 'n_layers': 1, 'rnn_dropout': 0.4275198695236406, 'bidirectional': True, 'fc_dropout': 0.0868469644869741, 'learning_rate_model': 0.001309253391186895}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:41:43,110] Trial 415 finished with value: 1.1074957847595215 and parameters: {'hidden_size': 138, 'n_layers': 5, 'rnn_dropout': 0.2505773561175088, 'bidirectional': False, 'fc_dropout': 0.18083533670023533, 'learning_rate_model': 1.0103208625366834e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 3: reducing lr to 0.0003806787380750788
Epoch 6: reducing lr to 0.0009029360662646598
Epoch 9: reducing lr to 0.001362248946334438
Epoch 12: reducing lr to 0.0015127057310470415
Epoch 17: reducing lr to 0.0014373101828869346
Epoch 23: reducing lr to 0.0011940962194901795
Epoch 26: reducing lr to 0.0010268610111704355
Epoch 29: reducing lr to 0.000842641864020724
Epoch 32: reducing lr to 0.0006530139483417324
Epoch 35: reducing lr to 0.0004698923529146866
Epoch 38: reducing lr to 0.0003047832059059688
Epoch 41: reducing lr to 0.00016806082892029274
Epoch 44: reducing lr to 6.831619082651296e-05
Epoch 47: reducing lr to 1.1816497836528508e-05
[I 2024-06-22 00:42:14,445] Trial 416 finished with value: 1.1178183555603027 and parameters: {'hidden_size': 142, 'n_layers': 7, 'rnn_dropout': 0.3129214084280226, 'bidirectional': False, 'fc_dropout': 0.6815810064519052, 'learning_rate_model': 0.015131072032774386}. Best is trial 153 with value: 0.9688200950622559.
Epoch 4: reducing lr to 0.0018836261754570209
Epoch 7: reducing lr to 0.0037368244517464676
Epoch 10: reducing lr to 0.005043447553178213
Epoch 13: reducing lr to 0.005229375153339525
Epoch 16: reducing lr to 0.005071765167868131
Epoch 19: reducing lr to 0.004760326864208097
Epoch 22: reducing lr to 0.0043146291452831305
Epoch 25: reducing lr to 0.0037626764827081395
Epoch 28: reducing lr to 0.0031391505838533353
Epoch 31: reducing lr to 0.0024832296990899877
Epoch 34: reducing lr to 0.0018361284662768303
Epoch 37: reducing lr to 0.001238505418849302
Epoch 40: reducing lr to 0.0007279118857581111
Epoch 43: reducing lr to 0.0003364300825377989
Epoch 46: reducing lr to 8.865882072410098e-05
Epoch 49: reducing lr to 1.6613386110003737e-07
[I 2024-06-22 00:42:32,439] Trial 417 finished with value: 1.1096389293670654 and parameters: {'hidden_size': 142, 'n_layers': 4, 'rnn_dropout': 0.552347215883451, 'bidirectional': False, 'fc_dropout': 0.6222564887203075, 'learning_rate_model': 0.05247114549766932}. Best is trial 153 with value: 0.9688200950622559.
Epoch 14: reducing lr to 0.0006823402518942236
Epoch 17: reducing lr to 0.0006546929531365072
Epoch 22: reducing lr to 0.0005667346813599775
Epoch 25: reducing lr to 0.0004942346574141881
Epoch 28: reducing lr to 0.00041233335380076177
Epoch 31: reducing lr to 0.00032617690764823464
Epoch 34: reducing lr to 0.00024117893942495453
Epoch 37: reducing lr to 0.00016268002423371774
Epoch 40: reducing lr to 9.561260000393216e-05
Epoch 43: reducing lr to 4.4190726295772685e-05
Epoch 46: reducing lr to 1.1645503430521894e-05
Epoch 49: reducing lr to 2.1821996204745427e-08
[I 2024-06-22 00:42:41,384] Trial 418 finished with value: 1.0794868469238281 and parameters: {'hidden_size': 51, 'n_layers': 3, 'rnn_dropout': 0.004968213752412876, 'bidirectional': True, 'fc_dropout': 0.5131398260428972, 'learning_rate_model': 0.0068921839914617405}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 0.0010447755784244897
Epoch 14: reducing lr to 0.001076118698308914
Epoch 17: reducing lr to 0.0010325161480148073
Epoch 22: reducing lr to 0.0008937971721565076
Epoch 25: reducing lr to 0.0007794573963931359
Epoch 28: reducing lr to 0.0006502908640222064
Epoch 31: reducing lr to 0.0005144135470572501
Epoch 34: reducing lr to 0.00038036326544273685
Epoch 37: reducing lr to 0.0002565626392892166
Epoch 40: reducing lr to 0.0001507906156386513
Epoch 43: reducing lr to 6.96931871258041e-05
Epoch 46: reducing lr to 1.8366121532499037e-05
Epoch 49: reducing lr to 3.4415466602238934e-08
[I 2024-06-22 00:42:43,440] Trial 419 finished with value: 1.0946133136749268 and parameters: {'hidden_size': 37, 'n_layers': 1, 'rnn_dropout': 0.7250680556642687, 'bidirectional': False, 'fc_dropout': 0.7705932519309364, 'learning_rate_model': 0.01086966223201368}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:42:47,494] Trial 420 finished with value: 1.0359175205230713 and parameters: {'hidden_size': 76, 'n_layers': 1, 'rnn_dropout': 0.42881921335072887, 'bidirectional': True, 'fc_dropout': 0.6169218666030231, 'learning_rate_model': 0.00034644677219478504}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:43:38,443] Trial 421 finished with value: 1.103704810142517 and parameters: {'hidden_size': 121, 'n_layers': 6, 'rnn_dropout': 0.7526779359208366, 'bidirectional': True, 'fc_dropout': 0.08806120627761437, 'learning_rate_model': 1.9754932570600042e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 0.00014897219025113647
Epoch 41: reducing lr to 1.7214524928086446e-05
Epoch 44: reducing lr to 6.9976494673406645e-06
Epoch 47: reducing lr to 1.2103676857744836e-06
[I 2024-06-22 00:44:00,511] Trial 422 finished with value: 0.982204258441925 and parameters: {'hidden_size': 75, 'n_layers': 5, 'rnn_dropout': 0.5561216517821522, 'bidirectional': True, 'fc_dropout': 0.2444085290288249, 'learning_rate_model': 0.001549880590083267}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 0.002240778840779208
Epoch 13: reducing lr to 0.0023233855553262414
Epoch 19: reducing lr to 0.0021149897168631117
Epoch 22: reducing lr to 0.0019169684214257736
Epoch 25: reducing lr to 0.0016717385792654458
Epoch 28: reducing lr to 0.00139470910169088
Epoch 31: reducing lr to 0.0011032866918599927
Epoch 34: reducing lr to 0.0008157828098346676
Epoch 37: reducing lr to 0.0005502618412278435
Epoch 40: reducing lr to 0.00032340765604484367
Epoch 43: reducing lr to 0.00014947422420943831
Epoch 46: reducing lr to 3.939067620556715e-05
Epoch 49: reducing lr to 7.38124540333035e-08
[I 2024-06-22 00:44:49,949] Trial 423 finished with value: 1.1708760261535645 and parameters: {'hidden_size': 155, 'n_layers': 4, 'rnn_dropout': 0.22958700219765965, 'bidirectional': True, 'fc_dropout': 0.5221543314676185, 'learning_rate_model': 0.02331267081552816}. Best is trial 153 with value: 0.9688200950622559.
Epoch 15: reducing lr to 0.00019310547811979322
Epoch 22: reducing lr to 0.00016203109154601137
Epoch 25: reducing lr to 0.00014130312411536343
Epoch 29: reducing lr to 0.0001097359084726936
Epoch 32: reducing lr to 8.504096689985754e-05
Epoch 35: reducing lr to 6.119333305542546e-05
Epoch 38: reducing lr to 3.969143169284663e-05
Epoch 41: reducing lr to 2.18862941988706e-05
Epoch 44: reducing lr to 8.896708772538416e-06
Epoch 47: reducing lr to 1.538843701486426e-06
[I 2024-06-22 00:45:06,555] Trial 424 finished with value: 1.0930304527282715 and parameters: {'hidden_size': 134, 'n_layers': 4, 'rnn_dropout': 0.24571368741668903, 'bidirectional': False, 'fc_dropout': 0.6219835449679528, 'learning_rate_model': 0.0019704954222894244}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:45:10,446] Trial 425 finished with value: 1.10296630859375 and parameters: {'hidden_size': 42, 'n_layers': 3, 'rnn_dropout': 0.293487454872947, 'bidirectional': False, 'fc_dropout': 0.4885060134941166, 'learning_rate_model': 7.106542572380849e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 7: reducing lr to 0.00598434197000311
Epoch 10: reducing lr to 0.008076835092397218
Epoch 13: reducing lr to 0.008374589069174824
Epoch 16: reducing lr to 0.00812218437017773
Epoch 19: reducing lr to 0.0076234311277586855
Epoch 22: reducing lr to 0.006909667984817689
Epoch 25: reducing lr to 0.00602574273578472
Epoch 28: reducing lr to 0.0050271964422448166
Epoch 31: reducing lr to 0.003976771159928881
Epoch 34: reducing lr to 0.002940470119735605
Epoch 37: reducing lr to 0.0019834059784724968
Epoch 40: reducing lr to 0.0011657153566233189
Epoch 43: reducing lr to 0.0005387763564760464
Epoch 46: reducing lr to 0.00014198277406963778
Epoch 49: reducing lr to 2.660552697764107e-07
[I 2024-06-22 00:45:14,168] Trial 426 finished with value: 1.1497894525527954 and parameters: {'hidden_size': 62, 'n_layers': 1, 'rnn_dropout': 0.24607087133984198, 'bidirectional': True, 'fc_dropout': 0.796461769477117, 'learning_rate_model': 0.0840299784671679}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 0.002125748376744615
Epoch 18: reducing lr to 0.002056946449886566
Epoch 21: reducing lr to 0.0018868042253274188
Epoch 24: reducing lr to 0.0016675890151916241
Epoch 27: reducing lr to 0.001413075147150959
Epoch 30: reducing lr to 0.0011392542915289136
Epoch 33: reducing lr to 0.000863331803686997
Epoch 36: reducing lr to 0.0006026450061539498
Epoch 39: reducing lr to 0.0003735736988758738
Epoch 42: reducing lr to 0.0001905111523938359
Epoch 45: reducing lr to 6.49601263332425e-05
Epoch 48: reducing lr to 4.809294971613171e-06
[I 2024-06-22 00:45:54,824] Trial 427 finished with value: 1.156302571296692 and parameters: {'hidden_size': 96, 'n_layers': 7, 'rnn_dropout': 0.17151832334065126, 'bidirectional': True, 'fc_dropout': 0.774087118010037, 'learning_rate_model': 0.022115914003569243}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:46:01,406] Trial 428 finished with value: 1.1054426431655884 and parameters: {'hidden_size': 55, 'n_layers': 4, 'rnn_dropout': 0.13478157731715773, 'bidirectional': False, 'fc_dropout': 0.028703319172813394, 'learning_rate_model': 5.826872431714904e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 0.0002850273447692371
Epoch 17: reducing lr to 0.0004537117525195525
Epoch 23: reducing lr to 0.000376937069584847
Epoch 26: reducing lr to 0.0003241463913073716
Epoch 29: reducing lr to 0.00026599443976892763
Epoch 32: reducing lr to 0.00020613511714412362
Epoch 35: reducing lr to 0.00014832962673947013
Epoch 38: reducing lr to 9.621007638892011e-05
Epoch 41: reducing lr to 5.3051299661816334e-05
Epoch 44: reducing lr to 2.1565184073976325e-05
Epoch 47: reducing lr to 3.7300813741445416e-06
[I 2024-06-22 00:48:19,000] Trial 429 finished with value: 1.0506585836410522 and parameters: {'hidden_size': 196, 'n_layers': 7, 'rnn_dropout': 0.4452733272816815, 'bidirectional': True, 'fc_dropout': 0.7616728685702719, 'learning_rate_model': 0.004776383894881025}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:48:26,265] Trial 430 finished with value: 1.085825800895691 and parameters: {'hidden_size': 139, 'n_layers': 1, 'rnn_dropout': 0.6823758798797543, 'bidirectional': True, 'fc_dropout': 0.592625450415668, 'learning_rate_model': 4.229506516204067e-05}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:48:30,645] Trial 431 finished with value: 1.095895528793335 and parameters: {'hidden_size': 148, 'n_layers': 1, 'rnn_dropout': 0.5655595584309073, 'bidirectional': False, 'fc_dropout': 0.3932457478252316, 'learning_rate_model': 6.569729148584553e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 24: reducing lr to 1.2454465941182992e-05
Epoch 27: reducing lr to 1.0553617307500334e-05
Epoch 30: reducing lr to 8.508573541162938e-06
Epoch 33: reducing lr to 6.447833637069277e-06
Epoch 36: reducing lr to 4.5008821930299795e-06
Epoch 39: reducing lr to 2.790052505015258e-06
Epoch 42: reducing lr to 1.422841381952796e-06
Epoch 45: reducing lr to 4.851577178681177e-07
Epoch 48: reducing lr to 3.59184426614674e-08
[I 2024-06-22 00:48:45,112] Trial 432 finished with value: 1.091306209564209 and parameters: {'hidden_size': 150, 'n_layers': 3, 'rnn_dropout': 0.7070540719980095, 'bidirectional': False, 'fc_dropout': 0.7766001184425293, 'learning_rate_model': 0.0001651737299816249}. Best is trial 153 with value: 0.9688200950622559.
Epoch 16: reducing lr to 1.0191607720399958e-05
Epoch 21: reducing lr to 8.99551548313026e-06
Epoch 24: reducing lr to 7.950386481168224e-06
Epoch 27: reducing lr to 6.736967828666597e-06
Epoch 34: reducing lr to 3.689662362742823e-06
Epoch 42: reducing lr to 9.082797240240193e-07
Epoch 45: reducing lr to 3.0970347340375464e-07
Epoch 48: reducing lr to 2.2928763249181327e-08
[I 2024-06-22 00:50:24,031] Trial 433 finished with value: 1.0746655464172363 and parameters: {'hidden_size': 175, 'n_layers': 6, 'rnn_dropout': 0.33969016807568225, 'bidirectional': True, 'fc_dropout': 0.7945894275724366, 'learning_rate_model': 0.00010543968694375883}. Best is trial 153 with value: 0.9688200950622559.
Epoch 12: reducing lr to 0.0004941932534152389
Epoch 16: reducing lr to 0.00047780495592183455
Epoch 22: reducing lr to 0.00040647607299366007
Epoch 25: reducing lr to 0.0003544772700357863
Epoch 28: reducing lr to 0.0002957356377327134
Epoch 31: reducing lr to 0.00023394211239007663
Epoch 34: reducing lr to 0.00017297947595333878
Epoch 37: reducing lr to 0.00011667812043257247
Epoch 40: reducing lr to 6.857571180406766e-05
Epoch 43: reducing lr to 3.1694677382967506e-05
Epoch 46: reducing lr to 8.352441906526055e-06
Epoch 49: reducing lr to 1.565127318641987e-08
[I 2024-06-22 00:51:13,190] Trial 434 finished with value: 1.0129356384277344 and parameters: {'hidden_size': 186, 'n_layers': 3, 'rnn_dropout': 0.748341609873127, 'bidirectional': True, 'fc_dropout': 0.4594266834807066, 'learning_rate_model': 0.004943244123469622}. Best is trial 153 with value: 0.9688200950622559.
Epoch 15: reducing lr to 0.0006285915665160758
Epoch 18: reducing lr to 0.0005965783367800154
Epoch 22: reducing lr to 0.0005274390900295089
Epoch 27: reducing lr to 0.0004098356673694563
Epoch 30: reducing lr to 0.0003304191173510107
Epoch 33: reducing lr to 0.00025039302873503704
Epoch 36: reducing lr to 0.00017478576336293645
Epoch 39: reducing lr to 0.0001083479717969408
Epoch 42: reducing lr to 5.525414939189415e-05
Epoch 45: reducing lr to 1.8840453589369192e-05
Epoch 48: reducing lr to 1.394844865994227e-06
[I 2024-06-22 00:51:17,870] Trial 435 finished with value: 0.9911534190177917 and parameters: {'hidden_size': 91, 'n_layers': 1, 'rnn_dropout': 0.26026506381516834, 'bidirectional': True, 'fc_dropout': 0.08433579715145374, 'learning_rate_model': 0.006414301740012138}. Best is trial 153 with value: 0.9688200950622559.
Epoch 5: reducing lr to 0.0011405256336860653
Epoch 8: reducing lr to 0.0019519952275198094
Epoch 11: reducing lr to 0.0023800417068962324
Epoch 14: reducing lr to 0.0023696793288896724
Epoch 17: reducing lr to 0.002273663840745857
Epoch 20: reducing lr to 0.002109984737099278
Epoch 23: reducing lr to 0.0018889265719755372
Epoch 26: reducing lr to 0.00162437918993969
Epoch 29: reducing lr to 0.0013329651175742903
Epoch 34: reducing lr to 0.0008375832229335428
Epoch 37: reducing lr to 0.0005649666564147997
Epoch 40: reducing lr to 0.0003320501775789089
Epoch 43: reducing lr to 0.00015346866953988084
Epoch 46: reducing lr to 4.044332527241776e-05
Epoch 49: reducing lr to 7.578496677845825e-08
[I 2024-06-22 00:51:31,060] Trial 436 finished with value: 1.1113911867141724 and parameters: {'hidden_size': 111, 'n_layers': 4, 'rnn_dropout': 0.29888777514112486, 'bidirectional': False, 'fc_dropout': 0.2215384451751942, 'learning_rate_model': 0.023935662435466326}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.0002692207155921405
Epoch 12: reducing lr to 0.0002989554299077684
Epoch 15: reducing lr to 0.00029304941338276295
Epoch 18: reducing lr to 0.0002781248444028828
Epoch 21: reducing lr to 0.0002551194911354301
Epoch 24: reducing lr to 0.00022547885746063248
Epoch 27: reducing lr to 0.00019106540447497524
Epoch 30: reducing lr to 0.00015404140568864546
Epoch 33: reducing lr to 0.00011673323998383511
Epoch 36: reducing lr to 8.14851298515743e-05
Epoch 39: reducing lr to 5.0511828773467346e-05
Epoch 42: reducing lr to 2.5759486650452902e-05
Epoch 45: reducing lr to 8.783420214863134e-06
Epoch 48: reducing lr to 6.502767321634427e-07
[I 2024-06-22 00:51:41,722] Trial 437 finished with value: 1.0925276279449463 and parameters: {'hidden_size': 93, 'n_layers': 4, 'rnn_dropout': 0.5836987028545443, 'bidirectional': False, 'fc_dropout': 0.5616555713830786, 'learning_rate_model': 0.0029903477270443474}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 7.351377320795492e-05
Epoch 37: reducing lr to 4.020622704232809e-05
Epoch 40: reducing lr to 2.363057125159085e-05
Epoch 43: reducing lr to 1.0921699716866392e-05
Epoch 46: reducing lr to 2.8781760831132762e-06
Epoch 49: reducing lr to 5.393287455285508e-09
[I 2024-06-22 00:52:00,203] Trial 438 finished with value: 0.979478120803833 and parameters: {'hidden_size': 66, 'n_layers': 5, 'rnn_dropout': 0.2057717987627643, 'bidirectional': True, 'fc_dropout': 0.1332301148892321, 'learning_rate_model': 0.0017033973020565553}. Best is trial 153 with value: 0.9688200950622559.
Epoch 37: reducing lr to 2.2493959095136548e-05
Epoch 40: reducing lr to 1.3220467132327478e-05
Epoch 43: reducing lr to 6.1103039193883994e-06
Epoch 46: reducing lr to 1.6102375140545298e-06
Epoch 49: reducing lr to 3.01735318959228e-09
[I 2024-06-22 00:52:45,488] Trial 439 finished with value: 0.9769910573959351 and parameters: {'hidden_size': 147, 'n_layers': 4, 'rnn_dropout': 0.7022476011839601, 'bidirectional': True, 'fc_dropout': 0.6143895835862447, 'learning_rate_model': 0.0009529904209834922}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00020503671287862255
Epoch 23: reducing lr to 0.00017397352468143866
Epoch 27: reducing lr to 0.0001408555304093047
Epoch 31: reducing lr to 0.00010433016303286726
Epoch 34: reducing lr to 7.714291686594727e-05
Epoch 37: reducing lr to 5.203444220765771e-05
Epoch 40: reducing lr to 3.0582416818925756e-05
Epoch 43: reducing lr to 1.4134739679213864e-05
Epoch 46: reducing lr to 3.7249027844008398e-06
Epoch 49: reducing lr to 6.979931345104176e-09
[I 2024-06-22 00:53:04,542] Trial 440 finished with value: 0.9773353934288025 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.08763337377301479, 'bidirectional': True, 'fc_dropout': 0.2283068259596248, 'learning_rate_model': 0.0022045174340091364}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 0.002880310562153115
Epoch 9: reducing lr to 0.004345490422861187
Epoch 12: reducing lr to 0.004825438319890129
Epoch 21: reducing lr to 0.004117882619009707
Epoch 24: reducing lr to 0.0036394532772034034
Epoch 27: reducing lr to 0.0030839858792439173
Epoch 30: reducing lr to 0.002486381672642822
Epoch 33: reducing lr to 0.001884190729021749
Epoch 36: reducing lr to 0.001315251133616531
Epoch 39: reducing lr to 0.0008153112129336996
Epoch 42: reducing lr to 0.00041578376422914327
Epoch 45: reducing lr to 0.00014177314825013985
Epoch 48: reducing lr to 1.049611396214678e-05
[I 2024-06-22 00:53:14,974] Trial 441 finished with value: 1.1155157089233398 and parameters: {'hidden_size': 86, 'n_layers': 2, 'rnn_dropout': 0.33506811444199425, 'bidirectional': True, 'fc_dropout': 0.30989464559904767, 'learning_rate_model': 0.048267189916329334}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 6.680485045137681e-06
Epoch 12: reducing lr to 7.418326907975345e-06
Epoch 15: reducing lr to 7.271774087978358e-06
Epoch 18: reducing lr to 6.901433493437104e-06
Epoch 21: reducing lr to 6.330575050678349e-06
[I 2024-06-22 00:53:43,230] Trial 442 finished with value: 1.1073591709136963 and parameters: {'hidden_size': 169, 'n_layers': 5, 'rnn_dropout': 0.254917491824306, 'bidirectional': False, 'fc_dropout': 0.4516880792549694, 'learning_rate_model': 7.42029573257119e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 3: reducing lr to 0.00017208616089749867
Epoch 9: reducing lr to 0.000615805843391028
Epoch 13: reducing lr to 0.0006816889863354704
Epoch 16: reducing lr to 0.0006611433211112552
Epoch 23: reducing lr to 0.0005397922542071551
Epoch 27: reducing lr to 0.00043703617786905143
Epoch 30: reducing lr to 0.0003523488062149892
Epoch 33: reducing lr to 0.00026701144130719873
Epoch 36: reducing lr to 0.00018638617389345156
Epoch 39: reducing lr to 0.00011553895193634322
Epoch 42: reducing lr to 5.8921329167454206e-05
Epoch 45: reducing lr to 2.0090881496154495e-05
Epoch 48: reducing lr to 1.4874197574532913e-06
[I 2024-06-22 00:53:54,941] Trial 443 finished with value: 1.1074049472808838 and parameters: {'hidden_size': 62, 'n_layers': 7, 'rnn_dropout': 0.19105130663711362, 'bidirectional': False, 'fc_dropout': 0.6129798455343353, 'learning_rate_model': 0.006840014521299888}. Best is trial 153 with value: 0.9688200950622559.
Epoch 24: reducing lr to 9.544583699197116e-06
Epoch 27: reducing lr to 8.087852517838625e-06
Epoch 30: reducing lr to 6.520616195662408e-06
Epoch 33: reducing lr to 4.941351007593724e-06
Epoch 36: reducing lr to 3.4492885535580054e-06
Epoch 42: reducing lr to 1.0904063429828463e-06
Epoch 45: reducing lr to 3.71804657652296e-07
Epoch 48: reducing lr to 2.7526397675039104e-08
[I 2024-06-22 00:54:31,760] Trial 444 finished with value: 1.0750348567962646 and parameters: {'hidden_size': 97, 'n_layers': 6, 'rnn_dropout': 0.41401015371409555, 'bidirectional': True, 'fc_dropout': 0.7071288527490794, 'learning_rate_model': 0.00012658226359631904}. Best is trial 153 with value: 0.9688200950622559.
Epoch 5: reducing lr to 0.0005831337577394041
Epoch 13: reducing lr to 0.0012196573518587013
Epoch 22: reducing lr to 0.0010063093588203935
Epoch 25: reducing lr to 0.0008775763643329613
Epoch 28: reducing lr to 0.000732150204550389
Epoch 31: reducing lr to 0.0005791684991111846
Epoch 34: reducing lr to 0.00042824381827367004
Epoch 37: reducing lr to 0.0002888590309784408
Epoch 40: reducing lr to 0.0001697723068124596
Epoch 43: reducing lr to 7.84662433888715e-05
Epoch 46: reducing lr to 2.0678069431338646e-05
Epoch 49: reducing lr to 3.8747723990137375e-08
[I 2024-06-22 00:55:41,114] Trial 445 finished with value: 1.1406636238098145 and parameters: {'hidden_size': 130, 'n_layers': 7, 'rnn_dropout': 0.15153441763500305, 'bidirectional': True, 'fc_dropout': 0.10298528367723812, 'learning_rate_model': 0.012237947458371016}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:56:07,546] Trial 446 finished with value: 1.1072170734405518 and parameters: {'hidden_size': 192, 'n_layers': 4, 'rnn_dropout': 0.5508441598065185, 'bidirectional': False, 'fc_dropout': 0.25476569585094605, 'learning_rate_model': 1.2462785417518953e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 1.9826552890738932e-05
Epoch 12: reducing lr to 2.2016343096047413e-05
Epoch 24: reducing lr to 1.6605217333866725e-05
Epoch 27: reducing lr to 1.4070865011563447e-05
Epoch 30: reducing lr to 1.134426104816008e-05
Epoch 33: reducing lr to 8.596729830230026e-06
Epoch 36: reducing lr to 6.0009098232191615e-06
Epoch 39: reducing lr to 3.7199048467811733e-06
Epoch 42: reducing lr to 1.8970376161068266e-06
Epoch 45: reducing lr to 6.468482377685735e-07
Epoch 48: reducing lr to 4.788913065437259e-08
[I 2024-06-22 00:56:46,432] Trial 447 finished with value: 1.1080975532531738 and parameters: {'hidden_size': 165, 'n_layers': 7, 'rnn_dropout': 0.5900708079333449, 'bidirectional': False, 'fc_dropout': 0.4674155241820646, 'learning_rate_model': 0.00022022186235388098}. Best is trial 153 with value: 0.9688200950622559.
Epoch 11: reducing lr to 2.7685152737264172e-06
Epoch 14: reducing lr to 2.756461534625895e-06
Epoch 17: reducing lr to 2.644774271049703e-06
Epoch 20: reducing lr to 2.4543792468270617e-06
Epoch 23: reducing lr to 2.1972396745440556e-06
Epoch 26: reducing lr to 1.8895125176339807e-06
Epoch 29: reducing lr to 1.5505334535340756e-06
Epoch 32: reducing lr to 1.2016017904651924e-06
Epoch 35: reducing lr to 8.646423158678275e-07
Epoch 38: reducing lr to 5.60827294501649e-07
Epoch 41: reducing lr to 3.092463697758698e-07
Epoch 44: reducing lr to 1.2570766278937327e-07
Epoch 47: reducing lr to 2.1743371628518472e-08
[I 2024-06-22 00:57:30,128] Trial 448 finished with value: 1.1075869798660278 and parameters: {'hidden_size': 179, 'n_layers': 7, 'rnn_dropout': 0.7900151269635534, 'bidirectional': False, 'fc_dropout': 0.2557200894896421, 'learning_rate_model': 2.784247303202293e-05}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:58:13,884] Trial 449 finished with value: 0.9803737998008728 and parameters: {'hidden_size': 176, 'n_layers': 3, 'rnn_dropout': 0.335465838533509, 'bidirectional': True, 'fc_dropout': 0.33673846761190396, 'learning_rate_model': 0.00035043062036654225}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.008155404769366803
Epoch 12: reducing lr to 0.009056147605638021
Epoch 16: reducing lr to 0.00875583018916234
Epoch 19: reducing lr to 0.008218167105208026
Epoch 22: reducing lr to 0.0074487202926214
Epoch 25: reducing lr to 0.006495836311205901
Epoch 28: reducing lr to 0.005419389214738237
Epoch 31: reducing lr to 0.0042870158310298855
Epoch 34: reducing lr to 0.0031698685810732743
Epoch 37: reducing lr to 0.0021381398343330816
Epoch 40: reducing lr to 0.0012566577223941111
Epoch 43: reducing lr to 0.0005808085697439819
Epoch 46: reducing lr to 0.0001530594484046105
Epoch 49: reducing lr to 2.868113621809498e-07
[I 2024-06-22 00:58:23,863] Trial 450 finished with value: 1.0920192003250122 and parameters: {'hidden_size': 38, 'n_layers': 4, 'rnn_dropout': 0.1862832777951968, 'bidirectional': True, 'fc_dropout': 0.05754157184737157, 'learning_rate_model': 0.09058551107987103}. Best is trial 153 with value: 0.9688200950622559.
Epoch 16: reducing lr to 0.00042062435248892084
Epoch 23: reducing lr to 0.00034341989120118834
Epoch 27: reducing lr to 0.00027804570274025145
Epoch 30: reducing lr to 0.0002241669600704994
Epoch 33: reducing lr to 0.00016987468680497274
Epoch 36: reducing lr to 0.00011858028539870526
Epoch 39: reducing lr to 7.350675003989786e-05
Epoch 42: reducing lr to 3.748619268692044e-05
Epoch 45: reducing lr to 1.2781969885209809e-05
Epoch 48: reducing lr to 9.463076346387898e-07
[I 2024-06-22 00:58:46,275] Trial 451 finished with value: 1.0290098190307617 and parameters: {'hidden_size': 160, 'n_layers': 2, 'rnn_dropout': 0.024594254679757645, 'bidirectional': True, 'fc_dropout': 0.43720919949138237, 'learning_rate_model': 0.004351668673292754}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.00015703320920539685
Epoch 12: reducing lr to 0.00017437711085694206
Epoch 15: reducing lr to 0.00017093220236800264
Epoch 18: reducing lr to 0.00016222688057371525
Epoch 21: reducing lr to 0.00014880813438048057
Epoch 24: reducing lr to 0.00013151910883652307
Epoch 27: reducing lr to 0.00011144615512532451
Epoch 30: reducing lr to 8.985050140957524e-05
Epoch 33: reducing lr to 6.808909654402732e-05
Epoch 36: reducing lr to 4.75292973461094e-05
Epoch 39: reducing lr to 2.946294291538778e-05
Epoch 42: reducing lr to 1.5025199109612239e-05
Epoch 45: reducing lr to 5.123263494437379e-06
Epoch 48: reducing lr to 3.7929860597326387e-07
[I 2024-06-22 00:58:51,129] Trial 452 finished with value: 1.0932831764221191 and parameters: {'hidden_size': 163, 'n_layers': 1, 'rnn_dropout': 0.5714364111031028, 'bidirectional': False, 'fc_dropout': 0.6769933844702254, 'learning_rate_model': 0.0017442339055707746}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 0.00010663469672060066
Epoch 13: reducing lr to 0.00011056580397335613
Epoch 16: reducing lr to 0.00010723342214055817
Epoch 19: reducing lr to 0.00010064861507995174
Epoch 22: reducing lr to 9.122513231632559e-05
Epoch 25: reducing lr to 7.955507841822919e-05
Epoch 28: reducing lr to 6.637173618640159e-05
Epoch 31: reducing lr to 5.25034597976898e-05
Epoch 34: reducing lr to 3.882165920771951e-05
Epoch 37: reducing lr to 2.6185986536648164e-05
Epoch 40: reducing lr to 1.5390397611693767e-05
Epoch 43: reducing lr to 7.1132136184300174e-06
Epoch 46: reducing lr to 1.8745325216206311e-06
Epoch 49: reducing lr to 3.5126039691279686e-09
[I 2024-06-22 00:58:58,842] Trial 453 finished with value: 1.0924932956695557 and parameters: {'hidden_size': 126, 'n_layers': 2, 'rnn_dropout': 0.18100127086965243, 'bidirectional': False, 'fc_dropout': 0.0895643554732649, 'learning_rate_model': 0.0011094087184867307}. Best is trial 153 with value: 0.9688200950622559.
Epoch 8: reducing lr to 0.0002362228286374848
Epoch 11: reducing lr to 0.000288023339581915
Epoch 14: reducing lr to 0.0002867693251203988
Epoch 17: reducing lr to 0.0002751499062393602
Epoch 22: reducing lr to 0.0002381834013818654
Epoch 27: reducing lr to 0.00018507549991450672
Epoch 30: reducing lr to 0.00014921220429046014
Epoch 33: reducing lr to 0.00011307365038697004
Epoch 36: reducing lr to 7.893056926929872e-05
Epoch 39: reducing lr to 4.892828184952683e-05
Epoch 42: reducing lr to 2.495192618712956e-05
Epoch 45: reducing lr to 8.508059801259817e-06
Epoch 48: reducing lr to 6.298905425533661e-07
[I 2024-06-22 00:59:02,888] Trial 454 finished with value: 1.0929991006851196 and parameters: {'hidden_size': 134, 'n_layers': 1, 'rnn_dropout': 0.27535708714421453, 'bidirectional': False, 'fc_dropout': 0.5202318153624044, 'learning_rate_model': 0.002896600261160456}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 00:59:27,875] Trial 455 finished with value: 1.0872448682785034 and parameters: {'hidden_size': 166, 'n_layers': 2, 'rnn_dropout': 0.49454547347235644, 'bidirectional': True, 'fc_dropout': 0.24420917725265057, 'learning_rate_model': 1.9294161071421722e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 9.221396636299655e-05
Epoch 16: reducing lr to 9.273172322295456e-05
Epoch 41: reducing lr to 1.0655811799487318e-05
Epoch 47: reducing lr to 7.492190648114376e-07
[I 2024-06-22 01:01:22,426] Trial 456 finished with value: 0.9767650365829468 and parameters: {'hidden_size': 193, 'n_layers': 6, 'rnn_dropout': 0.08116868625920325, 'bidirectional': True, 'fc_dropout': 0.729148112259034, 'learning_rate_model': 0.0009593779641667671}. Best is trial 153 with value: 0.9688200950622559.
Epoch 16: reducing lr to 0.0004208020110371984
Epoch 23: reducing lr to 0.0003435649410038434
Epoch 27: reducing lr to 0.0002781631405338821
Epoch 30: reducing lr to 0.00022426164117125415
Epoch 33: reducing lr to 0.00016994643655048386
Epoch 36: reducing lr to 0.00011863036999612241
Epoch 39: reducing lr to 7.353779698813897e-05
Epoch 42: reducing lr to 3.750202568026427e-05
Epoch 45: reducing lr to 1.2787368588828597e-05
Epoch 48: reducing lr to 9.467073253357356e-07
[I 2024-06-22 01:01:49,500] Trial 457 finished with value: 1.0106362104415894 and parameters: {'hidden_size': 177, 'n_layers': 2, 'rnn_dropout': 0.29375386072261245, 'bidirectional': True, 'fc_dropout': 0.26481154725263584, 'learning_rate_model': 0.004353506681802027}. Best is trial 153 with value: 0.9688200950622559.
Epoch 34: reducing lr to 1.980889184328812e-06
Epoch 37: reducing lr to 1.3361494219987332e-06
Epoch 40: reducing lr to 7.853005974938346e-07
Epoch 43: reducing lr to 3.6295429433285564e-07
Epoch 46: reducing lr to 9.564869903892673e-08
Epoch 49: reducing lr to 1.7923188635628044e-10
[I 2024-06-22 01:02:37,463] Trial 458 finished with value: 1.0770444869995117 and parameters: {'hidden_size': 102, 'n_layers': 7, 'rnn_dropout': 0.21573621311227864, 'bidirectional': True, 'fc_dropout': 0.4413903764973661, 'learning_rate_model': 5.6607980604122834e-05}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 01:02:54,050] Trial 459 finished with value: 1.1044340133666992 and parameters: {'hidden_size': 48, 'n_layers': 7, 'rnn_dropout': 0.30131817099706404, 'bidirectional': True, 'fc_dropout': 0.11268639751949348, 'learning_rate_model': 2.4290374320837906e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 0.00032298475510573614
Epoch 15: reducing lr to 0.0003293018908946563
Epoch 22: reducing lr to 0.00027631088123106754
Epoch 27: reducing lr to 0.00021470167181662919
Epoch 30: reducing lr to 0.00017309751820962185
Epoch 33: reducing lr to 0.0001311740440398972
Epoch 36: reducing lr to 9.156547023990167e-05
Epoch 39: reducing lr to 5.676053241548064e-05
Epoch 42: reducing lr to 2.894613425275922e-05
Epoch 45: reducing lr to 9.869997185419765e-06
Epoch 48: reducing lr to 7.307209901373425e-07
[I 2024-06-22 01:02:59,097] Trial 460 finished with value: 1.0927602052688599 and parameters: {'hidden_size': 166, 'n_layers': 1, 'rnn_dropout': 0.41305283654363933, 'bidirectional': False, 'fc_dropout': 0.7060816020024561, 'learning_rate_model': 0.0033602768542725305}. Best is trial 153 with value: 0.9688200950622559.
Epoch 28: reducing lr to 5.0511623405919183e-05
Epoch 38: reducing lr to 1.7006739404804243e-05
Epoch 41: reducing lr to 9.377704106454626e-06
Epoch 44: reducing lr to 3.8120068035304814e-06
Epoch 47: reducing lr to 6.593542409461796e-07
[I 2024-06-22 01:04:33,016] Trial 461 finished with value: 0.9714576005935669 and parameters: {'hidden_size': 194, 'n_layers': 5, 'rnn_dropout': 0.007615772973246404, 'bidirectional': True, 'fc_dropout': 0.25118746243755713, 'learning_rate_model': 0.0008443057031695725}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 01:04:40,275] Trial 462 finished with value: 1.0985257625579834 and parameters: {'hidden_size': 122, 'n_layers': 2, 'rnn_dropout': 0.4466292627581215, 'bidirectional': False, 'fc_dropout': 0.15677361163401723, 'learning_rate_model': 4.23649248161928e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 12: reducing lr to 5.920117885835112e-05
Epoch 15: reducing lr to 5.8031629468513664e-05
Epoch 18: reducing lr to 5.507616524485239e-05
Epoch 21: reducing lr to 5.052048939074197e-05
Epoch 24: reducing lr to 4.46508503739898e-05
Epoch 27: reducing lr to 3.783606535414289e-05
Epoch 30: reducing lr to 3.0504322375341328e-05
Epoch 33: reducing lr to 2.3116306738867064e-05
Epoch 36: reducing lr to 1.6136237258267187e-05
Epoch 39: reducing lr to 1.0002694417034046e-05
Epoch 42: reducing lr to 5.101067998541616e-06
Epoch 45: reducing lr to 1.7393523552610974e-06
Epoch 48: reducing lr to 1.2877220239856198e-07
[I 2024-06-22 01:04:55,315] Trial 463 finished with value: 1.0923995971679688 and parameters: {'hidden_size': 155, 'n_layers': 3, 'rnn_dropout': 0.5153565540305731, 'bidirectional': False, 'fc_dropout': 0.704546894325612, 'learning_rate_model': 0.0005921689085628344}. Best is trial 153 with value: 0.9688200950622559.
Epoch 13: reducing lr to 0.0003160043035077847
Epoch 16: reducing lr to 0.000306480138148764
Epoch 19: reducing lr to 0.0002876603286403783
Epoch 22: reducing lr to 0.00026072739820146164
Epoch 25: reducing lr to 0.00022737362043799572
Epoch 28: reducing lr to 0.0001896947655826793
Epoch 33: reducing lr to 0.00012377604190508054
Epoch 36: reducing lr to 8.640132706456242e-05
Epoch 39: reducing lr to 5.355933096547931e-05
Epoch 42: reducing lr to 2.7313619492968284e-05
Epoch 45: reducing lr to 9.313345442441088e-06
Epoch 48: reducing lr to 6.895095181227301e-07
[I 2024-06-22 01:05:02,744] Trial 464 finished with value: 1.0933113098144531 and parameters: {'hidden_size': 53, 'n_layers': 5, 'rnn_dropout': 0.5094687802772401, 'bidirectional': False, 'fc_dropout': 0.6260647824356789, 'learning_rate_model': 0.0031707627204098725}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.00015804545291444496
Epoch 12: reducing lr to 0.00017550115420013102
Epoch 15: reducing lr to 0.0001720340396634147
Epoch 18: reducing lr to 0.00016327260294116973
Epoch 21: reducing lr to 0.0001497673588569084
Epoch 24: reducing lr to 0.00013236688741287032
Epoch 27: reducing lr to 0.00011216454246513652
Epoch 30: reducing lr to 9.04296821145158e-05
Epoch 33: reducing lr to 6.852800217411804e-05
Epoch 36: reducing lr to 4.783567350996369e-05
Epoch 39: reducing lr to 2.965286247932624e-05
Epoch 42: reducing lr to 1.5122052274320904e-05
Epoch 45: reducing lr to 5.156288300262068e-06
Epoch 48: reducing lr to 3.817435832471168e-07
[I 2024-06-22 01:05:10,615] Trial 465 finished with value: 1.092909574508667 and parameters: {'hidden_size': 127, 'n_layers': 2, 'rnn_dropout': 0.5210922895982945, 'bidirectional': False, 'fc_dropout': 0.7333200896647462, 'learning_rate_model': 0.0017554773222146585}. Best is trial 153 with value: 0.9688200950622559.
Epoch 15: reducing lr to 0.00010445776113696783
Epoch 18: reducing lr to 9.913788336080495e-05
Epoch 21: reducing lr to 9.093760181530202e-05
Epoch 24: reducing lr to 8.037216782719055e-05
Epoch 27: reducing lr to 6.810545754656354e-05
Epoch 30: reducing lr to 5.490821556298581e-05
Epoch 33: reducing lr to 4.160968199260357e-05
Epoch 36: reducing lr to 2.9045457324062392e-05
Epoch 39: reducing lr to 1.800499268605813e-05
Epoch 42: reducing lr to 9.1819951880586e-06
Epoch 45: reducing lr to 3.130859059497296e-06
Epoch 48: reducing lr to 2.3179180185745138e-07
[I 2024-06-22 01:05:15,563] Trial 466 finished with value: 1.0917922258377075 and parameters: {'hidden_size': 64, 'n_layers': 3, 'rnn_dropout': 0.7982098924550564, 'bidirectional': False, 'fc_dropout': 0.4776129502276577, 'learning_rate_model': 0.0010659124854827172}. Best is trial 153 with value: 0.9688200950622559.
Epoch 12: reducing lr to 0.0018731982131037176
Epoch 18: reducing lr to 0.0017426777018766959
Epoch 23: reducing lr to 0.0014786609574584538
Epoch 26: reducing lr to 0.0012715719731549821
Epoch 29: reducing lr to 0.001043451612282495
Epoch 32: reducing lr to 0.0008086335207567857
Epoch 35: reducing lr to 0.0005818722688527451
Epoch 38: reducing lr to 0.00037741600694003643
Epoch 41: reducing lr to 0.00020811135831971805
Epoch 44: reducing lr to 8.459660326248681e-05
Epoch 47: reducing lr to 1.4632484149582765e-05
[I 2024-06-22 01:05:33,181] Trial 467 finished with value: 1.2069649696350098 and parameters: {'hidden_size': 80, 'n_layers': 4, 'rnn_dropout': 0.005387765595362648, 'bidirectional': True, 'fc_dropout': 0.01942514025974287, 'learning_rate_model': 0.018736953600696844}. Best is trial 153 with value: 0.9688200950622559.
Epoch 12: reducing lr to 0.0033084689137766153
Epoch 17: reducing lr to 0.0031435698047131465
Epoch 21: reducing lr to 0.0028233469650659334
Epoch 24: reducing lr to 0.0024953210946946787
Epoch 27: reducing lr to 0.0021144755637943504
Epoch 30: reducing lr to 0.0017047397410127883
Epoch 33: reducing lr to 0.0012918591102696955
Epoch 36: reducing lr to 0.0009017766264762507
Epoch 39: reducing lr to 0.0005590024416903272
Epoch 42: reducing lr to 0.00028507413578057473
Epoch 45: reducing lr to 9.720403053551235e-05
Epoch 48: reducing lr to 7.196458530218941e-06
[I 2024-06-22 01:05:35,928] Trial 468 finished with value: 1.076688289642334 and parameters: {'hidden_size': 41, 'n_layers': 1, 'rnn_dropout': 0.3698683233080926, 'bidirectional': True, 'fc_dropout': 0.05247881290803456, 'learning_rate_model': 0.033093469817092956}. Best is trial 153 with value: 0.9688200950622559.
Epoch 12: reducing lr to 0.0008236968990396967
Epoch 18: reducing lr to 0.0007663034317564663
Epoch 21: reducing lr to 0.0007029179359534308
Epoch 24: reducing lr to 0.00062125058490035
Epoch 27: reducing lr to 0.0005264329242267191
Epoch 30: reducing lr to 0.00042442255766553
Epoch 33: reducing lr to 0.00032162924024897656
Epoch 36: reducing lr to 0.00022451189060956629
Epoch 39: reducing lr to 0.00013917270791289957
Epoch 42: reducing lr to 7.097382135316478e-05
Epoch 45: reducing lr to 2.420051710108565e-05
Epoch 48: reducing lr to 1.7916748592455151e-06
[I 2024-06-22 01:07:05,575] Trial 469 finished with value: 1.1206055879592896 and parameters: {'hidden_size': 167, 'n_layers': 6, 'rnn_dropout': 0.37425443825229743, 'bidirectional': True, 'fc_dropout': 0.4657720337114457, 'learning_rate_model': 0.008239155082671501}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 01:08:13,556] Trial 470 finished with value: 1.0635943412780762 and parameters: {'hidden_size': 161, 'n_layers': 5, 'rnn_dropout': 0.44997293174081054, 'bidirectional': True, 'fc_dropout': 0.22899823426094815, 'learning_rate_model': 0.0001530054405241054}. Best is trial 153 with value: 0.9688200950622559.
Epoch 11: reducing lr to 0.00817173721154594
Epoch 14: reducing lr to 0.008136158578738392
Epoch 17: reducing lr to 0.007806494886259338
Epoch 20: reducing lr to 0.007244512036065717
Epoch 23: reducing lr to 0.006485521456773108
Epoch 26: reducing lr to 0.005577213136067851
Epoch 29: reducing lr to 0.004576659569205376
Epoch 32: reducing lr to 0.0035467292370715544
Epoch 35: reducing lr to 0.002552136827384759
Epoch 38: reducing lr to 0.0016553758309453491
Epoch 41: reducing lr to 0.0009127925323061433
Epoch 44: reducing lr to 0.00037104725248985617
Epoch 47: reducing lr to 6.417920851925774e-05
[I 2024-06-22 01:09:21,715] Trial 471 finished with value: 1.1075732707977295 and parameters: {'hidden_size': 142, 'n_layers': 6, 'rnn_dropout': 0.478095829591981, 'bidirectional': True, 'fc_dropout': 0.6125488950166031, 'learning_rate_model': 0.08218172935380007}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 9.49283507143846e-05
Epoch 42: reducing lr to 1.894779546042398e-05
Epoch 45: reducing lr to 6.460782853809485e-06
Epoch 48: reducing lr to 4.783212756101323e-07
[I 2024-06-22 01:09:25,206] Trial 472 finished with value: 0.9881418943405151 and parameters: {'hidden_size': 18, 'n_layers': 2, 'rnn_dropout': 0.6338502974165983, 'bidirectional': True, 'fc_dropout': 0.4439139048606762, 'learning_rate_model': 0.002199597292307988}. Best is trial 153 with value: 0.9688200950622559.
Epoch 16: reducing lr to 0.00037048628967635956
Epoch 22: reducing lr to 0.00031517842219760115
Epoch 25: reducing lr to 0.0002748589501270219
Epoch 28: reducing lr to 0.00022931114001795575
Epoch 31: reducing lr to 0.0001813969155075661
Epoch 34: reducing lr to 0.00013412695586731757
Epoch 37: reducing lr to 9.047131761552285e-05
Epoch 40: reducing lr to 5.317307975424316e-05
Epoch 43: reducing lr to 2.4575809188605e-05
Epoch 46: reducing lr to 6.476419244576389e-06
Epoch 49: reducing lr to 1.2135876908915196e-08
[I 2024-06-22 01:10:12,337] Trial 473 finished with value: 1.0496361255645752 and parameters: {'hidden_size': 149, 'n_layers': 4, 'rnn_dropout': 0.3238787772181782, 'bidirectional': True, 'fc_dropout': 0.07400268652048317, 'learning_rate_model': 0.0038329534919439657}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 0.0049516026349253756
Epoch 9: reducing lr to 0.007470424235016433
Epoch 12: reducing lr to 0.008295512787196339
Epoch 15: reducing lr to 0.008131630714140529
Epoch 18: reducing lr to 0.007717498905749606
Epoch 21: reducing lr to 0.007079138859026256
Epoch 24: reducing lr to 0.006256660887156855
Epoch 27: reducing lr to 0.005301745168174353
Epoch 30: reducing lr to 0.004274391172764754
Epoch 33: reducing lr to 0.0032391520210069947
Epoch 36: reducing lr to 0.002261075963258575
Epoch 39: reducing lr to 0.0014016187015711459
Epoch 42: reducing lr to 0.0007147826382225969
Epoch 45: reducing lr to 0.0002437252092400366
Epoch 48: reducing lr to 1.8044090881848273e-05
[I 2024-06-22 01:11:06,325] Trial 474 finished with value: 1.0946547985076904 and parameters: {'hidden_size': 139, 'n_layers': 5, 'rnn_dropout': 0.0025165763714979587, 'bidirectional': True, 'fc_dropout': 0.42128746856504595, 'learning_rate_model': 0.08297714416999552}. Best is trial 153 with value: 0.9688200950622559.
Epoch 5: reducing lr to 0.004598596968772328
Epoch 9: reducing lr to 0.008688654565587161
Epoch 12: reducing lr to 0.00964829342817123
Epoch 15: reducing lr to 0.009457686485716762
Epoch 18: reducing lr to 0.008976020637227937
Epoch 21: reducing lr to 0.008233560803627995
Epoch 24: reducing lr to 0.007276958238557977
Epoch 27: reducing lr to 0.006166320802119269
Epoch 30: reducing lr to 0.00497143230557993
Epoch 33: reducing lr to 0.0037673727904277967
Epoch 36: reducing lr to 0.0026297981711962013
Epoch 39: reducing lr to 0.0016301859636746185
Epoch 42: reducing lr to 0.00083134494609884
Epoch 45: reducing lr to 0.0002834704007954478
Epoch 48: reducing lr to 2.0986609018476544e-05
[I 2024-06-22 01:11:27,274] Trial 475 finished with value: 1.056039810180664 and parameters: {'hidden_size': 108, 'n_layers': 3, 'rnn_dropout': 0.6510719939156764, 'bidirectional': True, 'fc_dropout': 0.6424000317972007, 'learning_rate_model': 0.09650854086072257}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 01:11:31,893] Trial 476 finished with value: 1.108809471130371 and parameters: {'hidden_size': 36, 'n_layers': 4, 'rnn_dropout': 0.6508212635880003, 'bidirectional': False, 'fc_dropout': 0.6007220477316534, 'learning_rate_model': 2.4739192264457364e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 28: reducing lr to 7.393394903763992e-05
Epoch 32: reducing lr to 5.333411084242923e-05
Epoch 37: reducing lr to 2.916954573349747e-05
Epoch 40: reducing lr to 1.7143937134570725e-05
Epoch 43: reducing lr to 7.923673590244304e-06
Epoch 46: reducing lr to 2.0881115951776764e-06
Epoch 49: reducing lr to 3.912820392606292e-09
[I 2024-06-22 01:11:51,214] Trial 477 finished with value: 0.973401665687561 and parameters: {'hidden_size': 82, 'n_layers': 4, 'rnn_dropout': 0.19086847675406704, 'bidirectional': True, 'fc_dropout': 0.772434063673871, 'learning_rate_model': 0.0012358116928590523}. Best is trial 153 with value: 0.9688200950622559.
Epoch 4: reducing lr to 0.00045645579067993893
Epoch 7: reducing lr to 0.0009055380425153699
Epoch 10: reducing lr to 0.0012221697015227598
Epoch 13: reducing lr to 0.001267225207146249
Epoch 16: reducing lr to 0.0012290318588721713
Epoch 19: reducing lr to 0.0011535615670502453
Epoch 22: reducing lr to 0.001045556429222523
Epoch 25: reducing lr to 0.0009118027193323764
Epoch 28: reducing lr to 0.0007607047940223639
Epoch 31: reducing lr to 0.0006017566492263316
Epoch 34: reducing lr to 0.00044494575504663744
Epoch 37: reducing lr to 0.00030012482178693627
Epoch 40: reducing lr to 0.00017639359639840864
Epoch 43: reducing lr to 8.152650527700842e-05
Epoch 46: reducing lr to 2.148453479871148e-05
Epoch 49: reducing lr to 4.0258923938822634e-08
[I 2024-06-22 01:11:58,241] Trial 478 finished with value: 1.1073917150497437 and parameters: {'hidden_size': 35, 'n_layers': 7, 'rnn_dropout': 0.6673887915970793, 'bidirectional': False, 'fc_dropout': 0.7053527354831539, 'learning_rate_model': 0.012715239636234955}. Best is trial 153 with value: 0.9688200950622559.
Epoch 41: reducing lr to 1.4915980358447397e-05
[I 2024-06-22 01:12:40,364] Trial 479 finished with value: 0.9817693829536438 and parameters: {'hidden_size': 121, 'n_layers': 5, 'rnn_dropout': 0.675512546114687, 'bidirectional': True, 'fc_dropout': 0.5531907388431758, 'learning_rate_model': 0.0013429350235452983}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 01:12:42,646] Trial 480 finished with value: 1.1051146984100342 and parameters: {'hidden_size': 64, 'n_layers': 1, 'rnn_dropout': 0.1883982090399309, 'bidirectional': False, 'fc_dropout': 0.6645620553924543, 'learning_rate_model': 2.7066340582671316e-05}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 01:12:54,613] Trial 481 finished with value: 1.1047310829162598 and parameters: {'hidden_size': 29, 'n_layers': 6, 'rnn_dropout': 0.2278666976946454, 'bidirectional': True, 'fc_dropout': 0.07883120851244244, 'learning_rate_model': 3.4864881188637084e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 10: reducing lr to 0.0006571265211476778
Epoch 13: reducing lr to 0.0006813516084101294
Epoch 16: reducing lr to 0.0006608161115384135
Epoch 22: reducing lr to 0.000562166496307788
Epoch 25: reducing lr to 0.0004902508613831146
Epoch 29: reducing lr to 0.00038072847992712825
Epoch 32: reducing lr to 0.00029504943741703947
Epoch 35: reducing lr to 0.00021231012710542282
Epoch 38: reducing lr to 0.00013770933019895923
Epoch 41: reducing lr to 7.593444696042604e-05
Epoch 44: reducing lr to 3.0867110451505195e-05
Epoch 47: reducing lr to 5.339014653149218e-06
[I 2024-06-22 01:13:00,148] Trial 482 finished with value: 1.0938819646835327 and parameters: {'hidden_size': 45, 'n_layers': 4, 'rnn_dropout': 0.6513086517790563, 'bidirectional': False, 'fc_dropout': 0.354873630896267, 'learning_rate_model': 0.006836629297312474}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 01:13:03,333] Trial 483 finished with value: 1.0922870635986328 and parameters: {'hidden_size': 106, 'n_layers': 1, 'rnn_dropout': 0.06434514717563769, 'bidirectional': False, 'fc_dropout': 0.5535337834214279, 'learning_rate_model': 0.0001180403062074297}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 0.0020574238448998533
Epoch 9: reducing lr to 0.003104010981057286
Epoch 15: reducing lr to 0.003378746673084943
Epoch 19: reducing lr to 0.003127898818038686
Epoch 22: reducing lr to 0.0028350413298880574
Epoch 28: reducing lr to 0.002062662014809718
Epoch 31: reducing lr to 0.0016316718289037617
Epoch 34: reducing lr to 0.0012064768288532
Epoch 37: reducing lr to 0.0008137927806765602
Epoch 40: reducing lr to 0.0004782937794079123
Epoch 43: reducing lr to 0.00022106029429088333
Epoch 46: reducing lr to 5.8255625813575376e-05
Epoch 49: reducing lr to 1.091626526061873e-07
[I 2024-06-22 01:13:26,001] Trial 484 finished with value: 1.1542633771896362 and parameters: {'hidden_size': 146, 'n_layers': 5, 'rnn_dropout': 0.03463212869912598, 'bidirectional': False, 'fc_dropout': 0.06281833186728285, 'learning_rate_model': 0.03447755556815082}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 01:14:06,238] Trial 485 finished with value: 1.1067428588867188 and parameters: {'hidden_size': 188, 'n_layers': 6, 'rnn_dropout': 0.0939020832807584, 'bidirectional': False, 'fc_dropout': 0.7103840719521407, 'learning_rate_model': 2.416039983331099e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 13: reducing lr to 0.00026793867752349175
Epoch 16: reducing lr to 0.00025986317904931285
Epoch 22: reducing lr to 0.0002210696294093966
Epoch 25: reducing lr to 0.00019278910599514708
Epoch 28: reducing lr to 0.00016084136848503318
Epoch 31: reducing lr to 0.0001272338017547525
Epoch 34: reducing lr to 9.407812952629238e-05
Epoch 37: reducing lr to 6.345758227352303e-05
Epoch 40: reducing lr to 3.7296185931335896e-05
Epoch 43: reducing lr to 1.723774423350198e-05
Epoch 46: reducing lr to 4.542632050492197e-06
Epoch 49: reducing lr to 8.512238217680407e-09
[I 2024-06-22 01:14:17,198] Trial 486 finished with value: 1.0927908420562744 and parameters: {'hidden_size': 166, 'n_layers': 2, 'rnn_dropout': 0.47382990861273105, 'bidirectional': False, 'fc_dropout': 0.23573187919909558, 'learning_rate_model': 0.0026884759499057934}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 0.0030958151746195454
Epoch 12: reducing lr to 0.005186477240058248
Epoch 15: reducing lr to 0.005084015745059482
Epoch 18: reducing lr to 0.0048250944156970794
Epoch 21: reducing lr to 0.004425982276613493
Epoch 24: reducing lr to 0.003911756888626159
Epoch 30: reducing lr to 0.0026724125562035106
Epoch 33: reducing lr to 0.0020251657329696353
Epoch 36: reducing lr to 0.0014136581212415677
Epoch 39: reducing lr to 0.0008763127345374905
Epoch 42: reducing lr to 0.0004468926731632732
Epoch 45: reducing lr to 0.0001523806041867498
Epoch 48: reducing lr to 1.1281432393268898e-05
[I 2024-06-22 01:14:29,818] Trial 487 finished with value: 1.1073811054229736 and parameters: {'hidden_size': 69, 'n_layers': 7, 'rnn_dropout': 0.6516044863538806, 'bidirectional': False, 'fc_dropout': 0.586000892086763, 'learning_rate_model': 0.05187853731561344}. Best is trial 153 with value: 0.9688200950622559.
Epoch 15: reducing lr to 7.538742637983652e-05
Epoch 18: reducing lr to 7.15480573388484e-05
Epoch 21: reducing lr to 6.562989372346193e-05
Epoch 24: reducing lr to 5.8004793699487795e-05
Epoch 27: reducing lr to 4.915187833792851e-05
Epoch 30: reducing lr to 3.96273959287248e-05
Epoch 33: reducing lr to 3.0029811129042805e-05
Epoch 36: reducing lr to 2.0962178892722887e-05
Epoch 39: reducing lr to 1.2994248065588041e-05
Epoch 42: reducing lr to 6.626668796319882e-06
Epoch 45: reducing lr to 2.259548781100246e-06
Epoch 48: reducing lr to 1.672847207118025e-07
[I 2024-06-22 01:14:34,501] Trial 488 finished with value: 1.0935215950012207 and parameters: {'hidden_size': 47, 'n_layers': 3, 'rnn_dropout': 0.07498934085971376, 'bidirectional': False, 'fc_dropout': 0.5061700316950029, 'learning_rate_model': 0.0007692716955833606}. Best is trial 153 with value: 0.9688200950622559.
Epoch 5: reducing lr to 0.0021495044369060994
Epoch 8: reducing lr to 0.003678849714944925
Epoch 12: reducing lr to 0.00450986456809731
Epoch 15: reducing lr to 0.00442076990046429
Epoch 18: reducing lr to 0.00419562669146738
Epoch 21: reducing lr to 0.0038485815563131026
Epoch 24: reducing lr to 0.003401440510481748
Epoch 27: reducing lr to 0.0028822995390875195
Epoch 30: reducing lr to 0.0023237774197627094
Epoch 33: reducing lr to 0.0017609685265951355
Epoch 36: reducing lr to 0.0012292364117881982
Epoch 39: reducing lr to 0.0007619915347432838
Epoch 42: reducing lr to 0.00038859236031636436
Epoch 45: reducing lr to 0.00013250147564117452
Epoch 48: reducing lr to 9.809689674299126e-06
[I 2024-06-22 01:14:43,764] Trial 489 finished with value: 1.1311099529266357 and parameters: {'hidden_size': 148, 'n_layers': 2, 'rnn_dropout': 0.13844537534574758, 'bidirectional': False, 'fc_dropout': 0.3377012542526669, 'learning_rate_model': 0.04511061486539399}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 01:14:49,033] Trial 490 finished with value: 1.0917434692382812 and parameters: {'hidden_size': 65, 'n_layers': 3, 'rnn_dropout': 0.1721677281306616, 'bidirectional': False, 'fc_dropout': 0.4603416456894996, 'learning_rate_model': 0.00012985537014035867}. Best is trial 153 with value: 0.9688200950622559.
Epoch 5: reducing lr to 0.00025851772802037466
Epoch 8: reducing lr to 0.0004424498287637222
Epoch 11: reducing lr to 0.0005394731661330706
Epoch 18: reducing lr to 0.0005046018334630545
Epoch 22: reducing lr to 0.0004461220186195183
Epoch 27: reducing lr to 0.00034664991405720426
Epoch 30: reducing lr to 0.00027947728260881334
Epoch 33: reducing lr to 0.00021178908719351867
Epoch 36: reducing lr to 0.00014783845007214783
Epoch 39: reducing lr to 9.164359791512143e-05
Epoch 42: reducing lr to 4.673542998573947e-05
Epoch 45: reducing lr to 1.5935757030307445e-05
Epoch 48: reducing lr to 1.1797969074374589e-06
[I 2024-06-22 01:14:58,044] Trial 491 finished with value: 1.0980098247528076 and parameters: {'hidden_size': 64, 'n_layers': 6, 'rnn_dropout': 0.7281429660580356, 'bidirectional': False, 'fc_dropout': 0.10648771590072253, 'learning_rate_model': 0.005425387109872358}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 01:15:14,906] Trial 492 finished with value: 1.0825002193450928 and parameters: {'hidden_size': 55, 'n_layers': 5, 'rnn_dropout': 0.16134298784238643, 'bidirectional': True, 'fc_dropout': 0.535693411748701, 'learning_rate_model': 3.9349049956044136e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 6: reducing lr to 0.0006569107349648891
Epoch 9: reducing lr to 0.000991073443597948
Epoch 12: reducing lr to 0.0011005348780435628
Epoch 15: reducing lr to 0.001078793251948745
Epoch 18: reducing lr to 0.0010238519227104954
Epoch 21: reducing lr to 0.0009391630657114569
Epoch 27: reducing lr to 0.0007033628446791655
Epoch 33: reducing lr to 0.00042972627079854094
Epoch 36: reducing lr to 0.0002999685520722363
Epoch 39: reducing lr to 0.00018594754855636998
Epoch 42: reducing lr to 9.482755843594169e-05
Epoch 45: reducing lr to 3.233411849368987e-05
Epoch 48: reducing lr to 2.393842534811376e-06
[I 2024-06-22 01:15:20,439] Trial 493 finished with value: 1.0942864418029785 and parameters: {'hidden_size': 72, 'n_layers': 3, 'rnn_dropout': 0.5080639460360391, 'bidirectional': False, 'fc_dropout': 0.14667752541513865, 'learning_rate_model': 0.011008269600943211}. Best is trial 153 with value: 0.9688200950622559.
Epoch 8: reducing lr to 0.0005713681837740696
Epoch 11: reducing lr to 0.0006966615943542497
Epoch 14: reducing lr to 0.0006936284244890183
Epoch 17: reducing lr to 0.0006655237476427448
Epoch 20: reducing lr to 0.000617613265663165
Epoch 23: reducing lr to 0.0005529073685715707
Epoch 29: reducing lr to 0.0003901719878845771
Epoch 32: reducing lr to 0.0003023677806904976
Epoch 35: reducing lr to 0.00021757622218491565
Epoch 38: reducing lr to 0.00014112504303399003
Epoch 41: reducing lr to 7.78179087761865e-05
Epoch 44: reducing lr to 3.1632731671194394e-05
Epoch 47: reducing lr to 5.471442433103123e-06
[I 2024-06-22 01:15:38,915] Trial 494 finished with value: 1.0942223072052002 and parameters: {'hidden_size': 147, 'n_layers': 4, 'rnn_dropout': 0.5201303890525476, 'bidirectional': False, 'fc_dropout': 0.3324573765733087, 'learning_rate_model': 0.0070062035912650976}. Best is trial 153 with value: 0.9688200950622559.
Epoch 9: reducing lr to 0.007780834277948037
Epoch 12: reducing lr to 0.008640206796452644
Epoch 15: reducing lr to 0.008469515117980515
Epoch 18: reducing lr to 0.008038175361502867
Epoch 21: reducing lr to 0.007373290265695905
Epoch 24: reducing lr to 0.0065166368019767135
Epoch 27: reducing lr to 0.005522042556045767
Epoch 30: reducing lr to 0.004452000088363569
Epoch 33: reducing lr to 0.0033737448213984125
Epoch 36: reducing lr to 0.002355027881482556
Epoch 39: reducing lr to 0.0014598585695680788
Epoch 42: reducing lr to 0.0007444831883436254
Epoch 45: reducing lr to 0.00025385244569725156
Epoch 48: reducing lr to 1.879385647066828e-05
[I 2024-06-22 01:15:48,009] Trial 495 finished with value: 1.049367070198059 and parameters: {'hidden_size': 47, 'n_layers': 3, 'rnn_dropout': 0.39835676310994095, 'bidirectional': True, 'fc_dropout': 0.6632059927588773, 'learning_rate_model': 0.08642499908075395}. Best is trial 153 with value: 0.9688200950622559.
Epoch 34: reducing lr to 3.0949385133525674e-06
Epoch 42: reducing lr to 7.61877272881272e-07
Epoch 45: reducing lr to 2.5978344718886455e-07
Epoch 48: reducing lr to 1.923295561133024e-08
[I 2024-06-22 01:16:13,113] Trial 496 finished with value: 1.075984001159668 and parameters: {'hidden_size': 65, 'n_layers': 7, 'rnn_dropout': 0.23224011102032735, 'bidirectional': True, 'fc_dropout': 0.47808691910149603, 'learning_rate_model': 8.844423035919478e-05}. Best is trial 153 with value: 0.9688200950622559.
Epoch 5: reducing lr to 0.0006561202462289808
Epoch 9: reducing lr to 0.0012396829319211962
Epoch 14: reducing lr to 0.001363226339534234
Epoch 17: reducing lr to 0.0013079906623499227
Epoch 20: reducing lr to 0.0012138295399558136
Epoch 23: reducing lr to 0.0010866594585055983
Epoch 27: reducing lr to 0.0008798005023026593
Epoch 30: reducing lr to 0.0007093157783989539
Epoch 33: reducing lr to 0.0005375225486550412
Epoch 36: reducing lr to 0.0003752152744271514
Epoch 39: reducing lr to 0.00023259225001637102
Epoch 42: reducing lr to 0.00011861492851834125
Epoch 45: reducing lr to 4.044509019414909e-05
Epoch 48: reducing lr to 2.9943348308671125e-06
[I 2024-06-22 01:16:22,253] Trial 497 finished with value: 1.0941921472549438 and parameters: {'hidden_size': 143, 'n_layers': 2, 'rnn_dropout': 0.44774919920141837, 'bidirectional': False, 'fc_dropout': 0.3851748122989864, 'learning_rate_model': 0.013769679757267702}. Best is trial 153 with value: 0.9688200950622559.
Epoch 5: reducing lr to 0.0009051541580921467
Epoch 8: reducing lr to 0.001549159917655929
Epoch 11: reducing lr to 0.0018888699945018828
Epoch 14: reducing lr to 0.0018806461113524558
Epoch 17: reducing lr to 0.0018044454405672314
Epoch 22: reducing lr to 0.001562017441751977
Epoch 27: reducing lr to 0.001213733439149033
Epoch 30: reducing lr to 0.0009785403360257133
Epoch 33: reducing lr to 0.0007415420767454869
Epoch 36: reducing lr to 0.000517630217600227
Epoch 39: reducing lr to 0.0003208738694657692
Epoch 42: reducing lr to 0.00016363585238720056
Epoch 45: reducing lr to 5.579623822623142e-05
Epoch 48: reducing lr to 4.130850462940108e-06
[I 2024-06-22 01:16:29,689] Trial 498 finished with value: 1.0959656238555908 and parameters: {'hidden_size': 90, 'n_layers': 3, 'rnn_dropout': 0.6297817091020858, 'bidirectional': False, 'fc_dropout': 0.4618580250594632, 'learning_rate_model': 0.01899603458287186}. Best is trial 153 with value: 0.9688200950622559.
Epoch 11: reducing lr to 7.284420692903033e-05
Epoch 14: reducing lr to 7.252705315580043e-05
Epoch 17: reducing lr to 6.958837688534932e-05
Epoch 23: reducing lr to 5.781300289151995e-05
Epoch 26: reducing lr to 4.9716193418091395e-05
Epoch 29: reducing lr to 4.0797094677969135e-05
Epoch 32: reducing lr to 3.161612662989753e-05
Epoch 35: reducing lr to 2.275016662338851e-05
Epoch 38: reducing lr to 1.4756291894007958e-05
Epoch 41: reducing lr to 8.136782471741192e-06
Epoch 44: reducing lr to 3.3075761176742437e-06
Epoch 47: reducing lr to 5.721040000298519e-07
[I 2024-06-22 01:16:40,721] Trial 499 finished with value: 1.0925798416137695 and parameters: {'hidden_size': 123, 'n_layers': 3, 'rnn_dropout': 0.1284694628073942, 'bidirectional': False, 'fc_dropout': 0.08670311032332174, 'learning_rate_model': 0.0007325814259390815}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.599699372375032e-05
Epoch 36: reducing lr to 2.6241185895450958e-05
Epoch 39: reducing lr to 1.6266652469943095e-05
Epoch 42: reducing lr to 8.295494883510488e-06
Epoch 45: reducing lr to 2.8285818906580964e-06
Epoch 48: reducing lr to 2.0941284186783714e-07
[I 2024-06-22 01:17:48,054] Trial 500 finished with value: 0.9719099998474121 and parameters: {'hidden_size': 185, 'n_layers': 4, 'rnn_dropout': 0.05912222071490574, 'bidirectional': True, 'fc_dropout': 0.27905053795758994, 'learning_rate_model': 0.000963001111249918}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.144720152522403e-05
Epoch 27: reducing lr to 5.784635038879348e-05
Epoch 33: reducing lr to 3.5341782154018696e-05
Epoch 36: reducing lr to 2.4670177135536165e-05
Epoch 39: reducing lr to 1.529279962554096e-05
Epoch 42: reducing lr to 7.79885973974275e-06
Epoch 45: reducing lr to 2.6592401945142945e-06
Epoch 48: reducing lr to 1.968757023374846e-07
[I 2024-06-22 01:18:55,795] Trial 501 finished with value: 0.9727346897125244 and parameters: {'hidden_size': 183, 'n_layers': 4, 'rnn_dropout': 0.058107119834401755, 'bidirectional': True, 'fc_dropout': 0.3247115491525525, 'learning_rate_model': 0.000905348107776338}. Best is trial 153 with value: 0.9688200950622559.
Epoch 22: reducing lr to 8.240328427316126e-05
Epoch 27: reducing lr to 6.402977261627806e-05
Epoch 33: reducing lr to 3.911960322416893e-05
Epoch 36: reducing lr to 2.7307268682895195e-05
Epoch 39: reducing lr to 1.6927506681206085e-05
Epoch 42: reducing lr to 8.632510304378926e-06
Epoch 45: reducing lr to 2.9434967606842145e-06
Epoch 48: reducing lr to 2.1792051477082224e-07
[I 2024-06-22 01:20:03,575] Trial 502 finished with value: 0.972783088684082 and parameters: {'hidden_size': 183, 'n_layers': 4, 'rnn_dropout': 0.0706231064515491, 'bidirectional': True, 'fc_dropout': 0.7397362828030346, 'learning_rate_model': 0.0010021243015311619}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.701825167096625e-05
Epoch 26: reducing lr to 6.623171441175299e-05
Epoch 36: reducing lr to 2.6593818523756147e-05
Epoch 39: reducing lr to 1.6485245960231873e-05
Epoch 42: reducing lr to 8.406970873030132e-06
Epoch 45: reducing lr to 2.866592759162784e-06
Epoch 48: reducing lr to 2.1222696014444858e-07
[I 2024-06-22 01:21:08,690] Trial 503 finished with value: 0.9723117351531982 and parameters: {'hidden_size': 184, 'n_layers': 4, 'rnn_dropout': 0.07268358750832977, 'bidirectional': True, 'fc_dropout': 0.7427138565956712, 'learning_rate_model': 0.0009759420512773183}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.540048800192129e-05
Epoch 26: reducing lr to 6.48405212985719e-05
Epoch 36: reducing lr to 2.603521699106094e-05
Epoch 39: reducing lr to 1.6138974376404336e-05
Epoch 42: reducing lr to 8.23038296367054e-06
Epoch 45: reducing lr to 2.806380153460794e-06
Epoch 48: reducing lr to 2.0776914581778928e-07
[I 2024-06-22 01:22:13,892] Trial 504 finished with value: 0.9726629257202148 and parameters: {'hidden_size': 184, 'n_layers': 4, 'rnn_dropout': 0.05583335597367087, 'bidirectional': True, 'fc_dropout': 0.278909341809633, 'learning_rate_model': 0.0009554424481391592}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.367622627432487e-05
Epoch 27: reducing lr to 5.9651052937097734e-05
Epoch 33: reducing lr to 3.64443824716928e-05
Epoch 36: reducing lr to 2.543984248597537e-05
Epoch 39: reducing lr to 1.5769907589473405e-05
Epoch 42: reducing lr to 8.042170198424819e-06
Epoch 45: reducing lr to 2.74220372675683e-06
Epoch 48: reducing lr to 2.0301787171065223e-07
[I 2024-06-22 01:23:20,464] Trial 505 finished with value: 0.972718358039856 and parameters: {'hidden_size': 183, 'n_layers': 4, 'rnn_dropout': 0.05416203828300756, 'bidirectional': True, 'fc_dropout': 0.2820181171048837, 'learning_rate_model': 0.0009335933475576695}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.667370433128693e-05
Epoch 26: reducing lr to 6.593542151353893e-05
Epoch 33: reducing lr to 3.7927102777584753e-05
Epoch 38: reducing lr to 1.9570330291099444e-05
Epoch 41: reducing lr to 1.0791296460018222e-05
Epoch 44: reducing lr to 4.386627585763745e-06
Epoch 47: reducing lr to 7.587451049263783e-07
[I 2024-06-22 01:24:25,737] Trial 506 finished with value: 0.9724721908569336 and parameters: {'hidden_size': 182, 'n_layers': 4, 'rnn_dropout': 0.05617023461229681, 'bidirectional': True, 'fc_dropout': 0.27711878812401847, 'learning_rate_model': 0.0009715760960634121}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.7756822804568e-05
Epoch 31: reducing lr to 4.662998013618946e-05
Epoch 37: reducing lr to 2.3256601312665407e-05
Epoch 40: reducing lr to 1.3668697980793163e-05
Epoch 43: reducing lr to 6.3174695726710765e-06
Epoch 46: reducing lr to 1.6648315098590415e-06
Epoch 49: reducing lr to 3.1196544749171813e-09
[I 2024-06-22 01:25:33,315] Trial 507 finished with value: 0.9696123600006104 and parameters: {'hidden_size': 186, 'n_layers': 4, 'rnn_dropout': 0.055828250296416894, 'bidirectional': True, 'fc_dropout': 0.27244827091142565, 'learning_rate_model': 0.0009853009059838738}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.520511973642329e-05
Epoch 26: reducing lr to 6.467251469124376e-05
Epoch 36: reducing lr to 2.596775780982489e-05
Epoch 39: reducing lr to 1.609715709491997e-05
Epoch 42: reducing lr to 8.209057430022071e-06
Epoch 45: reducing lr to 2.7991086140127255e-06
Epoch 48: reducing lr to 2.0723080052696077e-07
[I 2024-06-22 01:26:37,003] Trial 508 finished with value: 0.9725885391235352 and parameters: {'hidden_size': 184, 'n_layers': 4, 'rnn_dropout': 0.05185993747671741, 'bidirectional': True, 'fc_dropout': 0.28221603238641696, 'learning_rate_model': 0.000952966825781498}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.695028357946727e-05
Epoch 26: reducing lr to 6.61732653671745e-05
Epoch 33: reducing lr to 3.8063914343732114e-05
Epoch 38: reducing lr to 1.9640924861764323e-05
Epoch 41: reducing lr to 1.0830223086660752e-05
Epoch 44: reducing lr to 4.4024511353144e-06
Epoch 47: reducing lr to 7.614820686939602e-07
[I 2024-06-22 01:27:42,334] Trial 509 finished with value: 0.9723843336105347 and parameters: {'hidden_size': 182, 'n_layers': 4, 'rnn_dropout': 0.05238686527282633, 'bidirectional': True, 'fc_dropout': 0.284282283211418, 'learning_rate_model': 0.0009750807889505347}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.633134188494914e-05
Epoch 26: reducing lr to 6.564100751063445e-05
Epoch 36: reducing lr to 2.6356633781241186e-05
Epoch 39: reducing lr to 1.6338217476342627e-05
Epoch 42: reducing lr to 8.33199084637209e-06
Epoch 45: reducing lr to 2.8410262138819744e-06
Epoch 48: reducing lr to 2.1033415197733346e-07
[I 2024-06-22 01:28:46,082] Trial 510 finished with value: 0.972213625907898 and parameters: {'hidden_size': 184, 'n_layers': 4, 'rnn_dropout': 0.06333771013690329, 'bidirectional': True, 'fc_dropout': 0.2772680650765265, 'learning_rate_model': 0.0009672378268751337}. Best is trial 153 with value: 0.9688200950622559.
Epoch 22: reducing lr to 8.64511592099378e-05
Epoch 27: reducing lr to 6.717509035533509e-05
Epoch 33: reducing lr to 4.104126524073166e-05
Epoch 36: reducing lr to 2.864867648560963e-05
Epoch 39: reducing lr to 1.775903215548003e-05
Epoch 42: reducing lr to 9.056562845618968e-06
Epoch 45: reducing lr to 3.0880893806161954e-06
Epoch 48: reducing lr to 2.2862536710444547e-07
[I 2024-06-22 01:29:52,509] Trial 511 finished with value: 0.9726461172103882 and parameters: {'hidden_size': 183, 'n_layers': 4, 'rnn_dropout': 0.06407334036769013, 'bidirectional': True, 'fc_dropout': 0.2847783381124447, 'learning_rate_model': 0.0010513513909546337}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.378910629422236e-05
Epoch 27: reducing lr to 5.9742444860692945e-05
Epoch 33: reducing lr to 3.650021924328909e-05
Epoch 36: reducing lr to 2.5478819101244906e-05
Epoch 39: reducing lr to 1.5794068809075687e-05
Epoch 42: reducing lr to 8.054491680915484e-06
Epoch 45: reducing lr to 2.7464050821585918e-06
Epoch 48: reducing lr to 2.0332891724810172e-07
[I 2024-06-22 01:30:58,916] Trial 512 finished with value: 0.9727122187614441 and parameters: {'hidden_size': 183, 'n_layers': 4, 'rnn_dropout': 0.05623258919178429, 'bidirectional': True, 'fc_dropout': 0.2759696237243719, 'learning_rate_model': 0.0009350237144613174}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.21176625433965e-05
Epoch 26: reducing lr to 6.201746113406015e-05
Epoch 36: reducing lr to 2.490168224319174e-05
Epoch 39: reducing lr to 1.5436307359766598e-05
Epoch 42: reducing lr to 7.87204429183253e-06
Epoch 45: reducing lr to 2.684194522330056e-06
Epoch 48: reducing lr to 1.987231852482803e-07
[I 2024-06-22 01:32:02,647] Trial 513 finished with value: 0.9730163812637329 and parameters: {'hidden_size': 184, 'n_layers': 4, 'rnn_dropout': 0.05467296482856637, 'bidirectional': True, 'fc_dropout': 0.2750963056234658, 'learning_rate_model': 0.0009138439004901498}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.499696789925864e-05
Epoch 27: reducing lr to 6.072037519434543e-05
Epoch 33: reducing lr to 3.709769515285743e-05
Epoch 36: reducing lr to 2.589588455818849e-05
Epoch 39: reducing lr to 1.605260357470514e-05
Epoch 42: reducing lr to 8.186336498369566e-06
Epoch 45: reducing lr to 2.791361274437007e-06
Epoch 48: reducing lr to 2.066572295786177e-07
[I 2024-06-22 01:33:09,042] Trial 514 finished with value: 0.9727257490158081 and parameters: {'hidden_size': 183, 'n_layers': 4, 'rnn_dropout': 0.05920968704348517, 'bidirectional': True, 'fc_dropout': 0.27531428454018786, 'learning_rate_model': 0.000950329215519875}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.152545429234595e-05
Epoch 27: reducing lr to 5.790970678189489e-05
Epoch 33: reducing lr to 3.5380490349574763e-05
Epoch 36: reducing lr to 2.4697197222887826e-05
Epoch 39: reducing lr to 1.5309549111345764e-05
Epoch 42: reducing lr to 7.807401464848871e-06
Epoch 45: reducing lr to 2.6621527355126907e-06
Epoch 48: reducing lr to 1.970913310557292e-07
[I 2024-06-22 01:34:15,520] Trial 515 finished with value: 0.9726414084434509 and parameters: {'hidden_size': 183, 'n_layers': 4, 'rnn_dropout': 0.05198552419285366, 'bidirectional': True, 'fc_dropout': 0.27823555978034775, 'learning_rate_model': 0.000906339693074708}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.505978717683537e-05
Epoch 26: reducing lr to 6.45475362040343e-05
Epoch 36: reducing lr to 2.5917575578582025e-05
Epoch 39: reducing lr to 1.6066049624432684e-05
Epoch 42: reducing lr to 8.193193572185117e-06
Epoch 45: reducing lr to 2.793699386278428e-06
Epoch 48: reducing lr to 2.068303306816655e-07
[I 2024-06-22 01:35:19,327] Trial 516 finished with value: 0.9725753664970398 and parameters: {'hidden_size': 184, 'n_layers': 4, 'rnn_dropout': 0.05148816396652389, 'bidirectional': True, 'fc_dropout': 0.27795132706780706, 'learning_rate_model': 0.0009511252342983836}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.781710686802574e-05
Epoch 27: reducing lr to 6.300366612037e-05
Epoch 33: reducing lr to 3.8492693626563064e-05
Epoch 36: reducing lr to 2.686965716818708e-05
Epoch 39: reducing lr to 1.6656235616896593e-05
Epoch 42: reducing lr to 8.494170364420047e-06
Epoch 45: reducing lr to 2.896325874026176e-06
Epoch 48: reducing lr to 2.1442823849588798e-07
[I 2024-06-22 01:36:25,715] Trial 517 finished with value: 0.9727483987808228 and parameters: {'hidden_size': 183, 'n_layers': 4, 'rnn_dropout': 0.056598093372647554, 'bidirectional': True, 'fc_dropout': 0.276374966697184, 'learning_rate_model': 0.0009860647996230286}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.147963033787671e-05
Epoch 26: reducing lr to 6.146878642508311e-05
Epoch 33: reducing lr to 3.5357823258608075e-05
Epoch 36: reducing lr to 2.468137455874321e-05
Epoch 39: reducing lr to 1.5299740797810895e-05
Epoch 42: reducing lr to 7.802399525150792e-06
Epoch 45: reducing lr to 2.6604471837346703e-06
Epoch 48: reducing lr to 1.9696506126450248e-07
[I 2024-06-22 01:37:32,159] Trial 518 finished with value: 0.972774863243103 and parameters: {'hidden_size': 183, 'n_layers': 4, 'rnn_dropout': 0.05418789607573603, 'bidirectional': True, 'fc_dropout': 0.2799678762445611, 'learning_rate_model': 0.0009057590316970206}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.041044363318741e-05
Epoch 26: reducing lr to 6.054934113852488e-05
Epoch 33: reducing lr to 3.482894371129981e-05
Epoch 36: reducing lr to 2.4312192493769914e-05
Epoch 39: reducing lr to 1.5070888474863952e-05
Epoch 42: reducing lr to 7.685691844969286e-06
Epoch 45: reducing lr to 2.6206524234101957e-06
Epoch 48: reducing lr to 1.940188733254148e-07
[I 2024-06-22 01:38:38,504] Trial 519 finished with value: 0.9727081656455994 and parameters: {'hidden_size': 183, 'n_layers': 4, 'rnn_dropout': 0.05032269140598372, 'bidirectional': True, 'fc_dropout': 0.2783798907601341, 'learning_rate_model': 0.0008922107591365012}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.41713182630791e-05
Epoch 27: reducing lr to 6.0051898093307965e-05
Epoch 33: reducing lr to 3.668928266147244e-05
Epoch 36: reducing lr to 2.5610793997024257e-05
Epoch 39: reducing lr to 1.5875878746056155e-05
Epoch 42: reducing lr to 8.096212244805058e-06
Epoch 45: reducing lr to 2.7606308797925172e-06
Epoch 48: reducing lr to 2.0438211804818703e-07
[I 2024-06-22 01:39:44,888] Trial 520 finished with value: 0.9725663661956787 and parameters: {'hidden_size': 183, 'n_layers': 4, 'rnn_dropout': 0.05244755840727419, 'bidirectional': True, 'fc_dropout': 0.2746733619898856, 'learning_rate_model': 0.0009398669396036172}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.912286892342553e-05
Epoch 26: reducing lr to 5.944209345877921e-05
Epoch 33: reducing lr to 3.419203724151509e-05
Epoch 36: reducing lr to 2.386760270596909e-05
Epoch 39: reducing lr to 1.4795291647851062e-05
Epoch 42: reducing lr to 7.545145898431023e-06
Epoch 45: reducing lr to 2.5727293368715406e-06
Epoch 48: reducing lr to 1.9047090825631206e-07
[I 2024-06-22 01:40:51,274] Trial 521 finished with value: 0.972775936126709 and parameters: {'hidden_size': 183, 'n_layers': 4, 'rnn_dropout': 0.051849592736534146, 'bidirectional': True, 'fc_dropout': 0.27657358161712625, 'learning_rate_model': 0.0008758951680116054}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.88527853592217e-05
Epoch 26: reducing lr to 5.920983555752127e-05
Epoch 36: reducing lr to 2.3774344898412374e-05
Epoch 39: reducing lr to 1.4737482052214733e-05
Epoch 42: reducing lr to 7.515664774044477e-06
Epoch 45: reducing lr to 2.562676919779243e-06
Epoch 48: reducing lr to 1.897266818869461e-07
[I 2024-06-22 01:41:55,059] Trial 522 finished with value: 0.9731007218360901 and parameters: {'hidden_size': 184, 'n_layers': 4, 'rnn_dropout': 0.05418092493182881, 'bidirectional': True, 'fc_dropout': 0.27630150392450503, 'learning_rate_model': 0.0008724727856289017}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.00389336767674e-05
Epoch 27: reducing lr to 6.480254094454783e-05
Epoch 33: reducing lr to 3.9591733440330775e-05
Epoch 36: reducing lr to 2.763683712437877e-05
Epoch 39: reducing lr to 1.7131802909434223e-05
Epoch 42: reducing lr to 8.736695127846352e-06
Epoch 45: reducing lr to 2.9790215014115044e-06
Epoch 48: reducing lr to 2.2055057364834227e-07
[I 2024-06-22 01:43:01,458] Trial 523 finished with value: 0.9725857973098755 and parameters: {'hidden_size': 183, 'n_layers': 4, 'rnn_dropout': 0.05267743155541006, 'bidirectional': True, 'fc_dropout': 0.2783656962181947, 'learning_rate_model': 0.0010142188302100887}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.39339912006772e-05
Epoch 27: reducing lr to 6.795612649447376e-05
Epoch 36: reducing lr to 2.8981770963866288e-05
Epoch 39: reducing lr to 1.796551413914668e-05
Epoch 42: reducing lr to 9.161862337460271e-06
Epoch 45: reducing lr to 3.123994198821786e-06
Epoch 48: reducing lr to 2.3128356485436087e-07
[I 2024-06-22 01:44:07,803] Trial 524 finished with value: 0.9723727703094482 and parameters: {'hidden_size': 183, 'n_layers': 4, 'rnn_dropout': 0.049386578446654006, 'bidirectional': True, 'fc_dropout': 0.2871830578125008, 'learning_rate_model': 0.0010635753184093746}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.707895190906964e-05
Epoch 27: reducing lr to 7.050240535792835e-05
Epoch 36: reducing lr to 3.0067702058493563e-05
Epoch 39: reducing lr to 1.8638672120382167e-05
Epoch 42: reducing lr to 9.505152304431106e-06
Epoch 45: reducing lr to 3.241048551510064e-06
Epoch 48: reducing lr to 2.399496334346603e-07
[I 2024-06-22 01:45:14,141] Trial 525 finished with value: 0.9723834991455078 and parameters: {'hidden_size': 183, 'n_layers': 4, 'rnn_dropout': 0.05109813710145513, 'bidirectional': True, 'fc_dropout': 0.2840712476766255, 'learning_rate_model': 0.0011034269034342803}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.620541307229992e-05
Epoch 26: reducing lr to 7.41321982189853e-05
Epoch 33: reducing lr to 4.264201906106588e-05
Epoch 36: reducing lr to 2.9766075719353167e-05
Epoch 39: reducing lr to 1.8451696925963513e-05
Epoch 42: reducing lr to 9.40980067805873e-06
Epoch 45: reducing lr to 3.208535737339352e-06
Epoch 48: reducing lr to 2.3754256124237667e-07
[I 2024-06-22 01:46:19,545] Trial 526 finished with value: 0.9725981950759888 and parameters: {'hidden_size': 182, 'n_layers': 4, 'rnn_dropout': 0.048075918545869706, 'bidirectional': True, 'fc_dropout': 0.2828147924476181, 'learning_rate_model': 0.001092357796232658}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.340878496611667e-05
Epoch 36: reducing lr to 2.8800421231998594e-05
Epoch 39: reducing lr to 1.7853097227976504e-05
Epoch 42: reducing lr to 9.104533153526733e-06
Epoch 45: reducing lr to 3.1044462039453174e-06
Epoch 48: reducing lr to 2.2983633747395564e-07
[I 2024-06-22 01:47:28,585] Trial 527 finished with value: 0.9717097878456116 and parameters: {'hidden_size': 187, 'n_layers': 4, 'rnn_dropout': 0.04569425125528675, 'bidirectional': True, 'fc_dropout': 0.2913135398506585, 'learning_rate_model': 0.0010569201316350705}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.82514413413292e-05
Epoch 26: reducing lr to 7.58916767458629e-05
Epoch 36: reducing lr to 3.0472553772289176e-05
Epoch 39: reducing lr to 1.888963570702845e-05
Epoch 42: reducing lr to 9.633136052336144e-06
Epoch 45: reducing lr to 3.284688203719654e-06
Epoch 48: reducing lr to 2.431804762883074e-07
[I 2024-06-22 01:48:37,380] Trial 528 finished with value: 0.9717564582824707 and parameters: {'hidden_size': 187, 'n_layers': 4, 'rnn_dropout': 0.044069035996031805, 'bidirectional': True, 'fc_dropout': 0.29223959309807906, 'learning_rate_model': 0.001118284183582743}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.29529019255689e-05
Epoch 27: reducing lr to 7.525817694263785e-05
Epoch 36: reducing lr to 3.209593247618406e-05
Epoch 39: reducing lr to 1.9895952163478774e-05
Epoch 42: reducing lr to 1.0146326644629231e-05
Epoch 45: reducing lr to 3.4596749448605192e-06
Epoch 48: reducing lr to 2.5613554429338664e-07
[I 2024-06-22 01:49:48,178] Trial 529 finished with value: 0.9709271192550659 and parameters: {'hidden_size': 189, 'n_layers': 4, 'rnn_dropout': 0.04595429359177604, 'bidirectional': True, 'fc_dropout': 0.2669892168997746, 'learning_rate_model': 0.001177859063394148}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.096610527017364e-05
Epoch 26: reducing lr to 7.82261473701408e-05
Epoch 36: reducing lr to 3.140990665047573e-05
Epoch 39: reducing lr to 1.9470691516469014e-05
Epoch 42: reducing lr to 9.929456730678205e-06
Epoch 45: reducing lr to 3.3857270587073133e-06
Epoch 48: reducing lr to 2.506608443949947e-07
[I 2024-06-22 01:50:58,682] Trial 530 finished with value: 0.9705502390861511 and parameters: {'hidden_size': 189, 'n_layers': 4, 'rnn_dropout': 0.039599014575045596, 'bidirectional': True, 'fc_dropout': 0.289408129058645, 'learning_rate_model': 0.0011526832334931908}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001111741937030975
Epoch 23: reducing lr to 9.4331234931544e-05
Epoch 26: reducing lr to 8.111998489377832e-05
Epoch 29: reducing lr to 6.656703734647975e-05
Epoch 32: reducing lr to 5.158680780423132e-05
Epoch 36: reducing lr to 3.257186041574315e-05
Epoch 39: reducing lr to 2.0190975201857772e-05
Epoch 42: reducing lr to 1.0296779364382907e-05
Epoch 45: reducing lr to 3.5109760238764918e-06
Epoch 48: reducing lr to 2.59933597580495e-07
[I 2024-06-22 01:52:09,815] Trial 531 finished with value: 0.9735861420631409 and parameters: {'hidden_size': 190, 'n_layers': 4, 'rnn_dropout': 0.040707694012435416, 'bidirectional': True, 'fc_dropout': 0.2955213145755758, 'learning_rate_model': 0.0011953247044858384}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00010896890502108027
Epoch 23: reducing lr to 9.246004884216427e-05
Epoch 26: reducing lr to 7.951086160164625e-05
Epoch 29: reducing lr to 6.524659121446053e-05
Epoch 36: reducing lr to 3.192575404218208e-05
Epoch 39: reducing lr to 1.979046022973697e-05
Epoch 42: reducing lr to 1.0092528987230214e-05
Epoch 45: reducing lr to 3.4413311231096297e-06
Epoch 48: reducing lr to 2.547774673516403e-07
[I 2024-06-22 01:53:18,527] Trial 532 finished with value: 0.9719338417053223 and parameters: {'hidden_size': 187, 'n_layers': 4, 'rnn_dropout': 0.0423044257786852, 'bidirectional': True, 'fc_dropout': 0.2910924917511491, 'learning_rate_model': 0.0011716138417906879}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.893264868159494e-05
Epoch 27: reducing lr to 7.200322821353334e-05
Epoch 33: reducing lr to 4.3991062336782694e-05
Epoch 36: reducing lr to 3.070776950350924e-05
Epoch 39: reducing lr to 1.9035443620225064e-05
Epoch 42: reducing lr to 9.707493625299153e-06
Epoch 45: reducing lr to 3.3100425059366926e-06
Epoch 48: reducing lr to 2.4505757113160544e-07
[I 2024-06-22 01:54:27,154] Trial 533 finished with value: 0.9733160734176636 and parameters: {'hidden_size': 188, 'n_layers': 4, 'rnn_dropout': 0.04115199588295722, 'bidirectional': True, 'fc_dropout': 0.2918657369681881, 'learning_rate_model': 0.0011269161490530101}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.117838678924478e-05
Epoch 26: reducing lr to 7.840869850108674e-05
Epoch 29: reducing lr to 6.43421564765482e-05
Epoch 36: reducing lr to 3.1483205849972535e-05
Epoch 39: reducing lr to 1.9516128967706544e-05
Epoch 42: reducing lr to 9.952628440097647e-06
Epoch 45: reducing lr to 3.393628103619023e-06
Epoch 48: reducing lr to 2.512457948516652e-07
[I 2024-06-22 01:55:35,841] Trial 534 finished with value: 0.9719313383102417 and parameters: {'hidden_size': 187, 'n_layers': 4, 'rnn_dropout': 0.043379560541971895, 'bidirectional': True, 'fc_dropout': 0.2630691215894125, 'learning_rate_model': 0.0011553731733019473}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00010758050718356095
Epoch 23: reducing lr to 9.128199413156076e-05
Epoch 26: reducing lr to 7.849779545873448e-05
Epoch 29: reducing lr to 6.441526941554791e-05
Epoch 32: reducing lr to 4.991927319375251e-05
Epoch 36: reducing lr to 3.1518980679957636e-05
Epoch 39: reducing lr to 1.9538305432171882e-05
Epoch 42: reducing lr to 9.963937758216204e-06
Epoch 45: reducing lr to 3.397484333159855e-06
Epoch 48: reducing lr to 2.515312891446502e-07
[I 2024-06-22 01:56:44,685] Trial 535 finished with value: 0.9726426601409912 and parameters: {'hidden_size': 192, 'n_layers': 4, 'rnn_dropout': 0.025397945324026228, 'bidirectional': True, 'fc_dropout': 0.2628193811784105, 'learning_rate_model': 0.0011566860408365055}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.19496924779691e-05
Epoch 27: reducing lr to 7.444594071812011e-05
Epoch 36: reducing lr to 3.1749531857993394e-05
Epoch 39: reducing lr to 1.9681221834829342e-05
Epoch 42: reducing lr to 1.0036820749305203e-05
Epoch 45: reducing lr to 3.4223358352855745e-06
Epoch 48: reducing lr to 2.5337115939977945e-07
[I 2024-06-22 01:57:56,849] Trial 536 finished with value: 0.9716652631759644 and parameters: {'hidden_size': 193, 'n_layers': 4, 'rnn_dropout': 0.02475425586747119, 'bidirectional': True, 'fc_dropout': 0.2588643897794197, 'learning_rate_model': 0.0011651468261658338}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00012220981475112239
Epoch 23: reducing lr to 0.00010369495259858493
Epoch 27: reducing lr to 8.395534651485802e-05
Epoch 30: reducing lr to 6.768676740702106e-05
Epoch 33: reducing lr to 5.1293323558889274e-05
Epoch 36: reducing lr to 3.5805081151638496e-05
Epoch 39: reducing lr to 2.2195216865285827e-05
Epoch 42: reducing lr to 1.131888126857045e-05
Epoch 45: reducing lr to 3.85949036535826e-06
Epoch 48: reducing lr to 2.8573570673010635e-07
[I 2024-06-22 01:59:10,126] Trial 537 finished with value: 0.9732704162597656 and parameters: {'hidden_size': 194, 'n_layers': 4, 'rnn_dropout': 0.024811858142879173, 'bidirectional': True, 'fc_dropout': 0.26375085775774654, 'learning_rate_model': 0.0013139776942549964}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.200195139019678e-05
Epoch 27: reducing lr to 7.448825150543055e-05
Epoch 36: reducing lr to 3.176757646427671e-05
Epoch 39: reducing lr to 1.969240750839368e-05
Epoch 42: reducing lr to 1.004252510046124e-05
Epoch 45: reducing lr to 3.424280893971586e-06
Epoch 48: reducing lr to 2.535151609817706e-07
[I 2024-06-22 02:00:22,257] Trial 538 finished with value: 0.9705631732940674 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.039042939533330455, 'bidirectional': True, 'fc_dropout': 0.26296040954613376, 'learning_rate_model': 0.0011658090285515085}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 5.6601629161713954e-05
Epoch 38: reducing lr to 1.444709874617422e-05
Epoch 41: reducing lr to 7.966289952092833e-06
Epoch 44: reducing lr to 3.2382714523240885e-06
Epoch 47: reducing lr to 5.601165279787404e-07
[I 2024-06-22 02:01:32,982] Trial 539 finished with value: 0.9715548753738403 and parameters: {'hidden_size': 189, 'n_layers': 4, 'rnn_dropout': 0.0368762853896764, 'bidirectional': True, 'fc_dropout': 0.26226055864096964, 'learning_rate_model': 0.0007172314207509478}. Best is trial 153 with value: 0.9688200950622559.
Epoch 38: reducing lr to 1.3740408457639032e-05
Epoch 41: reducing lr to 7.576613114984609e-06
Epoch 44: reducing lr to 3.0798690611447438e-06
Epoch 47: reducing lr to 5.327180227338328e-07
[I 2024-06-22 02:02:43,459] Trial 540 finished with value: 0.9717699885368347 and parameters: {'hidden_size': 189, 'n_layers': 4, 'rnn_dropout': 0.027123367253969837, 'bidirectional': True, 'fc_dropout': 0.25782244822888906, 'learning_rate_model': 0.0006821475268437915}. Best is trial 153 with value: 0.9688200950622559.
Epoch 38: reducing lr to 1.3030145003416411e-05
Epoch 41: reducing lr to 7.184965994817263e-06
Epoch 44: reducing lr to 2.92066575618724e-06
Epoch 47: reducing lr to 5.051809852345506e-07
[I 2024-06-22 02:03:54,103] Trial 541 finished with value: 0.9717322587966919 and parameters: {'hidden_size': 189, 'n_layers': 4, 'rnn_dropout': 0.03600517115749993, 'bidirectional': True, 'fc_dropout': 0.2538266123833845, 'learning_rate_model': 0.000646886241839114}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 5.52001203726843e-05
Epoch 32: reducing lr to 3.01872227423143e-05
Epoch 37: reducing lr to 1.6510026331004362e-05
Epoch 40: reducing lr to 9.70350570745444e-06
Epoch 43: reducing lr to 4.484816486634106e-06
Epoch 46: reducing lr to 1.1818757046623768e-06
Epoch 49: reducing lr to 2.2146648528736587e-09
[I 2024-06-22 02:05:05,257] Trial 542 finished with value: 0.9744265079498291 and parameters: {'hidden_size': 190, 'n_layers': 4, 'rnn_dropout': 0.035088091131182775, 'bidirectional': True, 'fc_dropout': 0.24913129651021626, 'learning_rate_model': 0.0006994721061368976}. Best is trial 153 with value: 0.9688200950622559.
Epoch 31: reducing lr to 2.6813191394692303e-05
Epoch 37: reducing lr to 1.3373020969884302e-05
Epoch 40: reducing lr to 7.859780639083097e-06
Epoch 43: reducing lr to 3.63267409266404e-06
Epoch 46: reducing lr to 9.573121365994428e-07
Epoch 49: reducing lr to 1.7938650687175477e-09
[I 2024-06-22 02:06:14,071] Trial 543 finished with value: 0.9756311178207397 and parameters: {'hidden_size': 188, 'n_layers': 4, 'rnn_dropout': 0.02158800969964301, 'bidirectional': True, 'fc_dropout': 0.2571927422823858, 'learning_rate_model': 0.0005665681541435088}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00013117740757923445
Epoch 23: reducing lr to 0.00011130394959386067
Epoch 27: reducing lr to 9.011587760494701e-05
Epoch 30: reducing lr to 7.26535319111111e-05
Epoch 33: reducing lr to 5.5057159069265116e-05
Epoch 36: reducing lr to 3.843241013989766e-05
Epoch 39: reducing lr to 2.3823872206797183e-05
Epoch 42: reducing lr to 1.2149445644214036e-05
Epoch 45: reducing lr to 4.142694608741133e-06
Epoch 48: reducing lr to 3.067026108991965e-07
[I 2024-06-22 02:07:25,113] Trial 544 finished with value: 0.975511908531189 and parameters: {'hidden_size': 190, 'n_layers': 4, 'rnn_dropout': 0.03760300365198592, 'bidirectional': True, 'fc_dropout': 0.24303474497806052, 'learning_rate_model': 0.0014103956208453986}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 5.8072456673527914e-05
Epoch 32: reducing lr to 3.175801380434446e-05
Epoch 37: reducing lr to 1.736912496409167e-05
Epoch 40: reducing lr to 1.0208427281914575e-05
Epoch 43: reducing lr to 4.7181837530496326e-06
Epoch 46: reducing lr to 1.2433745649305708e-06
Epoch 49: reducing lr to 2.3299048597553995e-09
[I 2024-06-22 02:08:37,827] Trial 545 finished with value: 0.9717950820922852 and parameters: {'hidden_size': 194, 'n_layers': 4, 'rnn_dropout': 0.07500603679576287, 'bidirectional': True, 'fc_dropout': 0.26322460140701864, 'learning_rate_model': 0.0007358691123086225}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 5.870440926720002e-05
Epoch 26: reducing lr to 5.048275681336403e-05
Epoch 37: reducing lr to 1.755813821063951e-05
Epoch 40: reducing lr to 1.031951681501952e-05
Epoch 43: reducing lr to 4.7695276883841314e-06
Epoch 46: reducing lr to 1.2569051408046258e-06
Epoch 49: reducing lr to 2.3552592102213213e-09
[I 2024-06-22 02:09:51,295] Trial 546 finished with value: 0.9746361970901489 and parameters: {'hidden_size': 195, 'n_layers': 4, 'rnn_dropout': 0.07319688075890286, 'bidirectional': True, 'fc_dropout': 0.300056256433997, 'learning_rate_model': 0.0007438769428838116}. Best is trial 153 with value: 0.9688200950622559.
Epoch 38: reducing lr to 1.2758718353782077e-05
Epoch 41: reducing lr to 7.035298339760573e-06
Epoch 44: reducing lr to 2.8598263318603695e-06
Epoch 47: reducing lr to 4.946577268789983e-07
[I 2024-06-22 02:11:03,783] Trial 547 finished with value: 0.9729163646697998 and parameters: {'hidden_size': 193, 'n_layers': 4, 'rnn_dropout': 0.022686220378877894, 'bidirectional': True, 'fc_dropout': 0.26053446709887806, 'learning_rate_model': 0.000633411168056673}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 5.567612578864258e-05
Epoch 31: reducing lr to 3.338840947899263e-05
Epoch 37: reducing lr to 1.6652396708063554e-05
Epoch 40: reducing lr to 9.78718163495869e-06
Epoch 43: reducing lr to 4.523490259857857e-06
Epoch 46: reducing lr to 1.1920673352713438e-06
Epoch 49: reducing lr to 2.2337625008050402e-09
[I 2024-06-22 02:12:12,764] Trial 548 finished with value: 0.9754945039749146 and parameters: {'hidden_size': 188, 'n_layers': 4, 'rnn_dropout': 0.03741988472745602, 'bidirectional': True, 'fc_dropout': 0.2601725231702747, 'learning_rate_model': 0.0007055038413683603}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00012471031268608608
Epoch 23: reducing lr to 0.00010581662355739424
Epoch 27: reducing lr to 8.567313138358632e-05
Epoch 36: reducing lr to 3.653767805200373e-05
Epoch 39: reducing lr to 2.2649346462411406e-05
Epoch 42: reducing lr to 1.1550473463483756e-05
Epoch 45: reducing lr to 3.938458226558639e-06
Epoch 48: reducing lr to 2.9158205831880236e-07
[I 2024-06-22 02:13:24,962] Trial 549 finished with value: 0.9720041751861572 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.07027901301045511, 'bidirectional': True, 'fc_dropout': 0.24431205727435562, 'learning_rate_model': 0.0013408625931296413}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001274240924414422
Epoch 23: reducing lr to 0.00010811926400953572
Epoch 27: reducing lr to 8.753743598293386e-05
Epoch 30: reducing lr to 7.057473186338693e-05
Epoch 33: reducing lr to 5.3481835449199755e-05
Epoch 36: reducing lr to 3.733276234671507e-05
Epoch 39: reducing lr to 2.3142211379336557e-05
Epoch 42: reducing lr to 1.1801819485916386e-05
Epoch 45: reducing lr to 4.0241617098740265e-06
Epoch 48: reducing lr to 2.979270787894237e-07
[I 2024-06-22 02:14:37,745] Trial 550 finished with value: 0.9750401973724365 and parameters: {'hidden_size': 194, 'n_layers': 4, 'rnn_dropout': 0.07294312287487036, 'bidirectional': True, 'fc_dropout': 0.2543964167053387, 'learning_rate_model': 0.0013700406593342295}. Best is trial 153 with value: 0.9688200950622559.
Epoch 31: reducing lr to 2.5204751684679956e-05
Epoch 37: reducing lr to 1.2570815158044696e-05
Epoch 40: reducing lr to 7.388296916545318e-06
Epoch 43: reducing lr to 3.414761305701639e-06
Epoch 46: reducing lr to 8.998859677888091e-07
Epoch 49: reducing lr to 1.6862566990741985e-09
[I 2024-06-22 02:15:49,933] Trial 551 finished with value: 0.9737076759338379 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.01834246342651071, 'bidirectional': True, 'fc_dropout': 0.23739751725418556, 'learning_rate_model': 0.0005325814979436351}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00011509044629455487
Epoch 23: reducing lr to 9.765417284501897e-05
Epoch 27: reducing lr to 7.906450327976029e-05
Epoch 30: reducing lr to 6.37436550000027e-05
Epoch 33: reducing lr to 4.830521601186991e-05
Epoch 36: reducing lr to 3.3719245690264696e-05
Epoch 39: reducing lr to 2.0902228023438812e-05
Epoch 42: reducing lr to 1.0659496533954832e-05
Epoch 45: reducing lr to 3.6346546267433668e-06
Epoch 48: reducing lr to 2.6909008966939043e-07
[I 2024-06-22 02:17:07,973] Trial 552 finished with value: 0.9727553129196167 and parameters: {'hidden_size': 198, 'n_layers': 4, 'rnn_dropout': 0.07323184744541533, 'bidirectional': True, 'fc_dropout': 0.29770493628031186, 'learning_rate_model': 0.0012374315398551805}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.150562913022055e-05
Epoch 26: reducing lr to 5.28916610658874e-05
Epoch 32: reducing lr to 3.363550865332751e-05
Epoch 37: reducing lr to 1.8395966341903655e-05
Epoch 40: reducing lr to 1.0811936989923793e-05
Epoch 43: reducing lr to 4.997116993254167e-06
Epoch 46: reducing lr to 1.3168813451529043e-06
Epoch 49: reducing lr to 2.4676459791988724e-09
[I 2024-06-22 02:18:16,816] Trial 553 finished with value: 0.973236083984375 and parameters: {'hidden_size': 187, 'n_layers': 4, 'rnn_dropout': 0.037412774479371774, 'bidirectional': True, 'fc_dropout': 0.2631190558857591, 'learning_rate_model': 0.0007793727922426669}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00013334875689994128
Epoch 23: reducing lr to 0.00011314633815605751
Epoch 27: reducing lr to 9.160754490675895e-05
Epoch 30: reducing lr to 7.385614903911658e-05
Epoch 37: reducing lr to 3.3841393996989084e-05
Epoch 40: reducing lr to 1.988974173719694e-05
Epoch 43: reducing lr to 9.192743771907984e-06
Epoch 46: reducing lr to 2.422547400899016e-06
Epoch 49: reducing lr to 4.539504925974748e-09
[I 2024-06-22 02:19:29,079] Trial 554 finished with value: 0.970982551574707 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.020094623478982936, 'bidirectional': True, 'fc_dropout': 0.2461216645655389, 'learning_rate_model': 0.0014337415737024157}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.000127474173460737
Epoch 23: reducing lr to 0.00010816175772358346
Epoch 27: reducing lr to 8.757184049731163e-05
Epoch 30: reducing lr to 7.060246958896499e-05
Epoch 33: reducing lr to 5.350285521698255e-05
Epoch 37: reducing lr to 3.235053575911685e-05
Epoch 40: reducing lr to 1.9013513490786952e-05
Epoch 43: reducing lr to 8.787764066219222e-06
Epoch 46: reducing lr to 2.3158238200208594e-06
Epoch 49: reducing lr to 4.339520306073305e-09
[I 2024-06-22 02:20:41,376] Trial 555 finished with value: 0.9712649583816528 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.019533009590005656, 'bidirectional': True, 'fc_dropout': 0.2416166169133698, 'learning_rate_model': 0.001370579121342317}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00013561884319661713
Epoch 23: reducing lr to 0.00011507250498159342
Epoch 27: reducing lr to 9.316704225191224e-05
Epoch 30: reducing lr to 7.511345233729994e-05
Epoch 33: reducing lr to 5.692129735187628e-05
Epoch 36: reducing lr to 3.973366375061638e-05
Epoch 39: reducing lr to 2.4630506493264003e-05
Epoch 42: reducing lr to 1.256080444152121e-05
Epoch 45: reducing lr to 4.282958940280768e-06
Epoch 48: reducing lr to 3.170870202661054e-07
[I 2024-06-22 02:21:54,197] Trial 556 finished with value: 0.9756370782852173 and parameters: {'hidden_size': 194, 'n_layers': 4, 'rnn_dropout': 0.023077603127442142, 'bidirectional': True, 'fc_dropout': 0.2409243127745941, 'learning_rate_model': 0.0014581491285615773}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00011410619782080335
Epoch 23: reducing lr to 9.681903862082642e-05
Epoch 27: reducing lr to 7.838834709837003e-05
Epoch 30: reducing lr to 6.319852204444421e-05
Epoch 33: reducing lr to 4.7892111912749136e-05
Epoch 36: reducing lr to 3.343088017275027e-05
Epoch 39: reducing lr to 2.0723473081630498e-05
Epoch 42: reducing lr to 1.0568336984815088e-05
Epoch 45: reducing lr to 3.6035712190048056e-06
Epoch 48: reducing lr to 2.6678884296660235e-07
[I 2024-06-22 02:23:09,163] Trial 557 finished with value: 0.9724018573760986 and parameters: {'hidden_size': 200, 'n_layers': 4, 'rnn_dropout': 0.02425930223716873, 'bidirectional': True, 'fc_dropout': 0.2506074131912692, 'learning_rate_model': 0.0012268490793322859}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001419641931200945
Epoch 23: reducing lr to 0.00012045653048622563
Epoch 27: reducing lr to 9.752615246469224e-05
Epoch 30: reducing lr to 7.862786912339221e-05
Epoch 33: reducing lr to 5.958453751292323e-05
Epoch 36: reducing lr to 4.1592726947859e-05
Epoch 39: reducing lr to 2.5782921443933174e-05
Epoch 42: reducing lr to 1.3148500794204805e-05
Epoch 45: reducing lr to 4.483350512302694e-06
Epoch 48: reducing lr to 3.3192292398238467e-07
[I 2024-06-22 02:24:20,244] Trial 558 finished with value: 0.9778774976730347 and parameters: {'hidden_size': 190, 'n_layers': 4, 'rnn_dropout': 0.03613485791683459, 'bidirectional': True, 'fc_dropout': 0.23602442153651454, 'learning_rate_model': 0.0015263731765128106}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.367957149420416e-05
Epoch 27: reducing lr to 7.584651604601489e-05
Epoch 30: reducing lr to 6.11492382957522e-05
Epoch 33: reducing lr to 4.633915587108227e-05
Epoch 36: reducing lr to 3.2346845970268316e-05
Epoch 39: reducing lr to 2.0051490965137718e-05
Epoch 42: reducing lr to 1.0225646672873097e-05
Epoch 45: reducing lr to 3.4867213355349504e-06
Epoch 48: reducing lr to 2.5813791217680315e-07
[I 2024-06-22 02:25:29,082] Trial 559 finished with value: 0.9734771251678467 and parameters: {'hidden_size': 188, 'n_layers': 4, 'rnn_dropout': 0.01914251409396329, 'bidirectional': True, 'fc_dropout': 0.24494041469239414, 'learning_rate_model': 0.0011870671066050542}. Best is trial 153 with value: 0.9688200950622559.
Epoch 38: reducing lr to 1.2737181564505728e-05
Epoch 41: reducing lr to 7.023422716078147e-06
Epoch 44: reducing lr to 2.8549989287177894e-06
Epoch 47: reducing lr to 4.938227418176208e-07
[I 2024-06-22 02:26:41,370] Trial 560 finished with value: 0.9729883670806885 and parameters: {'hidden_size': 193, 'n_layers': 4, 'rnn_dropout': 0.03657121625990702, 'bidirectional': True, 'fc_dropout': 0.221354160277066, 'learning_rate_model': 0.0006323419663960161}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00013720282892258783
Epoch 23: reducing lr to 0.00011641651589516742
Epoch 26: reducing lr to 0.00010011218466137505
Epoch 29: reducing lr to 8.215203126477101e-05
Epoch 36: reducing lr to 4.019774053182184e-05
Epoch 39: reducing lr to 2.491818311539972e-05
Epoch 42: reducing lr to 1.2707510713843803e-05
Epoch 45: reducing lr to 4.332982562857218e-06
Epoch 48: reducing lr to 3.2079049761594114e-07
[I 2024-06-22 02:27:56,914] Trial 561 finished with value: 0.9722726941108704 and parameters: {'hidden_size': 196, 'n_layers': 4, 'rnn_dropout': 0.07177483424242383, 'bidirectional': True, 'fc_dropout': 0.29377988327578675, 'learning_rate_model': 0.0014751798549085763}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00013351710830255723
Epoch 23: reducing lr to 0.00011328918421756016
Epoch 27: reducing lr to 9.172319846839529e-05
Epoch 30: reducing lr to 7.394939165023352e-05
Epoch 33: reducing lr to 5.603916715492308e-05
Epoch 36: reducing lr to 3.9117896607901244e-05
Epoch 39: reducing lr to 2.42487985112824e-05
Epoch 42: reducing lr to 1.2366145053710823e-05
Epoch 45: reducing lr to 4.216584356613496e-06
Epoch 48: reducing lr to 3.1217300655503583e-07
[I 2024-06-22 02:29:12,933] Trial 562 finished with value: 0.9724052548408508 and parameters: {'hidden_size': 197, 'n_layers': 4, 'rnn_dropout': 0.07825230309153808, 'bidirectional': True, 'fc_dropout': 0.2579552768765487, 'learning_rate_model': 0.0014355516573547344}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.209987802279249e-05
Epoch 26: reducing lr to 7.920113331866743e-05
Epoch 29: reducing lr to 6.499242827043004e-05
Epoch 36: reducing lr to 3.180138978825372e-05
Epoch 39: reducing lr to 1.9713368054619726e-05
Epoch 42: reducing lr to 1.0053214337493558e-05
Epoch 45: reducing lr to 3.4279256894562604e-06
Epoch 48: reducing lr to 2.537850018454867e-07
[I 2024-06-22 02:30:25,206] Trial 563 finished with value: 0.9706587791442871 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.014050168925933887, 'bidirectional': True, 'fc_dropout': 0.20569961469490253, 'learning_rate_model': 0.0011670499125837563}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00013058000506449482
Epoch 23: reducing lr to 0.00011079705392779362
Epoch 27: reducing lr to 8.970547574617689e-05
Epoch 30: reducing lr to 7.23226563169872e-05
Epoch 36: reducing lr to 3.8257382908541446e-05
Epoch 39: reducing lr to 2.3715374551371427e-05
Epoch 42: reducing lr to 1.2094115160752846e-05
Epoch 45: reducing lr to 4.123828126907625e-06
Epoch 48: reducing lr to 3.0530583904335874e-07
[I 2024-06-22 02:31:38,828] Trial 564 finished with value: 0.9759492874145508 and parameters: {'hidden_size': 195, 'n_layers': 4, 'rnn_dropout': 0.016647619114921394, 'bidirectional': True, 'fc_dropout': 0.2010170467130371, 'learning_rate_model': 0.0014039724576939095}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.19751173901425e-05
Epoch 37: reducing lr to 1.853638747651488e-05
Epoch 40: reducing lr to 1.0894467281143768e-05
Epoch 43: reducing lr to 5.035261270371018e-06
Epoch 46: reducing lr to 1.3269334385954471e-06
Epoch 49: reducing lr to 2.48648215457007e-09
[I 2024-06-22 02:32:51,162] Trial 565 finished with value: 0.9719443917274475 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.03536468654207392, 'bidirectional': True, 'fc_dropout': 0.26313811934917924, 'learning_rate_model': 0.0007853219448850344}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.026103768181154e-05
Epoch 37: reducing lr to 1.8023716472775755e-05
Epoch 40: reducing lr to 1.0593153042686929e-05
Epoch 43: reducing lr to 4.895998296242944e-06
Epoch 46: reducing lr to 1.2902337149453178e-06
Epoch 49: reducing lr to 2.417712158069279e-09
[I 2024-06-22 02:34:03,591] Trial 566 finished with value: 0.9720669984817505 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.007496047718460418, 'bidirectional': True, 'fc_dropout': 0.208713453822127, 'learning_rate_model': 0.0007636018664580664}. Best is trial 153 with value: 0.9688200950622559.
Epoch 38: reducing lr to 1.0156634017393869e-05
Epoch 41: reducing lr to 5.600480272295176e-06
Epoch 44: reducing lr to 2.2765773646380034e-06
Epoch 47: reducing lr to 3.937744651520323e-07
[I 2024-06-22 02:35:18,633] Trial 567 finished with value: 0.9747906923294067 and parameters: {'hidden_size': 200, 'n_layers': 4, 'rnn_dropout': 0.004855451496673824, 'bidirectional': True, 'fc_dropout': 0.21049908152604951, 'learning_rate_model': 0.0005042297539685528}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 5.881964089135696e-05
Epoch 37: reducing lr to 1.759260330122538e-05
Epoch 40: reducing lr to 1.0339773124519144e-05
Epoch 43: reducing lr to 4.778889854341589e-06
Epoch 46: reducing lr to 1.2593723357324648e-06
Epoch 49: reducing lr to 2.3598823781722197e-09
[I 2024-06-22 02:36:30,981] Trial 568 finished with value: 0.9722553491592407 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.0013659190036710983, 'bidirectional': True, 'fc_dropout': 0.21786682808141067, 'learning_rate_model': 0.0007453371083019023}. Best is trial 153 with value: 0.9688200950622559.
Epoch 24: reducing lr to 5.3079494489077416e-05
Epoch 37: reducing lr to 1.6615752354407506e-05
Epoch 40: reducing lr to 9.76564449820806e-06
Epoch 43: reducing lr to 4.513536114532834e-06
Epoch 46: reducing lr to 1.1894441370746299e-06
Epoch 49: reducing lr to 2.228847005179488e-09
[I 2024-06-22 02:37:43,378] Trial 569 finished with value: 0.9722509384155273 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.013945532527392597, 'bidirectional': True, 'fc_dropout': 0.21360614553228638, 'learning_rate_model': 0.0007039513481914305}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 2.6377559298479826e-05
Epoch 37: reducing lr to 1.442644135510638e-05
Epoch 40: reducing lr to 8.478911736479079e-06
Epoch 43: reducing lr to 3.918827307461067e-06
Epoch 46: reducing lr to 1.0327215838728263e-06
Epoch 49: reducing lr to 1.935171512184447e-09
[I 2024-06-22 02:38:54,545] Trial 570 finished with value: 0.9748234748840332 and parameters: {'hidden_size': 190, 'n_layers': 4, 'rnn_dropout': 0.02162931934439781, 'bidirectional': True, 'fc_dropout': 0.2031746104433993, 'learning_rate_model': 0.0006111978937166743}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.0423692732934425e-05
Epoch 26: reducing lr to 5.196125170288508e-05
Epoch 37: reducing lr to 1.8072365627139532e-05
Epoch 40: reducing lr to 1.0621745810352314e-05
Epoch 43: reducing lr to 4.909213449579301e-06
Epoch 46: reducing lr to 1.2937162807778735e-06
Epoch 49: reducing lr to 2.424237984866625e-09
[I 2024-06-22 02:40:03,457] Trial 571 finished with value: 0.9748567342758179 and parameters: {'hidden_size': 188, 'n_layers': 4, 'rnn_dropout': 0.013448857686330546, 'bidirectional': True, 'fc_dropout': 0.23884954501930805, 'learning_rate_model': 0.0007656629610791395}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.034097274071861e-05
Epoch 37: reducing lr to 1.8047624571497567e-05
Epoch 40: reducing lr to 1.0607204647920646e-05
Epoch 43: reducing lr to 4.90249273987144e-06
Epoch 46: reducing lr to 1.2919451841130608e-06
Epoch 49: reducing lr to 2.4209192047959863e-09
[I 2024-06-22 02:41:15,654] Trial 572 finished with value: 0.9721719026565552 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.03302626467862483, 'bidirectional': True, 'fc_dropout': 0.2234297081292957, 'learning_rate_model': 0.0007646147690320175}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00010964008650819915
Epoch 23: reducing lr to 9.302954592088563e-05
Epoch 27: reducing lr to 7.532023081337936e-05
Epoch 36: reducing lr to 3.212239706682546e-05
Epoch 39: reducing lr to 1.991235730234857e-05
Epoch 42: reducing lr to 1.0154692763337994e-05
Epoch 45: reducing lr to 3.4625276079272874e-06
Epoch 48: reducing lr to 2.5634673997477263e-07
[I 2024-06-22 02:42:28,104] Trial 573 finished with value: 0.9719566702842712 and parameters: {'hidden_size': 193, 'n_layers': 4, 'rnn_dropout': 0.03204488704118225, 'bidirectional': True, 'fc_dropout': 0.2337242077494008, 'learning_rate_model': 0.0011788302630303985}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00012040732176226291
Epoch 23: reducing lr to 0.00010216553840693675
Epoch 27: reducing lr to 8.271707507341548e-05
Epoch 30: reducing lr to 6.668844395862883e-05
Epoch 33: reducing lr to 5.05367897544769e-05
Epoch 36: reducing lr to 3.5276986023822425e-05
Epoch 39: reducing lr to 2.186785590113276e-05
Epoch 42: reducing lr to 1.1151937196444322e-05
Epoch 45: reducing lr to 3.8025660967281464e-06
Epoch 48: reducing lr to 2.815213430220958e-07
[I 2024-06-22 02:43:40,830] Trial 574 finished with value: 0.9730603098869324 and parameters: {'hidden_size': 194, 'n_layers': 4, 'rnn_dropout': 0.02996401289374344, 'bidirectional': True, 'fc_dropout': 0.2340481892943827, 'learning_rate_model': 0.0012945976175710115}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.000172358708254328
Epoch 23: reducing lr to 0.00014624625787039523
Epoch 27: reducing lr to 0.00011840648892082973
Epoch 30: reducing lr to 9.546208559389177e-05
Epoch 33: reducing lr to 7.234157918237292e-05
Epoch 36: reducing lr to 5.049772433421613e-05
Epoch 39: reducing lr to 3.1303041544707055e-05
Epoch 42: reducing lr to 1.5963593090357685e-05
Epoch 45: reducing lr to 5.443235269179253e-06
Epoch 48: reducing lr to 4.029875784889499e-07
[I 2024-06-22 02:44:54,612] Trial 575 finished with value: 0.9840258955955505 and parameters: {'hidden_size': 196, 'n_layers': 4, 'rnn_dropout': 0.036016332600780104, 'bidirectional': True, 'fc_dropout': 0.2280545283513016, 'learning_rate_model': 0.0018531694734829926}. Best is trial 153 with value: 0.9688200950622559.
Epoch 31: reducing lr to 2.733075455109947e-05
Epoch 37: reducing lr to 1.3631154469995833e-05
Epoch 40: reducing lr to 8.011494503216294e-06
Epoch 43: reducing lr to 3.702793991557161e-06
Epoch 46: reducing lr to 9.757907087243203e-07
Epoch 49: reducing lr to 1.8284912515304994e-09
[I 2024-06-22 02:46:03,916] Trial 576 finished with value: 0.9752570390701294 and parameters: {'hidden_size': 192, 'n_layers': 4, 'rnn_dropout': 0.0029787151154000123, 'bidirectional': True, 'fc_dropout': 0.24861108735013687, 'learning_rate_model': 0.0005775043682577429}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.103354904830264e-05
Epoch 26: reducing lr to 5.2485696603091755e-05
Epoch 37: reducing lr to 1.8254770008812607e-05
Epoch 40: reducing lr to 1.0728951088111648e-05
Epoch 43: reducing lr to 4.958762139675905e-06
Epoch 46: reducing lr to 1.306773758870351e-06
Epoch 49: reducing lr to 2.448705818235836e-09
[I 2024-06-22 02:47:12,909] Trial 577 finished with value: 0.9749050736427307 and parameters: {'hidden_size': 188, 'n_layers': 4, 'rnn_dropout': 0.03387889252650354, 'bidirectional': True, 'fc_dropout': 0.22289488998941703, 'learning_rate_model': 0.0007733907971503556}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00011034476783687148
Epoch 23: reducing lr to 9.362746759454655e-05
Epoch 27: reducing lr to 7.580433076273053e-05
Epoch 36: reducing lr to 3.2328854888650484e-05
Epoch 39: reducing lr to 2.0040338470985907e-05
Epoch 42: reducing lr to 1.0219959242201929e-05
Epoch 45: reducing lr to 3.484782046363319e-06
Epoch 48: reducing lr to 2.579943377383105e-07
[I 2024-06-22 02:48:25,352] Trial 578 finished with value: 0.971898078918457 and parameters: {'hidden_size': 193, 'n_layers': 4, 'rnn_dropout': 0.025835203774878697, 'bidirectional': True, 'fc_dropout': 0.2619369754696852, 'learning_rate_model': 0.0011864068684717785}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00011079459089779709
Epoch 23: reducing lr to 9.400914218488469e-05
Epoch 26: reducing lr to 8.084300178460606e-05
Epoch 29: reducing lr to 6.633974508309042e-05
Epoch 32: reducing lr to 5.141066533531198e-05
Epoch 35: reducing lr to 3.699381699374862e-05
Epoch 38: reducing lr to 2.39950577448544e-05
Epoch 41: reducing lr to 1.323114009055559e-05
Epoch 44: reducing lr to 5.378416238250379e-06
Epoch 47: reducing lr to 9.302925569229645e-07
[I 2024-06-22 02:49:39,210] Trial 579 finished with value: 0.9711701273918152 and parameters: {'hidden_size': 196, 'n_layers': 4, 'rnn_dropout': 0.015047914655047035, 'bidirectional': True, 'fc_dropout': 0.2636374838176426, 'learning_rate_model': 0.001191243284185373}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001446867526328369
Epoch 23: reducing lr to 0.0001227666205571056
Epoch 27: reducing lr to 9.939648855647912e-05
Epoch 33: reducing lr to 6.072723727300241e-05
Epoch 36: reducing lr to 4.239038353945458e-05
Epoch 39: reducing lr to 2.6277380902341053e-05
Epoch 42: reducing lr to 1.3400659983989249e-05
Epoch 45: reducing lr to 4.569331267857755e-06
Epoch 48: reducing lr to 3.382884721838666e-07
[I 2024-06-22 02:50:53,939] Trial 580 finished with value: 0.9761190414428711 and parameters: {'hidden_size': 200, 'n_layers': 4, 'rnn_dropout': 0.020380446213558303, 'bidirectional': True, 'fc_dropout': 0.2593845927430225, 'learning_rate_model': 0.0015556456410714921}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001111937938963099
Epoch 23: reducing lr to 9.434786568342118e-05
Epoch 27: reducing lr to 7.638759223944137e-05
Epoch 30: reducing lr to 6.158546660013365e-05
Epoch 36: reducing lr to 3.257760288830957e-05
Epoch 39: reducing lr to 2.0194534903996537e-05
Epoch 42: reducing lr to 1.0298594703521272e-05
Epoch 45: reducing lr to 3.5115950147244362e-06
Epoch 48: reducing lr to 2.5997942430129857e-07
[I 2024-06-22 02:52:07,829] Trial 581 finished with value: 0.9713373184204102 and parameters: {'hidden_size': 196, 'n_layers': 4, 'rnn_dropout': 0.037442652601620316, 'bidirectional': True, 'fc_dropout': 0.2462013764304502, 'learning_rate_model': 0.001195535442197344}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00010644451415788669
Epoch 23: reducing lr to 9.031810474845756e-05
Epoch 27: reducing lr to 7.312494572494238e-05
Epoch 36: reducing lr to 3.118615698200184e-05
Epoch 39: reducing lr to 1.933199130254459e-05
Epoch 42: reducing lr to 9.858723866797452e-06
Epoch 45: reducing lr to 3.36160870282173e-06
Epoch 48: reducing lr to 2.488752523059417e-07
[I 2024-06-22 02:53:23,807] Trial 582 finished with value: 0.971368134021759 and parameters: {'hidden_size': 197, 'n_layers': 4, 'rnn_dropout': 0.00046921628568135504, 'bidirectional': True, 'fc_dropout': 0.26295984397082606, 'learning_rate_model': 0.001144472050498618}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001069734034610102
Epoch 23: reducing lr to 9.076686699663687e-05
Epoch 26: reducing lr to 7.805481275598787e-05
Epoch 29: reducing lr to 6.405175793121773e-05
Epoch 36: reducing lr to 3.134111118479755e-05
Epoch 39: reducing lr to 1.942804588543109e-05
Epoch 42: reducing lr to 9.90770876411085e-06
Epoch 45: reducing lr to 3.3783114788950168e-06
Epoch 48: reducing lr to 2.501118351378416e-07
[I 2024-06-22 02:54:37,567] Trial 583 finished with value: 0.9710862040519714 and parameters: {'hidden_size': 196, 'n_layers': 4, 'rnn_dropout': 0.0022729434543887128, 'bidirectional': True, 'fc_dropout': 0.2605951300372224, 'learning_rate_model': 0.001150158572063597}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00011407642177804693
Epoch 23: reducing lr to 9.679377366687417e-05
Epoch 27: reducing lr to 7.836789163828651e-05
Epoch 30: reducing lr to 6.318203037326004e-05
Epoch 36: reducing lr to 3.3422156375652315e-05
Epoch 39: reducing lr to 2.0718065285802366e-05
Epoch 42: reducing lr to 1.0565579174460029e-05
Epoch 45: reducing lr to 3.6026308661340396e-06
Epoch 48: reducing lr to 2.667192243468574e-07
[I 2024-06-22 02:55:54,062] Trial 584 finished with value: 0.9728833436965942 and parameters: {'hidden_size': 198, 'n_layers': 4, 'rnn_dropout': 0.016229547415250588, 'bidirectional': True, 'fc_dropout': 0.26646222613294185, 'learning_rate_model': 0.0012265289327378017}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00014819646653245342
Epoch 23: reducing lr to 0.0001257446106407702
Epoch 26: reducing lr to 0.00010813386385809246
Epoch 29: reducing lr to 8.873461901264326e-05
Epoch 36: reducing lr to 4.3418660942957045e-05
Epoch 39: reducing lr to 2.691479992875667e-05
Epoch 42: reducing lr to 1.3725724177870167e-05
Epoch 45: reducing lr to 4.680171031491449e-06
Epoch 48: reducing lr to 3.4649444634040836e-07
[I 2024-06-22 02:57:07,827] Trial 585 finished with value: 0.9750309586524963 and parameters: {'hidden_size': 196, 'n_layers': 4, 'rnn_dropout': 0.0012344852534883592, 'bidirectional': True, 'fc_dropout': 0.2604574453135945, 'learning_rate_model': 0.0015933814463888016}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00010702592506822294
Epoch 23: reducing lr to 9.081143154803085e-05
Epoch 27: reducing lr to 7.352436171738296e-05
Epoch 36: reducing lr to 3.135649898660623e-05
Epoch 39: reducing lr to 1.9437584632090464e-05
Epoch 42: reducing lr to 9.912573232953104e-06
Epoch 45: reducing lr to 3.379970156125022e-06
Epoch 48: reducing lr to 2.50234634592084e-07
[I 2024-06-22 02:58:23,695] Trial 586 finished with value: 0.9713985919952393 and parameters: {'hidden_size': 197, 'n_layers': 4, 'rnn_dropout': 0.0005379190567577109, 'bidirectional': True, 'fc_dropout': 0.2458349496985161, 'learning_rate_model': 0.001150723274828956}. Best is trial 153 with value: 0.9688200950622559.
Epoch 22: reducing lr to 0.00015837271891111564
Epoch 27: reducing lr to 0.0001230602550607711
Epoch 30: reducing lr to 9.921406089215506e-05
Epoch 33: reducing lr to 7.518484220602289e-05
Epoch 36: reducing lr to 5.2482451706782686e-05
Epoch 39: reducing lr to 3.253335447895294e-05
Epoch 42: reducing lr to 1.659101503042875e-05
Epoch 45: reducing lr to 5.657172395584368e-06
Epoch 48: reducing lr to 4.1882632148928286e-07
[I 2024-06-22 02:59:40,215] Trial 587 finished with value: 0.9846661686897278 and parameters: {'hidden_size': 198, 'n_layers': 4, 'rnn_dropout': 0.0014625260425463974, 'bidirectional': True, 'fc_dropout': 0.24541839882031244, 'learning_rate_model': 0.001926005155259137}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.159784620166044e-05
Epoch 26: reducing lr to 7.876941190872316e-05
Epoch 36: reducing lr to 3.162804200568688e-05
Epoch 39: reducing lr to 1.9605911472943704e-05
Epoch 42: reducing lr to 9.998414769780409e-06
Epoch 45: reducing lr to 3.4092402382534085e-06
Epoch 48: reducing lr to 2.5240163251439866e-07
[I 2024-06-22 03:00:53,932] Trial 588 finished with value: 0.9728049635887146 and parameters: {'hidden_size': 195, 'n_layers': 4, 'rnn_dropout': 0.0002696020204796517, 'bidirectional': True, 'fc_dropout': 0.2608632472866475, 'learning_rate_model': 0.0011606883819765014}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.131737452446761e-05
Epoch 27: reducing lr to 7.3933992242679e-05
Epoch 30: reducing lr to 5.9607316795685754e-05
Epoch 33: reducing lr to 4.517068112431645e-05
Epoch 36: reducing lr to 3.153119726145425e-05
Epoch 39: reducing lr to 1.9545878370619393e-05
Epoch 42: reducing lr to 9.967799725038238e-06
Epoch 45: reducing lr to 3.3988011791791273e-06
Epoch 48: reducing lr to 2.5162878127245414e-07
[I 2024-06-22 03:02:08,961] Trial 589 finished with value: 0.9718697667121887 and parameters: {'hidden_size': 200, 'n_layers': 4, 'rnn_dropout': 0.018493260445055493, 'bidirectional': True, 'fc_dropout': 0.24552211641077668, 'learning_rate_model': 0.001157134365908542}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.000133971044216619
Epoch 23: reducing lr to 0.00011367434856125294
Epoch 27: reducing lr to 9.203504205508433e-05
Epoch 30: reducing lr to 7.420080725621707e-05
Epoch 33: reducing lr to 5.622969098283645e-05
Epoch 36: reducing lr to 3.9250890936333246e-05
Epoch 39: reducing lr to 2.4331240384515402e-05
Epoch 42: reducing lr to 1.2408187885747405e-05
Epoch 45: reducing lr to 4.230920040620371e-06
Epoch 48: reducing lr to 3.132343427454101e-07
[I 2024-06-22 03:03:23,740] Trial 590 finished with value: 0.9741371870040894 and parameters: {'hidden_size': 200, 'n_layers': 4, 'rnn_dropout': 0.019338733200946694, 'bidirectional': True, 'fc_dropout': 0.24357893909790557, 'learning_rate_model': 0.0014404322937169865}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001518671469955864
Epoch 23: reducing lr to 0.00012885918075450676
Epoch 26: reducing lr to 0.00011081223312528455
Epoch 29: reducing lr to 9.09324880984216e-05
Epoch 33: reducing lr to 6.374095832379513e-05
Epoch 36: reducing lr to 4.4494098395602455e-05
Epoch 39: reducing lr to 2.758145300476634e-05
Epoch 42: reducing lr to 1.4065696842272577e-05
Epoch 45: reducing lr to 4.796094256730144e-06
Epoch 48: reducing lr to 3.5507677238722727e-07
[I 2024-06-22 03:04:37,599] Trial 591 finished with value: 0.9762229323387146 and parameters: {'hidden_size': 196, 'n_layers': 4, 'rnn_dropout': 0.018558893850487243, 'bidirectional': True, 'fc_dropout': 0.24794636662977781, 'learning_rate_model': 0.0016328479349119755}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.075524516353159e-05
Epoch 27: reducing lr to 7.347887110031973e-05
Epoch 30: reducing lr to 5.9240387467373855e-05
Epoch 33: reducing lr to 4.489262050063247e-05
Epoch 36: reducing lr to 3.133709825391675e-05
Epoch 39: reducing lr to 1.9425558309134663e-05
Epoch 42: reducing lr to 9.906440176337328e-06
Epoch 45: reducing lr to 3.377878918275879e-06
Epoch 48: reducing lr to 2.500798106987298e-07
[I 2024-06-22 03:05:52,511] Trial 592 finished with value: 0.9717784523963928 and parameters: {'hidden_size': 200, 'n_layers': 4, 'rnn_dropout': 0.00023398528940882235, 'bidirectional': True, 'fc_dropout': 0.23960270608225837, 'learning_rate_model': 0.0011500113052093867}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 0.0001425747583319417
Epoch 27: reducing lr to 0.00011543390435180788
Epoch 30: reducing lr to 9.306551826764681e-05
Epoch 33: reducing lr to 7.052545015147178e-05
Epoch 36: reducing lr to 4.922998337259555e-05
Epoch 39: reducing lr to 3.0517181419072713e-05
Epoch 42: reducing lr to 1.5562828479236862e-05
Epoch 45: reducing lr to 5.306583322869669e-06
Epoch 48: reducing lr to 3.9287061050653077e-07
[I 2024-06-22 03:07:10,288] Trial 593 finished with value: 0.9831926822662354 and parameters: {'hidden_size': 199, 'n_layers': 4, 'rnn_dropout': 0.0013558755507210654, 'bidirectional': True, 'fc_dropout': 0.2287638247398046, 'learning_rate_model': 0.001806645815608624}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00011628732188202274
Epoch 23: reducing lr to 9.866972104433203e-05
Epoch 27: reducing lr to 7.988672942326334e-05
Epoch 30: reducing lr to 6.440655298138873e-05
Epoch 33: reducing lr to 4.880756295423905e-05
Epoch 36: reducing lr to 3.406990678589743e-05
Epoch 39: reducing lr to 2.111959938005789e-05
Epoch 42: reducing lr to 1.0770349272708983e-05
Epoch 45: reducing lr to 3.6724529804007437e-06
Epoch 48: reducing lr to 2.718884745008237e-07
[I 2024-06-22 03:08:25,285] Trial 594 finished with value: 0.9725674986839294 and parameters: {'hidden_size': 200, 'n_layers': 4, 'rnn_dropout': 0.017169952827141535, 'bidirectional': True, 'fc_dropout': 0.2398469722588444, 'learning_rate_model': 0.00125030012842095}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00012939926667171158
Epoch 23: reducing lr to 0.00010979519812823833
Epoch 26: reducing lr to 9.441819371956258e-05
Epoch 29: reducing lr to 7.747954386021261e-05
Epoch 32: reducing lr to 6.004356656391989e-05
Epoch 37: reducing lr to 3.2839088028729686e-05
Epoch 40: reducing lr to 1.9300652326397135e-05
Epoch 43: reducing lr to 8.920475379297363e-06
Epoch 46: reducing lr to 2.3507969960980543e-06
Epoch 49: reducing lr to 4.4050550010834095e-09
[I 2024-06-22 03:09:39,160] Trial 595 finished with value: 0.9711711406707764 and parameters: {'hidden_size': 196, 'n_layers': 4, 'rnn_dropout': 0.018178577424229635, 'bidirectional': True, 'fc_dropout': 0.2508981627623564, 'learning_rate_model': 0.001391277373309507}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00012955261529202524
Epoch 23: reducing lr to 0.00010992531433819141
Epoch 27: reducing lr to 8.899968247966093e-05
Epoch 30: reducing lr to 7.175362924901299e-05
Epoch 33: reducing lr to 5.437520896015537e-05
Epoch 37: reducing lr to 3.287800501003176e-05
Epoch 40: reducing lr to 1.9323525163945098e-05
Epoch 43: reducing lr to 8.931046865729569e-06
Epoch 46: reducing lr to 2.353582881097719e-06
Epoch 49: reducing lr to 4.4102753483498545e-09
[I 2024-06-22 03:10:53,000] Trial 596 finished with value: 0.9711508750915527 and parameters: {'hidden_size': 196, 'n_layers': 4, 'rnn_dropout': 0.014462578614957854, 'bidirectional': True, 'fc_dropout': 0.24578128538405455, 'learning_rate_model': 0.0013929261497760068}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00015130630359761084
Epoch 23: reducing lr to 0.00012838330547651258
Epoch 27: reducing lr to 0.00010394396861077857
Epoch 36: reducing lr to 4.4329782269121335e-05
Epoch 39: reducing lr to 2.7479595057670326e-05
Epoch 42: reducing lr to 1.401375240683688e-05
Epoch 45: reducing lr to 4.778382343039985e-06
Epoch 48: reducing lr to 3.5376547848655053e-07
[I 2024-06-22 03:12:06,723] Trial 597 finished with value: 0.9763771295547485 and parameters: {'hidden_size': 196, 'n_layers': 4, 'rnn_dropout': 0.0010974128426422339, 'bidirectional': True, 'fc_dropout': 0.22869982527235866, 'learning_rate_model': 0.0016268178487326384}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001318532357651629
Epoch 23: reducing lr to 0.00011187738939366195
Epoch 26: reducing lr to 9.620877055362713e-05
Epoch 29: reducing lr to 7.894889071895517e-05
Epoch 32: reducing lr to 6.118225197071022e-05
Epoch 37: reducing lr to 3.346185900071734e-05
Epoch 40: reducing lr to 1.9666676072208543e-05
Epoch 43: reducing lr to 9.089646128427079e-06
Epoch 46: reducing lr to 2.3953782624512675e-06
Epoch 49: reducing lr to 4.488593873517612e-09
[I 2024-06-22 03:13:20,588] Trial 598 finished with value: 0.9712052345275879 and parameters: {'hidden_size': 196, 'n_layers': 4, 'rnn_dropout': 0.01671344216222039, 'bidirectional': True, 'fc_dropout': 0.21666303804959663, 'learning_rate_model': 0.0014176620025453236}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00013432313233054883
Epoch 23: reducing lr to 0.00011397309510921907
Epoch 26: reducing lr to 9.801096911607751e-05
Epoch 29: reducing lr to 8.04277743648224e-05
Epoch 32: reducing lr to 6.232832800842064e-05
Epoch 37: reducing lr to 3.408867206402737e-05
Epoch 40: reducing lr to 2.0035075492984386e-05
Epoch 43: reducing lr to 9.259914879306757e-06
Epoch 46: reducing lr to 2.44024888325096e-06
Epoch 49: reducing lr to 4.572674954482492e-09
[I 2024-06-22 03:14:34,277] Trial 599 finished with value: 0.9714718461036682 and parameters: {'hidden_size': 196, 'n_layers': 4, 'rnn_dropout': 0.016751566571718765, 'bidirectional': True, 'fc_dropout': 0.2079496627349067, 'learning_rate_model': 0.0014442178810616549}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00017891691528750136
Epoch 23: reducing lr to 0.0001518108925015951
Epoch 26: reducing lr to 0.0001305495185701459
Epoch 32: reducing lr to 8.302063828329892e-05
Epoch 35: reducing lr to 5.9739555582974305e-05
Epoch 38: reducing lr to 3.874847751200259e-05
Epoch 41: reducing lr to 2.1366338839796765e-05
Epoch 44: reducing lr to 8.68534857778058e-06
Epoch 47: reducing lr to 1.5022852040955537e-06
[I 2024-06-22 03:15:47,946] Trial 600 finished with value: 0.98453688621521 and parameters: {'hidden_size': 195, 'n_layers': 4, 'rnn_dropout': 0.018517793341555358, 'bidirectional': True, 'fc_dropout': 0.2063816641263914, 'learning_rate_model': 0.0019236821223520303}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001325609835870844
Epoch 23: reducing lr to 0.00011247791298495747
Epoch 27: reducing lr to 9.106636266545742e-05
Epoch 30: reducing lr to 7.341983523645358e-05
Epoch 36: reducing lr to 3.883777080050754e-05
Epoch 39: reducing lr to 2.4075151284556807e-05
Epoch 42: reducing lr to 1.2277590282930555e-05
Epoch 45: reducing lr to 4.186389121190186e-06
Epoch 48: reducing lr to 3.099375153070181e-07
[I 2024-06-22 03:17:01,744] Trial 601 finished with value: 0.9761568307876587 and parameters: {'hidden_size': 195, 'n_layers': 4, 'rnn_dropout': 0.01815294885284367, 'bidirectional': True, 'fc_dropout': 0.2009727372258169, 'learning_rate_model': 0.0014252715783642238}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00014124132621577473
Epoch 23: reducing lr to 0.00011984317836282069
Epoch 27: reducing lr to 9.702955944096596e-05
Epoch 30: reducing lr to 7.822750419264939e-05
Epoch 33: reducing lr to 5.928113924586264e-05
Epoch 36: reducing lr to 4.1380941108696154e-05
Epoch 39: reducing lr to 2.5651637489867876e-05
Epoch 42: reducing lr to 1.3081549995861514e-05
Epoch 45: reducing lr to 4.460521757850043e-06
Epoch 48: reducing lr to 3.3023280697995743e-07
[I 2024-06-22 03:18:10,600] Trial 602 finished with value: 0.9751679301261902 and parameters: {'hidden_size': 188, 'n_layers': 4, 'rnn_dropout': 0.026068190856549614, 'bidirectional': True, 'fc_dropout': 0.21480333872298157, 'learning_rate_model': 0.001518601043070618}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00018327076813688456
Epoch 22: reducing lr to 0.00016203097098172343
Epoch 27: reducing lr to 0.00012590282438698346
Epoch 30: reducing lr to 0.00010150580688343173
Epoch 33: reducing lr to 7.69215371782982e-05
Epoch 36: reducing lr to 5.3694744069677994e-05
Epoch 39: reducing lr to 3.32848425648097e-05
Epoch 42: reducing lr to 1.6974250953293804e-05
Epoch 45: reducing lr to 5.787847443485439e-06
Epoch 48: reducing lr to 4.2850079237256174e-07
[I 2024-06-22 03:19:23,029] Trial 603 finished with value: 0.9893522262573242 and parameters: {'hidden_size': 193, 'n_layers': 4, 'rnn_dropout': 0.03413181248186818, 'bidirectional': True, 'fc_dropout': 0.22171432216051454, 'learning_rate_model': 0.0019704939560808393}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00014197451384627077
Epoch 23: reducing lr to 0.00012046528761603397
Epoch 27: reducing lr to 9.753324256743218e-05
Epoch 30: reducing lr to 7.863358533033997e-05
Epoch 33: reducing lr to 5.9588869279141846e-05
Epoch 36: reducing lr to 4.159575071840506e-05
Epoch 39: reducing lr to 2.5784795849488554e-05
Epoch 42: reducing lr to 1.3149456683667808e-05
Epoch 45: reducing lr to 4.483676449653327e-06
Epoch 48: reducing lr to 3.319470546137536e-07
[I 2024-06-22 03:20:38,820] Trial 604 finished with value: 0.9734548926353455 and parameters: {'hidden_size': 197, 'n_layers': 4, 'rnn_dropout': 0.013768792318845792, 'bidirectional': True, 'fc_dropout': 0.1956372269822993, 'learning_rate_model': 0.0015264841430829813}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001283929246620339
Epoch 23: reducing lr to 0.00010894131755241057
Epoch 27: reducing lr to 8.820300155113458e-05
Epoch 30: reducing lr to 7.111132641845734e-05
Epoch 33: reducing lr to 5.388846911169477e-05
Epoch 36: reducing lr to 3.761661120411863e-05
Epoch 39: reducing lr to 2.3318166488064128e-05
Epoch 42: reducing lr to 1.1891551205879925e-05
Epoch 45: reducing lr to 4.0547582591662345e-06
Epoch 48: reducing lr to 3.0019228113684247e-07
[I 2024-06-22 03:21:49,888] Trial 605 finished with value: 0.9753077030181885 and parameters: {'hidden_size': 190, 'n_layers': 4, 'rnn_dropout': 0.037710737360722175, 'bidirectional': True, 'fc_dropout': 0.24911551207724106, 'learning_rate_model': 0.0013804573671078687}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 0.00014018194466841613
Epoch 27: reducing lr to 0.0001134965921178707
Epoch 33: reducing lr to 6.93418306754644e-05
Epoch 36: reducing lr to 4.840376295148269e-05
Epoch 39: reducing lr to 3.0005015524308687e-05
Epoch 42: reducing lr to 1.530163954885466e-05
Epoch 45: reducing lr to 5.217523623732485e-06
Epoch 48: reducing lr to 3.8627711404324183e-07
[I 2024-06-22 03:23:02,698] Trial 606 finished with value: 0.9777637124061584 and parameters: {'hidden_size': 194, 'n_layers': 4, 'rnn_dropout': 0.025177628499526022, 'bidirectional': True, 'fc_dropout': 0.23020399386226012, 'learning_rate_model': 0.0017763251133797282}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00012713958108230601
Epoch 23: reducing lr to 0.00010787785629641984
Epoch 27: reducing lr to 8.734198318896362e-05
Epoch 30: reducing lr to 7.041715324149207e-05
Epoch 33: reducing lr to 5.336242169155622e-05
Epoch 36: reducing lr to 3.7249406093183544e-05
Epoch 39: reducing lr to 2.3090539659438082e-05
Epoch 42: reducing lr to 1.1775468490293175e-05
Epoch 45: reducing lr to 4.015176598067276e-06
Epoch 48: reducing lr to 2.9726186990713164e-07
[I 2024-06-22 03:24:11,632] Trial 607 finished with value: 0.975121021270752 and parameters: {'hidden_size': 188, 'n_layers': 4, 'rnn_dropout': 0.001085304699806072, 'bidirectional': True, 'fc_dropout': 0.2162237404466774, 'learning_rate_model': 0.0013669816449626878}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001056644488625375
Epoch 23: reducing lr to 8.965621982546864e-05
Epoch 27: reducing lr to 7.25890587153055e-05
Epoch 36: reducing lr to 3.095761313500859e-05
Epoch 39: reducing lr to 1.9190319224613544e-05
Epoch 42: reducing lr to 9.786475443233673e-06
Epoch 45: reducing lr to 3.336973574310305e-06
Epoch 48: reducing lr to 2.4705140117813487e-07
[I 2024-06-22 03:25:27,433] Trial 608 finished with value: 0.9709330797195435 and parameters: {'hidden_size': 197, 'n_layers': 4, 'rnn_dropout': 0.015357313840130773, 'bidirectional': True, 'fc_dropout': 0.2510138899545806, 'learning_rate_model': 0.0011360849303623292}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.486820269663586e-05
Epoch 26: reducing lr to 7.298226643284475e-05
Epoch 29: reducing lr to 5.988922781023007e-05
Epoch 36: reducing lr to 2.9304347112341707e-05
Epoch 39: reducing lr to 1.8165475913864036e-05
Epoch 42: reducing lr to 9.263836722302534e-06
Epoch 45: reducing lr to 3.1587652284380175e-06
Epoch 48: reducing lr to 2.3385782305443414e-07
[I 2024-06-22 03:26:43,116] Trial 609 finished with value: 0.9711276292800903 and parameters: {'hidden_size': 197, 'n_layers': 4, 'rnn_dropout': 0.0004079662491093468, 'bidirectional': True, 'fc_dropout': 0.242262054762764, 'learning_rate_model': 0.0010754132433675755}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00015532126453220968
Epoch 23: reducing lr to 0.00013178999735839004
Epoch 27: reducing lr to 0.00010670215490861654
Epoch 36: reducing lr to 4.5506087154099084e-05
Epoch 39: reducing lr to 2.820877486070456e-05
Epoch 42: reducing lr to 1.4385611788255803e-05
Epoch 45: reducing lr to 4.905178239719243e-06
Epoch 48: reducing lr to 3.6315275808009117e-07
[I 2024-06-22 03:27:56,802] Trial 610 finished with value: 0.9778722524642944 and parameters: {'hidden_size': 196, 'n_layers': 4, 'rnn_dropout': 0.012540768231452906, 'bidirectional': True, 'fc_dropout': 0.23368543778498707, 'learning_rate_model': 0.0016699859782491723}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.647941045164523e-05
Epoch 26: reducing lr to 7.436782179891146e-05
Epoch 29: reducing lr to 6.102621416346166e-05
Epoch 36: reducing lr to 2.986068493761255e-05
Epoch 39: reducing lr to 1.8510344247773202e-05
Epoch 42: reducing lr to 9.439709017153276e-06
Epoch 45: reducing lr to 3.2187338252811103e-06
Epoch 48: reducing lr to 2.3829757229035068e-07
[I 2024-06-22 03:29:10,593] Trial 611 finished with value: 0.9706838130950928 and parameters: {'hidden_size': 196, 'n_layers': 4, 'rnn_dropout': 0.018104343442142283, 'bidirectional': True, 'fc_dropout': 0.20615624908885127, 'learning_rate_model': 0.001095829772791997}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001201763339733659
Epoch 23: reducing lr to 0.00010196954541022607
Epoch 27: reducing lr to 8.25583917475546e-05
Epoch 30: reducing lr to 6.656050974342054e-05
Epoch 33: reducing lr to 5.0439840655763544e-05
Epoch 36: reducing lr to 3.5209311127633945e-05
Epoch 39: reducing lr to 2.1825904900075738e-05
Epoch 42: reducing lr to 1.1130543469906571e-05
Epoch 45: reducing lr to 3.795271304999834e-06
Epoch 48: reducing lr to 2.8098127625871514e-07
[I 2024-06-22 03:30:26,281] Trial 612 finished with value: 0.9720659852027893 and parameters: {'hidden_size': 197, 'n_layers': 4, 'rnn_dropout': 0.0005295285870716911, 'bidirectional': True, 'fc_dropout': 0.19967976278879188, 'learning_rate_model': 0.001292114078888999}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.586537685235763e-05
Epoch 27: reducing lr to 6.951984919820417e-05
Epoch 33: reducing lr to 4.2473818127325555e-05
Epoch 36: reducing lr to 2.964866378999282e-05
Epoch 39: reducing lr to 1.8378914428314972e-05
Epoch 42: reducing lr to 9.372683831924098e-06
Epoch 45: reducing lr to 3.1958797065311655e-06
Epoch 48: reducing lr to 2.366055774530567e-07
[I 2024-06-22 03:31:42,478] Trial 613 finished with value: 0.9728676080703735 and parameters: {'hidden_size': 198, 'n_layers': 4, 'rnn_dropout': 0.015984973437752206, 'bidirectional': True, 'fc_dropout': 0.2096372309391494, 'learning_rate_model': 0.001088049003981481}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001315876672186843
Epoch 23: reducing lr to 0.0001116520546454271
Epoch 27: reducing lr to 9.039771659031176e-05
Epoch 30: reducing lr to 7.288075710450887e-05
Epoch 33: reducing lr to 5.522935129844334e-05
Epoch 36: reducing lr to 3.855260817565817e-05
Epoch 39: reducing lr to 2.3898381784339773e-05
Epoch 42: reducing lr to 1.2187443248233609e-05
Epoch 45: reducing lr to 4.155650958678975e-06
Epoch 48: reducing lr to 3.0766182868592877e-07
[I 2024-06-22 03:32:55,138] Trial 614 finished with value: 0.9747804403305054 and parameters: {'hidden_size': 194, 'n_layers': 4, 'rnn_dropout': 0.0005358454902153198, 'bidirectional': True, 'fc_dropout': 0.19222193267386298, 'learning_rate_model': 0.001414806657849161}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.600838709085535e-05
Epoch 27: reducing lr to 6.963563568373085e-05
Epoch 36: reducing lr to 2.969804414136584e-05
Epoch 39: reducing lr to 1.8409524821375306e-05
Epoch 42: reducing lr to 9.388294195487433e-06
Epoch 45: reducing lr to 3.201202498275598e-06
Epoch 48: reducing lr to 2.3699964804707336e-07
[I 2024-06-22 03:34:10,954] Trial 615 finished with value: 0.9708996415138245 and parameters: {'hidden_size': 197, 'n_layers': 4, 'rnn_dropout': 0.01876260022726497, 'bidirectional': True, 'fc_dropout': 0.22311481163220725, 'learning_rate_model': 0.0010898611680138376}. Best is trial 153 with value: 0.9688200950622559.
Epoch 22: reducing lr to 0.00017453139792443078
Epoch 27: reducing lr to 0.00013561602334267894
Epoch 30: reducing lr to 0.00010933681545863814
Epoch 33: reducing lr to 8.285590916898708e-05
Epoch 36: reducing lr to 5.783720659633957e-05
Epoch 39: reducing lr to 3.5852714251685244e-05
Epoch 42: reducing lr to 1.828378691832093e-05
Epoch 45: reducing lr to 6.234370498210511e-06
Epoch 48: reducing lr to 4.615589343900852e-07
[I 2024-06-22 03:35:26,711] Trial 616 finished with value: 0.9984918832778931 and parameters: {'hidden_size': 197, 'n_layers': 4, 'rnn_dropout': 0.01561092205895364, 'bidirectional': True, 'fc_dropout': 0.22297930615213032, 'learning_rate_model': 0.0021225143728554404}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00012627260539624765
Epoch 23: reducing lr to 0.00010714222796040583
Epoch 27: reducing lr to 8.674639072946094e-05
Epoch 30: reducing lr to 6.993697264610143e-05
Epoch 33: reducing lr to 5.299853877042388e-05
Epoch 36: reducing lr to 3.699539920462853e-05
Epoch 39: reducing lr to 2.293308329303918e-05
Epoch 42: reducing lr to 1.1695170562723146e-05
Epoch 45: reducing lr to 3.987796765161445e-06
Epoch 48: reducing lr to 2.9523481577177923e-07
[I 2024-06-22 03:36:41,598] Trial 617 finished with value: 0.9736799597740173 and parameters: {'hidden_size': 200, 'n_layers': 4, 'rnn_dropout': 0.01629752728459038, 'bidirectional': True, 'fc_dropout': 0.21143980912195898, 'learning_rate_model': 0.001357660080117327}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 0.00014030690453722706
Epoch 27: reducing lr to 0.00011359776434297488
Epoch 30: reducing lr to 9.158517917235633e-05
Epoch 33: reducing lr to 6.940364281599939e-05
Epoch 36: reducing lr to 4.844691064701991e-05
Epoch 39: reducing lr to 3.003176235545342e-05
Epoch 42: reducing lr to 1.5315279614093718e-05
Epoch 45: reducing lr to 5.222174586943769e-06
Epoch 48: reducing lr to 3.866214461011822e-07
[I 2024-06-22 03:37:54,020] Trial 618 finished with value: 0.9781777858734131 and parameters: {'hidden_size': 193, 'n_layers': 4, 'rnn_dropout': 0.015770723064335586, 'bidirectional': True, 'fc_dropout': 0.23640402231970725, 'learning_rate_model': 0.0017779085509162704}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.940520124704393e-05
Epoch 27: reducing lr to 7.238582460211877e-05
Epoch 30: reducing lr to 5.835914777079666e-05
Epoch 36: reducing lr to 3.087093832253379e-05
Epoch 39: reducing lr to 1.913659036273808e-05
Epoch 42: reducing lr to 9.759075368165467e-06
Epoch 45: reducing lr to 3.327630749412093e-06
Epoch 48: reducing lr to 2.463597091611905e-07
[I 2024-06-22 03:39:08,943] Trial 619 finished with value: 0.9716746211051941 and parameters: {'hidden_size': 200, 'n_layers': 4, 'rnn_dropout': 0.0011624355808199623, 'bidirectional': True, 'fc_dropout': 0.22112038356975, 'learning_rate_model': 0.0011329041312527477}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001461510838784738
Epoch 23: reducing lr to 0.00012400910471776136
Epoch 27: reducing lr to 0.00010040245061763058
Epoch 33: reducing lr to 6.134183943513484e-05
Epoch 36: reducing lr to 4.281940390242363e-05
Epoch 39: reducing lr to 2.654332639637285e-05
Epoch 42: reducing lr to 1.3536284046107138e-05
Epoch 45: reducing lr to 4.61557609971301e-06
Epoch 48: reducing lr to 3.417121883903711e-07
[I 2024-06-22 03:40:21,777] Trial 620 finished with value: 0.9780392050743103 and parameters: {'hidden_size': 194, 'n_layers': 4, 'rnn_dropout': 0.029133647382193893, 'bidirectional': True, 'fc_dropout': 0.17702535928526908, 'learning_rate_model': 0.0015713898642149924}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00012092576992732956
Epoch 23: reducing lr to 0.00010260544135590112
Epoch 27: reducing lr to 8.307323710047365e-05
Epoch 30: reducing lr to 6.697559012960477e-05
Epoch 33: reducing lr to 5.0754390358269914e-05
Epoch 36: reducing lr to 3.5428881177750396e-05
Epoch 39: reducing lr to 2.1962014209780055e-05
Epoch 42: reducing lr to 1.1199955051935318e-05
Epoch 45: reducing lr to 3.818939132740743e-06
Epoch 48: reducing lr to 2.8273351369066757e-07
[I 2024-06-22 03:41:37,612] Trial 621 finished with value: 0.9720796942710876 and parameters: {'hidden_size': 197, 'n_layers': 4, 'rnn_dropout': 0.029123189185318827, 'bidirectional': True, 'fc_dropout': 0.19571397289351142, 'learning_rate_model': 0.0013001718779191865}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.666440143598567e-05
Epoch 26: reducing lr to 5.732793852814896e-05
Epoch 33: reducing lr to 3.297594171196293e-05
Epoch 38: reducing lr to 1.7015538327503494e-05
Epoch 41: reducing lr to 9.382555929697458e-06
Epoch 44: reducing lr to 3.8139790542009317e-06
Epoch 47: reducing lr to 6.5969537670767e-07
[I 2024-06-22 03:42:50,098] Trial 622 finished with value: 0.9721435308456421 and parameters: {'hidden_size': 193, 'n_layers': 4, 'rnn_dropout': 0.018306670003211126, 'bidirectional': True, 'fc_dropout': 0.234589376607004, 'learning_rate_model': 0.0008447425288561373}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.585444070612446e-05
Epoch 26: reducing lr to 7.38303801301736e-05
Epoch 29: reducing lr to 6.05851896775563e-05
Epoch 36: reducing lr to 2.9644887621591638e-05
Epoch 39: reducing lr to 1.8376573618745824e-05
Epoch 42: reducing lr to 9.371490090689374e-06
Epoch 45: reducing lr to 3.1954726669408943e-06
Epoch 48: reducing lr to 2.3657544245231192e-07
[I 2024-06-22 03:44:03,902] Trial 623 finished with value: 0.970721423625946 and parameters: {'hidden_size': 196, 'n_layers': 4, 'rnn_dropout': 0.03669800590277701, 'bidirectional': True, 'fc_dropout': 0.2468376224495135, 'learning_rate_model': 0.0010879104258554354}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001802489773610174
Epoch 22: reducing lr to 0.0001593593845716546
Epoch 27: reducing lr to 0.0001238269232639847
Epoch 30: reducing lr to 9.983216676037504e-05
Epoch 33: reducing lr to 7.565324549231971e-05
Epoch 36: reducing lr to 5.280941858110302e-05
Epoch 39: reducing lr to 3.2736038021340945e-05
Epoch 42: reducing lr to 1.669437743347748e-05
Epoch 45: reducing lr to 5.692416708979065e-06
Epoch 48: reducing lr to 4.2143561905004857e-07
[I 2024-06-22 03:45:19,618] Trial 624 finished with value: 0.9911622405052185 and parameters: {'hidden_size': 197, 'n_layers': 4, 'rnn_dropout': 0.03873343524643788, 'bidirectional': True, 'fc_dropout': 0.20592777544355845, 'learning_rate_model': 0.0019380042114209614}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.46273253240943e-05
Epoch 27: reducing lr to 6.851747596350137e-05
Epoch 36: reducing lr to 2.9221173981390665e-05
Epoch 39: reducing lr to 1.8113917709848104e-05
Epoch 42: reducing lr to 9.237543616304991e-06
Epoch 45: reducing lr to 3.1497998557244824e-06
Epoch 48: reducing lr to 2.3319407554739033e-07
[I 2024-06-22 03:46:34,671] Trial 625 finished with value: 0.9715563058853149 and parameters: {'hidden_size': 200, 'n_layers': 4, 'rnn_dropout': 0.013717014641675188, 'bidirectional': True, 'fc_dropout': 0.2425616605424934, 'learning_rate_model': 0.0010723609492429467}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001480524788853018
Epoch 23: reducing lr to 0.00012562243721078362
Epoch 27: reducing lr to 0.00010170866548249193
Epoch 33: reducing lr to 6.213988392523656e-05
Epoch 36: reducing lr to 4.3376475383625374e-05
Epoch 39: reducing lr to 2.6888649516361723e-05
Epoch 42: reducing lr to 1.3712388267938798e-05
Epoch 45: reducing lr to 4.675623778572016e-06
Epoch 48: reducing lr to 3.4615779242923655e-07
[I 2024-06-22 03:47:43,878] Trial 626 finished with value: 0.974912703037262 and parameters: {'hidden_size': 192, 'n_layers': 4, 'rnn_dropout': 0.0016951699353825837, 'bidirectional': True, 'fc_dropout': 0.22948077616025064, 'learning_rate_model': 0.0015918333174026742}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.698752792661231e-05
Epoch 26: reducing lr to 5.760581060368613e-05
Epoch 31: reducing lr to 4.017174292783557e-05
Epoch 37: reducing lr to 2.0035569532282128e-05
Epoch 40: reducing lr to 1.1775587719294262e-05
Epoch 43: reducing lr to 5.442502074557061e-06
Epoch 46: reducing lr to 1.4342528827352656e-06
Epoch 49: reducing lr to 2.68758333639256e-09
[I 2024-06-22 03:48:57,760] Trial 627 finished with value: 0.9718695878982544 and parameters: {'hidden_size': 196, 'n_layers': 4, 'rnn_dropout': 0.0387952912954902, 'bidirectional': True, 'fc_dropout': 0.24373371146460515, 'learning_rate_model': 0.0008488370483140891}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.418904666665196e-05
Epoch 27: reducing lr to 6.81626290241515e-05
Epoch 36: reducing lr to 2.9069839683013552e-05
Epoch 39: reducing lr to 1.8020107070028283e-05
Epoch 42: reducing lr to 9.189703061274884e-06
Epoch 45: reducing lr to 3.133487275282037e-06
Epoch 48: reducing lr to 2.3198638068096938e-07
[I 2024-06-22 03:50:10,142] Trial 628 finished with value: 0.970199704170227 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.014259192783557684, 'bidirectional': True, 'fc_dropout': 0.22113745283077083, 'learning_rate_model': 0.001066807271216046}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001250034132232408
Epoch 23: reducing lr to 0.0001060653191827724
Epoch 27: reducing lr to 8.587448474633077e-05
Epoch 30: reducing lr to 6.923402161402537e-05
Epoch 33: reducing lr to 5.246583945391623e-05
Epoch 36: reducing lr to 3.662355076639893e-05
Epoch 39: reducing lr to 2.2702578111593828e-05
Epoch 42: reducing lr to 1.157761997529678e-05
Epoch 45: reducing lr to 3.947714591945773e-06
Epoch 48: reducing lr to 2.9226734934307675e-07
[I 2024-06-22 03:51:25,859] Trial 629 finished with value: 0.9719587564468384 and parameters: {'hidden_size': 197, 'n_layers': 4, 'rnn_dropout': 0.014364829674726238, 'bidirectional': True, 'fc_dropout': 0.21978875117523708, 'learning_rate_model': 0.0013440139567805866}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.110380377638972e-05
Epoch 26: reducing lr to 6.974507798973655e-05
Epoch 36: reducing lr to 2.8004528698341168e-05
Epoch 39: reducing lr to 1.7359731291695695e-05
Epoch 42: reducing lr to 8.852931626557477e-06
Epoch 45: reducing lr to 3.0186555991844334e-06
Epoch 48: reducing lr to 2.2348486700463945e-07
[I 2024-06-22 03:52:35,154] Trial 630 finished with value: 0.9726177453994751 and parameters: {'hidden_size': 192, 'n_layers': 4, 'rnn_dropout': 0.0015376919764266556, 'bidirectional': True, 'fc_dropout': 0.20472969632138435, 'learning_rate_model': 0.0010277124046138436}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00011389577681478061
Epoch 23: reducing lr to 9.664049652672612e-05
Epoch 27: reducing lr to 7.824379268176593e-05
Epoch 30: reducing lr to 6.308197888691452e-05
Epoch 33: reducing lr to 4.78037950065539e-05
Epoch 36: reducing lr to 3.336923094095986e-05
Epoch 39: reducing lr to 2.0685257330537212e-05
Epoch 42: reducing lr to 1.0548848121433394e-05
Epoch 45: reducing lr to 3.5969259438518475e-06
Epoch 48: reducing lr to 2.6629686288308536e-07
[I 2024-06-22 03:53:50,085] Trial 631 finished with value: 0.9723214507102966 and parameters: {'hidden_size': 200, 'n_layers': 4, 'rnn_dropout': 0.02525412412475364, 'bidirectional': True, 'fc_dropout': 0.1787191894395929, 'learning_rate_model': 0.0012245866709579695}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001456419923799426
Epoch 23: reducing lr to 0.00012357714089459348
Epoch 27: reducing lr to 0.00010005271640639727
Epoch 30: reducing lr to 8.066484416977462e-05
Epoch 33: reducing lr to 6.11281659670225e-05
Epoch 36: reducing lr to 4.267025006845671e-05
Epoch 39: reducing lr to 2.6450867404947456e-05
Epoch 42: reducing lr to 1.348913279038803e-05
Epoch 45: reducing lr to 4.599498555223915e-06
Epoch 48: reducing lr to 3.4052189430949353e-07
[I 2024-06-22 03:55:03,700] Trial 632 finished with value: 0.979629397392273 and parameters: {'hidden_size': 195, 'n_layers': 4, 'rnn_dropout': 0.0156672395104103, 'bidirectional': True, 'fc_dropout': 0.22101213426042024, 'learning_rate_model': 0.001565916205043124}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.408261689465972e-05
Epoch 27: reducing lr to 6.807645946465811e-05
Epoch 36: reducing lr to 2.9033090289454503e-05
Epoch 39: reducing lr to 1.7997326483209998e-05
Epoch 42: reducing lr to 9.178085659246793e-06
Epoch 45: reducing lr to 3.1295259958822456e-06
Epoch 48: reducing lr to 2.3169310906692286e-07
[I 2024-06-22 03:56:12,809] Trial 633 finished with value: 0.9727400541305542 and parameters: {'hidden_size': 192, 'n_layers': 4, 'rnn_dropout': 0.027418005216456244, 'bidirectional': True, 'fc_dropout': 0.1892850052790805, 'learning_rate_model': 0.0010654586390705276}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00012265697593486134
Epoch 23: reducing lr to 0.00010407436858776852
Epoch 27: reducing lr to 8.426253601682438e-05
Epoch 30: reducing lr to 6.79344307808575e-05
Epoch 33: reducing lr to 5.148100392913784e-05
Epoch 36: reducing lr to 3.593609061682951e-05
Epoch 39: reducing lr to 2.22764283413611e-05
Epoch 42: reducing lr to 1.1360296635715574e-05
Epoch 45: reducing lr to 3.873612097593233e-06
Epoch 48: reducing lr to 2.8678120309322265e-07
[I 2024-06-22 03:57:26,622] Trial 634 finished with value: 0.9713361263275146 and parameters: {'hidden_size': 196, 'n_layers': 4, 'rnn_dropout': 0.00013793435679675461, 'bidirectional': True, 'fc_dropout': 0.24786939130873223, 'learning_rate_model': 0.001318785489949362}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.264833887916884e-05
Epoch 26: reducing lr to 7.107329832201859e-05
Epoch 36: reducing lr to 2.8537844962158193e-05
Epoch 39: reducing lr to 1.7690328786589546e-05
Epoch 42: reducing lr to 9.021526230300358e-06
Epoch 45: reducing lr to 3.07614266291075e-06
Epoch 48: reducing lr to 2.2774089700513654e-07
[I 2024-06-22 03:58:38,858] Trial 635 finished with value: 0.970192015171051 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.0006998984306963857, 'bidirectional': True, 'fc_dropout': 0.24370968748026425, 'learning_rate_model': 0.001047284086958904}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00011778526346998442
Epoch 23: reducing lr to 9.994072355976386e-05
Epoch 27: reducing lr to 8.09157810205712e-05
Epoch 30: reducing lr to 6.523619848948598e-05
Epoch 33: reducing lr to 4.9436271889770185e-05
Epoch 36: reducing lr to 3.450877432060899e-05
Epoch 39: reducing lr to 2.139164903878657e-05
Epoch 42: reducing lr to 1.0909086271991075e-05
Epoch 45: reducing lr to 3.7197592555831323e-06
Epoch 48: reducing lr to 2.7539077420684485e-07
[I 2024-06-22 03:59:48,092] Trial 636 finished with value: 0.9728239178657532 and parameters: {'hidden_size': 192, 'n_layers': 4, 'rnn_dropout': 0.0012961258662084335, 'bidirectional': True, 'fc_dropout': 0.23488434965384158, 'learning_rate_model': 0.0012664057238503097}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.525985328298563e-05
Epoch 27: reducing lr to 6.902959446723055e-05
Epoch 36: reducing lr to 2.9439581091199146e-05
Epoch 39: reducing lr to 1.824930612431893e-05
Epoch 42: reducing lr to 9.306587563829215e-06
Epoch 45: reducing lr to 3.173342328159092e-06
Epoch 48: reducing lr to 2.3493703235320535e-07
[I 2024-06-22 04:01:03,111] Trial 637 finished with value: 0.9713005423545837 and parameters: {'hidden_size': 200, 'n_layers': 4, 'rnn_dropout': 0.036586578094187944, 'bidirectional': True, 'fc_dropout': 0.2442010617434876, 'learning_rate_model': 0.0010803760705979197}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00016103806722353334
Epoch 23: reducing lr to 0.0001366407009233983
Epoch 26: reducing lr to 0.00011750393814758391
Epoch 33: reducing lr to 6.759013344563634e-05
Epoch 36: reducing lr to 4.718099832803054e-05
Epoch 39: reducing lr to 2.9247035787360725e-05
Epoch 42: reducing lr to 1.491509308262412e-05
Epoch 45: reducing lr to 5.085719753121823e-06
Epoch 48: reducing lr to 3.7651907125687593e-07
[I 2024-06-22 04:02:18,093] Trial 638 finished with value: 0.9827944040298462 and parameters: {'hidden_size': 200, 'n_layers': 4, 'rnn_dropout': 0.04541603998635639, 'bidirectional': True, 'fc_dropout': 0.2295414917346525, 'learning_rate_model': 0.0017314520007135196}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.22399872074176e-05
Epoch 27: reducing lr to 6.658459693891076e-05
Epoch 36: reducing lr to 2.839684422510148e-05
Epoch 39: reducing lr to 1.7602923819571465e-05
Epoch 42: reducing lr to 8.976952372339621e-06
Epoch 45: reducing lr to 3.060943954552174e-06
Epoch 48: reducing lr to 2.2661566717863385e-07
[I 2024-06-22 04:03:27,325] Trial 639 finished with value: 0.9724768400192261 and parameters: {'hidden_size': 192, 'n_layers': 4, 'rnn_dropout': 0.039019744593301715, 'bidirectional': True, 'fc_dropout': 0.24949090538922225, 'learning_rate_model': 0.001042109630780984}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 0.00016685227388705688
Epoch 27: reducing lr to 0.00013508989704838362
Epoch 30: reducing lr to 0.00010891264011320755
Epoch 33: reducing lr to 8.253446726723334e-05
Epoch 36: reducing lr to 5.761282547655083e-05
Epoch 39: reducing lr to 3.571362260731514e-05
Epoch 42: reducing lr to 1.8212854436893462e-05
Epoch 45: reducing lr to 6.210184077117767e-06
Epoch 48: reducing lr to 4.597683031227345e-07
[I 2024-06-22 04:04:38,488] Trial 640 finished with value: 1.0011802911758423 and parameters: {'hidden_size': 190, 'n_layers': 4, 'rnn_dropout': 0.02952425376823424, 'bidirectional': True, 'fc_dropout': 0.26559190824053774, 'learning_rate_model': 0.0021142800168106736}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001300769490030735
Epoch 23: reducing lr to 0.00011037021116929874
Epoch 27: reducing lr to 8.935988774214433e-05
Epoch 30: reducing lr to 7.204403517112045e-05
Epoch 33: reducing lr to 5.459527981738073e-05
Epoch 36: reducing lr to 3.8109997339389755e-05
Epoch 39: reducing lr to 2.362401168987474e-05
Epoch 42: reducing lr to 1.2047522897747943e-05
Epoch 45: reducing lr to 4.107941186679096e-06
Epoch 48: reducing lr to 3.041296562668154e-07
[I 2024-06-22 04:05:53,459] Trial 641 finished with value: 0.9740480184555054 and parameters: {'hidden_size': 200, 'n_layers': 4, 'rnn_dropout': 0.04061201001847302, 'bidirectional': True, 'fc_dropout': 0.22303539742760214, 'learning_rate_model': 0.0013985636904438033}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.774334918956124e-05
Epoch 26: reducing lr to 5.8255777812080115e-05
Epoch 31: reducing lr to 4.0625001294196546e-05
Epoch 37: reducing lr to 2.0261631158028054e-05
Epoch 40: reducing lr to 1.1908451848744058e-05
Epoch 43: reducing lr to 5.503909905520772e-06
Epoch 46: reducing lr to 1.4504355791083047e-06
Epoch 49: reducing lr to 2.7179073787040285e-09
[I 2024-06-22 04:07:07,151] Trial 642 finished with value: 0.9717571139335632 and parameters: {'hidden_size': 196, 'n_layers': 4, 'rnn_dropout': 0.016924872679735636, 'bidirectional': True, 'fc_dropout': 0.2471960089402731, 'learning_rate_model': 0.0008584144892161845}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.225167561793284e-05
Epoch 26: reducing lr to 7.073218842578187e-05
Epoch 31: reducing lr to 4.932549790353815e-05
Epoch 36: reducing lr to 2.8400880144657293e-05
Epoch 39: reducing lr to 1.7605425646320932e-05
Epoch 42: reducing lr to 8.978228227408026e-06
Epoch 45: reducing lr to 3.0613789931595475e-06
Epoch 48: reducing lr to 2.2664787507455444e-07
[I 2024-06-22 04:08:14,872] Trial 643 finished with value: 0.9719420671463013 and parameters: {'hidden_size': 187, 'n_layers': 4, 'rnn_dropout': 0.029118376664113304, 'bidirectional': True, 'fc_dropout': 0.2662414153543698, 'learning_rate_model': 0.0010422577412754048}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00012381267946371533
Epoch 23: reducing lr to 0.0001050549823206883
Epoch 27: reducing lr to 8.505647789809614e-05
Epoch 30: reducing lr to 6.857452532734148e-05
Epoch 33: reducing lr to 5.1966070330427934e-05
Epoch 36: reducing lr to 3.627468910601076e-05
Epoch 39: reducing lr to 2.248632220714533e-05
Epoch 42: reducing lr to 1.1467336083008763e-05
Epoch 45: reducing lr to 3.910110202462349e-06
Epoch 48: reducing lr to 2.8948332456569647e-07
[I 2024-06-22 04:09:25,854] Trial 644 finished with value: 0.9746040105819702 and parameters: {'hidden_size': 190, 'n_layers': 4, 'rnn_dropout': 0.017744049775896133, 'bidirectional': True, 'fc_dropout': 0.2334344086612702, 'learning_rate_model': 0.0013312114040314547}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00015125508815355514
Epoch 23: reducing lr to 0.00012833984920375342
Epoch 26: reducing lr to 0.00011036563484230282
Epoch 29: reducing lr to 9.056600966994251e-05
Epoch 33: reducing lr to 6.348406789085138e-05
Epoch 36: reducing lr to 4.431477714752479e-05
Epoch 39: reducing lr to 2.7470293530702274e-05
Epoch 42: reducing lr to 1.4009008912776576e-05
Epoch 45: reducing lr to 4.776764915558453e-06
Epoch 48: reducing lr to 3.5364573293967386e-07
[I 2024-06-22 04:10:39,667] Trial 645 finished with value: 0.9757214784622192 and parameters: {'hidden_size': 196, 'n_layers': 4, 'rnn_dropout': 0.04247417624701133, 'bidirectional': True, 'fc_dropout': 0.24718803988488366, 'learning_rate_model': 0.0016262671895959102}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.814019374323739e-05
Epoch 26: reducing lr to 7.579600956326717e-05
Epoch 29: reducing lr to 6.21981846510319e-05
Epoch 33: reducing lr to 4.359906979963032e-05
Epoch 36: reducing lr to 3.04341408198958e-05
Epoch 39: reducing lr to 1.8865823896486913e-05
Epoch 42: reducing lr to 9.620992757772024e-06
Epoch 45: reducing lr to 3.280547606494366e-06
Epoch 48: reducing lr to 2.4287392895625765e-07
[I 2024-06-22 04:11:52,458] Trial 646 finished with value: 0.9712298512458801 and parameters: {'hidden_size': 194, 'n_layers': 4, 'rnn_dropout': 0.0013294647513761096, 'bidirectional': True, 'fc_dropout': 0.21163873768928548, 'learning_rate_model': 0.0011168745020238157}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.944190205681396e-05
Epoch 37: reducing lr to 2.0769658176331847e-05
Epoch 40: reducing lr to 1.2207036658532909e-05
Epoch 43: reducing lr to 5.641911378181399e-06
Epoch 46: reducing lr to 1.4868028615224902e-06
Epoch 49: reducing lr to 2.786054428209183e-09
[I 2024-06-22 04:13:04,765] Trial 647 finished with value: 0.9709059000015259 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.03235730979437555, 'bidirectional': True, 'fc_dropout': 0.19657088959868757, 'learning_rate_model': 0.0008799378182129461}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.433942255306871e-05
Epoch 31: reducing lr to 4.458059977190508e-05
Epoch 36: reducing lr to 2.566883913417107e-05
Epoch 39: reducing lr to 1.5911860354406452e-05
Epoch 42: reducing lr to 8.114561763768592e-06
Epoch 45: reducing lr to 2.7668876634770858e-06
Epoch 48: reducing lr to 2.0484533633317306e-07
[I 2024-06-22 04:14:12,436] Trial 648 finished with value: 0.9719163179397583 and parameters: {'hidden_size': 187, 'n_layers': 4, 'rnn_dropout': 0.01476740178683576, 'bidirectional': True, 'fc_dropout': 0.18967695221389863, 'learning_rate_model': 0.000941997084628226}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.395228930617034e-05
Epoch 37: reducing lr to 1.9127747788306786e-05
Epoch 40: reducing lr to 1.1242029910396447e-05
Epoch 43: reducing lr to 5.195899565107402e-06
Epoch 46: reducing lr to 1.3692661624322215e-06
Epoch 49: reducing lr to 2.5658075821443324e-09
[I 2024-06-22 04:15:24,720] Trial 649 finished with value: 0.9716044664382935 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.02669514647721285, 'bidirectional': True, 'fc_dropout': 0.1728315579863043, 'learning_rate_model': 0.0008103758142418678}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.829900909262295e-05
Epoch 26: reducing lr to 5.8733616599785534e-05
Epoch 37: reducing lr to 2.042782571645825e-05
Epoch 40: reducing lr to 1.200613006039214e-05
Epoch 43: reducing lr to 5.5490553268964335e-06
Epoch 46: reducing lr to 1.4623326716336452e-06
Epoch 49: reducing lr to 2.7402008166330614e-09
[I 2024-06-22 04:16:35,719] Trial 650 finished with value: 0.9733136296272278 and parameters: {'hidden_size': 190, 'n_layers': 4, 'rnn_dropout': 0.04503836455230174, 'bidirectional': True, 'fc_dropout': 0.20667687181310132, 'learning_rate_model': 0.0008654555717367714}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.049539043914433e-05
Epoch 26: reducing lr to 6.922187397611012e-05
Epoch 36: reducing lr to 2.7794448184604496e-05
Epoch 39: reducing lr to 1.722950445205222e-05
Epoch 42: reducing lr to 8.786519924214027e-06
Epoch 45: reducing lr to 2.9960106646489483e-06
Epoch 48: reducing lr to 2.2180835903056958e-07
[I 2024-06-22 04:17:48,021] Trial 651 finished with value: 0.9711552858352661 and parameters: {'hidden_size': 193, 'n_layers': 4, 'rnn_dropout': 0.016382010689371684, 'bidirectional': True, 'fc_dropout': 0.19701172905457115, 'learning_rate_model': 0.001020002853338745}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.97394526252209e-05
Epoch 36: reducing lr to 2.7533428587267905e-05
Epoch 39: reducing lr to 1.7067700976605823e-05
Epoch 42: reducing lr to 8.70400510408248e-06
Epoch 45: reducing lr to 2.967874919981221e-06
Epoch 48: reducing lr to 2.1972534129351736e-07
[I 2024-06-22 04:19:00,140] Trial 652 finished with value: 0.9699122905731201 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.02746460385174353, 'bidirectional': True, 'fc_dropout': 0.1680821984780284, 'learning_rate_model': 0.0010104239355529932}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.813141071894846e-05
Epoch 26: reducing lr to 6.718897364006925e-05
Epoch 36: reducing lr to 2.697818390556961e-05
Epoch 39: reducing lr to 1.6723510271621874e-05
Epoch 42: reducing lr to 8.52847837924329e-06
Epoch 45: reducing lr to 2.9080241549360172e-06
Epoch 48: reducing lr to 2.1529431568402275e-07
[I 2024-06-22 04:20:08,902] Trial 653 finished with value: 0.9728888273239136 and parameters: {'hidden_size': 188, 'n_layers': 4, 'rnn_dropout': 0.017645637455282467, 'bidirectional': True, 'fc_dropout': 0.18172046955080226, 'learning_rate_model': 0.0009900475223976813}. Best is trial 153 with value: 0.9688200950622559.
Epoch 31: reducing lr to 4.1296149572123884e-05
Epoch 37: reducing lr to 2.0596364903911084e-05
Epoch 40: reducing lr to 1.2105186290502874e-05
Epoch 43: reducing lr to 5.594837647976859e-06
Epoch 46: reducing lr to 1.474397605204334e-06
Epoch 49: reducing lr to 2.762808764514464e-09
[I 2024-06-22 04:21:14,898] Trial 654 finished with value: 0.9703714847564697 and parameters: {'hidden_size': 186, 'n_layers': 4, 'rnn_dropout': 0.028013163499426525, 'bidirectional': True, 'fc_dropout': 0.17632500612076168, 'learning_rate_model': 0.0008725959879935796}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.491315060966942e-05
Epoch 31: reducing lr to 4.4924658684267095e-05
Epoch 37: reducing lr to 2.2406075942475214e-05
Epoch 40: reducing lr to 1.3168815205410978e-05
Epoch 43: reducing lr to 6.0864311644908e-06
Epoch 46: reducing lr to 1.603946369455652e-06
Epoch 49: reducing lr to 3.0055644906798288e-09
[I 2024-06-22 04:22:22,536] Trial 655 finished with value: 0.9715744256973267 and parameters: {'hidden_size': 187, 'n_layers': 4, 'rnn_dropout': 0.04658152087855759, 'bidirectional': True, 'fc_dropout': 0.16514965838951767, 'learning_rate_model': 0.0009492671189939285}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.488500513677969e-05
Epoch 26: reducing lr to 5.579774970981763e-05
Epoch 32: reducing lr to 3.548358390300604e-05
Epoch 37: reducing lr to 1.940671752277001e-05
Epoch 40: reducing lr to 1.1405989940277655e-05
Epoch 43: reducing lr to 5.271679460263788e-06
Epoch 46: reducing lr to 1.389236303296214e-06
Epoch 49: reducing lr to 2.6032287499581053e-09
[I 2024-06-22 04:23:30,260] Trial 656 finished with value: 0.9727046489715576 and parameters: {'hidden_size': 187, 'n_layers': 4, 'rnn_dropout': 0.061643519582402787, 'bidirectional': True, 'fc_dropout': 0.18281797536122213, 'learning_rate_model': 0.0008221947867741523}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.973506177774952e-05
Epoch 26: reducing lr to 5.9968547661683275e-05
Epoch 37: reducing lr to 2.085734049802169e-05
Epoch 40: reducing lr to 1.2258570550236187e-05
Epoch 43: reducing lr to 5.665729578953284e-06
Epoch 46: reducing lr to 1.493079629569737e-06
Epoch 49: reducing lr to 2.7978161875274152e-09
[I 2024-06-22 04:24:39,459] Trial 657 finished with value: 0.9730638265609741 and parameters: {'hidden_size': 192, 'n_layers': 4, 'rnn_dropout': 0.029510958311754935, 'bidirectional': True, 'fc_dropout': 0.16642937949320585, 'learning_rate_model': 0.0008836526116962367}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.43479513715997e-05
Epoch 27: reducing lr to 6.8291284269488e-05
Epoch 36: reducing lr to 2.912470827317586e-05
Epoch 39: reducing lr to 1.8054119568214996e-05
Epoch 42: reducing lr to 9.207048394323866e-06
Epoch 45: reducing lr to 3.1394016535848122e-06
Epoch 48: reducing lr to 2.324242491309964e-07
[I 2024-06-22 04:25:51,908] Trial 658 finished with value: 0.971021294593811 and parameters: {'hidden_size': 193, 'n_layers': 4, 'rnn_dropout': 0.029311385262911507, 'bidirectional': True, 'fc_dropout': 0.19906376383228447, 'learning_rate_model': 0.0010688208430686874}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.729725852486921e-05
Epoch 26: reducing lr to 5.787216290438456e-05
Epoch 37: reducing lr to 2.012820810441229e-05
Epoch 40: reducing lr to 1.1830034568461754e-05
Epoch 43: reducing lr to 5.467666601085251e-06
Epoch 46: reducing lr to 1.4408844456122801e-06
Epoch 49: reducing lr to 2.7000099301201058e-09
[I 2024-06-22 04:26:59,507] Trial 659 finished with value: 0.9724019169807434 and parameters: {'hidden_size': 187, 'n_layers': 4, 'rnn_dropout': 0.043127827885592836, 'bidirectional': True, 'fc_dropout': 0.19015021493083525, 'learning_rate_model': 0.0008527618208043499}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.316510036738203e-05
Epoch 27: reducing lr to 6.73336034620241e-05
Epoch 36: reducing lr to 2.871627878712112e-05
Epoch 39: reducing lr to 1.7800938155812366e-05
Epoch 42: reducing lr to 9.077933623164487e-06
Epoch 45: reducing lr to 3.0953763472412476e-06
Epoch 48: reducing lr to 2.2916485453968252e-07
[I 2024-06-22 04:28:11,876] Trial 660 finished with value: 0.9699820280075073 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.028247091355759826, 'bidirectional': True, 'fc_dropout': 0.165878561466365, 'learning_rate_model': 0.0010538322655514597}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.818220547658901e-05
Epoch 31: reducing lr to 4.0888178970555314e-05
Epoch 37: reducing lr to 2.039289045249039e-05
Epoch 40: reducing lr to 1.1985597413956143e-05
Epoch 43: reducing lr to 5.539565442103599e-06
Epoch 46: reducing lr to 1.459831819188484e-06
Epoch 49: reducing lr to 2.735514579340747e-09
[I 2024-06-22 04:29:19,600] Trial 661 finished with value: 0.972346842288971 and parameters: {'hidden_size': 187, 'n_layers': 4, 'rnn_dropout': 0.06095107803935591, 'bidirectional': True, 'fc_dropout': 0.17878744846466893, 'learning_rate_model': 0.0008639754867159406}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.144554020445038e-05
Epoch 27: reducing lr to 6.594138266714192e-05
Epoch 36: reducing lr to 2.812252769667766e-05
Epoch 39: reducing lr to 1.743287770761564e-05
Epoch 42: reducing lr to 8.89023405987203e-06
Epoch 45: reducing lr to 3.0313749111522724e-06
Epoch 48: reducing lr to 2.2442653578736825e-07
[I 2024-06-22 04:30:28,761] Trial 662 finished with value: 0.9725221395492554 and parameters: {'hidden_size': 192, 'n_layers': 4, 'rnn_dropout': 0.047392541848048135, 'bidirectional': True, 'fc_dropout': 0.16497379425679454, 'learning_rate_model': 0.001032042741168645}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.335246267486023e-05
Epoch 26: reducing lr to 5.447984242862706e-05
Epoch 37: reducing lr to 1.8948343225235084e-05
Epoch 40: reducing lr to 1.1136587728366703e-05
Epoch 43: reducing lr to 5.147165751719692e-06
Epoch 46: reducing lr to 1.3564234273481562e-06
Epoch 49: reducing lr to 2.5417421462481034e-09
[I 2024-06-22 04:31:39,730] Trial 663 finished with value: 0.9735677242279053 and parameters: {'hidden_size': 190, 'n_layers': 4, 'rnn_dropout': 0.03102615365044941, 'bidirectional': True, 'fc_dropout': 0.16059022265780143, 'learning_rate_model': 0.0008027750699991597}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.200821677051829e-05
Epoch 26: reducing lr to 7.052282640440202e-05
Epoch 31: reducing lr to 4.9179498095296845e-05
Epoch 37: reducing lr to 2.452816785722037e-05
Epoch 40: reducing lr to 1.4416041017995126e-05
Epoch 43: reducing lr to 6.662880445345654e-06
Epoch 46: reducing lr to 1.7558570222199085e-06
Epoch 49: reducing lr to 3.2902231752920258e-09
[I 2024-06-22 04:32:45,738] Trial 664 finished with value: 0.9691593050956726 and parameters: {'hidden_size': 186, 'n_layers': 4, 'rnn_dropout': 0.02871790071106125, 'bidirectional': True, 'fc_dropout': 0.19070568031104143, 'learning_rate_model': 0.0010391727358151088}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.578334758825759e-05
Epoch 26: reducing lr to 7.376924372448996e-05
Epoch 36: reducing lr to 2.962033970685947e-05
Epoch 39: reducing lr to 1.8361356608378964e-05
Epoch 42: reducing lr to 9.363729881151864e-06
Epoch 45: reducing lr to 3.1928266056179986e-06
Epoch 48: reducing lr to 2.3637954244205753e-07
[I 2024-06-22 04:33:48,632] Trial 665 finished with value: 0.9729560613632202 and parameters: {'hidden_size': 178, 'n_layers': 4, 'rnn_dropout': 0.06291256112270241, 'bidirectional': True, 'fc_dropout': 0.16753204624940532, 'learning_rate_model': 0.0010870095645429882}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 2.938199109916492e-05
Epoch 37: reducing lr to 1.606962822799109e-05
Epoch 40: reducing lr to 9.444668718314316e-06
Epoch 43: reducing lr to 4.3651858674286646e-06
Epoch 46: reducing lr to 1.1503496605546672e-06
Epoch 49: reducing lr to 2.155589586710395e-09
[I 2024-06-22 04:34:51,411] Trial 666 finished with value: 0.9717938899993896 and parameters: {'hidden_size': 180, 'n_layers': 4, 'rnn_dropout': 0.03966051198196517, 'bidirectional': True, 'fc_dropout': 0.1875476600658278, 'learning_rate_model': 0.0006808139778894034}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.11565246125831e-05
Epoch 36: reducing lr to 2.8022732803468575e-05
Epoch 39: reducing lr to 1.7371015837021266e-05
Epoch 42: reducing lr to 8.858686399285543e-06
Epoch 45: reducing lr to 3.02061785052109e-06
Epoch 48: reducing lr to 2.236301414371188e-07
[I 2024-06-22 04:35:57,397] Trial 667 finished with value: 0.9712620973587036 and parameters: {'hidden_size': 185, 'n_layers': 4, 'rnn_dropout': 0.030045312219029056, 'bidirectional': True, 'fc_dropout': 0.19666415833831274, 'learning_rate_model': 0.0010283804603006882}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 3.583069722849439e-05
Epoch 37: reducing lr to 1.9596561093096932e-05
Epoch 40: reducing lr to 1.1517567482993707e-05
Epoch 43: reducing lr to 5.323248946405962e-06
Epoch 46: reducing lr to 1.4028263181730771e-06
Epoch 49: reducing lr to 2.6286944805565747e-09
[I 2024-06-22 04:37:03,600] Trial 668 finished with value: 0.9705935716629028 and parameters: {'hidden_size': 186, 'n_layers': 4, 'rnn_dropout': 0.04914910528306541, 'bidirectional': True, 'fc_dropout': 0.16444858626383405, 'learning_rate_model': 0.0008302377952655292}. Best is trial 153 with value: 0.9688200950622559.
Epoch 31: reducing lr to 2.729117117245868e-05
Epoch 37: reducing lr to 1.3611412345873792e-05
Epoch 40: reducing lr to 7.999891383375364e-06
Epoch 43: reducing lr to 3.6974312015792057e-06
Epoch 46: reducing lr to 9.743774622285036e-07
Epoch 49: reducing lr to 1.8258430311396922e-09
[I 2024-06-22 04:38:12,657] Trial 669 finished with value: 0.9740646481513977 and parameters: {'hidden_size': 187, 'n_layers': 4, 'rnn_dropout': 0.06629500141525911, 'bidirectional': True, 'fc_dropout': 0.16880048495407135, 'learning_rate_model': 0.000576667963465745}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.187913699741266e-05
Epoch 26: reducing lr to 5.321285852043487e-05
Epoch 37: reducing lr to 1.8507680314274452e-05
Epoch 40: reducing lr to 1.0877595102562097e-05
Epoch 43: reducing lr to 5.027463199555192e-06
Epoch 46: reducing lr to 1.324878426875811e-06
Epoch 49: reducing lr to 2.482631358575061e-09
[I 2024-06-22 04:39:15,470] Trial 670 finished with value: 0.9740135669708252 and parameters: {'hidden_size': 179, 'n_layers': 4, 'rnn_dropout': 0.05582520492262477, 'bidirectional': True, 'fc_dropout': 0.12574364216402745, 'learning_rate_model': 0.0007841057227645513}. Best is trial 153 with value: 0.9688200950622559.
Epoch 38: reducing lr to 1.3143973358379778e-05
Epoch 41: reducing lr to 7.2477322080438535e-06
Epoch 44: reducing lr to 2.9461800216338336e-06
Epoch 47: reducing lr to 5.095941303294742e-07
[I 2024-06-22 04:40:26,135] Trial 671 finished with value: 0.9716928005218506 and parameters: {'hidden_size': 189, 'n_layers': 4, 'rnn_dropout': 0.04734099781608772, 'bidirectional': True, 'fc_dropout': 0.17399180090560745, 'learning_rate_model': 0.0006525372915195034}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 3.581756358736123e-05
Epoch 37: reducing lr to 1.958937802883225e-05
Epoch 40: reducing lr to 1.1513345751078079e-05
Epoch 43: reducing lr to 5.3212977245003745e-06
Epoch 46: reducing lr to 1.4023121161379376e-06
Epoch 49: reducing lr to 2.6277309400028067e-09
[I 2024-06-22 04:41:32,234] Trial 672 finished with value: 0.9706289768218994 and parameters: {'hidden_size': 186, 'n_layers': 4, 'rnn_dropout': 0.04498934278832495, 'bidirectional': True, 'fc_dropout': 0.1605369606095201, 'learning_rate_model': 0.0008299334739404746}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.5124079237365e-05
Epoch 37: reducing lr to 1.9478223158429417e-05
Epoch 40: reducing lr to 1.144801624173989e-05
Epoch 43: reducing lr to 5.291103393773228e-06
Epoch 46: reducing lr to 1.3943550579146387e-06
Epoch 49: reducing lr to 2.6128205588915946e-09
[I 2024-06-22 04:42:34,890] Trial 673 finished with value: 0.9709715843200684 and parameters: {'hidden_size': 180, 'n_layers': 4, 'rnn_dropout': 0.062409254888982066, 'bidirectional': True, 'fc_dropout': 0.1496958986537791, 'learning_rate_model': 0.0008252242306146758}. Best is trial 153 with value: 0.9688200950622559.
Epoch 31: reducing lr to 3.022018941599008e-05
Epoch 37: reducing lr to 1.5072253833011085e-05
Epoch 40: reducing lr to 8.858477761369387e-06
Epoch 43: reducing lr to 4.094257097221122e-06
Epoch 46: reducing lr to 1.0789522840607487e-06
Epoch 49: reducing lr to 2.021801186039875e-09
[I 2024-06-22 04:43:37,508] Trial 674 finished with value: 0.9722040891647339 and parameters: {'hidden_size': 180, 'n_layers': 4, 'rnn_dropout': 0.09448740305780902, 'bidirectional': True, 'fc_dropout': 0.15784748196964082, 'learning_rate_model': 0.0006385587110184121}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.33711122602583e-05
Epoch 26: reducing lr to 5.4495880107841836e-05
Epoch 37: reducing lr to 1.8953921204846457e-05
Epoch 40: reducing lr to 1.1139866097274775e-05
Epoch 43: reducing lr to 5.14868096522824e-06
Epoch 46: reducing lr to 1.356822728866623e-06
Epoch 49: reducing lr to 2.5424903797870333e-09
[I 2024-06-22 04:44:40,109] Trial 675 finished with value: 0.9739420413970947 and parameters: {'hidden_size': 179, 'n_layers': 4, 'rnn_dropout': 0.06761648505921505, 'bidirectional': True, 'fc_dropout': 0.13020918078633803, 'learning_rate_model': 0.0008030113894979016}. Best is trial 153 with value: 0.9688200950622559.
Epoch 37: reducing lr to 1.2725694581988321e-05
Epoch 40: reducing lr to 7.479324837644504e-06
Epoch 43: reducing lr to 3.456833061374017e-06
Epoch 46: reducing lr to 9.109730626632312e-07
Epoch 49: reducing lr to 1.707032318059784e-09
[I 2024-06-22 04:45:46,070] Trial 676 finished with value: 0.9757236838340759 and parameters: {'hidden_size': 185, 'n_layers': 4, 'rnn_dropout': 0.06796034788046394, 'bidirectional': True, 'fc_dropout': 0.1575758652568739, 'learning_rate_model': 0.0005391431977671949}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 3.578921855483397e-05
Epoch 37: reducing lr to 1.9573875535033034e-05
Epoch 40: reducing lr to 1.150423440661112e-05
Epoch 43: reducing lr to 5.3170865961605e-06
Epoch 46: reducing lr to 1.4012023649833599e-06
Epoch 49: reducing lr to 2.625651426163035e-09
[I 2024-06-22 04:46:52,202] Trial 677 finished with value: 0.970813512802124 and parameters: {'hidden_size': 186, 'n_layers': 4, 'rnn_dropout': 0.0877026909436546, 'bidirectional': True, 'fc_dropout': 0.14269107795646796, 'learning_rate_model': 0.0008292766874659585}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 2.9019925514099693e-05
Epoch 37: reducing lr to 1.587160695276466e-05
Epoch 40: reducing lr to 9.328284859449803e-06
Epoch 43: reducing lr to 4.311394973214764e-06
Epoch 46: reducing lr to 1.1361742419633137e-06
Epoch 49: reducing lr to 2.129026893855652e-09
[I 2024-06-22 04:47:50,622] Trial 678 finished with value: 0.9743980169296265 and parameters: {'hidden_size': 172, 'n_layers': 4, 'rnn_dropout': 0.08450787985433936, 'bidirectional': True, 'fc_dropout': 0.15183433005313093, 'learning_rate_model': 0.0006724245086259633}. Best is trial 153 with value: 0.9688200950622559.
Epoch 37: reducing lr to 1.1744567902131257e-05
Epoch 42: reducing lr to 4.286227209830917e-06
Epoch 45: reducing lr to 1.4615095103094176e-06
Epoch 48: reducing lr to 1.0820222705291852e-07
[I 2024-06-22 04:48:56,688] Trial 679 finished with value: 0.9737140536308289 and parameters: {'hidden_size': 186, 'n_layers': 4, 'rnn_dropout': 0.06083289339108944, 'bidirectional': True, 'fc_dropout': 0.16911537445345018, 'learning_rate_model': 0.0004975762897933433}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.755973127872957e-05
Epoch 37: reducing lr to 2.0206712137522162e-05
Epoch 40: reducing lr to 1.1876174066853063e-05
Epoch 43: reducing lr to 5.488991593238476e-06
Epoch 46: reducing lr to 1.44650418283076e-06
Epoch 49: reducing lr to 2.7105405082873854e-09
[I 2024-06-22 04:49:58,946] Trial 680 finished with value: 0.9730625152587891 and parameters: {'hidden_size': 175, 'n_layers': 4, 'rnn_dropout': 0.08323162240343276, 'bidirectional': True, 'fc_dropout': 0.13624673606146204, 'learning_rate_model': 0.0008560877622824976}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.350054124481486e-05
Epoch 26: reducing lr to 5.460718234277669e-05
Epoch 37: reducing lr to 1.8992632641136754e-05
Epoch 40: reducing lr to 1.1162618129007224e-05
Epoch 43: reducing lr to 5.159196617003431e-06
Epoch 46: reducing lr to 1.3595938998585302e-06
Epoch 49: reducing lr to 2.5476831551091843e-09
[I 2024-06-22 04:51:01,592] Trial 681 finished with value: 0.9737509489059448 and parameters: {'hidden_size': 179, 'n_layers': 4, 'rnn_dropout': 0.05093276394719616, 'bidirectional': True, 'fc_dropout': 0.12568093666110292, 'learning_rate_model': 0.0008046514577407191}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 2.9600969951776176e-05
Epoch 37: reducing lr to 1.6189392363082502e-05
Epoch 40: reducing lr to 9.515058186211493e-06
Epoch 43: reducing lr to 4.3977188359895124e-06
Epoch 46: reducing lr to 1.1589230158429316e-06
Epoch 49: reducing lr to 2.1716548197409957e-09
[I 2024-06-22 04:52:07,524] Trial 682 finished with value: 0.9716769456863403 and parameters: {'hidden_size': 186, 'n_layers': 4, 'rnn_dropout': 0.06884304513680055, 'bidirectional': True, 'fc_dropout': 0.1531143904961954, 'learning_rate_model': 0.0006858879656670447}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.681089960142573e-05
Epoch 37: reducing lr to 1.9982741055098683e-05
Epoch 40: reducing lr to 1.1744538621031892e-05
Epoch 43: reducing lr to 5.428151641633091e-06
Epoch 46: reducing lr to 1.4304711386940597e-06
Epoch 49: reducing lr to 2.6804968927102676e-09
[I 2024-06-22 04:53:10,319] Trial 683 finished with value: 0.9708337783813477 and parameters: {'hidden_size': 180, 'n_layers': 4, 'rnn_dropout': 0.04715364739982992, 'bidirectional': True, 'fc_dropout': 0.13712166248839294, 'learning_rate_model': 0.0008465988903936433}. Best is trial 153 with value: 0.9688200950622559.
Epoch 31: reducing lr to 2.8437573826049918e-05
Epoch 37: reducing lr to 1.4183178179367275e-05
Epoch 40: reducing lr to 8.335937669274532e-06
Epoch 43: reducing lr to 3.852746813143694e-06
Epoch 46: reducing lr to 1.0153075088446578e-06
Epoch 49: reducing lr to 1.9025400436177043e-09
[I 2024-06-22 04:54:13,219] Trial 684 finished with value: 0.9751356840133667 and parameters: {'hidden_size': 178, 'n_layers': 4, 'rnn_dropout': 0.09670571648169954, 'bidirectional': True, 'fc_dropout': 0.13490342271253045, 'learning_rate_model': 0.0006008916832680428}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.700356667880633e-05
Epoch 26: reducing lr to 5.7619603101331664e-05
Epoch 37: reducing lr to 2.0040366627274893e-05
Epoch 40: reducing lr to 1.1778407135672425e-05
Epoch 43: reducing lr to 5.443805167010111e-06
Epoch 46: reducing lr to 1.4345962843695968e-06
Epoch 49: reducing lr to 2.6882268215925806e-09
[I 2024-06-22 04:55:15,878] Trial 685 finished with value: 0.9732528924942017 and parameters: {'hidden_size': 179, 'n_layers': 4, 'rnn_dropout': 0.049288864557019196, 'bidirectional': True, 'fc_dropout': 0.14103928955694003, 'learning_rate_model': 0.0008490402844610616}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.426485710926131e-05
Epoch 37: reducing lr to 1.9221234951458165e-05
Epoch 40: reducing lr to 1.1296975505456432e-05
Epoch 43: reducing lr to 5.221294604593359e-06
Epoch 46: reducing lr to 1.3759584719786384e-06
Epoch 49: reducing lr to 2.5783480063772297e-09
[I 2024-06-22 04:56:20,576] Trial 686 finished with value: 0.9721212387084961 and parameters: {'hidden_size': 181, 'n_layers': 4, 'rnn_dropout': 0.06690432131700041, 'bidirectional': True, 'fc_dropout': 0.15147775847232225, 'learning_rate_model': 0.000814336538567513}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 2.9222754651836903e-05
Epoch 37: reducing lr to 1.598253846949681e-05
Epoch 40: reducing lr to 9.393483096216122e-06
Epoch 43: reducing lr to 4.341528631705263e-06
Epoch 46: reducing lr to 1.144115311340092e-06
Epoch 49: reducing lr to 2.1439073141673972e-09
[I 2024-06-22 04:57:26,351] Trial 687 finished with value: 0.9736570119857788 and parameters: {'hidden_size': 185, 'n_layers': 4, 'rnn_dropout': 0.044831202277411566, 'bidirectional': True, 'fc_dropout': 0.12009680216016955, 'learning_rate_model': 0.0006771242892374508}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.06429725708987e-05
Epoch 26: reducing lr to 6.0749304002662044e-05
Epoch 32: reducing lr to 3.863243655592082e-05
Epoch 37: reducing lr to 2.112889119392404e-05
Epoch 40: reducing lr to 1.241817015805774e-05
Epoch 43: reducing lr to 5.739494151675551e-06
Epoch 46: reducing lr to 1.5125186761004178e-06
Epoch 49: reducing lr to 2.8342421610604712e-09
[I 2024-06-22 04:58:28,599] Trial 688 finished with value: 0.9740062355995178 and parameters: {'hidden_size': 174, 'n_layers': 4, 'rnn_dropout': 0.08537273206276949, 'bidirectional': True, 'fc_dropout': 0.17721397677223327, 'learning_rate_model': 0.0008951572654973674}. Best is trial 153 with value: 0.9688200950622559.
Epoch 37: reducing lr to 1.0837204362912187e-05
Epoch 42: reducing lr to 3.955081243166319e-06
Epoch 45: reducing lr to 1.3485959954889977e-06
Epoch 48: reducing lr to 9.984272362050114e-08
[I 2024-06-22 04:59:34,681] Trial 689 finished with value: 0.9750988483428955 and parameters: {'hidden_size': 186, 'n_layers': 4, 'rnn_dropout': 0.0638519217899194, 'bidirectional': True, 'fc_dropout': 0.1632756528637278, 'learning_rate_model': 0.00045913446825502583}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.476115124567574e-05
Epoch 26: reducing lr to 6.429072474341906e-05
Epoch 36: reducing lr to 2.5814458855134946e-05
Epoch 39: reducing lr to 1.6002128584017986e-05
Epoch 42: reducing lr to 8.160595798015626e-06
Epoch 45: reducing lr to 2.782584259937394e-06
Epoch 48: reducing lr to 2.0600742709082344e-07
[I 2024-06-22 05:00:39,517] Trial 690 finished with value: 0.971295952796936 and parameters: {'hidden_size': 181, 'n_layers': 4, 'rnn_dropout': 0.046859884935012336, 'bidirectional': True, 'fc_dropout': 0.1870974439939967, 'learning_rate_model': 0.0009473410486421555}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 3.170606074786809e-05
Epoch 37: reducing lr to 1.734071074600598e-05
Epoch 40: reducing lr to 1.0191727276606318e-05
Epoch 43: reducing lr to 4.710465258168364e-06
Epoch 46: reducing lr to 1.2413405237152935e-06
Epoch 49: reducing lr to 2.3260933594646676e-09
[I 2024-06-22 05:01:45,516] Trial 691 finished with value: 0.9712145328521729 and parameters: {'hidden_size': 186, 'n_layers': 4, 'rnn_dropout': 0.048320216163780474, 'bidirectional': True, 'fc_dropout': 0.14427904627635277, 'learning_rate_model': 0.0007346653012080128}. Best is trial 153 with value: 0.9688200950622559.
Epoch 37: reducing lr to 2.186173148823738e-05
Epoch 40: reducing lr to 1.2848885399569377e-05
Epoch 43: reducing lr to 5.938564351087297e-06
Epoch 46: reducing lr to 1.5649792913581973e-06
Epoch 49: reducing lr to 2.932545798491233e-09
[I 2024-06-22 05:02:48,131] Trial 692 finished with value: 0.9704854488372803 and parameters: {'hidden_size': 180, 'n_layers': 4, 'rnn_dropout': 0.07390751905313872, 'bidirectional': True, 'fc_dropout': 0.1727358716284835, 'learning_rate_model': 0.0009262051471814028}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 2.4902741847442484e-05
Epoch 37: reducing lr to 1.361983270621202e-05
Epoch 40: reducing lr to 8.004840316403253e-06
Epoch 43: reducing lr to 3.699718525058352e-06
Epoch 46: reducing lr to 9.74980236512972e-07
Epoch 49: reducing lr to 1.8269725433352721e-09
[I 2024-06-22 05:03:50,730] Trial 693 finished with value: 0.9751501083374023 and parameters: {'hidden_size': 179, 'n_layers': 4, 'rnn_dropout': 0.0762641059080781, 'bidirectional': True, 'fc_dropout': 0.17178038385190875, 'learning_rate_model': 0.0005770247047005637}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.305711726946649e-05
Epoch 37: reducing lr to 1.8860007178377385e-05
Epoch 40: reducing lr to 1.1084669620080712e-05
Epoch 43: reducing lr to 5.123169971739167e-06
Epoch 46: reducing lr to 1.350099862168204e-06
Epoch 49: reducing lr to 2.5298926958450088e-09
[I 2024-06-22 05:04:55,582] Trial 694 finished with value: 0.9724657535552979 and parameters: {'hidden_size': 181, 'n_layers': 4, 'rnn_dropout': 0.09803888933711145, 'bidirectional': True, 'fc_dropout': 0.12603070378066616, 'learning_rate_model': 0.0007990325804655527}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.195556125812243e-05
Epoch 32: reducing lr to 3.935025033608555e-05
Epoch 38: reducing lr to 1.8366063207517115e-05
Epoch 41: reducing lr to 1.012725027772723e-05
Epoch 44: reducing lr to 4.116694931031192e-06
Epoch 47: reducing lr to 7.120554609039899e-07
[I 2024-06-22 05:05:54,600] Trial 695 finished with value: 0.9702425003051758 and parameters: {'hidden_size': 176, 'n_layers': 4, 'rnn_dropout': 0.07756755365482589, 'bidirectional': True, 'fc_dropout': 0.15112835431217225, 'learning_rate_model': 0.0009117898229509591}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 2.8455910497075732e-05
Epoch 37: reducing lr to 1.556313529037835e-05
Epoch 40: reducing lr to 9.14698554008212e-06
Epoch 43: reducing lr to 4.227601115507117e-06
Epoch 46: reducing lr to 1.1140921958149776e-06
Epoch 49: reducing lr to 2.08764831969915e-09
[I 2024-06-22 05:06:55,378] Trial 696 finished with value: 0.974632740020752 and parameters: {'hidden_size': 174, 'n_layers': 4, 'rnn_dropout': 0.10521038764316165, 'bidirectional': True, 'fc_dropout': 0.14649808867627945, 'learning_rate_model': 0.0006593556425292627}. Best is trial 153 with value: 0.9688200950622559.
Epoch 38: reducing lr to 1.889509460390722e-05
Epoch 41: reducing lr to 1.0418964037801035e-05
Epoch 44: reducing lr to 4.235275643907322e-06
Epoch 47: reducing lr to 7.32566100039491e-07
[I 2024-06-22 05:07:54,318] Trial 697 finished with value: 0.9698590040206909 and parameters: {'hidden_size': 176, 'n_layers': 4, 'rnn_dropout': 0.05930332886809708, 'bidirectional': True, 'fc_dropout': 0.15714514054445378, 'learning_rate_model': 0.0009380537771691171}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.442171006674836e-05
Epoch 26: reducing lr to 6.399882287945114e-05
Epoch 31: reducing lr to 4.462994676153174e-05
Epoch 37: reducing lr to 2.2259089011124885e-05
Epoch 40: reducing lr to 1.308242597145799e-05
Epoch 43: reducing lr to 6.046503341250334e-06
Epoch 46: reducing lr to 1.5934242612782945e-06
Epoch 49: reducing lr to 2.9858475753831956e-09
[I 2024-06-22 05:08:55,137] Trial 698 finished with value: 0.9734466075897217 and parameters: {'hidden_size': 174, 'n_layers': 4, 'rnn_dropout': 0.07055015588134973, 'bidirectional': True, 'fc_dropout': 0.16276702723665593, 'learning_rate_model': 0.0009430397964939552}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.53025426801423e-05
Epoch 26: reducing lr to 6.475629338584615e-05
Epoch 31: reducing lr to 4.515817317028174e-05
Epoch 37: reducing lr to 2.2522540785181842e-05
Epoch 40: reducing lr to 1.3237265566619631e-05
Epoch 43: reducing lr to 6.118067906686847e-06
Epoch 46: reducing lr to 1.6122835438055033e-06
Epoch 49: reducing lr to 3.021187154660446e-09
[I 2024-06-22 05:09:56,010] Trial 699 finished with value: 0.973515510559082 and parameters: {'hidden_size': 174, 'n_layers': 4, 'rnn_dropout': 0.07915585444855464, 'bidirectional': True, 'fc_dropout': 0.17620022076675929, 'learning_rate_model': 0.0009542013272856455}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.136431756668225e-05
Epoch 26: reducing lr to 5.277014042738561e-05
Epoch 32: reducing lr to 3.3558229770314306e-05
Epoch 37: reducing lr to 1.835370089720641e-05
Epoch 40: reducing lr to 1.0787096146207048e-05
Epoch 43: reducing lr to 4.985635923545804e-06
Epoch 46: reducing lr to 1.3138557592917363e-06
Epoch 49: reducing lr to 2.4619764670409487e-09
[I 2024-06-22 05:10:54,372] Trial 700 finished with value: 0.9743602871894836 and parameters: {'hidden_size': 172, 'n_layers': 4, 'rnn_dropout': 0.07969776963213962, 'bidirectional': True, 'fc_dropout': 0.15681670964325806, 'learning_rate_model': 0.0007775821530864064}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.120088497071996e-05
Epoch 27: reducing lr to 6.574330055793815e-05
Epoch 38: reducing lr to 2.0725855789363676e-05
Epoch 41: reducing lr to 1.1428465993357716e-05
Epoch 44: reducing lr to 4.645634968436507e-06
Epoch 47: reducing lr to 8.035450292191325e-07
[I 2024-06-22 05:12:01,604] Trial 701 finished with value: 0.9720395803451538 and parameters: {'hidden_size': 185, 'n_layers': 4, 'rnn_dropout': 0.09111141493845532, 'bidirectional': True, 'fc_dropout': 0.13344674610991275, 'learning_rate_model': 0.0010289425756172033}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 2.9109932998980936e-05
Epoch 37: reducing lr to 1.592083393724289e-05
Epoch 40: reducing lr to 9.35721723758588e-06
Epoch 43: reducing lr to 4.324767089476049e-06
Epoch 46: reducing lr to 1.1396981719560352e-06
Epoch 49: reducing lr to 2.135630231133671e-09
[I 2024-06-22 05:13:04,028] Trial 702 finished with value: 0.973984956741333 and parameters: {'hidden_size': 177, 'n_layers': 4, 'rnn_dropout': 0.05743887807536145, 'bidirectional': True, 'fc_dropout': 0.16973746791072403, 'learning_rate_model': 0.0006745100838892261}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 3.879333009240988e-05
Epoch 37: reducing lr to 2.121688724929482e-05
Epoch 40: reducing lr to 1.2469888441748198e-05
Epoch 43: reducing lr to 5.763397575690352e-06
Epoch 46: reducing lr to 1.518817902877148e-06
Epoch 49: reducing lr to 2.846046004791915e-09
[I 2024-06-22 05:14:06,434] Trial 703 finished with value: 0.9727066159248352 and parameters: {'hidden_size': 177, 'n_layers': 4, 'rnn_dropout': 0.09865004938439012, 'bidirectional': True, 'fc_dropout': 0.18528034421482498, 'learning_rate_model': 0.0008988853507800877}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.364201782296351e-05
Epoch 37: reducing lr to 2.202588771757593e-05
Epoch 40: reducing lr to 1.2945365615673547e-05
Epoch 43: reducing lr to 5.983156076682467e-06
Epoch 46: reducing lr to 1.5767304694202024e-06
Epoch 49: reducing lr to 2.9545658137403695e-09
[I 2024-06-22 05:15:13,604] Trial 704 finished with value: 0.972075879573822 and parameters: {'hidden_size': 185, 'n_layers': 4, 'rnn_dropout': 0.05534291210566269, 'bidirectional': True, 'fc_dropout': 0.13628410731753657, 'learning_rate_model': 0.0009331598728231967}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 5.7849341197377393e-05
Epoch 37: reducing lr to 1.7302392457690777e-05
Epoch 40: reducing lr to 1.0169206311350901e-05
Epoch 43: reducing lr to 4.700056401893372e-06
Epoch 46: reducing lr to 1.2385974963514392e-06
Epoch 49: reducing lr to 2.3209533212323852e-09
[I 2024-06-22 05:16:18,313] Trial 705 finished with value: 0.9720619916915894 and parameters: {'hidden_size': 181, 'n_layers': 4, 'rnn_dropout': 0.04297910857419691, 'bidirectional': True, 'fc_dropout': 0.11687671352273749, 'learning_rate_model': 0.000733041889270682}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 2.3793918434556324e-05
Epoch 37: reducing lr to 1.3013393886070955e-05
Epoch 40: reducing lr to 7.648415533396697e-06
Epoch 43: reducing lr to 3.534984274235502e-06
Epoch 46: reducing lr to 9.315681126605178e-07
Epoch 49: reducing lr to 1.745624475600182e-09
[I 2024-06-22 05:17:25,596] Trial 706 finished with value: 0.9753642082214355 and parameters: {'hidden_size': 185, 'n_layers': 4, 'rnn_dropout': 0.08282763713581377, 'bidirectional': True, 'fc_dropout': 0.1884614289644769, 'learning_rate_model': 0.0005513320116507252}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.10185749965027e-05
Epoch 27: reducing lr to 7.369207286974791e-05
Epoch 33: reducing lr to 4.502287816493349e-05
Epoch 36: reducing lr to 3.142802404927033e-05
Epoch 39: reducing lr to 1.9481922313393565e-05
Epoch 42: reducing lr to 9.935184093366853e-06
Epoch 45: reducing lr to 3.3876799638214543e-06
Epoch 48: reducing lr to 2.5080542688390126e-07
[I 2024-06-22 05:18:30,259] Trial 707 finished with value: 0.9715774059295654 and parameters: {'hidden_size': 181, 'n_layers': 4, 'rnn_dropout': 0.06587169737276818, 'bidirectional': True, 'fc_dropout': 0.16123832232725024, 'learning_rate_model': 0.0011533481072241905}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.944596857405216e-05
Epoch 27: reducing lr to 6.432245402206868e-05
Epoch 36: reducing lr to 2.743209076893177e-05
Epoch 41: reducing lr to 1.1181473582280722e-05
Epoch 44: reducing lr to 4.545233341262344e-06
Epoch 47: reducing lr to 7.861787856400628e-07
[I 2024-06-22 05:19:40,773] Trial 708 finished with value: 0.9703981876373291 and parameters: {'hidden_size': 189, 'n_layers': 4, 'rnn_dropout': 0.04486600821401339, 'bidirectional': True, 'fc_dropout': 0.17978236312657214, 'learning_rate_model': 0.0010067050322969387}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 3.493456899715375e-05
Epoch 37: reducing lr to 1.9106449736325725e-05
Epoch 40: reducing lr to 1.1229512318672937e-05
Epoch 43: reducing lr to 5.190114119782072e-06
Epoch 46: reducing lr to 1.3677415343251354e-06
Epoch 49: reducing lr to 2.5629506486503013e-09
[I 2024-06-22 05:20:48,123] Trial 709 finished with value: 0.9706494808197021 and parameters: {'hidden_size': 186, 'n_layers': 4, 'rnn_dropout': 0.058444409093127106, 'bidirectional': True, 'fc_dropout': 0.17691467633010935, 'learning_rate_model': 0.0008094734902251076}. Best is trial 153 with value: 0.9688200950622559.
Epoch 31: reducing lr to 2.7320083064046753e-05
Epoch 37: reducing lr to 1.3625832089006749e-05
Epoch 40: reducing lr to 8.008366358338183e-06
Epoch 43: reducing lr to 3.7013482093682377e-06
Epoch 46: reducing lr to 9.754097043179212e-07
Epoch 49: reducing lr to 1.8277773041505512e-09
[I 2024-06-22 05:21:55,268] Trial 710 finished with value: 0.9747342467308044 and parameters: {'hidden_size': 185, 'n_layers': 4, 'rnn_dropout': 0.08107004149292177, 'bidirectional': True, 'fc_dropout': 0.17806059091127607, 'learning_rate_model': 0.0005772788775792024}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 3.101934705056614e-05
Epoch 37: reducing lr to 1.6965132597558954e-05
Epoch 40: reducing lr to 9.970987186070724e-06
Epoch 43: reducing lr to 4.608442460723658e-06
Epoch 46: reducing lr to 1.2144546375299996e-06
Epoch 49: reducing lr to 2.275713080945787e-09
[I 2024-06-22 05:22:57,993] Trial 711 finished with value: 0.9717388153076172 and parameters: {'hidden_size': 180, 'n_layers': 4, 'rnn_dropout': 0.06411737772532795, 'bidirectional': True, 'fc_dropout': 0.15275061434750195, 'learning_rate_model': 0.0007187533678624006}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.565085214493602e-05
Epoch 32: reducing lr to 3.590240172560612e-05
Epoch 37: reducing lr to 1.9635777789030563e-05
Epoch 40: reducing lr to 1.1540616473055263e-05
Epoch 43: reducing lr to 5.333901847918542e-06
Epoch 46: reducing lr to 1.4056336583439319e-06
Epoch 49: reducing lr to 2.6339550317092466e-09
[I 2024-06-22 05:23:56,933] Trial 712 finished with value: 0.9708940386772156 and parameters: {'hidden_size': 176, 'n_layers': 4, 'rnn_dropout': 0.1014274721299116, 'bidirectional': True, 'fc_dropout': 0.17188738665831702, 'learning_rate_model': 0.0008318992695933391}. Best is trial 153 with value: 0.9688200950622559.
Epoch 38: reducing lr to 9.755416295743017e-06
Epoch 42: reducing lr to 4.171958543908915e-06
[I 2024-06-22 05:24:54,813] Trial 713 finished with value: 0.9766574501991272 and parameters: {'hidden_size': 171, 'n_layers': 4, 'rnn_dropout': 0.10319412738037897, 'bidirectional': True, 'fc_dropout': 0.17034537961782978, 'learning_rate_model': 0.00048431115566823277}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 2.96888209785965e-05
Epoch 37: reducing lr to 1.6237439935341487e-05
Epoch 40: reducing lr to 9.54329738355117e-06
Epoch 43: reducing lr to 4.410770574362895e-06
Epoch 46: reducing lr to 1.162362517221217e-06
Epoch 49: reducing lr to 2.178099949966955e-09
[I 2024-06-22 05:25:58,584] Trial 714 finished with value: 0.9732713103294373 and parameters: {'hidden_size': 175, 'n_layers': 4, 'rnn_dropout': 0.09240276179219256, 'bidirectional': True, 'fc_dropout': 0.15755880248122575, 'learning_rate_model': 0.000687923573357121}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.324825090164954e-05
Epoch 28: reducing lr to 4.794800025636842e-05
Epoch 37: reducing lr to 1.891717410625322e-05
Epoch 40: reducing lr to 1.1118268573819455e-05
Epoch 43: reducing lr to 5.138698910063533e-06
Epoch 46: reducing lr to 1.354192175639544e-06
Epoch 49: reducing lr to 2.5375611019027264e-09
[I 2024-06-22 05:27:01,221] Trial 715 finished with value: 0.974575936794281 and parameters: {'hidden_size': 178, 'n_layers': 4, 'rnn_dropout': 0.10850539691029121, 'bidirectional': True, 'fc_dropout': 0.14200283819579573, 'learning_rate_model': 0.0008014545433771196}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 2.666100271510655e-05
Epoch 37: reducing lr to 1.458146251461495e-05
Epoch 40: reducing lr to 8.570022960405037e-06
Epoch 43: reducing lr to 3.96093756446505e-06
Epoch 46: reducing lr to 1.0438188249346216e-06
Epoch 49: reducing lr to 1.955966143674277e-09
[I 2024-06-22 05:28:03,603] Trial 716 finished with value: 0.9736512303352356 and parameters: {'hidden_size': 177, 'n_layers': 4, 'rnn_dropout': 0.08194050615625112, 'bidirectional': True, 'fc_dropout': 0.1755734556039204, 'learning_rate_model': 0.0006177655983806952}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.697227689839115e-05
Epoch 26: reducing lr to 6.619217848434259e-05
Epoch 36: reducing lr to 2.657794378326273e-05
Epoch 39: reducing lr to 1.6475405365081722e-05
Epoch 42: reducing lr to 8.401952470696306e-06
Epoch 45: reducing lr to 2.864881593986888e-06
Epoch 48: reducing lr to 2.1210027476774133e-07
[I 2024-06-22 05:29:08,491] Trial 717 finished with value: 0.9715718030929565 and parameters: {'hidden_size': 181, 'n_layers': 4, 'rnn_dropout': 0.0649772622387166, 'bidirectional': True, 'fc_dropout': 0.18530795570107284, 'learning_rate_model': 0.0009753594787976722}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.060881757482722e-05
Epoch 26: reducing lr to 5.21204494955219e-05
Epoch 32: reducing lr to 3.314507040794397e-05
Epoch 37: reducing lr to 1.8127735361725937e-05
Epoch 40: reducing lr to 1.0654288492284349e-05
Epoch 43: reducing lr to 4.92425419473349e-06
Epoch 46: reducing lr to 1.2976799415721926e-06
Epoch 49: reducing lr to 2.431665314342902e-09
[I 2024-06-22 05:30:06,821] Trial 718 finished with value: 0.9743090867996216 and parameters: {'hidden_size': 172, 'n_layers': 4, 'rnn_dropout': 0.07614979753292812, 'bidirectional': True, 'fc_dropout': 0.14285126857971095, 'learning_rate_model': 0.0007680087831929821}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.352956076224749e-05
Epoch 38: reducing lr to 1.8767813591725242e-05
Epoch 41: reducing lr to 1.034877988067358e-05
Epoch 44: reducing lr to 4.206746007928957e-06
Epoch 47: reducing lr to 7.276313930873554e-07
[I 2024-06-22 05:31:05,863] Trial 719 finished with value: 0.9699200987815857 and parameters: {'hidden_size': 176, 'n_layers': 4, 'rnn_dropout': 0.05511822992963338, 'bidirectional': True, 'fc_dropout': 0.17562150797354215, 'learning_rate_model': 0.0009317348654757867}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.77612094540531e-05
Epoch 26: reducing lr to 5.827113671612533e-05
Epoch 32: reducing lr to 3.705649026248028e-05
Epoch 37: reducing lr to 2.026697305647046e-05
Epoch 40: reducing lr to 1.1911591464695316e-05
Epoch 43: reducing lr to 5.505360989469638e-06
Epoch 46: reducing lr to 1.4508179806780674e-06
Epoch 49: reducing lr to 2.718623944172015e-09
[I 2024-06-22 05:32:07,926] Trial 720 finished with value: 0.9738860130310059 and parameters: {'hidden_size': 173, 'n_layers': 4, 'rnn_dropout': 0.08891586203226477, 'bidirectional': True, 'fc_dropout': 0.1636976270728484, 'learning_rate_model': 0.0008586408067810004}. Best is trial 153 with value: 0.9688200950622559.
Epoch 38: reducing lr to 1.2008483514627224e-05
Epoch 41: reducing lr to 6.6216105560834616e-06
Epoch 44: reducing lr to 2.6916635674979e-06
Epoch 47: reducing lr to 4.655709918425046e-07
[I 2024-06-22 05:33:06,908] Trial 721 finished with value: 0.9716867208480835 and parameters: {'hidden_size': 176, 'n_layers': 4, 'rnn_dropout': 0.06464104953118963, 'bidirectional': True, 'fc_dropout': 0.12614765174957, 'learning_rate_model': 0.0005961654892503049}. Best is trial 153 with value: 0.9688200950622559.
Epoch 27: reducing lr to 6.145226259870256e-05
Epoch 38: reducing lr to 1.9373087778402225e-05
Epoch 41: reducing lr to 1.0682534758126939e-05
Epoch 44: reducing lr to 4.342416300904823e-06
Epoch 47: reducing lr to 7.510979784463266e-07
[I 2024-06-22 05:34:06,031] Trial 722 finished with value: 0.9699340462684631 and parameters: {'hidden_size': 176, 'n_layers': 4, 'rnn_dropout': 0.10692416265629932, 'bidirectional': True, 'fc_dropout': 0.17031024794285426, 'learning_rate_model': 0.0009617839204785549}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.490109724624957e-05
Epoch 26: reducing lr to 6.441107106837423e-05
Epoch 36: reducing lr to 2.5862781148378967e-05
Epoch 39: reducing lr to 1.6032083097273706e-05
Epoch 42: reducing lr to 8.175871682953228e-06
Epoch 45: reducing lr to 2.7877930018033224e-06
Epoch 48: reducing lr to 2.0639305405121145e-07
[I 2024-06-22 05:35:02,717] Trial 723 finished with value: 0.9718222618103027 and parameters: {'hidden_size': 169, 'n_layers': 4, 'rnn_dropout': 0.05068535169476538, 'bidirectional': True, 'fc_dropout': 0.154659172413964, 'learning_rate_model': 0.00094911438397378}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.392911229862189e-05
Epoch 26: reducing lr to 7.217469724374141e-05
Epoch 33: reducing lr to 4.151603337733059e-05
Epoch 36: reducing lr to 2.8980086315967385e-05
Epoch 39: reducing lr to 1.7964469842520195e-05
Epoch 42: reducing lr to 9.161329778143714e-06
Epoch 45: reducing lr to 3.1238126077702805e-06
Epoch 48: reducing lr to 2.312701208390805e-07
[I 2024-06-22 05:36:07,963] Trial 724 finished with value: 0.9727792739868164 and parameters: {'hidden_size': 182, 'n_layers': 4, 'rnn_dropout': 0.06866421830651782, 'bidirectional': True, 'fc_dropout': 0.17895259151830736, 'learning_rate_model': 0.001063513495067809}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 5.61824749473324e-05
Epoch 32: reducing lr to 3.072444179467543e-05
Epoch 37: reducing lr to 1.680384271735138e-05
Epoch 40: reducing lr to 9.876191621135141e-06
Epoch 43: reducing lr to 4.56462935592419e-06
Epoch 46: reducing lr to 1.2029086480201462e-06
Epoch 49: reducing lr to 2.2540775594897913e-09
[I 2024-06-22 05:37:15,200] Trial 725 finished with value: 0.9737944006919861 and parameters: {'hidden_size': 185, 'n_layers': 4, 'rnn_dropout': 0.0559286785294978, 'bidirectional': True, 'fc_dropout': 0.1921412088613872, 'learning_rate_model': 0.0007119200794141868}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.688704888004774e-05
Epoch 27: reducing lr to 6.22506560779056e-05
Epoch 38: reducing lr to 1.9624784726573368e-05
Epoch 41: reducing lr to 1.0821323237697564e-05
Epoch 44: reducing lr to 4.398833375102206e-06
Epoch 47: reducing lr to 7.608563128489246e-07
[I 2024-06-22 05:38:17,740] Trial 726 finished with value: 0.9724999070167542 and parameters: {'hidden_size': 179, 'n_layers': 4, 'rnn_dropout': 0.07320052357476067, 'bidirectional': True, 'fc_dropout': 0.14169379001431728, 'learning_rate_model': 0.0009742795061256915}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00011649379351365467
Epoch 23: reducing lr to 9.884491209669248e-05
Epoch 27: reducing lr to 8.002857071002455e-05
Epoch 30: reducing lr to 6.45209087500713e-05
Epoch 33: reducing lr to 4.8894222247754844e-05
Epoch 36: reducing lr to 3.4130398928375075e-05
Epoch 39: reducing lr to 2.1157097862891935e-05
Epoch 42: reducing lr to 1.0789472351232002e-05
Epoch 45: reducing lr to 3.6789735309361105e-06
Epoch 48: reducing lr to 2.7237122065099615e-07
[I 2024-06-22 05:39:24,943] Trial 727 finished with value: 0.972676157951355 and parameters: {'hidden_size': 185, 'n_layers': 4, 'rnn_dropout': 0.04318215679885575, 'bidirectional': True, 'fc_dropout': 0.1637214257216633, 'learning_rate_model': 0.0012525200738403364}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.690757406651305e-05
Epoch 37: reducing lr to 2.0011655810236007e-05
Epoch 40: reducing lr to 1.1761532809041016e-05
Epoch 43: reducing lr to 5.436006103397534e-06
Epoch 46: reducing lr to 1.432541011019996e-06
Epoch 49: reducing lr to 2.684375535343033e-09
[I 2024-06-22 05:40:27,515] Trial 728 finished with value: 0.9707770943641663 and parameters: {'hidden_size': 180, 'n_layers': 4, 'rnn_dropout': 0.04993237888383131, 'bidirectional': True, 'fc_dropout': 0.11659834266439306, 'learning_rate_model': 0.0008478239074995432}. Best is trial 153 with value: 0.9688200950622559.
Epoch 27: reducing lr to 6.385764927887812e-05
Epoch 38: reducing lr to 2.013139618439777e-05
Epoch 41: reducing lr to 1.1100674395808141e-05
Epoch 44: reducing lr to 4.512388729718125e-06
Epoch 47: reducing lr to 7.804977270716969e-07
[I 2024-06-22 05:41:26,597] Trial 729 finished with value: 0.9700475931167603 and parameters: {'hidden_size': 176, 'n_layers': 4, 'rnn_dropout': 0.08915091713022513, 'bidirectional': True, 'fc_dropout': 0.11828007669515388, 'learning_rate_model': 0.0009994304144186332}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00011259266075611487
Epoch 23: reducing lr to 9.55348033529906e-05
Epoch 27: reducing lr to 7.734858176538285e-05
Epoch 36: reducing lr to 3.2987443468934414e-05
Epoch 39: reducing lr to 2.0448591040013628e-05
Epoch 42: reducing lr to 1.0428155556951217e-05
Epoch 45: reducing lr to 3.5557724253426627e-06
Epoch 48: reducing lr to 2.6325007986707867e-07
[I 2024-06-22 05:42:26,294] Trial 730 finished with value: 0.9718241691589355 and parameters: {'hidden_size': 172, 'n_layers': 4, 'rnn_dropout': 0.06205579780662421, 'bidirectional': True, 'fc_dropout': 0.19347415104874946, 'learning_rate_model': 0.0012105758041744852}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.400860250069419e-05
Epoch 26: reducing lr to 7.224305470769265e-05
Epoch 33: reducing lr to 4.155535367742697e-05
Epoch 37: reducing lr to 2.5126471282183776e-05
Epoch 40: reducing lr to 1.476768435172013e-05
Epoch 43: reducing lr to 6.825404781194018e-06
Epoch 46: reducing lr to 1.7986867711133197e-06
Epoch 49: reducing lr to 3.3704799562222706e-09
[I 2024-06-22 05:43:30,258] Trial 731 finished with value: 0.9722481369972229 and parameters: {'hidden_size': 175, 'n_layers': 4, 'rnn_dropout': 0.10527527832947564, 'bidirectional': True, 'fc_dropout': 0.11077189020500543, 'learning_rate_model': 0.0010645207606079087}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.838978688405918e-05
Epoch 37: reducing lr to 2.3445917088580834e-05
Epoch 40: reducing lr to 1.3779965320728088e-05
Epoch 43: reducing lr to 6.368895687686475e-06
Epoch 46: reducing lr to 1.6783837423980995e-06
Epoch 49: reducing lr to 3.145049406864421e-09
[I 2024-06-22 05:44:29,706] Trial 732 finished with value: 0.9718578457832336 and parameters: {'hidden_size': 171, 'n_layers': 4, 'rnn_dropout': 0.07592926513568268, 'bidirectional': True, 'fc_dropout': 0.19733827078949379, 'learning_rate_model': 0.0009933215536708992}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.918518834064977e-05
Epoch 37: reducing lr to 2.368381691385961e-05
Epoch 40: reducing lr to 1.3919787163898622e-05
Epoch 43: reducing lr to 6.433519270786031e-06
Epoch 46: reducing lr to 1.695413880206686e-06
Epoch 49: reducing lr to 3.1769614323605077e-09
[I 2024-06-22 05:45:32,490] Trial 733 finished with value: 0.9703048467636108 and parameters: {'hidden_size': 180, 'n_layers': 4, 'rnn_dropout': 0.05283728487371199, 'bidirectional': True, 'fc_dropout': 0.11244306953761518, 'learning_rate_model': 0.0010034005377077002}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 0.00010038609349204929
Epoch 27: reducing lr to 8.12763693236207e-05
Epoch 30: reducing lr to 6.552691322787236e-05
Epoch 36: reducing lr to 3.4662557182439416e-05
Epoch 39: reducing lr to 2.148697751895494e-05
Epoch 42: reducing lr to 1.095770087914216e-05
Epoch 45: reducing lr to 3.736335770829083e-06
Epoch 48: reducing lr to 2.7661800937278743e-07
[I 2024-06-22 05:46:35,259] Trial 734 finished with value: 0.9724326133728027 and parameters: {'hidden_size': 177, 'n_layers': 4, 'rnn_dropout': 0.0824849792155037, 'bidirectional': True, 'fc_dropout': 0.1797438518991368, 'learning_rate_model': 0.0012720492594520889}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00011803860368349453
Epoch 23: reducing lr to 0.00010015568257500125
Epoch 26: reducing lr to 8.612870872947e-05
Epoch 29: reducing lr to 7.06771947517761e-05
Epoch 33: reducing lr to 4.954260264207936e-05
Epoch 36: reducing lr to 3.458299803923728e-05
Epoch 39: reducing lr to 2.1437659590320467e-05
Epoch 42: reducing lr to 1.0932550245020705e-05
Epoch 45: reducing lr to 3.7277599559785338e-06
Epoch 48: reducing lr to 2.7598310261434806e-07
[I 2024-06-22 05:47:34,430] Trial 735 finished with value: 0.9702772498130798 and parameters: {'hidden_size': 176, 'n_layers': 4, 'rnn_dropout': 0.05706031852410155, 'bidirectional': True, 'fc_dropout': 0.16453794252692192, 'learning_rate_model': 0.0012691295917351272}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 0.00010022761456574729
Epoch 27: reducing lr to 8.11480588047424e-05
Epoch 36: reducing lr to 3.46078356104162e-05
Epoch 39: reducing lr to 2.1453056155863453e-05
Epoch 42: reducing lr to 1.0940402022202372e-05
Epoch 45: reducing lr to 3.730437239860625e-06
Epoch 48: reducing lr to 2.7618131417331965e-07
[I 2024-06-22 05:48:31,331] Trial 736 finished with value: 0.971663236618042 and parameters: {'hidden_size': 170, 'n_layers': 4, 'rnn_dropout': 0.09312072786297833, 'bidirectional': True, 'fc_dropout': 0.1671292475323383, 'learning_rate_model': 0.00127004108288272}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00015091626932883277
Epoch 23: reducing lr to 0.00012805236163951282
Epoch 27: reducing lr to 0.0001036760239922991
Epoch 30: reducing lr to 8.358603963845904e-05
Epoch 33: reducing lr to 6.334186046144223e-05
Epoch 36: reducing lr to 4.4215509870673164e-05
Epoch 39: reducing lr to 2.740875872428701e-05
Epoch 42: reducing lr to 1.3977628044910933e-05
Epoch 45: reducing lr to 4.76606472758843e-06
Epoch 48: reducing lr to 3.5285354913238473e-07
[I 2024-06-22 05:49:35,319] Trial 737 finished with value: 0.9755569696426392 and parameters: {'hidden_size': 175, 'n_layers': 4, 'rnn_dropout': 0.06063760762118972, 'bidirectional': True, 'fc_dropout': 0.1527436744840452, 'learning_rate_model': 0.0016226242712346843}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.593756353572873e-05
Epoch 26: reducing lr to 8.250134444239945e-05
Epoch 33: reducing lr to 4.7456085031826186e-05
Epoch 36: reducing lr to 3.3126513507216926e-05
Epoch 39: reducing lr to 2.053479918589301e-05
Epoch 42: reducing lr to 1.0472119072762522e-05
Epoch 45: reducing lr to 3.5707630204089923e-06
Epoch 48: reducing lr to 2.6435990211563353e-07
[I 2024-06-22 05:50:34,496] Trial 738 finished with value: 0.9698095321655273 and parameters: {'hidden_size': 176, 'n_layers': 4, 'rnn_dropout': 0.05802225473438846, 'bidirectional': True, 'fc_dropout': 0.12178675297744332, 'learning_rate_model': 0.0012156794074163965}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00012039441610272115
Epoch 23: reducing lr to 0.00010215458796275873
Epoch 27: reducing lr to 8.270820918059797e-05
Epoch 36: reducing lr to 3.5273204918449274e-05
Epoch 39: reducing lr to 2.186551203118335e-05
Epoch 42: reducing lr to 1.1150741894509351e-05
Epoch 45: reducing lr to 3.8021585249732854e-06
Epoch 48: reducing lr to 2.814911686228102e-07
[I 2024-06-22 05:51:34,391] Trial 739 finished with value: 0.9731084108352661 and parameters: {'hidden_size': 172, 'n_layers': 4, 'rnn_dropout': 0.11045857631100753, 'bidirectional': True, 'fc_dropout': 0.10691205726385035, 'learning_rate_model': 0.0012944588582675786}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.890575055266905e-05
Epoch 26: reducing lr to 6.785486586187308e-05
Epoch 33: reducing lr to 3.903119768445018e-05
Epoch 36: reducing lr to 2.7245557580859665e-05
Epoch 39: reducing lr to 1.6889252577357704e-05
Epoch 42: reducing lr to 8.613001882266024e-06
Epoch 45: reducing lr to 2.936844816432693e-06
Epoch 48: reducing lr to 2.1742804094347905e-07
[I 2024-06-22 05:52:38,405] Trial 740 finished with value: 0.9721358418464661 and parameters: {'hidden_size': 175, 'n_layers': 4, 'rnn_dropout': 0.07549199609844529, 'bidirectional': True, 'fc_dropout': 0.12035969061846546, 'learning_rate_model': 0.0009998596226377064}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00014250221386124688
Epoch 23: reducing lr to 0.00012091304075395221
Epoch 27: reducing lr to 9.78957604036914e-05
Epoch 30: reducing lr to 7.892585570360835e-05
Epoch 33: reducing lr to 5.9810353025478606e-05
Epoch 36: reducing lr to 4.17503564830764e-05
Epoch 39: reducing lr to 2.5880634439016405e-05
Epoch 42: reducing lr to 1.3198331430979219e-05
Epoch 45: reducing lr to 4.500341666990805e-06
Epoch 48: reducing lr to 3.3318085680082016e-07
[I 2024-06-22 05:53:37,589] Trial 741 finished with value: 0.9733226299285889 and parameters: {'hidden_size': 176, 'n_layers': 4, 'rnn_dropout': 0.06423800064330187, 'bidirectional': True, 'fc_dropout': 0.11942411822523706, 'learning_rate_model': 0.00153215787763817}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 0.00010158184337455355
Epoch 27: reducing lr to 8.224449354967987e-05
Epoch 30: reducing lr to 6.63074376617602e-05
Epoch 33: reducing lr to 5.0248061543455415e-05
Epoch 36: reducing lr to 3.507544055339638e-05
Epoch 39: reducing lr to 2.174291985070527e-05
Epoch 42: reducing lr to 1.1088223634664958e-05
Epoch 45: reducing lr to 3.780841168972882e-06
Epoch 48: reducing lr to 2.799129473537304e-07
[I 2024-06-22 05:54:40,417] Trial 742 finished with value: 0.9709570407867432 and parameters: {'hidden_size': 180, 'n_layers': 4, 'rnn_dropout': 0.06117919460679446, 'bidirectional': True, 'fc_dropout': 0.1323066702651608, 'learning_rate_model': 0.0012872012859890109}. Best is trial 153 with value: 0.9688200950622559.
Epoch 31: reducing lr to 3.36853929852144e-05
Epoch 37: reducing lr to 1.6800516586743853e-05
Epoch 40: reducing lr to 9.874236740707534e-06
Epoch 43: reducing lr to 4.563725839171033e-06
Epoch 46: reducing lr to 1.202670545858667e-06
Epoch 49: reducing lr to 2.2536313903356517e-09
[I 2024-06-22 05:55:37,394] Trial 743 finished with value: 0.9740308523178101 and parameters: {'hidden_size': 170, 'n_layers': 4, 'rnn_dropout': 0.08736017352233681, 'bidirectional': True, 'fc_dropout': 0.15848867166234684, 'learning_rate_model': 0.0007117791628865755}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.849646835796144e-05
Epoch 26: reducing lr to 6.750290433527937e-05
Epoch 36: reducing lr to 2.710423554127925e-05
Epoch 39: reducing lr to 1.6801648438072345e-05
Epoch 42: reducing lr to 8.568326452545084e-06
Epoch 45: reducing lr to 2.9216114743306855e-06
Epoch 48: reducing lr to 2.1630024702270524e-07
[I 2024-06-22 05:56:40,045] Trial 744 finished with value: 0.9720710515975952 and parameters: {'hidden_size': 177, 'n_layers': 4, 'rnn_dropout': 0.05391910717873871, 'bidirectional': True, 'fc_dropout': 0.17612132568269917, 'learning_rate_model': 0.00099467337527441}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 0.00010026091825294123
Epoch 27: reducing lr to 8.117502272659703e-05
Epoch 30: reducing lr to 6.544520522683373e-05
Epoch 33: reducing lr to 4.959465809457e-05
Epoch 36: reducing lr to 3.4619335121171016e-05
Epoch 39: reducing lr to 2.146018458922649e-05
Epoch 42: reducing lr to 1.0944037305036353e-05
Epoch 45: reducing lr to 3.731676791609629e-06
Epoch 48: reducing lr to 2.7627308385313067e-07
[I 2024-06-22 05:57:45,097] Trial 745 finished with value: 0.9718750715255737 and parameters: {'hidden_size': 181, 'n_layers': 4, 'rnn_dropout': 0.07739970170482652, 'bidirectional': True, 'fc_dropout': 0.12642343241844423, 'learning_rate_model': 0.0012704630928360749}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.225275216350455e-05
Epoch 26: reducing lr to 6.213363122290107e-05
Epoch 33: reducing lr to 3.5740252556839015e-05
Epoch 38: reducing lr to 1.8441918733587825e-05
Epoch 41: reducing lr to 1.0169077853337025e-05
Epoch 44: reducing lr to 4.133697707082735e-06
Epoch 47: reducing lr to 7.149963928265355e-07
[I 2024-06-22 05:58:50,689] Trial 746 finished with value: 0.9721940755844116 and parameters: {'hidden_size': 182, 'n_layers': 4, 'rnn_dropout': 0.045500024641142923, 'bidirectional': True, 'fc_dropout': 0.10697889406775288, 'learning_rate_model': 0.000915555697864073}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.000142138280847641
Epoch 23: reducing lr to 0.00012060424381590127
Epoch 27: reducing lr to 9.764574675030602e-05
Epoch 36: reducing lr to 4.164373124096754e-05
Epoch 39: reducing lr to 2.5814538502467705e-05
Epoch 42: reducing lr to 1.3164624526348814e-05
Epoch 45: reducing lr to 4.4888483514784075e-06
Epoch 48: reducing lr to 3.3232995413758155e-07
[I 2024-06-22 05:59:47,663] Trial 747 finished with value: 0.9752044677734375 and parameters: {'hidden_size': 170, 'n_layers': 4, 'rnn_dropout': 0.09305828171017361, 'bidirectional': True, 'fc_dropout': 0.14880440251583293, 'learning_rate_model': 0.001528244936087158}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001679958273981236
Epoch 23: reducing lr to 0.0001425443561491733
Epoch 27: reducing lr to 0.00011540928959742339
Epoch 30: reducing lr to 9.304567327594625e-05
Epoch 36: reducing lr to 4.9219485729326224e-05
Epoch 39: reducing lr to 3.0510674033488245e-05
Epoch 42: reducing lr to 1.555950991176142e-05
Epoch 45: reducing lr to 5.305451764114487e-06
Epoch 48: reducing lr to 3.9278683604151814e-07
[I 2024-06-22 06:00:50,160] Trial 748 finished with value: 0.9802061319351196 and parameters: {'hidden_size': 174, 'n_layers': 4, 'rnn_dropout': 0.0721032130546346, 'bidirectional': True, 'fc_dropout': 0.16430156061540171, 'learning_rate_model': 0.0018062605722673308}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.105162212634892e-05
Epoch 26: reducing lr to 7.829968744502837e-05
Epoch 36: reducing lr to 3.143943497271761e-05
Epoch 39: reducing lr to 1.9488995832357887e-05
Epoch 42: reducing lr to 9.938791371538249e-06
Epoch 45: reducing lr to 3.388909966594439e-06
Epoch 48: reducing lr to 2.5089648960936045e-07
[I 2024-06-22 06:01:52,790] Trial 749 finished with value: 0.971200168132782 and parameters: {'hidden_size': 177, 'n_layers': 4, 'rnn_dropout': 0.0545222595372515, 'bidirectional': True, 'fc_dropout': 0.18449491875757412, 'learning_rate_model': 0.0011537668661935415}. Best is trial 153 with value: 0.9688200950622559.
Epoch 38: reducing lr to 1.3566389861395696e-05
Epoch 41: reducing lr to 7.480657337351562e-06
Epoch 44: reducing lr to 3.040863343644717e-06
Epoch 47: reducing lr to 5.259712915289057e-07
[I 2024-06-22 06:02:58,339] Trial 750 finished with value: 0.9727606177330017 and parameters: {'hidden_size': 182, 'n_layers': 4, 'rnn_dropout': 0.04029357870552635, 'bidirectional': True, 'fc_dropout': 0.13468700722160443, 'learning_rate_model': 0.0006735083109559821}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.494947787095927e-05
Epoch 26: reducing lr to 5.5853192927819616e-05
Epoch 32: reducing lr to 3.551884202879172e-05
Epoch 37: reducing lr to 1.9426000932511687e-05
Epoch 40: reducing lr to 1.1417323458028387e-05
Epoch 43: reducing lr to 5.276917644152419e-06
Epoch 46: reducing lr to 1.3906167125711434e-06
Epoch 49: reducing lr to 2.6058154381242844e-09
[I 2024-06-22 06:03:52,992] Trial 751 finished with value: 0.9725393056869507 and parameters: {'hidden_size': 168, 'n_layers': 4, 'rnn_dropout': 0.059756290882413333, 'bidirectional': True, 'fc_dropout': 0.18079753870629026, 'learning_rate_model': 0.0008230117574412545}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.091012310271275e-05
Epoch 27: reducing lr to 6.550788877780699e-05
Epoch 36: reducing lr to 2.7937652229769427e-05
Epoch 39: reducing lr to 1.731827522804797e-05
Epoch 42: reducing lr to 8.831790302949826e-06
Epoch 45: reducing lr to 3.01144687132179e-06
Epoch 48: reducing lr to 2.2295117194247229e-07
[I 2024-06-22 06:04:57,028] Trial 752 finished with value: 0.9718330502510071 and parameters: {'hidden_size': 184, 'n_layers': 4, 'rnn_dropout': 0.10902129224984487, 'bidirectional': True, 'fc_dropout': 0.14935750382740925, 'learning_rate_model': 0.0010252581666915313}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00012148684262508759
Epoch 23: reducing lr to 0.00010308151119461958
Epoch 27: reducing lr to 8.345868120622104e-05
Epoch 30: reducing lr to 6.728634419848965e-05
Epoch 33: reducing lr to 5.098988112866975e-05
Epoch 36: reducing lr to 3.559326448457486e-05
Epoch 39: reducing lr to 2.20639138013087e-05
Epoch 42: reducing lr to 1.1251920724757017e-05
Epoch 45: reducing lr to 3.836658287913945e-06
Epoch 48: reducing lr to 2.8404534371140757e-07
[I 2024-06-22 06:05:59,858] Trial 753 finished with value: 0.9738135933876038 and parameters: {'hidden_size': 179, 'n_layers': 4, 'rnn_dropout': 0.036903580338210284, 'bidirectional': True, 'fc_dropout': 0.16253442935193174, 'learning_rate_model': 0.0013062044294880676}. Best is trial 153 with value: 0.9688200950622559.
Epoch 37: reducing lr to 1.1939980030224783e-05
Epoch 40: reducing lr to 7.0175335912459535e-06
Epoch 43: reducing lr to 3.243399993195327e-06
Epoch 46: reducing lr to 8.547274261686954e-07
Epoch 49: reducing lr to 1.601636095960573e-09
[I 2024-06-22 06:07:07,346] Trial 754 finished with value: 0.9763596653938293 and parameters: {'hidden_size': 185, 'n_layers': 4, 'rnn_dropout': 0.04961925388202678, 'bidirectional': True, 'fc_dropout': 0.12727007410604346, 'learning_rate_model': 0.000505855218612832}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 3.8844636240051525e-05
Epoch 37: reducing lr to 2.1244947659347667e-05
Epoch 40: reducing lr to 1.248638050200554e-05
Epoch 43: reducing lr to 5.771019961451804e-06
Epoch 46: reducing lr to 1.5208266166271702e-06
Epoch 49: reducing lr to 2.84981004506496e-09
[I 2024-06-22 06:08:08,771] Trial 755 finished with value: 0.9724560379981995 and parameters: {'hidden_size': 177, 'n_layers': 4, 'rnn_dropout': 0.0888600759183188, 'bidirectional': True, 'fc_dropout': 0.10491956211195812, 'learning_rate_model': 0.0009000741722710546}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.92388279935266e-05
Epoch 27: reducing lr to 7.225112253803737e-05
Epoch 36: reducing lr to 3.0813490899160044e-05
Epoch 39: reducing lr to 1.9100979271263882e-05
Epoch 42: reducing lr to 9.740914801468289e-06
Epoch 45: reducing lr to 3.3214383943079094e-06
Epoch 48: reducing lr to 2.4590126081840394e-07
[I 2024-06-22 06:09:09,694] Trial 756 finished with value: 0.9723197221755981 and parameters: {'hidden_size': 173, 'n_layers': 4, 'rnn_dropout': 0.07380180861663777, 'bidirectional': True, 'fc_dropout': 0.16973278187121055, 'learning_rate_model': 0.0011307959211753614}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 5.694248216388171e-05
Epoch 37: reducing lr to 1.703115633681936e-05
Epoch 40: reducing lr to 1.0009791590006634e-05
Epoch 43: reducing lr to 4.626377280960033e-06
Epoch 46: reducing lr to 1.2191809687785725e-06
Epoch 49: reducing lr to 2.2845695450105127e-09
[I 2024-06-22 06:10:18,847] Trial 757 finished with value: 0.9753769040107727 and parameters: {'hidden_size': 188, 'n_layers': 4, 'rnn_dropout': 0.036745570201548755, 'bidirectional': True, 'fc_dropout': 0.1928790628867908, 'learning_rate_model': 0.000721550562914039}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00015294548555762303
Epoch 23: reducing lr to 0.0001297741503606986
Epoch 27: reducing lr to 0.00010507004911203767
Epoch 30: reducing lr to 8.470993535155206e-05
Epoch 33: reducing lr to 6.419355346831123e-05
Epoch 36: reducing lr to 4.481003046538988e-05
Epoch 39: reducing lr to 2.7777296180597526e-05
Epoch 42: reducing lr to 1.4165570867741566e-05
Epoch 45: reducing lr to 4.8301491098468905e-06
Epoch 48: reducing lr to 3.5759800876865916e-07
[I 2024-06-22 06:11:24,239] Trial 758 finished with value: 0.9857494235038757 and parameters: {'hidden_size': 182, 'n_layers': 4, 'rnn_dropout': 0.06022093123317836, 'bidirectional': True, 'fc_dropout': 0.14772983140699786, 'learning_rate_model': 0.0016444420349460572}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.827430794664899e-05
Epoch 26: reducing lr to 6.731185786776797e-05
Epoch 32: reducing lr to 4.280577565833109e-05
Epoch 37: reducing lr to 2.3411378028077943e-05
Epoch 40: reducing lr to 1.3759665536584766e-05
Epoch 43: reducing lr to 6.359513428393191e-06
Epoch 46: reducing lr to 1.6759112523092387e-06
Epoch 49: reducing lr to 3.1404163165337645e-09
[I 2024-06-22 06:12:31,873] Trial 759 finished with value: 0.9696975946426392 and parameters: {'hidden_size': 186, 'n_layers': 4, 'rnn_dropout': 0.07404779309654459, 'bidirectional': True, 'fc_dropout': 0.18323619637923824, 'learning_rate_model': 0.0009918582544059808}. Best is trial 153 with value: 0.9688200950622559.
Epoch 31: reducing lr to 2.7885555761747957e-05
Epoch 37: reducing lr to 1.390786036877923e-05
Epoch 40: reducing lr to 8.174123999638645e-06
Epoch 43: reducing lr to 3.777958933873595e-06
Epoch 46: reducing lr to 9.955987921611672e-07
Epoch 49: reducing lr to 1.8656087470659765e-09
[I 2024-06-22 06:13:34,500] Trial 760 finished with value: 0.972726583480835 and parameters: {'hidden_size': 180, 'n_layers': 4, 'rnn_dropout': 0.08279819468858382, 'bidirectional': True, 'fc_dropout': 0.17287193841876564, 'learning_rate_model': 0.0005892274299853342}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.50587440382719e-05
Epoch 26: reducing lr to 5.594715618238992e-05
Epoch 32: reducing lr to 3.557859628491645e-05
Epoch 37: reducing lr to 1.9458681790583004e-05
Epoch 40: reducing lr to 1.1436531113210864e-05
Epoch 43: reducing lr to 5.2857951376304465e-06
Epoch 46: reducing lr to 1.3929561826233196e-06
Epoch 49: reducing lr to 2.6101992680552936e-09
[I 2024-06-22 06:14:38,311] Trial 761 finished with value: 0.9729709029197693 and parameters: {'hidden_size': 184, 'n_layers': 4, 'rnn_dropout': 0.10014824181930344, 'bidirectional': True, 'fc_dropout': 0.15709223531457667, 'learning_rate_model': 0.0008243963311643491}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 4.1129771259752385e-05
Epoch 36: reducing lr to 2.596929768339366e-05
Epoch 39: reducing lr to 1.609811164736629e-05
Epoch 42: reducing lr to 8.20954422255354e-06
Epoch 45: reducing lr to 2.799274599593854e-06
Epoch 48: reducing lr to 2.0724308919795303e-07
[I 2024-06-22 06:15:39,573] Trial 762 finished with value: 0.9720153212547302 and parameters: {'hidden_size': 177, 'n_layers': 4, 'rnn_dropout': 0.07500135298165678, 'bidirectional': True, 'fc_dropout': 0.1374854242485616, 'learning_rate_model': 0.0009530233361832695}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 5.7445692003802754e-05
Epoch 32: reducing lr to 3.141525577113247e-05
Epoch 37: reducing lr to 1.718166339461244e-05
Epoch 40: reducing lr to 1.0098249722357676e-05
Epoch 43: reducing lr to 4.6672613183693155e-06
Epoch 46: reducing lr to 1.2299550663735434e-06
Epoch 49: reducing lr to 2.3047586521674107e-09
[I 2024-06-22 06:16:48,556] Trial 763 finished with value: 0.9731966257095337 and parameters: {'hidden_size': 187, 'n_layers': 4, 'rnn_dropout': 0.11325158525150364, 'bidirectional': True, 'fc_dropout': 0.181133958456841, 'learning_rate_model': 0.0007279270208670648}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.63114809076988e-05
Epoch 26: reducing lr to 6.562392809705855e-05
Epoch 32: reducing lr to 4.173236682100639e-05
Epoch 37: reducing lr to 2.282430818334784e-05
Epoch 40: reducing lr to 1.341462456119182e-05
Epoch 43: reducing lr to 6.200040604688068e-06
Epoch 46: reducing lr to 1.6338856629784976e-06
Epoch 49: reducing lr to 3.061666414784416e-09
[I 2024-06-22 06:17:49,229] Trial 764 finished with value: 0.9732489585876465 and parameters: {'hidden_size': 173, 'n_layers': 4, 'rnn_dropout': 0.09221160589838549, 'bidirectional': True, 'fc_dropout': 0.1581636086386895, 'learning_rate_model': 0.0009669861571415637}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 0.00010221851391312121
Epoch 27: reducing lr to 8.275996604223373e-05
Epoch 30: reducing lr to 6.67230236626117e-05
Epoch 33: reducing lr to 5.0562994402932015e-05
Epoch 36: reducing lr to 3.529527806456742e-05
Epoch 39: reducing lr to 2.1879194956880865e-05
Epoch 42: reducing lr to 1.1157719767819485e-05
Epoch 45: reducing lr to 3.8045378267940196e-06
Epoch 48: reducing lr to 2.81667319208226e-07
[I 2024-06-22 06:18:54,010] Trial 765 finished with value: 0.9719953536987305 and parameters: {'hidden_size': 181, 'n_layers': 4, 'rnn_dropout': 0.07012313098584046, 'bidirectional': True, 'fc_dropout': 0.11958343525587577, 'learning_rate_model': 0.0012952689003260913}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.378386700539019e-05
Epoch 26: reducing lr to 5.4850827848261356e-05
Epoch 32: reducing lr to 3.488140583132996e-05
Epoch 37: reducing lr to 1.9077373684014356e-05
Epoch 40: reducing lr to 1.1212423330812046e-05
Epoch 43: reducing lr to 5.182215842931522e-06
Epoch 46: reducing lr to 1.3656601154875574e-06
Epoch 49: reducing lr to 2.559050369527194e-09
[I 2024-06-22 06:19:57,920] Trial 766 finished with value: 0.9732046127319336 and parameters: {'hidden_size': 184, 'n_layers': 4, 'rnn_dropout': 0.058495642305825886, 'bidirectional': True, 'fc_dropout': 0.19175615539934038, 'learning_rate_model': 0.0008082416395217453}. Best is trial 153 with value: 0.9688200950622559.
Epoch 31: reducing lr to 3.034198554875672e-05
Epoch 37: reducing lr to 1.5132999389687393e-05
Epoch 40: reducing lr to 8.89417999733761e-06
Epoch 43: reducing lr to 4.110758141411474e-06
Epoch 46: reducing lr to 1.0833007748603757e-06
Epoch 49: reducing lr to 2.029949631521602e-09
[I 2024-06-22 06:21:06,826] Trial 767 finished with value: 0.9758403301239014 and parameters: {'hidden_size': 188, 'n_layers': 4, 'rnn_dropout': 0.050287123337067106, 'bidirectional': True, 'fc_dropout': 0.17455543641949578, 'learning_rate_model': 0.0006411322879234378}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.569767601653221e-05
Epoch 26: reducing lr to 7.369557060222848e-05
Epoch 37: reducing lr to 2.5631663083093115e-05
Epoch 40: reducing lr to 1.5064602807523988e-05
Epoch 43: reducing lr to 6.962636089745942e-06
Epoch 46: reducing lr to 1.834851093551011e-06
Epoch 49: reducing lr to 3.438246687988318e-09
[I 2024-06-22 06:22:08,303] Trial 768 finished with value: 0.9728259444236755 and parameters: {'hidden_size': 178, 'n_layers': 4, 'rnn_dropout': 0.08071392392964138, 'bidirectional': True, 'fc_dropout': 0.14232450706720398, 'learning_rate_model': 0.0010859239713539475}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.471422045285904e-05
Epoch 36: reducing lr to 2.5798254008098496e-05
Epoch 39: reducing lr to 1.5992083359075754e-05
Epoch 42: reducing lr to 8.155473040750971e-06
Epoch 45: reducing lr to 2.7808375120178887e-06
Epoch 48: reducing lr to 2.0587810736101812e-07
[I 2024-06-22 06:23:15,610] Trial 769 finished with value: 0.9719045162200928 and parameters: {'hidden_size': 185, 'n_layers': 4, 'rnn_dropout': 0.06624914775381467, 'bidirectional': True, 'fc_dropout': 0.16764232197780699, 'learning_rate_model': 0.0009467463618865373}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00013880557731833944
Epoch 23: reducing lr to 0.0001177764469225025
Epoch 26: reducing lr to 0.00010128165503324137
Epoch 30: reducing lr to 7.68784474952118e-05
Epoch 33: reducing lr to 5.8258818276317036e-05
Epoch 36: reducing lr to 4.0667314407638707e-05
Epoch 39: reducing lr to 2.5209267332298675e-05
Epoch 42: reducing lr to 1.2855954755198822e-05
Epoch 45: reducing lr to 4.3835987265761e-06
Epoch 48: reducing lr to 3.24537843494053e-07
[I 2024-06-22 06:24:20,879] Trial 770 finished with value: 0.9837998151779175 and parameters: {'hidden_size': 182, 'n_layers': 4, 'rnn_dropout': 0.046228989601359564, 'bidirectional': True, 'fc_dropout': 0.19117227294514788, 'learning_rate_model': 0.0014924123140675186}. Best is trial 153 with value: 0.9688200950622559.
Epoch 24: reducing lr to 5.847542050559582e-05
Epoch 32: reducing lr to 3.346894166385479e-05
Epoch 37: reducing lr to 1.8304867355900674e-05
Epoch 40: reducing lr to 1.07583950080456e-05
Epoch 43: reducing lr to 4.9723706829727235e-06
Epoch 46: reducing lr to 1.3103599940588621e-06
Epoch 49: reducing lr to 2.455425906469546e-09
[I 2024-06-22 06:25:18,687] Trial 771 finished with value: 0.9729259610176086 and parameters: {'hidden_size': 171, 'n_layers': 4, 'rnn_dropout': 0.10592350348836775, 'bidirectional': True, 'fc_dropout': 0.15701291669582798, 'learning_rate_model': 0.0007755132466351126}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.03225922981402e-05
Epoch 27: reducing lr to 7.312857901450151e-05
Epoch 36: reducing lr to 3.11877064988928e-05
Epoch 39: reducing lr to 1.933295183279126e-05
Epoch 42: reducing lr to 9.859213707824154e-06
Epoch 45: reducing lr to 3.3617757278728982e-06
Epoch 48: reducing lr to 2.4888761793364756e-07
[I 2024-06-22 06:26:27,588] Trial 772 finished with value: 0.973170816898346 and parameters: {'hidden_size': 188, 'n_layers': 4, 'rnn_dropout': 0.06612838379935554, 'bidirectional': True, 'fc_dropout': 0.10215104710994435, 'learning_rate_model': 0.0011445289147917885}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.549199549108917e-05
Epoch 38: reducing lr to 1.9268708861532677e-05
Epoch 41: reducing lr to 1.0624979069522782e-05
Epoch 44: reducing lr to 4.319020097095204e-06
Epoch 47: reducing lr to 7.470511897077442e-07
[I 2024-06-22 06:27:27,858] Trial 773 finished with value: 0.9696767330169678 and parameters: {'hidden_size': 176, 'n_layers': 4, 'rnn_dropout': 0.035954711317253515, 'bidirectional': True, 'fc_dropout': 0.13229326841443706, 'learning_rate_model': 0.0009566019915557932}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00017707263501752694
Epoch 23: reducing lr to 0.00015024602182764013
Epoch 27: reducing lr to 0.00012164484875024266
Epoch 30: reducing lr to 9.807292716209281e-05
Epoch 36: reducing lr to 5.187881251148684e-05
Epoch 39: reducing lr to 3.215916449202799e-05
Epoch 42: reducing lr to 1.6400189589992715e-05
Epoch 45: reducing lr to 5.592105103918959e-06
Epoch 48: reducing lr to 4.140090925784613e-07
[I 2024-06-22 06:28:28,705] Trial 774 finished with value: 0.9841036200523376 and parameters: {'hidden_size': 174, 'n_layers': 4, 'rnn_dropout': 0.034816339490344024, 'bidirectional': True, 'fc_dropout': 0.1297310842000292, 'learning_rate_model': 0.001903852756423966}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00012292125976411455
Epoch 23: reducing lr to 0.00010429861325422836
Epoch 27: reducing lr to 8.444409296058059e-05
Epoch 36: reducing lr to 3.6013520600442105e-05
Epoch 39: reducing lr to 2.2324426424953723e-05
Epoch 42: reducing lr to 1.138477418926242e-05
Epoch 45: reducing lr to 3.881958406723972e-06
Epoch 48: reducing lr to 2.87399118494556e-07
[I 2024-06-22 06:29:25,277] Trial 775 finished with value: 0.9727177619934082 and parameters: {'hidden_size': 169, 'n_layers': 4, 'rnn_dropout': 0.03347924849361518, 'bidirectional': True, 'fc_dropout': 0.10845798626143008, 'learning_rate_model': 0.0013216270215996489}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.122591843792965e-05
Epoch 26: reducing lr to 6.985009028504484e-05
Epoch 33: reducing lr to 4.0178882495207696e-05
Epoch 37: reducing lr to 2.4294187098074597e-05
Epoch 40: reducing lr to 1.427852254368828e-05
Epoch 43: reducing lr to 6.59932144518819e-06
Epoch 46: reducing lr to 1.7391074320588377e-06
Epoch 49: reducing lr to 3.258836855663666e-09
[I 2024-06-22 06:30:20,833] Trial 776 finished with value: 0.973296046257019 and parameters: {'hidden_size': 165, 'n_layers': 4, 'rnn_dropout': 0.04351089159921331, 'bidirectional': True, 'fc_dropout': 0.12088527614996203, 'learning_rate_model': 0.001029259788911574}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 0.00010121193407258949
Epoch 27: reducing lr to 8.194500102041777e-05
Epoch 30: reducing lr to 6.606597976765557e-05
Epoch 36: reducing lr to 3.494771367524407e-05
Epoch 39: reducing lr to 2.1663743217977917e-05
Epoch 42: reducing lr to 1.104784597534647e-05
Epoch 45: reducing lr to 3.7670732723567867e-06
Epoch 48: reducing lr to 2.7889364705825996e-07
[I 2024-06-22 06:31:22,196] Trial 777 finished with value: 0.9727659821510315 and parameters: {'hidden_size': 177, 'n_layers': 4, 'rnn_dropout': 0.03670565715957787, 'bidirectional': True, 'fc_dropout': 0.1383717330905978, 'learning_rate_model': 0.001282513954932891}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.716597663924441e-05
Epoch 26: reducing lr to 6.63587502467429e-05
Epoch 37: reducing lr to 2.30798827533361e-05
Epoch 40: reducing lr to 1.356483445479473e-05
Epoch 43: reducing lr to 6.2694653906979745e-06
Epoch 46: reducing lr to 1.6521810532427552e-06
Epoch 49: reducing lr to 3.095949341184491e-09
[I 2024-06-22 06:32:24,576] Trial 778 finished with value: 0.9724150896072388 and parameters: {'hidden_size': 175, 'n_layers': 4, 'rnn_dropout': 0.08888638692568857, 'bidirectional': True, 'fc_dropout': 0.14740034989382692, 'learning_rate_model': 0.0009778139583310147}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00014097033905863363
Epoch 23: reducing lr to 0.00011961324592677375
Epoch 27: reducing lr to 9.684339746432612e-05
Epoch 30: reducing lr to 7.807741604536718e-05
Epoch 36: reducing lr to 4.130154718135694e-05
Epoch 39: reducing lr to 2.5602421976918255e-05
Epoch 42: reducing lr to 1.3056451590604651e-05
Epoch 45: reducing lr to 4.451963751897318e-06
Epoch 48: reducing lr to 3.295992186955806e-07
[I 2024-06-22 06:33:26,069] Trial 779 finished with value: 0.9738651514053345 and parameters: {'hidden_size': 178, 'n_layers': 4, 'rnn_dropout': 0.05211326362538977, 'bidirectional': True, 'fc_dropout': 0.11944552119965586, 'learning_rate_model': 0.0015156874384584346}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.982574410179404e-05
Epoch 27: reducing lr to 7.2726311966355e-05
Epoch 33: reducing lr to 4.4432837285410815e-05
Epoch 36: reducing lr to 3.101614858267401e-05
Epoch 39: reducing lr to 1.92266047716212e-05
Epoch 42: reducing lr to 9.804979961609752e-06
Epoch 45: reducing lr to 3.34328320939646e-06
Epoch 48: reducing lr to 2.4751853229386824e-07
[I 2024-06-22 06:34:20,610] Trial 780 finished with value: 0.9707046747207642 and parameters: {'hidden_size': 168, 'n_layers': 4, 'rnn_dropout': 0.03244363040605783, 'bidirectional': True, 'fc_dropout': 0.1403626017760028, 'learning_rate_model': 0.0011382330688410513}. Best is trial 153 with value: 0.9688200950622559.
Epoch 37: reducing lr to 2.1185484790682063e-05
Epoch 40: reducing lr to 1.2451432145539578e-05
Epoch 43: reducing lr to 5.754867349191386e-06
Epoch 46: reducing lr to 1.5165699474737497e-06
Epoch 49: reducing lr to 2.8418336601261294e-09
[I 2024-06-22 06:35:23,203] Trial 781 finished with value: 0.9706311821937561 and parameters: {'hidden_size': 180, 'n_layers': 4, 'rnn_dropout': 0.07334541361133418, 'bidirectional': True, 'fc_dropout': 0.15743490765163, 'learning_rate_model': 0.0008975549383735069}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 2.8182875155001062e-05
Epoch 37: reducing lr to 1.541380652550894e-05
Epoch 40: reducing lr to 9.059219930679287e-06
Epoch 43: reducing lr to 4.187037151938064e-06
Epoch 46: reducing lr to 1.1034024467093218e-06
Epoch 49: reducing lr to 2.0676172694539946e-09
[I 2024-06-22 06:36:21,476] Trial 782 finished with value: 0.9743819832801819 and parameters: {'hidden_size': 172, 'n_layers': 4, 'rnn_dropout': 0.04765699958937139, 'bidirectional': True, 'fc_dropout': 0.19845540513146492, 'learning_rate_model': 0.000653029104728782}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00011815999434857568
Epoch 23: reducing lr to 0.00010025868247960903
Epoch 26: reducing lr to 8.621728332209466e-05
Epoch 33: reducing lr to 4.9593552156025645e-05
Epoch 36: reducing lr to 3.461856312558598e-05
Epoch 39: reducing lr to 2.1459706036773095e-05
Epoch 42: reducing lr to 1.0943793257932257e-05
Epoch 45: reducing lr to 3.731593576897425e-06
Epoch 48: reducing lr to 2.762669230877432e-07
[I 2024-06-22 06:37:26,747] Trial 783 finished with value: 0.9752528071403503 and parameters: {'hidden_size': 182, 'n_layers': 4, 'rnn_dropout': 0.060583885612669405, 'bidirectional': True, 'fc_dropout': 0.13440981535192006, 'learning_rate_model': 0.0012704347620811605}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.10690406497251e-05
Epoch 36: reducing lr to 2.7992525254199295e-05
Epoch 39: reducing lr to 1.7352290474993423e-05
Epoch 42: reducing lr to 8.849137037781641e-06
Epoch 45: reducing lr to 3.017361727601772e-06
Epoch 48: reducing lr to 2.2338907577934032e-07
[I 2024-06-22 06:38:29,223] Trial 784 finished with value: 0.9722391963005066 and parameters: {'hidden_size': 175, 'n_layers': 4, 'rnn_dropout': 0.08267875652205343, 'bidirectional': True, 'fc_dropout': 0.16439343288594832, 'learning_rate_model': 0.0010272719012732739}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 0.00012675495880788397
Epoch 27: reducing lr to 0.00010262559770279191
Epoch 30: reducing lr to 8.27391613527085e-05
Epoch 33: reducing lr to 6.270009245286255e-05
Epoch 37: reducing lr to 3.7911651158991334e-05
Epoch 40: reducing lr to 2.2281970726446138e-05
Epoch 43: reducing lr to 1.0298396546713577e-05
Epoch 46: reducing lr to 2.7139181083136144e-06
Epoch 49: reducing lr to 5.085491667491601e-09
[I 2024-06-22 06:39:31,795] Trial 785 finished with value: 0.974475085735321 and parameters: {'hidden_size': 180, 'n_layers': 4, 'rnn_dropout': 0.12236996747751783, 'bidirectional': True, 'fc_dropout': 0.0996193065497807, 'learning_rate_model': 0.0016061841423903914}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.74854936374328e-05
Epoch 26: reducing lr to 5.803403536898992e-05
Epoch 36: reducing lr to 2.3302229430592744e-05
Epoch 39: reducing lr to 1.4444822327486435e-05
Epoch 42: reducing lr to 7.366417271918334e-06
Epoch 45: reducing lr to 2.51178679355192e-06
Epoch 48: reducing lr to 1.8595905331254347e-07
[I 2024-06-22 06:40:35,488] Trial 786 finished with value: 0.9727880954742432 and parameters: {'hidden_size': 184, 'n_layers': 4, 'rnn_dropout': 0.03342373400433887, 'bidirectional': True, 'fc_dropout': 0.18388846241803355, 'learning_rate_model': 0.00085514705492603}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 0.00016321200857845713
Epoch 27: reducing lr to 0.00013214260089046358
Epoch 30: reducing lr to 0.0001065364608964922
Epoch 33: reducing lr to 8.073378843345222e-05
Epoch 36: reducing lr to 5.635586945775145e-05
Epoch 39: reducing lr to 3.493444796142547e-05
Epoch 42: reducing lr to 1.781549921581881e-05
Epoch 45: reducing lr to 6.074694658068838e-06
Epoch 48: reducing lr to 4.497374023453276e-07
[I 2024-06-22 06:41:46,283] Trial 787 finished with value: 0.9906554222106934 and parameters: {'hidden_size': 189, 'n_layers': 4, 'rnn_dropout': 0.04800382492639343, 'bidirectional': True, 'fc_dropout': 0.14943892819305882, 'learning_rate_model': 0.00206815214561923}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.947470126130895e-05
Epoch 26: reducing lr to 7.694361703162227e-05
Epoch 29: reducing lr to 6.313991102468846e-05
Epoch 36: reducing lr to 3.089493615066755e-05
Epoch 39: reducing lr to 1.9151466379844954e-05
Epoch 42: reducing lr to 9.766661681577244e-06
Epoch 45: reducing lr to 3.330217515968496e-06
Epoch 48: reducing lr to 2.4655121930896964e-07
[I 2024-06-22 06:42:47,534] Trial 788 finished with value: 0.9714499115943909 and parameters: {'hidden_size': 177, 'n_layers': 4, 'rnn_dropout': 0.031577152354306456, 'bidirectional': True, 'fc_dropout': 0.12707072247637674, 'learning_rate_model': 0.0011337848054437873}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 3.183714845892792e-05
Epoch 37: reducing lr to 1.7412405369249197e-05
Epoch 40: reducing lr to 1.0233864652518787e-05
Epoch 43: reducing lr to 4.729940528642063e-06
Epoch 46: reducing lr to 1.2464728070725352e-06
Epoch 49: reducing lr to 2.3357105193109323e-09
[I 2024-06-22 06:43:48,341] Trial 789 finished with value: 0.9748470783233643 and parameters: {'hidden_size': 173, 'n_layers': 4, 'rnn_dropout': 0.09351736480180434, 'bidirectional': True, 'fc_dropout': 0.1682767059244382, 'learning_rate_model': 0.0007377027517918705}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00013189533586587763
Epoch 23: reducing lr to 0.00011191311130320661
Epoch 27: reducing lr to 9.060907791132741e-05
Epoch 30: reducing lr to 7.305116155363939e-05
Epoch 33: reducing lr to 5.535848452314835e-05
Epoch 36: reducing lr to 3.864274906084819e-05
Epoch 39: reducing lr to 2.395425923052534e-05
Epoch 42: reducing lr to 1.2215939035537904e-05
Epoch 45: reducing lr to 4.165367397428047e-06
Epoch 48: reducing lr to 3.0838118104336254e-07
[I 2024-06-22 06:44:53,184] Trial 790 finished with value: 0.9724124073982239 and parameters: {'hidden_size': 181, 'n_layers': 4, 'rnn_dropout': 0.05844923528821134, 'bidirectional': True, 'fc_dropout': 0.2014203265769962, 'learning_rate_model': 0.0014181146551688277}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.108039193579479e-05
Epoch 26: reducing lr to 6.112546204085974e-05
Epoch 36: reducing lr to 2.454352056462711e-05
Epoch 39: reducing lr to 1.521428646572338e-05
Epoch 42: reducing lr to 7.758820431301232e-06
Epoch 45: reducing lr to 2.645587668129493e-06
Epoch 48: reducing lr to 1.9586494342738659e-07
[I 2024-06-22 06:46:02,185] Trial 791 finished with value: 0.9735793471336365 and parameters: {'hidden_size': 188, 'n_layers': 4, 'rnn_dropout': 0.07254571020182811, 'bidirectional': True, 'fc_dropout': 0.15136163551628512, 'learning_rate_model': 0.0009007000549399125}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.497016712597722e-05
Epoch 26: reducing lr to 7.306995057027683e-05
Epoch 29: reducing lr to 5.996118138934844e-05
Epoch 36: reducing lr to 2.9339554656929535e-05
Epoch 39: reducing lr to 1.81873007237035e-05
Epoch 42: reducing lr to 9.274966707325041e-06
Epoch 45: reducing lr to 3.1625603093246753e-06
Epoch 48: reducing lr to 2.3413879023314546e-07
[I 2024-06-22 06:47:05,898] Trial 792 finished with value: 0.9709078073501587 and parameters: {'hidden_size': 184, 'n_layers': 4, 'rnn_dropout': 0.03327168256876744, 'bidirectional': True, 'fc_dropout': 0.1807953385481657, 'learning_rate_model': 0.0010767052926178478}. Best is trial 153 with value: 0.9688200950622559.
Epoch 31: reducing lr to 2.7858545946144097e-05
Epoch 37: reducing lr to 1.3894389281912093e-05
Epoch 40: reducing lr to 8.166206582326267e-06
Epoch 43: reducing lr to 3.7742996209649826e-06
Epoch 46: reducing lr to 9.946344599448194e-07
Epoch 49: reducing lr to 1.8638017273845188e-09
[I 2024-06-22 06:48:07,426] Trial 793 finished with value: 0.9749338626861572 and parameters: {'hidden_size': 178, 'n_layers': 4, 'rnn_dropout': 0.04821838246232723, 'bidirectional': True, 'fc_dropout': 0.1101007836823979, 'learning_rate_model': 0.0005886567071219056}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00012077481001318746
Epoch 23: reducing lr to 0.0001024773519616644
Epoch 27: reducing lr to 8.296953109349324e-05
Epoch 30: reducing lr to 6.689197991697881e-05
Epoch 33: reducing lr to 5.069103017941454e-05
Epoch 36: reducing lr to 3.538465288080461e-05
Epoch 39: reducing lr to 2.1934597524473948e-05
Epoch 42: reducing lr to 1.1185973381576268e-05
Epoch 45: reducing lr to 3.814171689672666e-06
Epoch 48: reducing lr to 2.8238055809666227e-07
[I 2024-06-22 06:49:16,271] Trial 794 finished with value: 0.9757997393608093 and parameters: {'hidden_size': 188, 'n_layers': 4, 'rnn_dropout': 0.06473393970087385, 'bidirectional': True, 'fc_dropout': 0.1689513577879051, 'learning_rate_model': 0.00129854878438686}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00016467107922408673
Epoch 23: reducing lr to 0.0001397233093698203
Epoch 27: reducing lr to 0.00011312526367369255
Epoch 36: reducing lr to 4.824539965921329e-05
Epoch 39: reducing lr to 2.990684768046177e-05
Epoch 42: reducing lr to 1.5251576953132385e-05
Epoch 45: reducing lr to 5.200453376128476e-06
Epoch 48: reducing lr to 3.850133256148424e-07
[I 2024-06-22 06:50:14,058] Trial 795 finished with value: 0.9802476167678833 and parameters: {'hidden_size': 171, 'n_layers': 4, 'rnn_dropout': 0.03410381612196462, 'bidirectional': True, 'fc_dropout': 0.1288342282523913, 'learning_rate_model': 0.0017705134847801588}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.023399596549314e-05
Epoch 26: reducing lr to 5.1798122290655915e-05
Epoch 32: reducing lr to 3.2940092169976214e-05
Epoch 37: reducing lr to 1.801562845692655e-05
Epoch 40: reducing lr to 1.058839943985304e-05
Epoch 43: reducing lr to 4.893801251483758e-06
Epoch 46: reducing lr to 1.2896547316512077e-06
Epoch 49: reducing lr to 2.4166272267599902e-09
[I 2024-06-22 06:51:17,647] Trial 796 finished with value: 0.9732087850570679 and parameters: {'hidden_size': 184, 'n_layers': 4, 'rnn_dropout': 0.07959406775817175, 'bidirectional': True, 'fc_dropout': 0.1884198136441154, 'learning_rate_model': 0.0007632592054975634}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.220361658680879e-05
Epoch 38: reducing lr to 1.8429377283120994e-05
Epoch 41: reducing lr to 1.016216235891189e-05
Epoch 44: reducing lr to 4.130886580659991e-06
Epoch 47: reducing lr to 7.14510158613367e-07
[I 2024-06-22 06:52:16,560] Trial 797 finished with value: 0.9699227213859558 and parameters: {'hidden_size': 176, 'n_layers': 4, 'rnn_dropout': 0.05037767265592705, 'bidirectional': True, 'fc_dropout': 0.15252237862663234, 'learning_rate_model': 0.0009149330730385185}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.969502342937068e-05
Epoch 27: reducing lr to 6.452409823092825e-05
Epoch 37: reducing lr to 2.3836305544000664e-05
Epoch 40: reducing lr to 1.4009409933919096e-05
Epoch 43: reducing lr to 6.474941586460752e-06
Epoch 46: reducing lr to 1.7063298293146797e-06
Epoch 49: reducing lr to 3.197416348853753e-09
[I 2024-06-22 06:53:11,127] Trial 798 finished with value: 0.9721287488937378 and parameters: {'hidden_size': 168, 'n_layers': 4, 'rnn_dropout': 0.10314129201226639, 'bidirectional': True, 'fc_dropout': 0.1401862953108029, 'learning_rate_model': 0.001009860947954678}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.000128420913489647
Epoch 23: reducing lr to 0.00010896506605541371
Epoch 27: reducing lr to 8.822222923530698e-05
Epoch 30: reducing lr to 7.11268282279356e-05
Epoch 33: reducing lr to 5.390021644961291e-05
Epoch 36: reducing lr to 3.762481138219816e-05
Epoch 39: reducing lr to 2.3323249697624194e-05
Epoch 42: reducing lr to 1.1894143487172913e-05
Epoch 45: reducing lr to 4.055642170255783e-06
Epoch 48: reducing lr to 3.002577211136306e-07
[I 2024-06-22 06:54:13,476] Trial 799 finished with value: 0.9733877778053284 and parameters: {'hidden_size': 175, 'n_layers': 4, 'rnn_dropout': 0.06039571212066105, 'bidirectional': True, 'fc_dropout': 0.19887216092663063, 'learning_rate_model': 0.0013807582978902846}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.396972568086846e-05
Epoch 27: reducing lr to 7.608143582440023e-05
Epoch 32: reducing lr to 5.138910967966436e-05
Epoch 36: reducing lr to 3.2447034011630544e-05
Epoch 39: reducing lr to 2.0113596544396856e-05
Epoch 42: reducing lr to 1.025731861741938e-05
Epoch 45: reducing lr to 3.497520774271693e-06
Epoch 48: reducing lr to 2.5893744397182706e-07
[I 2024-06-22 06:55:09,583] Trial 800 finished with value: 0.9707930088043213 and parameters: {'hidden_size': 166, 'n_layers': 4, 'rnn_dropout': 0.07018450636950754, 'bidirectional': True, 'fc_dropout': 0.15132954516542718, 'learning_rate_model': 0.0011907438152549677}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.489070421401598e-05
Epoch 26: reducing lr to 6.440213359799763e-05
Epoch 36: reducing lr to 2.5859192513125173e-05
Epoch 39: reducing lr to 1.6029858537654054e-05
Epoch 42: reducing lr to 8.174737225634669e-06
Epoch 45: reducing lr to 2.7874061767287356e-06
Epoch 48: reducing lr to 2.0636441562343857e-07
[I 2024-06-22 06:56:09,324] Trial 801 finished with value: 0.9731907844543457 and parameters: {'hidden_size': 172, 'n_layers': 4, 'rnn_dropout': 0.09311431758457622, 'bidirectional': True, 'fc_dropout': 0.11661918839355659, 'learning_rate_model': 0.0009489826879539798}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00014914215352050534
Epoch 23: reducing lr to 0.00012654702546808046
Epoch 27: reducing lr to 0.00010245724702459858
Epoch 30: reducing lr to 8.260343309154686e-05
Epoch 36: reducing lr to 4.369572870073252e-05
Epoch 39: reducing lr to 2.7086551500667956e-05
Epoch 42: reducing lr to 1.3813312222715756e-05
Epoch 45: reducing lr to 4.7100366345648705e-06
Epoch 48: reducing lr to 3.487055333994049e-07
[I 2024-06-22 06:57:11,910] Trial 802 finished with value: 0.9752736687660217 and parameters: {'hidden_size': 177, 'n_layers': 4, 'rnn_dropout': 0.045253409770936934, 'bidirectional': True, 'fc_dropout': 0.17092574949710457, 'learning_rate_model': 0.0016035493008330457}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.878256652874715e-05
Epoch 27: reducing lr to 7.188171603929273e-05
Epoch 36: reducing lr to 3.065594727371441e-05
Epoch 39: reducing lr to 1.900331953080134e-05
Epoch 42: reducing lr to 9.691111322920431e-06
Epoch 45: reducing lr to 3.30445649997966e-06
Epoch 48: reducing lr to 2.4464401358673374e-07
[I 2024-06-22 06:58:14,628] Trial 803 finished with value: 0.9711554050445557 and parameters: {'hidden_size': 179, 'n_layers': 4, 'rnn_dropout': 0.029918746250464874, 'bidirectional': True, 'fc_dropout': 0.09893244377233223, 'learning_rate_model': 0.0011250143727735884}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 4.2352377992450806e-05
Epoch 35: reducing lr to 3.0475702084069974e-05
Epoch 38: reducing lr to 1.9767255469902214e-05
Epoch 41: reducing lr to 1.0899883180492195e-05
Epoch 44: reducing lr to 4.4307677412347405e-06
Epoch 47: reducing lr to 7.663799283162209e-07
[I 2024-06-22 06:59:14,362] Trial 804 finished with value: 0.9788894653320312 and parameters: {'hidden_size': 172, 'n_layers': 4, 'rnn_dropout': 0.7737158850314543, 'bidirectional': True, 'fc_dropout': 0.14050002670489803, 'learning_rate_model': 0.0009813525174927837}. Best is trial 153 with value: 0.9688200950622559.
Epoch 38: reducing lr to 1.3950611941303379e-05
Epoch 41: reducing lr to 7.69252163954243e-06
Epoch 44: reducing lr to 3.1269855066185827e-06
Epoch 47: reducing lr to 5.408676483097227e-07
[I 2024-06-22 07:00:13,334] Trial 805 finished with value: 0.9705456495285034 and parameters: {'hidden_size': 176, 'n_layers': 4, 'rnn_dropout': 0.05267148378590366, 'bidirectional': True, 'fc_dropout': 0.1841716026717932, 'learning_rate_model': 0.0006925831545005417}. Best is trial 153 with value: 0.9688200950622559.
Epoch 37: reducing lr to 1.2853727936899883e-05
Epoch 40: reducing lr to 7.554574408131007e-06
Epoch 43: reducing lr to 3.4916122973022025e-06
Epoch 46: reducing lr to 9.201383727919146e-07
Epoch 49: reducing lr to 1.7242067892365987e-09
[I 2024-06-22 07:01:15,666] Trial 806 finished with value: 0.976118803024292 and parameters: {'hidden_size': 174, 'n_layers': 4, 'rnn_dropout': 0.08246551564565859, 'bidirectional': True, 'fc_dropout': 0.20608055578457113, 'learning_rate_model': 0.0005445675234842036}. Best is trial 153 with value: 0.9688200950622559.
Epoch 42: reducing lr to 4.021454025206542e-06
Epoch 45: reducing lr to 1.3712276590543363e-06
Epoch 48: reducing lr to 1.0151824908400286e-07
[I 2024-06-22 07:02:12,289] Trial 807 finished with value: 0.9776405096054077 and parameters: {'hidden_size': 169, 'n_layers': 4, 'rnn_dropout': 0.0658118558975868, 'bidirectional': True, 'fc_dropout': 0.19004556657460112, 'learning_rate_model': 0.0004668395013795159}. Best is trial 153 with value: 0.9688200950622559.
Epoch 37: reducing lr to 1.5195732252436701e-05
Epoch 40: reducing lr to 8.931050240881052e-06
Epoch 43: reducing lr to 4.127799021387752e-06
Epoch 46: reducing lr to 1.087791527623627e-06
Epoch 49: reducing lr to 2.0383646554184878e-09
[I 2024-06-22 07:03:14,995] Trial 808 finished with value: 0.9730371832847595 and parameters: {'hidden_size': 175, 'n_layers': 4, 'rnn_dropout': 0.0318042954692625, 'bidirectional': True, 'fc_dropout': 0.2052463713389603, 'learning_rate_model': 0.0006437900600403028}. Best is trial 153 with value: 0.9688200950622559.
Epoch 24: reducing lr to 5.411227135225084e-05
Epoch 31: reducing lr to 3.396315123145305e-05
Epoch 37: reducing lr to 1.6939047908764807e-05
Epoch 40: reducing lr to 9.955656324598035e-06
Epoch 43: reducing lr to 4.6013567638260745e-06
Epoch 46: reducing lr to 1.2125873564408401e-06
Epoch 49: reducing lr to 2.2722140651117684e-09
[I 2024-06-22 07:04:16,567] Trial 809 finished with value: 0.9746397733688354 and parameters: {'hidden_size': 178, 'n_layers': 4, 'rnn_dropout': 0.054129798679100956, 'bidirectional': True, 'fc_dropout': 0.18012058789133, 'learning_rate_model': 0.0007176482507751854}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 0.00010577053125359309
Epoch 27: reducing lr to 8.56358133151527e-05
Epoch 36: reducing lr to 3.6521762728868804e-05
Epoch 39: reducing lr to 2.2639480710481808e-05
Epoch 42: reducing lr to 1.154544223196238e-05
Epoch 45: reducing lr to 3.936742686911037e-06
Epoch 48: reducing lr to 2.9145504907995576e-07
[I 2024-06-22 07:05:12,095] Trial 810 finished with value: 0.9741997718811035 and parameters: {'hidden_size': 165, 'n_layers': 4, 'rnn_dropout': 0.11877259812453186, 'bidirectional': True, 'fc_dropout': 0.18901660301224546, 'learning_rate_model': 0.001340278531345014}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.188751547681834e-05
Epoch 27: reducing lr to 7.439559987176211e-05
Epoch 37: reducing lr to 2.7483007097998273e-05
Epoch 40: reducing lr to 1.615270084291949e-05
Epoch 43: reducing lr to 7.465538871002323e-06
Epoch 46: reducing lr to 1.9673801682066867e-06
Epoch 49: reducing lr to 3.6865870866025635e-09
[I 2024-06-22 07:06:11,331] Trial 811 finished with value: 0.9715427160263062 and parameters: {'hidden_size': 171, 'n_layers': 4, 'rnn_dropout': 0.07785544979455464, 'bidirectional': True, 'fc_dropout': 0.20761071971412037, 'learning_rate_model': 0.0011643589460370485}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.881985034360475e-05
Epoch 33: reducing lr to 3.4042147303098596e-05
Epoch 37: reducing lr to 2.0583606223999273e-05
Epoch 40: reducing lr to 1.209768757906162e-05
Epoch 43: reducing lr to 5.591371854714701e-06
Epoch 46: reducing lr to 1.4734842708759082e-06
Epoch 49: reducing lr to 2.761097307529218e-09
[I 2024-06-22 07:07:13,944] Trial 812 finished with value: 0.9726258516311646 and parameters: {'hidden_size': 175, 'n_layers': 4, 'rnn_dropout': 0.028512937098168444, 'bidirectional': True, 'fc_dropout': 0.12460748022374638, 'learning_rate_model': 0.0008720554473226858}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00017706197916946147
Epoch 23: reducing lr to 0.00015023698034711505
Epoch 27: reducing lr to 0.0001216375284264326
Epoch 30: reducing lr to 9.806702534552433e-05
Epoch 33: reducing lr to 7.431561373374138e-05
Epoch 36: reducing lr to 5.187569055679359e-05
Epoch 39: reducing lr to 3.2157229223087645e-05
Epoch 42: reducing lr to 1.6399202662066243e-05
Epoch 45: reducing lr to 5.591768583132765e-06
Epoch 48: reducing lr to 4.1398417840701926e-07
[I 2024-06-22 07:08:16,503] Trial 813 finished with value: 0.9853278398513794 and parameters: {'hidden_size': 180, 'n_layers': 4, 'rnn_dropout': 0.10007136882304413, 'bidirectional': True, 'fc_dropout': 0.1541934256822371, 'learning_rate_model': 0.0019037381866842122}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00013518649043193096
Epoch 23: reducing lr to 0.00011470565392686185
Epoch 30: reducing lr to 7.487399071074653e-05
Epoch 33: reducing lr to 5.673983230100626e-05
Epoch 36: reducing lr to 3.9606992862052725e-05
Epoch 39: reducing lr to 2.455198445807348e-05
Epoch 42: reducing lr to 1.2520760606911158e-05
Epoch 45: reducing lr to 4.269304870571731e-06
Epoch 48: reducing lr to 3.1607614709666754e-07
[I 2024-06-22 07:09:21,883] Trial 814 finished with value: 0.9829199910163879 and parameters: {'hidden_size': 182, 'n_layers': 4, 'rnn_dropout': 0.055108329721996335, 'bidirectional': True, 'fc_dropout': 0.17812524948241826, 'learning_rate_model': 0.0014535005502947333}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.021705797066106e-05
Epoch 38: reducing lr to 2.047474206649903e-05
Epoch 41: reducing lr to 1.128999911066828e-05
Epoch 44: reducing lr to 4.589348622345427e-06
Epoch 47: reducing lr to 7.938093065629941e-07
[I 2024-06-22 07:10:20,963] Trial 815 finished with value: 0.9696411490440369 and parameters: {'hidden_size': 176, 'n_layers': 4, 'rnn_dropout': 0.02949200136099807, 'bidirectional': True, 'fc_dropout': 0.1549776232685874, 'learning_rate_model': 0.0010164759444004701}. Best is trial 153 with value: 0.9688200950622559.
Epoch 31: reducing lr to 3.1759914089948643e-05
Epoch 38: reducing lr to 1.3517746718695188e-05
Epoch 41: reducing lr to 7.453834970747616e-06
Epoch 44: reducing lr to 3.029960137186042e-06
Epoch 47: reducing lr to 5.24085388436664e-07
[I 2024-06-22 07:11:19,178] Trial 816 finished with value: 0.9738057255744934 and parameters: {'hidden_size': 170, 'n_layers': 4, 'rnn_dropout': 0.03318754467013382, 'bidirectional': True, 'fc_dropout': 0.14495556833200468, 'learning_rate_model': 0.0006710934046165268}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.354671465491568e-05
Epoch 26: reducing lr to 6.32463720646017e-05
Epoch 36: reducing lr to 2.5395123105457618e-05
Epoch 39: reducing lr to 1.5742186486301793e-05
Epoch 42: reducing lr to 8.028033284271754e-06
Epoch 45: reducing lr to 2.7373833489584343e-06
Epoch 48: reducing lr to 2.0266099711672416e-07
[I 2024-06-22 07:12:21,410] Trial 817 finished with value: 0.9734868407249451 and parameters: {'hidden_size': 174, 'n_layers': 4, 'rnn_dropout': 0.026734040517314102, 'bidirectional': True, 'fc_dropout': 0.15872136852990615, 'learning_rate_model': 0.0009319522322016571}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 3.475145150950898e-05
Epoch 37: reducing lr to 1.900629893515763e-05
Epoch 40: reducing lr to 1.1170650276223271e-05
Epoch 43: reducing lr to 5.162908956372685e-06
Epoch 46: reducing lr to 1.3605722060436484e-06
Epoch 49: reducing lr to 2.549516360007831e-09
[I 2024-06-22 07:13:23,897] Trial 818 finished with value: 0.9728392362594604 and parameters: {'hidden_size': 177, 'n_layers': 4, 'rnn_dropout': 0.06943205553502473, 'bidirectional': True, 'fc_dropout': 0.11570743529991478, 'learning_rate_model': 0.0008052304508489199}. Best is trial 153 with value: 0.9688200950622559.
Epoch 37: reducing lr to 1.2548334705228466e-05
Epoch 40: reducing lr to 7.375084387513864e-06
Epoch 43: reducing lr to 3.4086546706547884e-06
Epoch 46: reducing lr to 8.982766971262043e-07
Epoch 49: reducing lr to 1.6832411576226704e-09
[I 2024-06-22 07:14:26,319] Trial 819 finished with value: 0.9751409292221069 and parameters: {'hidden_size': 175, 'n_layers': 4, 'rnn_dropout': 0.04846275135740509, 'bidirectional': True, 'fc_dropout': 0.13541061821040917, 'learning_rate_model': 0.0005316290797364789}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.084289001798508e-05
Epoch 27: reducing lr to 6.545345433539562e-05
Epoch 38: reducing lr to 2.063448053198344e-05
Epoch 41: reducing lr to 1.1378080666343133e-05
Epoch 44: reducing lr to 4.62515349373895e-06
Epoch 47: reducing lr to 8.000023946178134e-07
[I 2024-06-22 07:15:28,855] Trial 820 finished with value: 0.97225421667099 and parameters: {'hidden_size': 179, 'n_layers': 4, 'rnn_dropout': 0.0832182666522043, 'bidirectional': True, 'fc_dropout': 0.16616144685441397, 'learning_rate_model': 0.0010244062180533933}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 3.0880304651569545e-05
Epoch 37: reducing lr to 1.68890873883605e-05
Epoch 40: reducing lr to 9.92629282237391e-06
Epoch 43: reducing lr to 4.58778538840256e-06
Epoch 46: reducing lr to 1.2090109160358136e-06
Epoch 49: reducing lr to 2.265512330881271e-09
[I 2024-06-22 07:16:25,644] Trial 821 finished with value: 0.972693145275116 and parameters: {'hidden_size': 169, 'n_layers': 4, 'rnn_dropout': 0.027208922097576728, 'bidirectional': True, 'fc_dropout': 0.17724904721483292, 'learning_rate_model': 0.0007155315981587524}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.123985135329594e-05
Epoch 36: reducing lr to 2.8051504895441783e-05
Epoch 39: reducing lr to 1.7388851373220963e-05
Epoch 42: reducing lr to 8.86778197685214e-06
Epoch 45: reducing lr to 3.023719243066235e-06
Epoch 48: reducing lr to 2.2385975169826166e-07
[I 2024-06-22 07:17:25,280] Trial 822 finished with value: 0.9718604683876038 and parameters: {'hidden_size': 172, 'n_layers': 4, 'rnn_dropout': 0.05767289322640831, 'bidirectional': True, 'fc_dropout': 0.1554476359748329, 'learning_rate_model': 0.0010294363408029483}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.509184664249489e-05
Epoch 37: reducing lr to 1.946858258180695e-05
Epoch 40: reducing lr to 1.1442350145974579e-05
Epoch 43: reducing lr to 5.288484608308554e-06
Epoch 46: reducing lr to 1.39366493404321e-06
Epoch 49: reducing lr to 2.6115273661577072e-09
[I 2024-06-22 07:18:27,938] Trial 823 finished with value: 0.9708849787712097 and parameters: {'hidden_size': 180, 'n_layers': 4, 'rnn_dropout': 0.04335290549684251, 'bidirectional': True, 'fc_dropout': 0.19298261876348832, 'learning_rate_model': 0.0008248157930810647}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00011794685182555718
Epoch 23: reducing lr to 0.00010007783118001288
Epoch 26: reducing lr to 8.606176055488074e-05
Epoch 29: reducing lr to 7.062225709807711e-05
Epoch 36: reducing lr to 3.455611654264104e-05
Epoch 39: reducing lr to 2.1420996015558822e-05
Epoch 42: reducing lr to 1.092405233191708e-05
Epoch 45: reducing lr to 3.724862353912499e-06
Epoch 48: reducing lr to 2.7576857989351517e-07
[I 2024-06-22 07:19:26,931] Trial 824 finished with value: 0.970363974571228 and parameters: {'hidden_size': 176, 'n_layers': 4, 'rnn_dropout': 0.06930162769174181, 'bidirectional': True, 'fc_dropout': 0.13665445702917062, 'learning_rate_model': 0.0012681430924512387}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00016277604755180839
Epoch 23: reducing lr to 0.0001381153761622469
Epoch 27: reducing lr to 0.00011182342027407066
Epoch 30: reducing lr to 9.01546614116954e-05
Epoch 33: reducing lr to 6.831959030226232e-05
Epoch 36: reducing lr to 4.7690192510351883e-05
Epoch 39: reducing lr to 2.9562680241713428e-05
Epoch 42: reducing lr to 1.5076062093361248e-05
Epoch 45: reducing lr to 5.1406066568113584e-06
Epoch 48: reducing lr to 3.8058259952907826e-07
[I 2024-06-22 07:20:20,792] Trial 825 finished with value: 0.9767876267433167 and parameters: {'hidden_size': 164, 'n_layers': 4, 'rnn_dropout': 0.09165301669763432, 'bidirectional': True, 'fc_dropout': 0.10141511788307309, 'learning_rate_model': 0.001750138449007857}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00012252097700222667
Epoch 23: reducing lr to 0.00010395897357713267
Epoch 27: reducing lr to 8.416910786182516e-05
Epoch 30: reducing lr to 6.785910681330553e-05
Epoch 33: reducing lr to 5.142392310245055e-05
Epoch 36: reducing lr to 3.5896245594320986e-05
Epoch 39: reducing lr to 2.2251728804671494e-05
Epoch 42: reducing lr to 1.1347700627986732e-05
Epoch 45: reducing lr to 3.869317135103754e-06
Epoch 48: reducing lr to 2.864632273953454e-07
[I 2024-06-22 07:21:15,352] Trial 826 finished with value: 0.9733812808990479 and parameters: {'hidden_size': 168, 'n_layers': 4, 'rnn_dropout': 0.07393595838106964, 'bidirectional': True, 'fc_dropout': 0.13646091005363434, 'learning_rate_model': 0.0013173232541683132}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 0.00018027588747824658
Epoch 27: reducing lr to 0.0001459581611469514
Epoch 30: reducing lr to 0.00011767488926940208
Epoch 33: reducing lr to 8.917453737679616e-05
Epoch 36: reducing lr to 6.224789750210423e-05
Epoch 39: reducing lr to 3.858685806676526e-05
Epoch 42: reducing lr to 1.9678116579613425e-05
Epoch 45: reducing lr to 6.709806344404453e-06
Epoch 48: reducing lr to 4.967576224698417e-07
[I 2024-06-22 07:22:17,689] Trial 827 finished with value: 1.005728840827942 and parameters: {'hidden_size': 174, 'n_layers': 4, 'rnn_dropout': 0.09900256199578765, 'bidirectional': True, 'fc_dropout': 0.12786233728675891, 'learning_rate_model': 0.0022843782558580597}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 4.0331049565382815e-05
Epoch 36: reducing lr to 2.546498558021488e-05
Epoch 39: reducing lr to 1.5785493545750034e-05
Epoch 42: reducing lr to 8.050118559083921e-06
Epoch 45: reducing lr to 2.7449139434872854e-06
Epoch 48: reducing lr to 2.0321852143887583e-07
[I 2024-06-22 07:23:20,241] Trial 828 finished with value: 0.9721853733062744 and parameters: {'hidden_size': 177, 'n_layers': 4, 'rnn_dropout': 0.06718728408259948, 'bidirectional': True, 'fc_dropout': 0.11759375758498622, 'learning_rate_model': 0.0009345160508146558}. Best is trial 153 with value: 0.9688200950622559.
Epoch 31: reducing lr to 2.8587866651285515e-05
Epoch 37: reducing lr to 1.4258136399517706e-05
Epoch 40: reducing lr to 8.37999317242563e-06
Epoch 43: reducing lr to 3.873108613591466e-06
Epoch 46: reducing lr to 1.0206734178677288e-06
Epoch 49: reducing lr to 1.9125949843193727e-09
[I 2024-06-22 07:24:22,449] Trial 829 finished with value: 0.975071370601654 and parameters: {'hidden_size': 174, 'n_layers': 4, 'rnn_dropout': 0.11456647061468811, 'bidirectional': True, 'fc_dropout': 0.1398127701720629, 'learning_rate_model': 0.0006040674010452113}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.876957025059324e-05
Epoch 27: reducing lr to 7.996757110842562e-05
Epoch 36: reducing lr to 3.410438395998852e-05
Epoch 39: reducing lr to 2.114097144042587e-05
Epoch 42: reducing lr to 1.078124836935842e-05
Epoch 45: reducing lr to 3.6761693334140273e-06
Epoch 48: reducing lr to 2.7216361309534854e-07
[I 2024-06-22 07:25:24,187] Trial 830 finished with value: 0.9729512929916382 and parameters: {'hidden_size': 178, 'n_layers': 4, 'rnn_dropout': 0.08622660829816946, 'bidirectional': True, 'fc_dropout': 0.15533578523279482, 'learning_rate_model': 0.0012515653744770835}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00014103409420380798
Epoch 23: reducing lr to 0.00011966734212821412
Epoch 27: reducing lr to 9.688719579031262e-05
Epoch 30: reducing lr to 7.811272728195815e-05
Epoch 33: reducing lr to 5.919416080912073e-05
Epoch 36: reducing lr to 4.1320226189679234e-05
Epoch 39: reducing lr to 2.5614000910053085e-05
Epoch 42: reducing lr to 1.3062356492105077e-05
Epoch 45: reducing lr to 4.453977193854034e-06
Epoch 48: reducing lr to 3.29748283003548e-07
[I 2024-06-22 07:26:21,222] Trial 831 finished with value: 0.9787531495094299 and parameters: {'hidden_size': 167, 'n_layers': 4, 'rnn_dropout': 0.06853664873890872, 'bidirectional': True, 'fc_dropout': 0.17140870015824639, 'learning_rate_model': 0.00151637292218021}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.2472911327e-05
Epoch 37: reducing lr to 2.4667155223762127e-05
Epoch 40: reducing lr to 1.4497728634808275e-05
Epoch 43: reducing lr to 6.700635250844029e-06
Epoch 46: reducing lr to 1.7658064789001894e-06
Epoch 49: reducing lr to 3.308867024148267e-09
[I 2024-06-22 07:27:19,013] Trial 832 finished with value: 0.9717726707458496 and parameters: {'hidden_size': 171, 'n_layers': 4, 'rnn_dropout': 0.05121233022482854, 'bidirectional': True, 'fc_dropout': 0.14982322929914338, 'learning_rate_model': 0.001045061144715997}. Best is trial 153 with value: 0.9688200950622559.
Epoch 37: reducing lr to 1.6659531742370456e-05
Epoch 40: reducing lr to 9.791375138029601e-06
Epoch 43: reducing lr to 4.525428434809899e-06
Epoch 46: reducing lr to 1.1925780990660358e-06
Epoch 49: reducing lr to 2.2347195985958185e-09
[I 2024-06-22 07:28:25,185] Trial 833 finished with value: 0.972793459892273 and parameters: {'hidden_size': 181, 'n_layers': 4, 'rnn_dropout': 0.08253940376110389, 'bidirectional': True, 'fc_dropout': 0.1272347919011618, 'learning_rate_model': 0.0007058061278319886}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.337979530197525e-05
Epoch 26: reducing lr to 6.310283005119722e-05
Epoch 31: reducing lr to 4.4005121015923746e-05
Epoch 37: reducing lr to 2.1947458527623644e-05
Epoch 40: reducing lr to 1.2899270105159196e-05
Epoch 43: reducing lr to 5.961851415073858e-06
Epoch 46: reducing lr to 1.5711160898739602e-06
Epoch 49: reducing lr to 2.944045275079209e-09
[I 2024-06-22 07:29:27,449] Trial 834 finished with value: 0.9736617803573608 and parameters: {'hidden_size': 174, 'n_layers': 4, 'rnn_dropout': 0.0585537621717491, 'bidirectional': True, 'fc_dropout': 0.10045285553089828, 'learning_rate_model': 0.0009298371021880272}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00014464400152494578
Epoch 23: reducing lr to 0.00012273034626837247
Epoch 27: reducing lr to 9.936711952352375e-05
Epoch 30: reducing lr to 8.011209989948772e-05
Epoch 36: reducing lr to 4.237785830250465e-05
Epoch 39: reducing lr to 2.62696166314205e-05
Epoch 42: reducing lr to 1.339670044346211e-05
Epoch 45: reducing lr to 4.567981151344181e-06
Epoch 48: reducing lr to 3.3818851688932615e-07
[I 2024-06-22 07:30:29,884] Trial 835 finished with value: 0.9745411276817322 and parameters: {'hidden_size': 177, 'n_layers': 4, 'rnn_dropout': 0.11154809725273525, 'bidirectional': True, 'fc_dropout': 0.182050309363366, 'learning_rate_model': 0.0015551859889372669}. Best is trial 153 with value: 0.9688200950622559.
Epoch 42: reducing lr to 3.847712783447042e-06
Epoch 45: reducing lr to 1.3119857045956307e-06
Epoch 48: reducing lr to 9.713229650403465e-08
[I 2024-06-22 07:31:33,752] Trial 836 finished with value: 0.977211594581604 and parameters: {'hidden_size': 180, 'n_layers': 4, 'rnn_dropout': 0.027642715008067607, 'bidirectional': True, 'fc_dropout': 0.1685704850026875, 'learning_rate_model': 0.00044667036002823577}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.594657091557894e-05
Epoch 27: reducing lr to 7.768196432195247e-05
Epoch 36: reducing lr to 3.312962368720561e-05
Epoch 39: reducing lr to 2.05367271558101e-05
Epoch 42: reducing lr to 1.0473102278410538e-05
Epoch 45: reducing lr to 3.5710982719798542e-06
Epoch 48: reducing lr to 2.6438472232126823e-07
[I 2024-06-22 07:32:30,599] Trial 837 finished with value: 0.9715768098831177 and parameters: {'hidden_size': 170, 'n_layers': 4, 'rnn_dropout': 0.06698484472674698, 'bidirectional': True, 'fc_dropout': 0.14503978159680067, 'learning_rate_model': 0.0012157935450471129}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.450261329368449e-05
Epoch 26: reducing lr to 5.546891249531601e-05
Epoch 32: reducing lr to 3.527446574050161e-05
Epoch 38: reducing lr to 1.646375974404783e-05
Epoch 41: reducing lr to 9.078299119220067e-06
Epoch 44: reducing lr to 3.6902996313492326e-06
Epoch 47: reducing lr to 6.383028251782529e-07
[I 2024-06-22 07:33:30,840] Trial 838 finished with value: 0.9710085391998291 and parameters: {'hidden_size': 176, 'n_layers': 4, 'rnn_dropout': 0.09226250545987569, 'bidirectional': True, 'fc_dropout': 0.1179160131766901, 'learning_rate_model': 0.0008173492823431198}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.63200069001321e-05
Epoch 26: reducing lr to 7.423074298614978e-05
Epoch 33: reducing lr to 4.269870357792566e-05
Epoch 36: reducing lr to 2.9805644099512343e-05
Epoch 39: reducing lr to 1.8476224974787688e-05
Epoch 42: reducing lr to 9.422309232224973e-06
Epoch 45: reducing lr to 3.21280088008127e-06
Epoch 48: reducing lr to 2.3785832924808678e-07
[I 2024-06-22 07:34:37,409] Trial 839 finished with value: 0.9726283550262451 and parameters: {'hidden_size': 182, 'n_layers': 4, 'rnn_dropout': 0.04723313214569011, 'bidirectional': True, 'fc_dropout': 0.19831359808678506, 'learning_rate_model': 0.0010938098797709345}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 3.1489251335599546e-05
Epoch 37: reducing lr to 1.7222133123416494e-05
Epoch 40: reducing lr to 1.0122035162583896e-05
Epoch 43: reducing lr to 4.678254596230378e-06
Epoch 46: reducing lr to 1.2328521053175684e-06
Epoch 49: reducing lr to 2.3101872859143668e-09
[I 2024-06-22 07:35:41,302] Trial 840 finished with value: 0.9716413021087646 and parameters: {'hidden_size': 180, 'n_layers': 4, 'rnn_dropout': 0.026642766846455574, 'bidirectional': True, 'fc_dropout': 0.16032322977149457, 'learning_rate_model': 0.0007296415818177157}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00012830761930895989
Epoch 23: reducing lr to 0.00010886893601283105
Epoch 27: reducing lr to 8.814439872540012e-05
Epoch 30: reducing lr to 7.106407944730489e-05
Epoch 36: reducing lr to 3.7591618407134975e-05
Epoch 39: reducing lr to 2.330267369957488e-05
Epoch 42: reducing lr to 1.1883650357940798e-05
Epoch 45: reducing lr to 4.052064243231638e-06
Epoch 48: reducing lr to 2.9999283082757927e-07
[I 2024-06-22 07:36:39,749] Trial 841 finished with value: 0.9735913872718811 and parameters: {'hidden_size': 172, 'n_layers': 4, 'rnn_dropout': 0.07735535663375251, 'bidirectional': True, 'fc_dropout': 0.1378397726736528, 'learning_rate_model': 0.0013795401794714415}. Best is trial 153 with value: 0.9688200950622559.
Epoch 37: reducing lr to 2.2141992888763808e-05
Epoch 40: reducing lr to 1.3013604585660572e-05
Epoch 43: reducing lr to 6.0146951169896825e-06
Epoch 46: reducing lr to 1.5850418965652298e-06
Epoch 49: reducing lr to 2.9701402311688337e-09
[I 2024-06-22 07:37:42,228] Trial 842 finished with value: 0.9722710251808167 and parameters: {'hidden_size': 177, 'n_layers': 4, 'rnn_dropout': 0.056241563022899246, 'bidirectional': True, 'fc_dropout': 0.18078525713752064, 'learning_rate_model': 0.000938078843089868}. Best is trial 153 with value: 0.9688200950622559.
Epoch 38: reducing lr to 1.2101175586384836e-05
Epoch 41: reducing lr to 6.672721989102286e-06
Epoch 44: reducing lr to 2.712440201969857e-06
Epoch 47: reducing lr to 4.6916467956598094e-07
[I 2024-06-22 07:38:48,765] Trial 843 finished with value: 0.9736261367797852 and parameters: {'hidden_size': 182, 'n_layers': 4, 'rnn_dropout': 0.04260653217547417, 'bidirectional': True, 'fc_dropout': 0.15583515996265537, 'learning_rate_model': 0.0006007672205381639}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.447422289432185e-05
Epoch 26: reducing lr to 8.124294714895232e-05
Epoch 29: reducing lr to 6.666794013933768e-05
Epoch 36: reducing lr to 3.262123307547892e-05
Epoch 39: reducing lr to 2.022158082694797e-05
Epoch 42: reducing lr to 1.031238729642741e-05
Epoch 45: reducing lr to 3.516297986526297e-06
Epoch 48: reducing lr to 2.6032760679286106e-07
[I 2024-06-22 07:39:51,069] Trial 844 finished with value: 0.9726168513298035 and parameters: {'hidden_size': 174, 'n_layers': 4, 'rnn_dropout': 0.025329785944413, 'bidirectional': True, 'fc_dropout': 0.2029720962760044, 'learning_rate_model': 0.001197136586249885}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001617750536779763
Epoch 23: reducing lr to 0.00013726603347639249
Epoch 27: reducing lr to 0.00011113576038596787
Epoch 36: reducing lr to 4.7396920918722036e-05
Epoch 39: reducing lr to 2.9380884072921498e-05
Epoch 42: reducing lr to 1.498335161154319e-05
Epoch 45: reducing lr to 5.108994415031049e-06
Epoch 48: reducing lr to 3.782422008258117e-07
[I 2024-06-22 07:40:52,710] Trial 845 finished with value: 0.9761193990707397 and parameters: {'hidden_size': 178, 'n_layers': 4, 'rnn_dropout': 0.07130428312950941, 'bidirectional': True, 'fc_dropout': 0.12945755967454198, 'learning_rate_model': 0.0017393759449898303}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.477000663862844e-05
Epoch 26: reducing lr to 5.569885694710116e-05
Epoch 32: reducing lr to 3.5420694814082215e-05
Epoch 37: reducing lr to 1.9372322158780132e-05
Epoch 40: reducing lr to 1.1385774611477183e-05
Epoch 43: reducing lr to 5.262336234977953e-06
Epoch 46: reducing lr to 1.3867740997698403e-06
Epoch 49: reducing lr to 2.59861493515914e-09
[I 2024-06-22 07:41:47,361] Trial 846 finished with value: 0.9728681445121765 and parameters: {'hidden_size': 168, 'n_layers': 4, 'rnn_dropout': 0.09726916837740988, 'bidirectional': True, 'fc_dropout': 0.18319620034458858, 'learning_rate_model': 0.0008207375754282104}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.906831126901319e-05
Epoch 26: reducing lr to 6.799465967315655e-05
Epoch 36: reducing lr to 2.7301688564045535e-05
Epoch 39: reducing lr to 1.6924047620535962e-05
Epoch 42: reducing lr to 8.630746289309212e-06
Epoch 45: reducing lr to 2.9428952702184682e-06
Epoch 48: reducing lr to 2.1787598368328086e-07
[I 2024-06-22 07:42:51,064] Trial 847 finished with value: 0.9720832705497742 and parameters: {'hidden_size': 184, 'n_layers': 4, 'rnn_dropout': 0.05619731913310076, 'bidirectional': True, 'fc_dropout': 0.11277284018442786, 'learning_rate_model': 0.0010019195218891692}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00013173635353438897
Epoch 23: reducing lr to 0.00011177821489278899
Epoch 26: reducing lr to 9.612348561043201e-05
Epoch 32: reducing lr to 6.112801653194673e-05
Epoch 36: reducing lr to 3.859617035281053e-05
Epoch 39: reducing lr to 2.3925385548552034e-05
Epoch 42: reducing lr to 1.2201214341473138e-05
Epoch 45: reducing lr to 4.1603465995658374e-06
Epoch 48: reducing lr to 3.0800946843633546e-07
[I 2024-06-22 07:43:44,906] Trial 848 finished with value: 0.9749429821968079 and parameters: {'hidden_size': 164, 'n_layers': 4, 'rnn_dropout': 0.12612944615048532, 'bidirectional': True, 'fc_dropout': 0.1667863025500965, 'learning_rate_model': 0.0014164053060647301}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.687818097693992e-05
Epoch 26: reducing lr to 5.751177788046308e-05
Epoch 38: reducing lr to 1.7070103791145076e-05
Epoch 41: reducing lr to 9.412643929535794e-06
Epoch 44: reducing lr to 3.8262097301517055e-06
Epoch 47: reducing lr to 6.618108892115604e-07
[I 2024-06-22 07:44:51,473] Trial 849 finished with value: 0.9721482992172241 and parameters: {'hidden_size': 182, 'n_layers': 4, 'rnn_dropout': 0.044630224313264834, 'bidirectional': True, 'fc_dropout': 0.14662031253282753, 'learning_rate_model': 0.0008474514509517897}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.216149137765998e-05
Epoch 27: reducing lr to 7.461742109946374e-05
Epoch 36: reducing lr to 3.182266441267633e-05
Epoch 39: reducing lr to 1.9726555984589433e-05
Epoch 42: reducing lr to 1.0059939778132919e-05
Epoch 45: reducing lr to 3.4302189172704353e-06
Epoch 48: reducing lr to 2.539547799789007e-07
[I 2024-06-22 07:45:49,784] Trial 850 finished with value: 0.972043514251709 and parameters: {'hidden_size': 172, 'n_layers': 4, 'rnn_dropout': 0.02322818141079459, 'bidirectional': True, 'fc_dropout': 0.19441468425844943, 'learning_rate_model': 0.0011678306504300572}. Best is trial 153 with value: 0.9688200950622559.
Epoch 38: reducing lr to 1.3515060369843564e-05
Epoch 41: reducing lr to 7.452353688294951e-06
Epoch 44: reducing lr to 3.029358000594456e-06
Epoch 47: reducing lr to 5.239812382250428e-07
[I 2024-06-22 07:46:50,025] Trial 851 finished with value: 0.97079998254776 and parameters: {'hidden_size': 176, 'n_layers': 4, 'rnn_dropout': 0.08160197304299051, 'bidirectional': True, 'fc_dropout': 0.09557686122590447, 'learning_rate_model': 0.0006709600398602296}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.020597314947905e-05
Epoch 37: reducing lr to 2.3989127553732798e-05
Epoch 40: reducing lr to 1.409922864249836e-05
Epoch 43: reducing lr to 6.5164544620326205e-06
Epoch 46: reducing lr to 1.7172696435111598e-06
Epoch 49: reducing lr to 3.217915985065378e-09
[I 2024-06-22 07:47:53,918] Trial 852 finished with value: 0.9702489376068115 and parameters: {'hidden_size': 180, 'n_layers': 4, 'rnn_dropout': 0.06209030650535721, 'bidirectional': True, 'fc_dropout': 0.16941769644683632, 'learning_rate_model': 0.001016335482329627}. Best is trial 153 with value: 0.9688200950622559.
Epoch 22: reducing lr to 0.00016679181740905771
Epoch 27: reducing lr to 0.00012960214191894888
Epoch 36: reducing lr to 5.527242041711896e-05
Epoch 39: reducing lr to 3.4262828580995054e-05
Epoch 42: reducing lr to 1.747299388816634e-05
Epoch 45: reducing lr to 5.9579078501862274e-06
Epoch 48: reducing lr to 4.4109114132944923e-07
[I 2024-06-22 07:48:59,765] Trial 853 finished with value: 0.9874796867370605 and parameters: {'hidden_size': 185, 'n_layers': 4, 'rnn_dropout': 0.07277113079580953, 'bidirectional': True, 'fc_dropout': 0.14724331633681284, 'learning_rate_model': 0.0020283916472077375}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00013485717790659405
Epoch 23: reducing lr to 0.00011442623245179885
Epoch 30: reducing lr to 7.469159864712961e-05
Epoch 33: reducing lr to 5.660161481046753e-05
Epoch 36: reducing lr to 3.9510510744655275e-05
Epoch 39: reducing lr to 2.4492176144549774e-05
Epoch 42: reducing lr to 1.249026019757714e-05
Epoch 45: reducing lr to 4.258904899658413e-06
Epoch 48: reducing lr to 3.1530618972985814e-07
[I 2024-06-22 07:50:06,389] Trial 854 finished with value: 0.9791802167892456 and parameters: {'hidden_size': 182, 'n_layers': 4, 'rnn_dropout': 0.10121496461599531, 'bidirectional': True, 'fc_dropout': 0.12563831359287014, 'learning_rate_model': 0.001449959841934994}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.662669377238055e-05
Epoch 27: reducing lr to 7.013624010466976e-05
Epoch 36: reducing lr to 2.9911540751893362e-05
Epoch 39: reducing lr to 1.8541869265746018e-05
Epoch 42: reducing lr to 9.455785811427896e-06
Epoch 45: reducing lr to 3.2242156596723906e-06
Epoch 48: reducing lr to 2.3870341753821045e-07
[I 2024-06-22 07:51:12,487] Trial 855 finished with value: 0.9713273048400879 and parameters: {'hidden_size': 181, 'n_layers': 4, 'rnn_dropout': 0.06091947142433099, 'bidirectional': True, 'fc_dropout': 0.16656238075482427, 'learning_rate_model': 0.0010976960834786047}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001215182691293556
Epoch 23: reducing lr to 0.00010310817656414861
Epoch 27: reducing lr to 8.34802704955997e-05
Epoch 33: reducing lr to 5.100307131192161e-05
Epoch 36: reducing lr to 3.56024718345567e-05
Epoch 39: reducing lr to 2.206962134680304e-05
Epoch 42: reducing lr to 1.1254831398267349e-05
Epoch 45: reducing lr to 3.837650763769394e-06
Epoch 48: reducing lr to 2.841188212338681e-07
[I 2024-06-22 07:52:16,332] Trial 856 finished with value: 0.9742226004600525 and parameters: {'hidden_size': 184, 'n_layers': 4, 'rnn_dropout': 0.08980490349123652, 'bidirectional': True, 'fc_dropout': 0.13672015015109812, 'learning_rate_model': 0.0013065423215444516}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.992473732797037e-05
Epoch 26: reducing lr to 6.87311418046124e-05
Epoch 36: reducing lr to 2.7597405990717645e-05
Epoch 39: reducing lr to 1.7107359938361316e-05
Epoch 42: reducing lr to 8.724229960729293e-06
Epoch 45: reducing lr to 2.974771152702187e-06
Epoch 48: reducing lr to 2.2023590091249223e-07
[I 2024-06-22 07:53:17,906] Trial 857 finished with value: 0.972829282283783 and parameters: {'hidden_size': 178, 'n_layers': 4, 'rnn_dropout': 0.0340249341463836, 'bidirectional': True, 'fc_dropout': 0.15794877221111558, 'learning_rate_model': 0.0010127717833546305}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00015073911961398107
Epoch 23: reducing lr to 0.00012790205021549333
Epoch 27: reducing lr to 0.00010355432619146643
Epoch 30: reducing lr to 8.348792402008742e-05
Epoch 36: reducing lr to 4.416360847528055e-05
Epoch 39: reducing lr to 2.7376585560889582e-05
Epoch 42: reducing lr to 1.3961220716307651e-05
Epoch 45: reducing lr to 4.760470188237484e-06
Epoch 48: reducing lr to 3.5243935982136753e-07
[I 2024-06-22 07:54:24,894] Trial 858 finished with value: 0.9747357368469238 and parameters: {'hidden_size': 185, 'n_layers': 4, 'rnn_dropout': 0.017890387261349245, 'bidirectional': True, 'fc_dropout': 0.1689776780139323, 'learning_rate_model': 0.0016207195897299067}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.224976318405612e-05
Epoch 27: reducing lr to 7.468888928482632e-05
Epoch 36: reducing lr to 3.185314399834769e-05
Epoch 39: reducing lr to 1.9745449979301373e-05
Epoch 42: reducing lr to 1.0069575137144362e-05
Epoch 45: reducing lr to 3.4335043634544684e-06
Epoch 48: reducing lr to 2.5419801657194236e-07
[I 2024-06-22 07:55:27,502] Trial 859 finished with value: 0.9723610281944275 and parameters: {'hidden_size': 179, 'n_layers': 4, 'rnn_dropout': 0.0676252381747667, 'bidirectional': True, 'fc_dropout': 0.10872279291618604, 'learning_rate_model': 0.0011689491926708268}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.552828541722983e-05
Epoch 31: reducing lr to 4.5293548806355986e-05
Epoch 36: reducing lr to 2.6079344469895956e-05
Epoch 39: reducing lr to 1.6166328565557368e-05
Epoch 42: reducing lr to 8.244332762904333e-06
Epoch 45: reducing lr to 2.8111367291738842e-06
Epoch 48: reducing lr to 2.0812129685182777e-07
[I 2024-06-22 07:56:34,750] Trial 860 finished with value: 0.9716763496398926 and parameters: {'hidden_size': 187, 'n_layers': 4, 'rnn_dropout': 0.04290380731460686, 'bidirectional': True, 'fc_dropout': 0.13987701372587225, 'learning_rate_model': 0.0009570618418404988}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.89471988783144e-05
Epoch 27: reducing lr to 8.011138594817986e-05
Epoch 36: reducing lr to 3.416571777876204e-05
Epoch 39: reducing lr to 2.1178991670099115e-05
Epoch 42: reducing lr to 1.0800637522800244e-05
Epoch 45: reducing lr to 3.6827806096634665e-06
Epoch 48: reducing lr to 2.7265307608469354e-07
[I 2024-06-22 07:57:32,346] Trial 861 finished with value: 0.9718202352523804 and parameters: {'hidden_size': 171, 'n_layers': 4, 'rnn_dropout': 0.07919260281706897, 'bidirectional': True, 'fc_dropout': 0.1758804434812118, 'learning_rate_model': 0.0012538162077996102}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.269100702156765e-05
Epoch 26: reducing lr to 6.251050774202544e-05
Epoch 33: reducing lr to 3.5957037922689045e-05
Epoch 36: reducing lr to 2.509965374570287e-05
Epoch 39: reducing lr to 1.5559027942713242e-05
Epoch 42: reducing lr to 7.934628033005854e-06
Epoch 45: reducing lr to 2.7055342060280605e-06
Epoch 48: reducing lr to 2.0030305953882978e-07
[I 2024-06-22 07:58:39,777] Trial 862 finished with value: 0.9728686809539795 and parameters: {'hidden_size': 183, 'n_layers': 4, 'rnn_dropout': 0.04171326524656627, 'bidirectional': True, 'fc_dropout': 0.2099447613426136, 'learning_rate_model': 0.0009211090743155096}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00016255917711270154
Epoch 23: reducing lr to 0.00013793136172814423
Epoch 27: reducing lr to 0.0001116744352445039
Epoch 30: reducing lr to 9.003454619018738e-05
Epoch 36: reducing lr to 4.762665372103757e-05
Epoch 39: reducing lr to 2.952329317253697e-05
Epoch 42: reducing lr to 1.5055975893607888e-05
Epoch 45: reducing lr to 5.133757703051239e-06
Epoch 48: reducing lr to 3.800755401876235e-07
[I 2024-06-22 07:59:50,048] Trial 863 finished with value: 0.9803688526153564 and parameters: {'hidden_size': 189, 'n_layers': 4, 'rnn_dropout': 0.05978528908780718, 'bidirectional': True, 'fc_dropout': 0.15376521083825095, 'learning_rate_model': 0.0017478066975023824}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.38704442038317e-05
Epoch 37: reducing lr to 2.5085148960716965e-05
Epoch 40: reducing lr to 1.4743397813700172e-05
Epoch 43: reducing lr to 6.814179903361301e-06
Epoch 46: reducing lr to 1.7957286990410473e-06
Epoch 49: reducing lr to 3.3649369551814677e-09
[I 2024-06-22 08:00:53,541] Trial 864 finished with value: 0.9700788259506226 and parameters: {'hidden_size': 180, 'n_layers': 4, 'rnn_dropout': 0.10935532366609771, 'bidirectional': True, 'fc_dropout': 0.1910624276401293, 'learning_rate_model': 0.001062770078286308}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00014044506375203818
Epoch 23: reducing lr to 0.00011916755015242412
Epoch 27: reducing lr to 9.648254534724595e-05
Epoch 30: reducing lr to 7.77864886139282e-05
Epoch 36: reducing lr to 4.114765180873157e-05
Epoch 39: reducing lr to 2.550702375241716e-05
Epoch 42: reducing lr to 1.3007801415978558e-05
Epoch 45: reducing lr to 4.435375108922165e-06
Epoch 48: reducing lr to 3.2837108565845084e-07
[I 2024-06-22 08:01:55,202] Trial 865 finished with value: 0.9741334319114685 and parameters: {'hidden_size': 174, 'n_layers': 4, 'rnn_dropout': 0.12534948883288213, 'bidirectional': True, 'fc_dropout': 0.07746711142406373, 'learning_rate_model': 0.0015100397739266213}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.919603178416615e-05
Epoch 27: reducing lr to 8.031285045837599e-05
Epoch 32: reducing lr to 5.424721334673237e-05
Epoch 35: reducing lr to 3.9034925338564995e-05
Epoch 38: reducing lr to 2.5318968510960254e-05
Epoch 41: reducing lr to 1.3961159121974568e-05
Epoch 44: reducing lr to 5.675166645693983e-06
Epoch 47: reducing lr to 9.81620807299975e-07
[I 2024-06-22 08:02:57,462] Trial 866 finished with value: 0.9735211133956909 and parameters: {'hidden_size': 179, 'n_layers': 4, 'rnn_dropout': 0.11101335786947217, 'bidirectional': True, 'fc_dropout': 0.202718706723289, 'learning_rate_model': 0.0012569693110094798}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.529573674744314e-05
Epoch 26: reducing lr to 7.334992361202016e-05
Epoch 37: reducing lr to 2.5511445448216182e-05
Epoch 40: reducing lr to 1.4993946802331896e-05
Epoch 43: reducing lr to 6.929979931598719e-06
Epoch 46: reducing lr to 1.826245274330336e-06
Epoch 49: reducing lr to 3.422120622184437e-09
[I 2024-06-22 08:04:04,388] Trial 867 finished with value: 0.9696178436279297 and parameters: {'hidden_size': 186, 'n_layers': 4, 'rnn_dropout': 0.10795427233548853, 'bidirectional': True, 'fc_dropout': 0.21280593575136544, 'learning_rate_model': 0.0010808307703755674}. Best is trial 153 with value: 0.9688200950622559.
Epoch 16: reducing lr to 0.00021725462489875858
Epoch 23: reducing lr to 0.00017737812659729884
Epoch 27: reducing lr to 0.00014361202459179094
Epoch 32: reducing lr to 9.700256052080599e-05
Epoch 35: reducing lr to 6.980059387340633e-05
Epoch 38: reducing lr to 4.527430302476067e-05
Epoch 41: reducing lr to 2.4964751166365976e-05
Epoch 44: reducing lr to 1.0148091709262854e-05
Epoch 47: reducing lr to 1.755292592819143e-06
[I 2024-06-22 08:05:11,757] Trial 868 finished with value: 0.9993155002593994 and parameters: {'hidden_size': 183, 'n_layers': 4, 'rnn_dropout': 0.13660613766454133, 'bidirectional': True, 'fc_dropout': 0.19951695400554897, 'learning_rate_model': 0.0022476590803781333}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 0.00010567961372681682
Epoch 27: reducing lr to 8.556220305473506e-05
Epoch 30: reducing lr to 6.89822527976014e-05
Epoch 36: reducing lr to 3.6490369596004055e-05
Epoch 39: reducing lr to 2.2620020416869777e-05
Epoch 42: reducing lr to 1.1535518077844704e-05
Epoch 45: reducing lr to 3.933358767927125e-06
Epoch 48: reducing lr to 2.912045220955983e-07
[I 2024-06-22 08:06:15,009] Trial 869 finished with value: 0.9734479188919067 and parameters: {'hidden_size': 175, 'n_layers': 4, 'rnn_dropout': 0.13949458383977448, 'bidirectional': True, 'fc_dropout': 0.21411638369593744, 'learning_rate_model': 0.0013391264636772337}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.622179069714541e-05
Epoch 37: reducing lr to 2.5788422653886937e-05
Epoch 40: reducing lr to 1.5156735755067474e-05
Epoch 43: reducing lr to 7.005218572259239e-06
Epoch 46: reducing lr to 1.8460727793606072e-06
Epoch 49: reducing lr to 3.459274511004133e-09
[I 2024-06-22 08:07:18,453] Trial 870 finished with value: 0.9699113368988037 and parameters: {'hidden_size': 180, 'n_layers': 4, 'rnn_dropout': 0.1011347339744821, 'bidirectional': True, 'fc_dropout': 0.19246818488098152, 'learning_rate_model': 0.001092565326427644}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001382242880691794
Epoch 23: reducing lr to 0.00011728322335236229
Epoch 27: reducing lr to 9.495692326553446e-05
Epoch 37: reducing lr to 3.507871165242542e-05
Epoch 40: reducing lr to 2.0616955533877212e-05
Epoch 43: reducing lr to 9.528851208022936e-06
Epoch 46: reducing lr to 2.511121195185686e-06
Epoch 49: reducing lr to 4.7054794597659735e-09
[I 2024-06-22 08:08:19,460] Trial 871 finished with value: 0.9735270738601685 and parameters: {'hidden_size': 178, 'n_layers': 4, 'rnn_dropout': 0.12105664928746426, 'bidirectional': True, 'fc_dropout': 0.2007548615535097, 'learning_rate_model': 0.0014861623978160126}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00018281316454001216
Epoch 23: reducing lr to 0.00015511685759428558
Epoch 27: reducing lr to 0.00012558846118612186
Epoch 30: reducing lr to 0.00010125235990546849
Epoch 33: reducing lr to 7.672947396795877e-05
Epoch 36: reducing lr to 5.356067518204682e-05
Epoch 39: reducing lr to 3.3201734582921336e-05
Epoch 42: reducing lr to 1.6931868426230708e-05
Epoch 45: reducing lr to 5.773395930921748e-06
Epoch 48: reducing lr to 4.274308808649083e-07
[I 2024-06-22 08:09:17,359] Trial 872 finished with value: 0.988107442855835 and parameters: {'hidden_size': 172, 'n_layers': 4, 'rnn_dropout': 0.10905752765108892, 'bidirectional': True, 'fc_dropout': 0.21481249716653095, 'learning_rate_model': 0.0019655738854602784}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.875330961482665e-05
Epoch 27: reducing lr to 7.185802853778376e-05
Epoch 36: reducing lr to 3.0645845083097436e-05
Epoch 39: reducing lr to 1.8997057282417973e-05
Epoch 42: reducing lr to 9.687917767914616e-06
Epoch 45: reducing lr to 3.3033675677359355e-06
Epoch 48: reducing lr to 2.4456339495712313e-07
[I 2024-06-22 08:10:17,011] Trial 873 finished with value: 0.9696825742721558 and parameters: {'hidden_size': 176, 'n_layers': 4, 'rnn_dropout': 0.09880374036177715, 'bidirectional': True, 'fc_dropout': 0.20864027070228508, 'learning_rate_model': 0.0011246436417849442}. Best is trial 153 with value: 0.9688200950622559.
Epoch 27: reducing lr to 7.201959500390738e-05
Epoch 32: reducing lr to 4.864554443061446e-05
Epoch 36: reducing lr to 3.071474957424757e-05
Epoch 39: reducing lr to 1.9039770497271263e-05
Epoch 42: reducing lr to 9.709700200159267e-06
Epoch 45: reducing lr to 3.3107948995885883e-06
Epoch 48: reducing lr to 2.4511327427153597e-07
[I 2024-06-22 08:11:12,579] Trial 874 finished with value: 0.9730139970779419 and parameters: {'hidden_size': 168, 'n_layers': 4, 'rnn_dropout': 0.13683060961871013, 'bidirectional': True, 'fc_dropout': 0.21536618556266598, 'learning_rate_model': 0.0011271723042399133}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.516234665645104e-05
Epoch 37: reducing lr to 2.547154926864799e-05
Epoch 40: reducing lr to 1.4970498456558007e-05
Epoch 43: reducing lr to 6.919142453796199e-06
Epoch 46: reducing lr to 1.8233892930984021e-06
Epoch 49: reducing lr to 3.416768924692157e-09
[I 2024-06-22 08:12:16,019] Trial 875 finished with value: 0.9700803756713867 and parameters: {'hidden_size': 180, 'n_layers': 4, 'rnn_dropout': 0.10981912720104936, 'bidirectional': True, 'fc_dropout': 0.21353328972642857, 'learning_rate_model': 0.0010791405087012432}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00015291664698329467
Epoch 23: reducing lr to 0.00012974968084813058
Epoch 26: reducing lr to 0.0001115780171648457
Epoch 29: reducing lr to 9.156089027117293e-05
Epoch 33: reducing lr to 6.418144947873384e-05
Epoch 36: reducing lr to 4.4801581328172645e-05
Epoch 39: reducing lr to 2.7772058643722058e-05
Epoch 42: reducing lr to 1.416289988424122e-05
Epoch 45: reducing lr to 4.82923836303004e-06
Epoch 48: reducing lr to 3.5753058202035986e-07
[I 2024-06-22 08:13:21,556] Trial 876 finished with value: 0.9761312007904053 and parameters: {'hidden_size': 181, 'n_layers': 4, 'rnn_dropout': 0.12479044460158405, 'bidirectional': True, 'fc_dropout': 0.21318603771594283, 'learning_rate_model': 0.0016441319678415554}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.10033816448981e-05
Epoch 27: reducing lr to 7.367977175897327e-05
Epoch 37: reducing lr to 2.7218567949196007e-05
Epoch 40: reducing lr to 1.5997280933936158e-05
Epoch 43: reducing lr to 7.393706093120392e-06
Epoch 46: reducing lr to 1.9484502041311904e-06
Epoch 49: reducing lr to 3.651115060288035e-09
[I 2024-06-22 08:13:48,296] Trial 877 finished with value: 0.973690390586853 and parameters: {'hidden_size': 173, 'n_layers': 2, 'rnn_dropout': 0.11541442759335716, 'bidirectional': True, 'fc_dropout': 0.21784609705365407, 'learning_rate_model': 0.001153155583628691}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 0.00011575757496915746
Epoch 27: reducing lr to 9.372170076470908e-05
Epoch 30: reducing lr to 7.556063102578715e-05
Epoch 36: reducing lr to 3.997021322467069e-05
Epoch 39: reducing lr to 2.4777141180496506e-05
Epoch 42: reducing lr to 1.2635583643937315e-05
Epoch 45: reducing lr to 4.308457008897807e-06
Epoch 48: reducing lr to 3.189747587929075e-07
[I 2024-06-22 08:14:49,113] Trial 878 finished with value: 0.9742305278778076 and parameters: {'hidden_size': 177, 'n_layers': 4, 'rnn_dropout': 0.13245223833739647, 'bidirectional': True, 'fc_dropout': 0.2112515727274365, 'learning_rate_model': 0.00146683003983165}. Best is trial 153 with value: 0.9688200950622559.
Epoch 22: reducing lr to 8.598124453346913e-05
Epoch 36: reducing lr to 2.849295348935346e-05
Epoch 39: reducing lr to 1.7662501005105595e-05
Epoch 42: reducing lr to 9.00733491347312e-06
Epoch 45: reducing lr to 3.0713037349931664e-06
Epoch 48: reducing lr to 2.2738264906113115e-07
[I 2024-06-22 08:15:56,565] Trial 879 finished with value: 0.9722104072570801 and parameters: {'hidden_size': 183, 'n_layers': 4, 'rnn_dropout': 0.14772959179445988, 'bidirectional': True, 'fc_dropout': 0.22026402838838227, 'learning_rate_model': 0.0010456366561465602}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.588943002045637e-05
Epoch 27: reducing lr to 7.76357008970705e-05
Epoch 33: reducing lr to 4.743227550290484e-05
Epoch 36: reducing lr to 3.3109893369232836e-05
Epoch 39: reducing lr to 2.0524496526185986e-05
Epoch 42: reducing lr to 1.0466865031647171e-05
Epoch 45: reducing lr to 3.5689715075745786e-06
Epoch 48: reducing lr to 2.6422726823460697e-07
[I 2024-06-22 08:16:52,875] Trial 880 finished with value: 0.9708119034767151 and parameters: {'hidden_size': 170, 'n_layers': 4, 'rnn_dropout': 0.11303866613487824, 'bidirectional': True, 'fc_dropout': 0.19864931312893797, 'learning_rate_model': 0.0012150694802808028}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.583108724731586e-05
Epoch 26: reducing lr to 5.661133141020205e-05
Epoch 32: reducing lr to 3.600096667700097e-05
Epoch 37: reducing lr to 1.96896850317317e-05
Epoch 40: reducing lr to 1.1572299598614032e-05
Epoch 43: reducing lr to 5.3485453188596605e-06
Epoch 46: reducing lr to 1.4094926261721164e-06
Epoch 49: reducing lr to 2.641186181632886e-09
[I 2024-06-22 08:17:55,083] Trial 881 finished with value: 0.9735854864120483 and parameters: {'hidden_size': 179, 'n_layers': 4, 'rnn_dropout': 0.12551531674539992, 'bidirectional': True, 'fc_dropout': 0.1911970712409484, 'learning_rate_model': 0.000834183131038029}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.588267195998875e-05
Epoch 36: reducing lr to 2.9654635659565445e-05
Epoch 39: reducing lr to 1.8382616331396673e-05
Epoch 42: reducing lr to 9.374571689190987e-06
Epoch 45: reducing lr to 3.1965234244711202e-06
Epoch 48: reducing lr to 2.3665323483346416e-07
[I 2024-06-22 08:18:58,308] Trial 882 finished with value: 0.971967339515686 and parameters: {'hidden_size': 175, 'n_layers': 4, 'rnn_dropout': 0.112905480978397, 'bidirectional': True, 'fc_dropout': 0.21000426944321965, 'learning_rate_model': 0.0010882681601224265}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00012715680417411856
Epoch 23: reducing lr to 0.00010789247007922231
Epoch 27: reducing lr to 8.735381505896623e-05
Epoch 33: reducing lr to 5.3369650485927814e-05
Epoch 36: reducing lr to 3.72544521216166e-05
Epoch 39: reducing lr to 2.3093667642723622e-05
Epoch 42: reducing lr to 1.1777063666030099e-05
Epoch 45: reducing lr to 4.015720518022077e-06
Epoch 48: reducing lr to 2.9730213878670363e-07
[I 2024-06-22 08:20:05,168] Trial 883 finished with value: 0.9748558402061462 and parameters: {'hidden_size': 185, 'n_layers': 4, 'rnn_dropout': 0.13366728178859555, 'bidirectional': True, 'fc_dropout': 0.20372259717260469, 'learning_rate_model': 0.0013671668245124145}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00015290259907618203
Epoch 23: reducing lr to 0.0001297377612075917
Epoch 26: reducing lr to 0.00011156776689025582
Epoch 29: reducing lr to 9.15524788986569e-05
Epoch 32: reducing lr to 7.094953179849931e-05
Epoch 35: reducing lr to 5.105349207264649e-05
Epoch 38: reducing lr to 3.311449290476381e-05
Epoch 41: reducing lr to 1.8259697447262686e-05
Epoch 44: reducing lr to 7.422508762188807e-06
Epoch 47: reducing lr to 1.2838546421996667e-06
[I 2024-06-22 08:21:11,318] Trial 884 finished with value: 0.9815058708190918 and parameters: {'hidden_size': 182, 'n_layers': 4, 'rnn_dropout': 0.11473522877799444, 'bidirectional': True, 'fc_dropout': 0.22115780952139233, 'learning_rate_model': 0.0016439809273000527}. Best is trial 153 with value: 0.9688200950622559.
Epoch 27: reducing lr to 6.690881908585481e-05
Epoch 32: reducing lr to 4.519347729550975e-05
Epoch 38: reducing lr to 2.109329047433426e-05
Epoch 41: reducing lr to 1.163107354040635e-05
Epoch 44: reducing lr to 4.727994289974972e-06
Epoch 47: reducing lr to 8.177905357821319e-07
[I 2024-06-22 08:22:06,846] Trial 885 finished with value: 0.9712111949920654 and parameters: {'hidden_size': 166, 'n_layers': 4, 'rnn_dropout': 0.09678926737656122, 'bidirectional': True, 'fc_dropout': 0.19161526327749287, 'learning_rate_model': 0.0010471840028936944}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 3.6071468657321854e-05
Epoch 37: reducing lr to 1.9728244045968338e-05
Epoch 40: reducing lr to 1.1594962046705735e-05
Epoch 43: reducing lr to 5.3590195664041456e-06
Epoch 46: reducing lr to 1.4122528859808145e-06
Epoch 49: reducing lr to 2.6463585109728903e-09
[I 2024-06-22 08:23:07,531] Trial 886 finished with value: 0.972887396812439 and parameters: {'hidden_size': 177, 'n_layers': 4, 'rnn_dropout': 0.1073581457001215, 'bidirectional': True, 'fc_dropout': 0.193562928547031, 'learning_rate_model': 0.0008358167416912125}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00012005874240292105
Epoch 23: reducing lr to 0.00010186976903507894
Epoch 27: reducing lr to 8.247760902921052e-05
Epoch 36: reducing lr to 3.517485910157449e-05
Epoch 39: reducing lr to 2.1804548428724665e-05
Epoch 42: reducing lr to 1.1119652323178628e-05
Epoch 45: reducing lr to 3.7915576627354717e-06
Epoch 48: reducing lr to 2.807063383533301e-07
[I 2024-06-22 08:24:09,124] Trial 887 finished with value: 0.9734763503074646 and parameters: {'hidden_size': 173, 'n_layers': 4, 'rnn_dropout': 0.13237657026067579, 'bidirectional': True, 'fc_dropout': 0.21781827832984416, 'learning_rate_model': 0.0012908497557173161}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.047109298637185e-05
Epoch 27: reducing lr to 6.515243342897681e-05
Epoch 33: reducing lr to 3.980550360696886e-05
Epoch 38: reducing lr to 2.05395824078693e-05
Epoch 41: reducing lr to 1.1325752791668429e-05
Epoch 44: reducing lr to 4.603882379614435e-06
Epoch 47: reducing lr to 7.963231778612733e-07
[I 2024-06-22 08:25:11,323] Trial 888 finished with value: 0.9723408222198486 and parameters: {'hidden_size': 179, 'n_layers': 4, 'rnn_dropout': 0.09230711642091348, 'bidirectional': True, 'fc_dropout': 0.189217396220253, 'learning_rate_model': 0.0010196949665017276}. Best is trial 153 with value: 0.9688200950622559.
Epoch 32: reducing lr to 3.6337393513098864e-05
Epoch 37: reducing lr to 1.9873683936495028e-05
Epoch 40: reducing lr to 1.1680442031990006e-05
Epoch 43: reducing lr to 5.398527148389244e-06
Epoch 46: reducing lr to 1.4226642487282773e-06
Epoch 49: reducing lr to 2.6658679052899563e-09
[I 2024-06-22 08:26:18,033] Trial 889 finished with value: 0.9706817865371704 and parameters: {'hidden_size': 186, 'n_layers': 4, 'rnn_dropout': 0.09710514490543379, 'bidirectional': True, 'fc_dropout': 0.20465342323986838, 'learning_rate_model': 0.0008419785214790486}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001688477748057394
Epoch 23: reducing lr to 0.00014326723300017843
Epoch 27: reducing lr to 0.00011599455797349023
Epoch 30: reducing lr to 9.351753035338098e-05
Epoch 33: reducing lr to 7.086798685479329e-05
Epoch 36: reducing lr to 4.946908962676058e-05
Epoch 39: reducing lr to 3.0665401029092975e-05
Epoch 42: reducing lr to 1.563841594376472e-05
Epoch 45: reducing lr to 5.332356991147032e-06
Epoch 48: reducing lr to 3.9477875293546845e-07
[I 2024-06-22 08:27:24,134] Trial 890 finished with value: 0.9925799369812012 and parameters: {'hidden_size': 182, 'n_layers': 4, 'rnn_dropout': 0.10849145866478739, 'bidirectional': True, 'fc_dropout': 0.18801681219937152, 'learning_rate_model': 0.001815420555797011}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.079903622135454e-05
Epoch 27: reducing lr to 7.351432599317271e-05
Epoch 37: reducing lr to 2.715744945342921e-05
Epoch 40: reducing lr to 1.5961359508941793e-05
Epoch 43: reducing lr to 7.3771037430115685e-06
Epoch 46: reducing lr to 1.9440750163632295e-06
Epoch 49: reducing lr to 3.6429165885254467e-09
[I 2024-06-22 08:28:21,479] Trial 891 finished with value: 0.9718422889709473 and parameters: {'hidden_size': 171, 'n_layers': 4, 'rnn_dropout': 0.10741903344148028, 'bidirectional': True, 'fc_dropout': 0.22063802495548446, 'learning_rate_model': 0.0011505662065979811}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00013126429723668895
Epoch 23: reducing lr to 0.00011137767541473194
Epoch 27: reducing lr to 9.017556881154124e-05
Epoch 30: reducing lr to 7.270165636041312e-05
Epoch 33: reducing lr to 5.509362798399846e-05
Epoch 36: reducing lr to 3.845786710702204e-05
Epoch 39: reducing lr to 2.3839652729781206e-05
Epoch 42: reducing lr to 1.2157493227938774e-05
Epoch 45: reducing lr to 4.145438658361691e-06
Epoch 48: reducing lr to 3.069057654308703e-07
[I 2024-06-22 08:29:31,707] Trial 892 finished with value: 0.9759094715118408 and parameters: {'hidden_size': 189, 'n_layers': 4, 'rnn_dropout': 0.11729954644494675, 'bidirectional': True, 'fc_dropout': 0.18700531842961632, 'learning_rate_model': 0.0014113298426342883}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 5.956067104582436e-05
Epoch 32: reducing lr to 3.257187178946323e-05
Epoch 37: reducing lr to 1.7814240994761628e-05
Epoch 40: reducing lr to 1.0470037158088694e-05
Epoch 43: reducing lr to 4.839095959535027e-06
Epoch 46: reducing lr to 1.2752383434526306e-06
Epoch 49: reducing lr to 2.389612991575245e-09
[I 2024-06-22 08:30:34,828] Trial 893 finished with value: 0.9733546376228333 and parameters: {'hidden_size': 175, 'n_layers': 4, 'rnn_dropout': 0.14241131315308006, 'bidirectional': True, 'fc_dropout': 0.2088665377111419, 'learning_rate_model': 0.0007547271226597832}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.445826658067256e-05
Epoch 26: reducing lr to 6.403025959136794e-05
Epoch 29: reducing lr to 5.254321345232757e-05
Epoch 37: reducing lr to 2.2270022846112624e-05
Epoch 40: reducing lr to 1.3088852159283544e-05
Epoch 43: reducing lr to 6.049473429997144e-06
Epoch 46: reducing lr to 1.5942069634782658e-06
Epoch 49: reducing lr to 2.987314246579581e-09
[I 2024-06-22 08:31:03,485] Trial 894 finished with value: 0.9748426675796509 and parameters: {'hidden_size': 185, 'n_layers': 2, 'rnn_dropout': 0.10499572070181529, 'bidirectional': True, 'fc_dropout': 0.1813167444161053, 'learning_rate_model': 0.0009435030248640194}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.356055431566037e-05
Epoch 37: reducing lr to 2.7983403022782227e-05
Epoch 40: reducing lr to 1.644680059871514e-05
Epoch 43: reducing lr to 7.601467418196774e-06
Epoch 46: reducing lr to 2.0032011762630704e-06
Epoch 49: reducing lr to 3.7537104966445936e-09
[I 2024-06-22 08:32:06,863] Trial 895 finished with value: 0.9702348113059998 and parameters: {'hidden_size': 180, 'n_layers': 4, 'rnn_dropout': 0.1223928883097889, 'bidirectional': True, 'fc_dropout': 0.22358321260629233, 'learning_rate_model': 0.0011855589722752657}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.291296315870433e-05
Epoch 36: reducing lr to 2.8629217720098887e-05
Epoch 39: reducing lr to 1.7746969858550366e-05
Epoch 42: reducing lr to 9.050411443377598e-06
Epoch 45: reducing lr to 3.0859918873109162e-06
Epoch 48: reducing lr to 2.2847008009107585e-07
[I 2024-06-22 08:33:12,256] Trial 896 finished with value: 0.9716944694519043 and parameters: {'hidden_size': 181, 'n_layers': 4, 'rnn_dropout': 0.13146805529975739, 'bidirectional': True, 'fc_dropout': 0.21347294768636582, 'learning_rate_model': 0.001050637291642009}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.172499905755892e-05
Epoch 32: reducing lr to 3.375547521888604e-05
Epoch 37: reducing lr to 1.8461578577024417e-05
Epoch 40: reducing lr to 1.0850499538838547e-05
Epoch 43: reducing lr to 5.014940031685181e-06
Epoch 46: reducing lr to 1.3215782187412946e-06
Epoch 49: reducing lr to 2.476447243842175e-09
[I 2024-06-22 08:34:20,807] Trial 897 finished with value: 0.9727483987808228 and parameters: {'hidden_size': 187, 'n_layers': 4, 'rnn_dropout': 0.1260180288031793, 'bidirectional': True, 'fc_dropout': 0.22414631363758786, 'learning_rate_model': 0.0007821525533022897}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.023196642392006e-05
Epoch 26: reducing lr to 6.0395859966247675e-05
Epoch 37: reducing lr to 2.1005961710020596e-05
Epoch 40: reducing lr to 1.2345920306679159e-05
Epoch 43: reducing lr to 5.706101341449181e-06
Epoch 46: reducing lr to 1.5037187282687594e-06
Epoch 49: reducing lr to 2.8177523262156586e-09
[I 2024-06-22 08:35:26,919] Trial 898 finished with value: 0.9726086854934692 and parameters: {'hidden_size': 182, 'n_layers': 4, 'rnn_dropout': 0.15273452888562633, 'bidirectional': True, 'fc_dropout': 0.2205422066715913, 'learning_rate_model': 0.0008899491729547898}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.835750813747742e-05
Epoch 27: reducing lr to 7.15375727262981e-05
Epoch 33: reducing lr to 4.370656565413839e-05
Epoch 36: reducing lr to 3.050917783304054e-05
Epoch 39: reducing lr to 1.8912338601274685e-05
Epoch 42: reducing lr to 9.64471383353043e-06
Epoch 45: reducing lr to 3.2886359732837252e-06
Epoch 48: reducing lr to 2.434727477074617e-07
[I 2024-06-22 08:36:35,401] Trial 899 finished with value: 0.9740349054336548 and parameters: {'hidden_size': 188, 'n_layers': 4, 'rnn_dropout': 0.12027622680156162, 'bidirectional': True, 'fc_dropout': 0.22226412426496037, 'learning_rate_model': 0.0011196282162549925}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00013902235288955643
Epoch 23: reducing lr to 0.00011796038086125898
Epoch 27: reducing lr to 9.550517553702772e-05
Epoch 32: reducing lr to 6.450885012180441e-05
Epoch 36: reducing lr to 4.0730825402520114e-05
Epoch 39: reducing lr to 2.5248637171980176e-05
Epoch 42: reducing lr to 1.287603216843711e-05
Epoch 45: reducing lr to 4.390444684327223e-06
Epoch 48: reducing lr to 3.2504468102725894e-07
[I 2024-06-22 08:37:37,606] Trial 900 finished with value: 0.9740408658981323 and parameters: {'hidden_size': 179, 'n_layers': 4, 'rnn_dropout': 0.10229327921378484, 'bidirectional': True, 'fc_dropout': 0.20130017732849542, 'learning_rate_model': 0.0014947430455707006}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.526843133535e-05
Epoch 27: reducing lr to 6.09401622357783e-05
Epoch 36: reducing lr to 2.598961882504891e-05
Epoch 39: reducing lr to 1.6110708522767254e-05
Epoch 42: reducing lr to 8.215968243453352e-06
Epoch 45: reducing lr to 2.8014650498849588e-06
Epoch 48: reducing lr to 2.0740525824172825e-07
[I 2024-06-22 08:38:42,131] Trial 901 finished with value: 0.9725947380065918 and parameters: {'hidden_size': 184, 'n_layers': 4, 'rnn_dropout': 0.09834550463660838, 'bidirectional': True, 'fc_dropout': 0.23084710575100295, 'learning_rate_model': 0.0009537690830437138}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00011446965016955154
Epoch 23: reducing lr to 9.712742771504148e-05
Epoch 26: reducing lr to 8.352456611782373e-05
Epoch 33: reducing lr to 4.804465840797576e-05
Epoch 36: reducing lr to 3.3537364589473955e-05
Epoch 39: reducing lr to 2.0789481721911086e-05
Epoch 42: reducing lr to 1.0601999371020651e-05
Epoch 45: reducing lr to 3.6150493547103343e-06
Epoch 48: reducing lr to 2.6763862179934177e-07
[I 2024-06-22 08:39:52,370] Trial 902 finished with value: 0.9732985496520996 and parameters: {'hidden_size': 189, 'n_layers': 4, 'rnn_dropout': 0.11839668420643831, 'bidirectional': True, 'fc_dropout': 0.2076333977908726, 'learning_rate_model': 0.0012307568528621964}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 5.897637526065929e-05
Epoch 32: reducing lr to 3.225233866353726e-05
Epoch 37: reducing lr to 1.7639481615016804e-05
Epoch 40: reducing lr to 1.0367325108768664e-05
Epoch 43: reducing lr to 4.791623973012416e-06
Epoch 46: reducing lr to 1.2627281353559672e-06
Epoch 49: reducing lr to 2.366170663364257e-09
[I 2024-06-22 08:40:54,588] Trial 903 finished with value: 0.9747616052627563 and parameters: {'hidden_size': 179, 'n_layers': 4, 'rnn_dropout': 0.14694929173776392, 'bidirectional': True, 'fc_dropout': 0.19582336697720665, 'learning_rate_model': 0.0007473231785977596}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 7.120049217015149e-05
Epoch 27: reducing lr to 5.764660518543963e-05
Epoch 38: reducing lr to 1.8173338053918914e-05
Epoch 41: reducing lr to 1.0020979497579654e-05
Epoch 44: reducing lr to 4.07349619791482e-06
Epoch 47: reducing lr to 7.045834732209038e-07
[I 2024-06-22 08:42:01,373] Trial 904 finished with value: 0.9724955558776855 and parameters: {'hidden_size': 185, 'n_layers': 4, 'rnn_dropout': 0.1034800797878532, 'bidirectional': True, 'fc_dropout': 0.19801634692439327, 'learning_rate_model': 0.0009022219132856161}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.519754086153093e-05
Epoch 36: reducing lr to 3.287098928699498e-05
Epoch 39: reducing lr to 2.0376402240550536e-05
Epoch 42: reducing lr to 1.0391341478719739e-05
Epoch 45: reducing lr to 3.5432196317517968e-06
Epoch 48: reducing lr to 2.6232073920065545e-07
[I 2024-06-22 08:43:00,971] Trial 905 finished with value: 0.9700944423675537 and parameters: {'hidden_size': 176, 'n_layers': 4, 'rnn_dropout': 0.13233872521908663, 'bidirectional': True, 'fc_dropout': 0.22628975590594513, 'learning_rate_model': 0.0012063021594137572}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.0001702313914964526
Epoch 23: reducing lr to 0.00014444123091065966
Epoch 27: reducing lr to 0.00011694507098219684
Epoch 33: reducing lr to 7.144871188692564e-05
Epoch 36: reducing lr to 4.98744622066523e-05
Epoch 39: reducing lr to 3.0916687495497895e-05
Epoch 42: reducing lr to 1.5766564350464213e-05
Epoch 45: reducing lr to 5.376052788395673e-06
Epoch 48: reducing lr to 3.9801375246290537e-07
[I 2024-06-22 08:43:56,525] Trial 906 finished with value: 0.9824361801147461 and parameters: {'hidden_size': 168, 'n_layers': 4, 'rnn_dropout': 0.1599285326933825, 'bidirectional': True, 'fc_dropout': 0.22818544333366358, 'learning_rate_model': 0.0018302969507305806}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.000137236954632731
Epoch 23: reducing lr to 0.0001164454715392202
Epoch 27: reducing lr to 9.42786477853569e-05
Epoch 36: reducing lr to 4.020773869623706e-05
Epoch 39: reducing lr to 2.4924380878966035e-05
Epoch 42: reducing lr to 1.2710671383566645e-05
Epoch 45: reducing lr to 4.334060281940399e-06
Epoch 48: reducing lr to 3.20870286084048e-07
[I 2024-06-22 08:44:58,202] Trial 907 finished with value: 0.9734400510787964 and parameters: {'hidden_size': 173, 'n_layers': 4, 'rnn_dropout': 0.12758127704905364, 'bidirectional': True, 'fc_dropout': 0.22780319816160613, 'learning_rate_model': 0.0014755467683354566}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00012776788686244835
Epoch 23: reducing lr to 0.00010841097336416073
Epoch 27: reducing lr to 8.777361488398387e-05
Epoch 36: reducing lr to 3.743348737578636e-05
Epoch 39: reducing lr to 2.320464983198332e-05
Epoch 42: reducing lr to 1.1833661185702048e-05
Epoch 45: reducing lr to 4.035019031425825e-06
Epoch 48: reducing lr to 2.9873089591372223e-07
[I 2024-06-22 08:45:57,814] Trial 908 finished with value: 0.9722508788108826 and parameters: {'hidden_size': 176, 'n_layers': 4, 'rnn_dropout': 0.13749837684007402, 'bidirectional': True, 'fc_dropout': 0.218341808020295, 'learning_rate_model': 0.0013737370744014755}. Best is trial 153 with value: 0.9688200950622559.
Epoch 22: reducing lr to 0.00017426452245132998
Epoch 27: reducing lr to 0.00013540865326016065
Epoch 30: reducing lr to 0.00010916962883949759
Epoch 33: reducing lr to 8.272921442969297e-05
Epoch 36: reducing lr to 5.774876788527219e-05
Epoch 39: reducing lr to 3.5797891966459484e-05
Epoch 42: reducing lr to 1.8255829230810672e-05
Epoch 45: reducing lr to 6.224837539694269e-06
Epoch 48: reducing lr to 4.6085316591261415e-07
[I 2024-06-22 08:46:51,260] Trial 909 finished with value: 0.9887053966522217 and parameters: {'hidden_size': 164, 'n_layers': 4, 'rnn_dropout': 0.1231322941002661, 'bidirectional': True, 'fc_dropout': 0.2254741129669632, 'learning_rate_model': 0.002119268842056081}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 9.224069660412315e-05
Epoch 37: reducing lr to 2.7588641463866458e-05
Epoch 40: reducing lr to 1.621478576341296e-05
Epoch 43: reducing lr to 7.494233600865421e-06
Epoch 46: reducing lr to 1.974942039283914e-06
Epoch 49: reducing lr to 3.7007569439256523e-09
[I 2024-06-22 08:47:48,583] Trial 910 finished with value: 0.9721865653991699 and parameters: {'hidden_size': 171, 'n_layers': 4, 'rnn_dropout': 0.14664286530334905, 'bidirectional': True, 'fc_dropout': 0.20892217695051443, 'learning_rate_model': 0.0011688343048822068}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00015193767471869811
Epoch 23: reducing lr to 0.0001289190234841586
Epoch 27: reducing lr to 0.0001043777061248915
Epoch 33: reducing lr to 6.377055988508385e-05
Epoch 36: reducing lr to 4.451476163655896e-05
Epoch 39: reducing lr to 2.759426194415197e-05
Epoch 42: reducing lr to 1.40722290093139e-05
Epoch 45: reducing lr to 4.7983215824845766e-06
Epoch 48: reducing lr to 3.5524167149004156e-07
[I 2024-06-22 08:48:59,107] Trial 911 finished with value: 0.9797475337982178 and parameters: {'hidden_size': 190, 'n_layers': 4, 'rnn_dropout': 0.11469518295920918, 'bidirectional': True, 'fc_dropout': 0.23299770006916753, 'learning_rate_model': 0.0016336062361595805}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00011063705670830216
Epoch 23: reducing lr to 9.387547452205742e-05
Epoch 27: reducing lr to 7.600512652969387e-05
Epoch 37: reducing lr to 2.8077593775713144e-05
Epoch 40: reducing lr to 1.650215971749124e-05
Epoch 43: reducing lr to 7.627053582213951e-06
Epoch 46: reducing lr to 2.009943852516929e-06
Epoch 49: reducing lr to 3.7663453008283946e-09
[I 2024-06-22 08:49:24,900] Trial 912 finished with value: 0.9723811149597168 and parameters: {'hidden_size': 176, 'n_layers': 2, 'rnn_dropout': 0.13502299333786, 'bidirectional': True, 'fc_dropout': 0.2062375679459164, 'learning_rate_model': 0.0011895495052405276}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.000127346131484514
Epoch 23: reducing lr to 0.00010805311418556559
Epoch 33: reducing lr to 5.344911404628443e-05
Epoch 36: reducing lr to 3.7309921313896543e-05
Epoch 39: reducing lr to 2.312805244824277e-05
Epoch 42: reducing lr to 1.1794598864423327e-05
Epoch 45: reducing lr to 4.021699636244744e-06
Epoch 48: reducing lr to 2.9774480022879727e-07
[I 2024-06-22 08:50:30,992] Trial 913 finished with value: 0.9779097437858582 and parameters: {'hidden_size': 182, 'n_layers': 4, 'rnn_dropout': 0.09797244371539553, 'bidirectional': True, 'fc_dropout': 0.23249802114753987, 'learning_rate_model': 0.001369202437308977}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 8.576818640215224e-05
Epoch 26: reducing lr to 7.375620588830845e-05
Epoch 37: reducing lr to 2.5652752318321875e-05
Epoch 40: reducing lr to 1.5076997670518476e-05
Epoch 43: reducing lr to 6.968364811672139e-06
Epoch 46: reducing lr to 1.8363607734417235e-06
Epoch 49: reducing lr to 3.441075610653016e-09
[I 2024-06-22 08:51:32,615] Trial 914 finished with value: 0.9732983708381653 and parameters: {'hidden_size': 173, 'n_layers': 4, 'rnn_dropout': 0.1212121159746101, 'bidirectional': True, 'fc_dropout': 0.19121131767733215, 'learning_rate_model': 0.0010868174485348157}. Best is trial 153 with value: 0.9688200950622559.
Epoch 23: reducing lr to 6.250068079333925e-05
Epoch 32: reducing lr to 3.417967134702951e-05
Epoch 37: reducing lr to 1.869358035158122e-05
Epoch 40: reducing lr to 1.0986854896390207e-05
Epoch 43: reducing lr to 5.077961456521234e-06
Epoch 46: reducing lr to 1.3381861426349332e-06
Epoch 49: reducing lr to 2.5075681012848816e-09
[I 2024-06-22 08:52:28,655] Trial 915 finished with value: 0.9730224609375 and parameters: {'hidden_size': 169, 'n_layers': 4, 'rnn_dropout': 0.10379812592241644, 'bidirectional': True, 'fc_dropout': 0.21266060041289375, 'learning_rate_model': 0.000791981657546176}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00015398462887414728
Epoch 23: reducing lr to 0.00013065586282519712
Epoch 27: reducing lr to 0.00010578391679439246
Epoch 36: reducing lr to 4.511447909623107e-05
Epoch 39: reducing lr to 2.7966020885821385e-05
Epoch 42: reducing lr to 1.4261814691077019e-05
Epoch 45: reducing lr to 4.862966143622123e-06
Epoch 48: reducing lr to 3.6002760372831796e-07
[I 2024-06-22 08:53:29,698] Trial 916 finished with value: 0.9743455648422241 and parameters: {'hidden_size': 178, 'n_layers': 4, 'rnn_dropout': 0.1011854997664057, 'bidirectional': True, 'fc_dropout': 0.18868067499515168, 'learning_rate_model': 0.001655614714831283}. Best is trial 153 with value: 0.9688200950622559.
Epoch 22: reducing lr to 7.648295492033438e-05
Epoch 27: reducing lr to 5.9429502789430305e-05
Epoch 32: reducing lr to 4.014158255507698e-05
Epoch 37: reducing lr to 2.1954274847004277e-05
Epoch 40: reducing lr to 1.290327628859511e-05
Epoch 43: reducing lr to 5.963703013667546e-06
Epoch 46: reducing lr to 1.5716040383550599e-06
Epoch 49: reducing lr to 2.9449596202521347e-09
[I 2024-06-22 08:54:36,481] Trial 917 finished with value: 0.9723662734031677 and parameters: {'hidden_size': 185, 'n_layers': 4, 'rnn_dropout': 0.12982154158645515, 'bidirectional': True, 'fc_dropout': 0.22766644235632388, 'learning_rate_model': 0.0009301258858142725}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00011682945576282452
Epoch 23: reducing lr to 9.912972130851957e-05
Epoch 26: reducing lr to 8.524643508491321e-05
Epoch 29: reducing lr to 6.995320124112726e-05
Epoch 32: reducing lr to 5.4210950217504064e-05
Epoch 35: reducing lr to 3.900883130617807e-05
Epoch 38: reducing lr to 2.530204330926976e-05
Epoch 41: reducing lr to 1.395182637866513e-05
Epoch 44: reducing lr to 5.671372915310997e-06
Epoch 47: reducing lr to 9.809646142903669e-07
[I 2024-06-22 08:55:47,100] Trial 918 finished with value: 0.9746934175491333 and parameters: {'hidden_size': 190, 'n_layers': 4, 'rnn_dropout': 0.09290861312617661, 'bidirectional': True, 'fc_dropout': 0.19994596068257245, 'learning_rate_model': 0.0012561290532755057}. Best is trial 153 with value: 0.9688200950622559.
[I 2024-06-22 08:56:52,624] Trial 919 finished with value: 0.9890604019165039 and parameters: {'hidden_size': 181, 'n_layers': 4, 'rnn_dropout': 0.7993418746549786, 'bidirectional': True, 'fc_dropout': 0.18061722162806065, 'learning_rate_model': 0.0007071469835922215}. Best is trial 153 with value: 0.9688200950622559.
Epoch 27: reducing lr to 6.705546259996354e-05
Epoch 36: reducing lr to 2.8597657918396538e-05
Epoch 39: reducing lr to 1.77274062485688e-05
Epoch 42: reducing lr to 9.040434601073687e-06
Epoch 45: reducing lr to 3.082590002810581e-06
Epoch 48: reducing lr to 2.2821822303746054e-07
[I 2024-06-22 08:57:52,270] Trial 920 finished with value: 0.9696143865585327 and parameters: {'hidden_size': 176, 'n_layers': 4, 'rnn_dropout': 0.11607469161878226, 'bidirectional': True, 'fc_dropout': 0.21014383973535808, 'learning_rate_model': 0.001049479107548071}. Best is trial 153 with value: 0.9688200950622559.
Epoch 18: reducing lr to 0.00013623267512176147
Epoch 23: reducing lr to 0.00011559334099227687
Epoch 27: reducing lr to 9.358873073971761e-05
Epoch 31: reducing lr to 6.932015738211193e-05
Epoch 36: reducing lr to 3.9913504477304476e-05
Epoch 39: reducing lr to 2.474198798699803e-05
Epoch 42: reducing lr to 1.2617656591192798e-05
Epoch 45: reducing lr to 4.302344276932066e-06
Epoch 48: reducing lr to 3.185222053148822e-07
[I 2024-06-22 08:58:20,057] Trial 921 finished with value: 0.9685981869697571 and parameters: {'hidden_size': 184, 'n_layers': 2, 'rnn_dropout': 0.16045930829107158, 'bidirectional': True, 'fc_dropout': 0.22523006702219253, 'learning_rate_model': 0.0014647489377446925}. Best is trial 921 with value: 0.9685981869697571.
Epoch 18: reducing lr to 0.0001930003850009404
Epoch 23: reducing lr to 0.0001637607078853489
Epoch 31: reducing lr to 9.820564010150809e-05
Epoch 34: reducing lr to 7.261437450002972e-05
Epoch 37: reducing lr to 4.897984969809855e-05
Epoch 40: reducing lr to 2.8787128594897118e-05
Epoch 43: reducing lr to 1.330498407663837e-05
Epoch 46: reducing lr to 3.5062387676202103e-06
Epoch 49: reducing lr to 6.570186470399031e-09
[I 2024-06-22 08:58:49,530] Trial 922 finished with value: 0.9687913656234741 and parameters: {'hidden_size': 187, 'n_layers': 2, 'rnn_dropout': 0.1445906619168636, 'bidirectional': True, 'fc_dropout': 0.21215745751903486, 'learning_rate_model': 0.002075105026468696}. Best is trial 921 with value: 0.9685981869697571.
Epoch 18: reducing lr to 0.00018005517668000614
Epoch 22: reducing lr to 0.00015918804403087773
Epoch 30: reducing lr to 9.972482888701649e-05
Epoch 36: reducing lr to 5.2752638778883506e-05
Epoch 39: reducing lr to 3.270084078163932e-05
Epoch 42: reducing lr to 1.6676427918517455e-05
Epoch 45: reducing lr to 5.686296317890293e-06
Epoch 48: reducing lr to 4.2098249853211865e-07
[I 2024-06-22 08:59:18,916] Trial 923 finished with value: 0.9714757204055786 and parameters: {'hidden_size': 188, 'n_layers': 2, 'rnn_dropout': 0.16544324266713106, 'bidirectional': True, 'fc_dropout': 0.21211845490046546, 'learning_rate_model': 0.0019359205017573898}. Best is trial 921 with value: 0.9685981869697571.
Epoch 23: reducing lr to 0.0002107515889305898
Epoch 27: reducing lr to 0.0001706324390321968
Epoch 30: reducing lr to 0.00013756787021087477
Epoch 33: reducing lr to 0.00010424952392248154
Epoch 36: reducing lr to 7.277092621574236e-05
Epoch 39: reducing lr to 4.5109979837937124e-05
Epoch 42: reducing lr to 2.3004709028628902e-05
Epoch 45: reducing lr to 7.844101439636111e-06
Epoch 48: reducing lr to 5.807346712495554e-07
[I 2024-06-22 08:59:47,592] Trial 924 finished with value: 0.9777686595916748 and parameters: {'hidden_size': 185, 'n_layers': 2, 'rnn_dropout': 0.15596776841697055, 'bidirectional': True, 'fc_dropout': 0.20340007179681635, 'learning_rate_model': 0.0026705531941905933}. Best is trial 921 with value: 0.9685981869697571.
Epoch 22: reducing lr to 0.00016471196528844835
Epoch 27: reducing lr to 0.00012798603572205698
Epoch 30: reducing lr to 0.00010318534067073893
Epoch 33: reducing lr to 7.819429510840285e-05
Epoch 36: reducing lr to 5.458318720051682e-05
Epoch 39: reducing lr to 3.383557970398636e-05
Epoch 42: reducing lr to 1.7255110037769915e-05
Epoch 45: reducing lr to 5.883614233933936e-06
Epoch 48: reducing lr to 4.3559084545205343e-07
[I 2024-06-22 09:00:16,223] Trial 925 finished with value: 0.9742096662521362 and parameters: {'hidden_size': 185, 'n_layers': 2, 'rnn_dropout': 0.13668836331445733, 'bidirectional': True, 'fc_dropout': 0.23261403090772143, 'learning_rate_model': 0.0020030981122225963}. Best is trial 921 with value: 0.9685981869697571.
Epoch 18: reducing lr to 0.00015158792451304908
Epoch 27: reducing lr to 0.00010413743573606306
Epoch 31: reducing lr to 7.713346871505227e-05
Epoch 36: reducing lr to 4.441229167928451e-05
Epoch 39: reducing lr to 2.7530741827712325e-05
Epoch 42: reducing lr to 1.4039835694100532e-05
Epoch 45: reducing lr to 4.78727617216517e-06
Epoch 48: reducing lr to 3.54423929294816e-07
[I 2024-06-22 09:00:44,006] Trial 926 finished with value: 0.9693102836608887 and parameters: {'hidden_size': 184, 'n_layers': 2, 'rnn_dropout': 0.1713846209051529, 'bidirectional': True, 'fc_dropout': 0.20695238767756788, 'learning_rate_model': 0.0016298457855794058}. Best is trial 921 with value: 0.9685981869697571.
Epoch 23: reducing lr to 0.00019432554059031567
Epoch 27: reducing lr to 0.00015733329046499505
Epoch 31: reducing lr to 0.00011653506111554193
Epoch 34: reducing lr to 8.616735822383648e-05
Epoch 37: reducing lr to 5.8121608617368174e-05
Epoch 40: reducing lr to 3.41600521790783e-05
Epoch 43: reducing lr to 1.5788269705382768e-05
Epoch 46: reducing lr to 4.16065460851294e-06
Epoch 49: reducing lr to 7.796467505058121e-09
[I 2024-06-22 09:01:12,541] Trial 927 finished with value: 0.9742059707641602 and parameters: {'hidden_size': 182, 'n_layers': 2, 'rnn_dropout': 0.16301325267046302, 'bidirectional': True, 'fc_dropout': 0.1919669056287609, 'learning_rate_model': 0.0024624093975737366}. Best is trial 921 with value: 0.9685981869697571.
Epoch 23: reducing lr to 0.0001843572124536245
Epoch 27: reducing lr to 0.00014926255585431992
Epoch 31: reducing lr to 0.00011055715556025449
Epoch 36: reducing lr to 6.365714808072504e-05
Epoch 39: reducing lr to 3.946043860907852e-05
Epoch 42: reducing lr to 2.0123615918366975e-05
Epoch 45: reducing lr to 6.861711852103975e-06
Epoch 48: reducing lr to 5.080038812993373e-07
[I 2024-06-22 09:01:39,442] Trial 928 finished with value: 0.9692915678024292 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.16898976137297184, 'bidirectional': True, 'fc_dropout': 0.20178477521178625, 'learning_rate_model': 0.0023360950448266826}. Best is trial 921 with value: 0.9685981869697571.
Epoch 17: reducing lr to 0.0002574163785777477
Epoch 23: reducing lr to 0.00021385775189955731
Epoch 27: reducing lr to 0.00017314730578179446
Epoch 30: reducing lr to 0.0001395954147069188
Epoch 33: reducing lr to 0.00010578600586496003
Epoch 36: reducing lr to 7.384346074502295e-05
Epoch 39: reducing lr to 4.577483342036782e-05
Epoch 42: reducing lr to 2.3343764006383437e-05
Epoch 45: reducing lr to 7.959711753846534e-06
Epoch 48: reducing lr to 5.892938310631413e-07
[I 2024-06-22 09:02:07,217] Trial 929 finished with value: 0.9844703674316406 and parameters: {'hidden_size': 184, 'n_layers': 2, 'rnn_dropout': 0.1435775744181385, 'bidirectional': True, 'fc_dropout': 0.19026106398982617, 'learning_rate_model': 0.0027099131510029935}. Best is trial 921 with value: 0.9685981869697571.
Epoch 18: reducing lr to 0.00018233268788497817
Epoch 22: reducing lr to 0.00016120160765433427
Epoch 27: reducing lr to 0.00012525838471764977
Epoch 32: reducing lr to 8.46056176622327e-05
Epoch 35: reducing lr to 6.088006673373872e-05
Epoch 38: reducing lr to 3.948823980595207e-05
Epoch 41: reducing lr to 2.1774251946280365e-05
Epoch 44: reducing lr to 8.851163954285628e-06
Epoch 47: reducing lr to 1.5309659167352888e-06
[I 2024-06-22 09:02:34,612] Trial 930 finished with value: 0.97123122215271 and parameters: {'hidden_size': 179, 'n_layers': 2, 'rnn_dropout': 0.1788258253004505, 'bidirectional': True, 'fc_dropout': 0.20011165200306494, 'learning_rate_model': 0.001960407887879719}. Best is trial 921 with value: 0.9685981869697571.
Epoch 23: reducing lr to 0.00018837791447657396
Epoch 27: reducing lr to 0.00015251786793181753
Epoch 31: reducing lr to 0.000112968329894562
Epoch 34: reducing lr to 8.353007632889158e-05
Epoch 37: reducing lr to 5.634270916784023e-05
Epoch 40: reducing lr to 3.3114532286171366e-05
Epoch 43: reducing lr to 1.530504591037733e-05
Epoch 46: reducing lr to 4.03331150207061e-06
Epoch 49: reducing lr to 7.55784486396703e-09
[I 2024-06-22 09:03:03,187] Trial 931 finished with value: 0.9734845161437988 and parameters: {'hidden_size': 182, 'n_layers': 2, 'rnn_dropout': 0.15841851926780354, 'bidirectional': True, 'fc_dropout': 0.1784647120359607, 'learning_rate_model': 0.0023870436459013473}. Best is trial 921 with value: 0.9685981869697571.
Epoch 22: reducing lr to 0.0001746660972492173
Epoch 27: reducing lr to 0.00013572068867505888
Epoch 30: reducing lr to 0.00010942119910187788
Epoch 33: reducing lr to 8.29198554568892e-05
Epoch 36: reducing lr to 5.7881844024153205e-05
Epoch 39: reducing lr to 3.58803845531834e-05
Epoch 42: reducing lr to 1.829789792517542e-05
Epoch 45: reducing lr to 6.239182042188036e-06
Epoch 48: reducing lr to 4.6191515497588306e-07
[I 2024-06-22 09:03:31,861] Trial 932 finished with value: 0.9739771485328674 and parameters: {'hidden_size': 185, 'n_layers': 2, 'rnn_dropout': 0.1559961864379752, 'bidirectional': True, 'fc_dropout': 0.2004884795107033, 'learning_rate_model': 0.002124152480704649}. Best is trial 921 with value: 0.9685981869697571.
Epoch 18: reducing lr to 0.00023443661114479582
Epoch 23: reducing lr to 0.00019891932026522632
Epoch 27: reducing lr to 0.00016105258783439565
Epoch 30: reducing lr to 0.00012984436972237782
Epoch 33: reducing lr to 9.839662202244752e-05
Epoch 36: reducing lr to 6.868533353109793e-05
Epoch 39: reducing lr to 4.257736120554618e-05
Epoch 42: reducing lr to 2.1713151042392813e-05
Epoch 45: reducing lr to 7.40370848154215e-06
Epoch 48: reducing lr to 5.481303682955841e-07
[I 2024-06-22 09:03:59,263] Trial 933 finished with value: 0.9756819009780884 and parameters: {'hidden_size': 180, 'n_layers': 2, 'rnn_dropout': 0.17466711483584083, 'bidirectional': True, 'fc_dropout': 0.18248865607739245, 'learning_rate_model': 0.0025206197913671654}. Best is trial 921 with value: 0.9685981869697571.
Epoch 18: reducing lr to 0.00021915368716799358
Epoch 22: reducing lr to 0.00019375531126454793
Epoch 27: reducing lr to 0.00015055356874296152
Epoch 30: reducing lr to 0.00012137981454222609
Epoch 33: reducing lr to 9.198214568873861e-05
Epoch 36: reducing lr to 6.420773625842306e-05
Epoch 39: reducing lr to 3.980174279313315e-05
Epoch 42: reducing lr to 2.0297670605880443e-05
Epoch 45: reducing lr to 6.9210606846930015e-06
Epoch 48: reducing lr to 5.123977465556519e-07
[I 2024-06-22 09:04:25,955] Trial 934 finished with value: 0.9777058959007263 and parameters: {'hidden_size': 173, 'n_layers': 2, 'rnn_dropout': 0.15312332948066862, 'bidirectional': True, 'fc_dropout': 0.2097294360237519, 'learning_rate_model': 0.0023563005732306483}. Best is trial 921 with value: 0.9685981869697571.
Epoch 23: reducing lr to 0.000162101336901919
Epoch 31: reducing lr to 9.721053210705748e-05
Epoch 36: reducing lr to 5.5972362946444e-05
Epoch 39: reducing lr to 3.469671605539602e-05
Epoch 42: reducing lr to 1.7694263220044113e-05
Epoch 45: reducing lr to 6.033355841402691e-06
Epoch 48: reducing lr to 4.466769008600567e-07
[I 2024-06-22 09:04:52,843] Trial 935 finished with value: 0.9669202566146851 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.1792242756195256, 'bidirectional': True, 'fc_dropout': 0.1767009352692203, 'learning_rate_model': 0.002054078193396489}. Best is trial 935 with value: 0.9669202566146851.
Epoch 18: reducing lr to 0.0001731812153411343
Epoch 23: reducing lr to 0.0001469441546273134
Epoch 31: reducing lr to 8.812092320981818e-05
Epoch 36: reducing lr to 5.0738702794504885e-05
Epoch 39: reducing lr to 3.145242171684855e-05
Epoch 42: reducing lr to 1.603977240604588e-05
Epoch 45: reducing lr to 5.469210745726971e-06
Epoch 48: reducing lr to 4.0491066170627275e-07
[I 2024-06-22 09:05:19,636] Trial 936 finished with value: 0.9705429077148438 and parameters: {'hidden_size': 177, 'n_layers': 2, 'rnn_dropout': 0.17594821293887053, 'bidirectional': True, 'fc_dropout': 0.17436677119696398, 'learning_rate_model': 0.001862012920039483}. Best is trial 935 with value: 0.9669202566146851.
Epoch 18: reducing lr to 0.00021165944419548736
Epoch 22: reducing lr to 0.00018712959851203097
Epoch 27: reducing lr to 0.00014540519529272257
Epoch 30: reducing lr to 0.00011722907524191084
Epoch 33: reducing lr to 8.883669758867788e-05
Epoch 36: reducing lr to 6.20120699091439e-05
Epoch 39: reducing lr to 3.844067086650714e-05
Epoch 42: reducing lr to 1.9603565581857934e-05
Epoch 45: reducing lr to 6.6843860885736855e-06
Epoch 48: reducing lr to 4.948756447791404e-07
[I 2024-06-22 09:05:46,330] Trial 937 finished with value: 0.9770902991294861 and parameters: {'hidden_size': 173, 'n_layers': 2, 'rnn_dropout': 0.17319953633386248, 'bidirectional': True, 'fc_dropout': 0.1776544256715147, 'learning_rate_model': 0.002275723836237354}. Best is trial 935 with value: 0.9669202566146851.
Epoch 18: reducing lr to 0.00025861004495874755
Epoch 23: reducing lr to 0.0002194304639780903
Epoch 27: reducing lr to 0.00017765918376482328
Epoch 30: reducing lr to 0.00014323299644868534
Epoch 33: reducing lr to 0.00010854258095932606
Epoch 36: reducing lr to 7.576767598603857e-05
Epoch 39: reducing lr to 4.6967635480750154e-05
Epoch 42: reducing lr to 2.3952056548885715e-05
Epoch 45: reducing lr to 8.167126175060227e-06
Epoch 48: reducing lr to 6.046496683943108e-07
[I 2024-06-22 09:06:10,354] Trial 938 finished with value: 0.9837661981582642 and parameters: {'hidden_size': 168, 'n_layers': 2, 'rnn_dropout': 0.14666765228125522, 'bidirectional': True, 'fc_dropout': 0.18550748760354763, 'learning_rate_model': 0.002780528153799163}. Best is trial 935 with value: 0.9669202566146851.
Epoch 23: reducing lr to 0.00019251442554138535
Epoch 27: reducing lr to 0.00015586694337961827
Epoch 30: reducing lr to 0.00012566358166495606
Epoch 33: reducing lr to 9.522840284496865e-05
Epoch 36: reducing lr to 6.647377193038453e-05
Epoch 39: reducing lr to 4.1206435969240016e-05
Epoch 42: reducing lr to 2.1014021132015292e-05
Epoch 45: reducing lr to 7.165320509337856e-06
Epoch 48: reducing lr to 5.304814174586936e-07
[I 2024-06-22 09:06:39,005] Trial 939 finished with value: 0.9752024412155151 and parameters: {'hidden_size': 185, 'n_layers': 2, 'rnn_dropout': 0.1693117088173086, 'bidirectional': True, 'fc_dropout': 0.16813531691293027, 'learning_rate_model': 0.0024394597291821}. Best is trial 935 with value: 0.9669202566146851.
Epoch 18: reducing lr to 0.00019742559241179523
Epoch 23: reducing lr to 0.00016751549364983094
Epoch 31: reducing lr to 0.00010045734714531955
Epoch 34: reducing lr to 7.427931246463819e-05
Epoch 37: reducing lr to 5.010288369549458e-05
Epoch 40: reducing lr to 2.9447173987007335e-05
Epoch 43: reducing lr to 1.361004727191105e-05
Epoch 46: reducing lr to 3.586631528383968e-06
Epoch 49: reducing lr to 6.72083092559915e-09
[I 2024-06-22 09:07:05,817] Trial 940 finished with value: 0.9717551469802856 and parameters: {'hidden_size': 177, 'n_layers': 2, 'rnn_dropout': 0.20486243609414412, 'bidirectional': True, 'fc_dropout': 0.1913143090213954, 'learning_rate_model': 0.0021226840514607273}. Best is trial 935 with value: 0.9669202566146851.
Epoch 16: reducing lr to 0.00033191773029959135
Epoch 23: reducing lr to 0.0002709951293897883
Epoch 27: reducing lr to 0.0002194078826558906
Epoch 30: reducing lr to 0.0001768917756532394
Epoch 33: reducing lr to 0.0001340493486552128
Epoch 36: reducing lr to 9.357256410600411e-05
Epoch 39: reducing lr to 5.800471011859669e-05
Epoch 42: reducing lr to 2.958062680059278e-05
Epoch 45: reducing lr to 1.0086345234060863e-05
Epoch 48: reducing lr to 7.467382247269144e-07
[I 2024-06-22 09:07:35,281] Trial 941 finished with value: 0.9969538450241089 and parameters: {'hidden_size': 187, 'n_layers': 2, 'rnn_dropout': 0.1979019214235514, 'bidirectional': True, 'fc_dropout': 0.17148305408821943, 'learning_rate_model': 0.0034339333433938777}. Best is trial 935 with value: 0.9669202566146851.
Epoch 18: reducing lr to 0.00020387674082795524
Epoch 23: reducing lr to 0.00017298928911038748
Epoch 27: reducing lr to 0.00014005865615121296
Epoch 31: reducing lr to 0.0001037399269163171
Epoch 34: reducing lr to 7.67064895246427e-05
Epoch 37: reducing lr to 5.174006322652043e-05
Epoch 40: reducing lr to 3.0409400249094125e-05
Epoch 43: reducing lr to 1.4054773985552708e-05
Epoch 46: reducing lr to 3.70382956750865e-06
Epoch 49: reducing lr to 6.940443171695939e-09
[I 2024-06-22 09:08:03,871] Trial 942 finished with value: 0.9721970558166504 and parameters: {'hidden_size': 182, 'n_layers': 2, 'rnn_dropout': 0.1616480970621773, 'bidirectional': True, 'fc_dropout': 0.19121978753586408, 'learning_rate_model': 0.0021920456255570897}. Best is trial 935 with value: 0.9669202566146851.
Epoch 18: reducing lr to 0.00022831026436019252
Epoch 23: reducing lr to 0.00019372111878913662
Epoch 27: reducing lr to 0.00015684392776712527
Epoch 30: reducing lr to 0.00012645124936859392
Epoch 33: reducing lr to 9.582530081967372e-05
Epoch 36: reducing lr to 6.689043396241345e-05
Epoch 39: reducing lr to 4.1464720655741545e-05
Epoch 42: reducing lr to 2.1145738416768332e-05
Epoch 45: reducing lr to 7.210233215761141e-06
Epoch 48: reducing lr to 5.338065103326507e-07
[I 2024-06-22 09:08:30,558] Trial 943 finished with value: 0.9744445085525513 and parameters: {'hidden_size': 174, 'n_layers': 2, 'rnn_dropout': 0.18532120655136558, 'bidirectional': True, 'fc_dropout': 0.1632898484098598, 'learning_rate_model': 0.002454750425321296}. Best is trial 935 with value: 0.9669202566146851.
Epoch 23: reducing lr to 0.00015138370476454723
Epoch 31: reducing lr to 9.078327652167017e-05
Epoch 36: reducing lr to 5.22716458062614e-05
Epoch 39: reducing lr to 3.240267797919201e-05
Epoch 42: reducing lr to 1.6524374015188232e-05
Epoch 45: reducing lr to 5.634449270378299e-06
Epoch 48: reducing lr to 4.17144024702598e-07
[I 2024-06-22 09:08:57,472] Trial 944 finished with value: 0.9664796590805054 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.16676884787168744, 'bidirectional': True, 'fc_dropout': 0.17832748876209184, 'learning_rate_model': 0.0019182689836825613}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.0002543430201048908
Epoch 23: reducing lr to 0.0002158098960158629
Epoch 27: reducing lr to 0.00017472783532179848
Epoch 30: reducing lr to 0.00014086967465336836
Epoch 33: reducing lr to 0.00010675164553479884
Epoch 36: reducing lr to 7.451752131164085e-05
Epoch 39: reducing lr to 4.6192676921212335e-05
Epoch 42: reducing lr to 2.3556851402808033e-05
Epoch 45: reducing lr to 8.032369884448552e-06
Epoch 48: reducing lr to 5.946730444649374e-07
[I 2024-06-22 09:09:23,151] Trial 945 finished with value: 0.9811298251152039 and parameters: {'hidden_size': 172, 'n_layers': 2, 'rnn_dropout': 0.1840428125296935, 'bidirectional': True, 'fc_dropout': 0.16077163440831221, 'learning_rate_model': 0.002734649878881412}. Best is trial 944 with value: 0.9664796590805054.
Epoch 32: reducing lr to 8.379770701806299e-05
Epoch 36: reducing lr to 5.290979093937674e-05
Epoch 39: reducing lr to 3.279825785683665e-05
Epoch 42: reducing lr to 1.6726107645207332e-05
Epoch 45: reducing lr to 5.703236015548014e-06
Epoch 48: reducing lr to 4.222366217514553e-07
[I 2024-06-22 09:09:46,976] Trial 946 finished with value: 0.9706104397773743 and parameters: {'hidden_size': 164, 'n_layers': 2, 'rnn_dropout': 0.19770577413191978, 'bidirectional': True, 'fc_dropout': 0.17432178476750962, 'learning_rate_model': 0.0019416876841474408}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.00022319304914017213
Epoch 22: reducing lr to 0.00019732653950324826
Epoch 27: reducing lr to 0.00015332851799530935
Epoch 30: reducing lr to 0.00012361704364563658
Epoch 33: reducing lr to 9.367752752883364e-05
Epoch 36: reducing lr to 6.539119016929948e-05
Epoch 39: reducing lr to 4.053535420685205e-05
Epoch 42: reducing lr to 2.067178997310947e-05
Epoch 45: reducing lr to 7.048627186987191e-06
Epoch 48: reducing lr to 5.218420776038988e-07
[I 2024-06-22 09:10:12,716] Trial 947 finished with value: 0.9770603775978088 and parameters: {'hidden_size': 176, 'n_layers': 2, 'rnn_dropout': 0.16572329391593565, 'bidirectional': True, 'fc_dropout': 0.16442525176324474, 'learning_rate_model': 0.0023997310582638954}. Best is trial 944 with value: 0.9664796590805054.
Epoch 23: reducing lr to 0.00016100913977275402
Epoch 31: reducing lr to 9.655555253612299e-05
Epoch 36: reducing lr to 5.5595235556312275e-05
Epoch 39: reducing lr to 3.446293850370265e-05
Epoch 42: reducing lr to 1.7575043823949234e-05
Epoch 45: reducing lr to 5.99270464101691e-06
Epoch 48: reducing lr to 4.4366730674993277e-07
[I 2024-06-22 09:10:39,620] Trial 948 finished with value: 0.9670920372009277 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.1706403898074419, 'bidirectional': True, 'fc_dropout': 0.17902749312049857, 'learning_rate_model': 0.002040238342666168}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.0001966797510676461
Epoch 27: reducing lr to 0.00013511448885645738
Epoch 31: reducing lr to 0.00010007783584737068
Epoch 34: reducing lr to 7.399869746647957e-05
Epoch 37: reducing lr to 4.991360325994065e-05
Epoch 40: reducing lr to 2.9335927417808473e-05
Epoch 43: reducing lr to 1.355863075682192e-05
Epoch 46: reducing lr to 3.57308182569638e-06
Epoch 49: reducing lr to 6.6954407342174895e-09
[I 2024-06-22 09:11:08,688] Trial 949 finished with value: 0.970066487789154 and parameters: {'hidden_size': 183, 'n_layers': 2, 'rnn_dropout': 0.16038985821432922, 'bidirectional': True, 'fc_dropout': 0.1851207509876351, 'learning_rate_model': 0.0021146649010212893}. Best is trial 944 with value: 0.9664796590805054.
Epoch 12: reducing lr to 0.0003097085784566272
Epoch 23: reducing lr to 0.0002444770553111864
Epoch 27: reducing lr to 0.0001979378492320451
Epoch 30: reducing lr to 0.0001595821316709635
Epoch 33: reducing lr to 0.00012093202597184352
Epoch 36: reducing lr to 8.441607412673722e-05
Epoch 39: reducing lr to 5.232869223851027e-05
Epoch 42: reducing lr to 2.6686031408580016e-05
Epoch 45: reducing lr to 9.099351664466253e-06
Epoch 48: reducing lr to 6.736665809478539e-07
[I 2024-06-22 09:11:38,115] Trial 950 finished with value: 0.9912067651748657 and parameters: {'hidden_size': 188, 'n_layers': 2, 'rnn_dropout': 0.18016966702562937, 'bidirectional': True, 'fc_dropout': 0.17885734569865464, 'learning_rate_model': 0.003097907751398381}. Best is trial 944 with value: 0.9664796590805054.
Epoch 23: reducing lr to 0.00017173406650432732
Epoch 27: reducing lr to 0.00013904238056398153
Epoch 31: reducing lr to 0.0001029871826158691
Epoch 36: reducing lr to 5.9298471464581514e-05
Epoch 39: reducing lr to 3.675853793941536e-05
Epoch 42: reducing lr to 1.8745729274365727e-05
Epoch 45: reducing lr to 6.391882714321117e-06
Epoch 48: reducing lr to 4.7322028346166373e-07
[I 2024-06-22 09:12:05,050] Trial 951 finished with value: 0.9678717255592346 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.16707904987560107, 'bidirectional': True, 'fc_dropout': 0.2022444510703385, 'learning_rate_model': 0.0021761399863301505}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.0002975843794071799
Epoch 23: reducing lr to 0.0002525001627696473
Epoch 27: reducing lr to 0.0002044336597794354
Epoch 30: reducing lr to 0.00016481920632901957
Epoch 33: reducing lr to 0.00012490070367988641
Epoch 36: reducing lr to 8.718639231908502e-05
Epoch 39: reducing lr to 5.404598517815191e-05
Epoch 42: reducing lr to 2.7561798246324197e-05
Epoch 45: reducing lr to 9.397968956438314e-06
Epoch 48: reducing lr to 6.957745835300781e-07
[I 2024-06-22 09:12:30,048] Trial 952 finished with value: 0.9842171669006348 and parameters: {'hidden_size': 170, 'n_layers': 2, 'rnn_dropout': 0.17272800988169462, 'bidirectional': True, 'fc_dropout': 0.20427918541361711, 'learning_rate_model': 0.003199573107086796}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.00025477097701343736
Epoch 23: reducing lr to 0.0002161730171893656
Epoch 27: reducing lr to 0.00017502183192611069
Epoch 30: reducing lr to 0.00014110670160401067
Epoch 33: reducing lr to 0.00010693126557778838
Epoch 36: reducing lr to 7.464290430048769e-05
Epoch 39: reducing lr to 4.6270400600063575e-05
Epoch 42: reducing lr to 2.359648810012051e-05
Epoch 45: reducing lr to 8.045885129264905e-06
Epoch 48: reducing lr to 5.95673639793232e-07
[I 2024-06-22 09:12:56,850] Trial 953 finished with value: 0.9781811237335205 and parameters: {'hidden_size': 177, 'n_layers': 2, 'rnn_dropout': 0.18908743919017806, 'bidirectional': True, 'fc_dropout': 0.20212106108699676, 'learning_rate_model': 0.002739251193702793}. Best is trial 944 with value: 0.9664796590805054.
Epoch 23: reducing lr to 0.000178479543773516
Epoch 31: reducing lr to 0.00010703237710461445
Epoch 34: reducing lr to 7.914096488419024e-05
Epoch 37: reducing lr to 5.338216560673598e-05
Epoch 40: reducing lr to 3.1374519837590666e-05
Epoch 43: reducing lr to 1.4500837951768276e-05
Epoch 46: reducing lr to 3.821380010423422e-06
Epoch 49: reducing lr to 7.160715771682355e-09
[I 2024-06-22 09:13:23,760] Trial 954 finished with value: 0.9687182903289795 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.20085453744879833, 'bidirectional': True, 'fc_dropout': 0.2045473130309423, 'learning_rate_model': 0.0022616157635661875}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.00021740707907265183
Epoch 22: reducing lr to 0.00019221112280236292
Epoch 27: reducing lr to 0.00014935368894469246
Epoch 30: reducing lr to 0.00012041244333606416
Epoch 33: reducing lr to 9.124906762665835e-05
Epoch 36: reducing lr to 6.369601430940305e-05
Epoch 39: reducing lr to 3.948453140112083e-05
Epoch 42: reducing lr to 2.013590250489641e-05
Epoch 45: reducing lr to 6.8659013087479875e-06
Epoch 48: reducing lr to 5.083140459173716e-07
[I 2024-06-22 09:13:51,183] Trial 955 finished with value: 0.9749948382377625 and parameters: {'hidden_size': 180, 'n_layers': 2, 'rnn_dropout': 0.17093230321412922, 'bidirectional': True, 'fc_dropout': 0.20666205926015463, 'learning_rate_model': 0.002337521360754482}. Best is trial 944 with value: 0.9664796590805054.
Epoch 12: reducing lr to 0.00030970755989353263
Epoch 23: reducing lr to 0.00024447625128016067
Epoch 27: reducing lr to 0.0001979371982581868
Epoch 30: reducing lr to 0.00015958160684058687
Epoch 33: reducing lr to 0.00012093162825312605
Epoch 36: reducing lr to 8.441579650091836e-05
Epoch 39: reducing lr to 5.232852014100181e-05
Epoch 42: reducing lr to 2.6685943644118086e-05
Epoch 45: reducing lr to 9.099321738708852e-06
Epoch 48: reducing lr to 6.736643654072991e-07
[I 2024-06-22 09:14:19,748] Trial 956 finished with value: 0.9837093353271484 and parameters: {'hidden_size': 182, 'n_layers': 2, 'rnn_dropout': 0.1967358879477093, 'bidirectional': True, 'fc_dropout': 0.21196111033461448, 'learning_rate_model': 0.0030978975630641674}. Best is trial 944 with value: 0.9664796590805054.
Epoch 23: reducing lr to 0.00020179917811948312
Epoch 27: reducing lr to 0.00016338422942358292
Epoch 30: reducing lr to 0.00013172419380119246
Epoch 33: reducing lr to 9.982116079719227e-05
Epoch 36: reducing lr to 6.967972661960258e-05
Epoch 39: reducing lr to 4.319377567910177e-05
Epoch 42: reducing lr to 2.202750355720499e-05
Epoch 45: reducing lr to 7.510895797448788e-06
Epoch 48: reducing lr to 5.56065935058015e-07
[I 2024-06-22 09:14:46,635] Trial 957 finished with value: 0.9713653326034546 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.2171171944302424, 'bidirectional': True, 'fc_dropout': 0.20058961551396523, 'learning_rate_model': 0.0025571121074181406}. Best is trial 944 with value: 0.9664796590805054.
Epoch 22: reducing lr to 0.00016917487651506186
Epoch 30: reducing lr to 0.00010598117286480529
Epoch 38: reducing lr to 4.1441386287498826e-05
Epoch 41: reducing lr to 2.2851238506992532e-05
Epoch 44: reducing lr to 9.288955555528414e-06
Epoch 47: reducing lr to 1.6066897451037873e-06
[I 2024-06-22 09:15:13,326] Trial 958 finished with value: 0.9746744632720947 and parameters: {'hidden_size': 173, 'n_layers': 2, 'rnn_dropout': 0.18065837261891796, 'bidirectional': True, 'fc_dropout': 0.2153062128426914, 'learning_rate_model': 0.002057372548432444}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.0002727264963267379
Epoch 23: reducing lr to 0.00023140826427543135
Epoch 27: reducing lr to 0.00018735686286345632
Epoch 30: reducing lr to 0.00015105149255150279
Epoch 33: reducing lr to 0.0001144674709446045
Epoch 36: reducing lr to 7.990351964011311e-05
Epoch 39: reducing lr to 4.953140419375304e-05
Epoch 42: reducing lr to 2.5259500122818162e-05
Epoch 45: reducing lr to 8.612935770294138e-06
Epoch 48: reducing lr to 6.376549897456774e-07
[I 2024-06-22 09:15:40,742] Trial 959 finished with value: 0.9816626310348511 and parameters: {'hidden_size': 179, 'n_layers': 2, 'rnn_dropout': 0.18690513615080073, 'bidirectional': True, 'fc_dropout': 0.19854087708823148, 'learning_rate_model': 0.0029323056706651272}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.00020599491564888037
Epoch 23: reducing lr to 0.00017478655914224744
Epoch 27: reducing lr to 0.00014151379378833293
Epoch 31: reducing lr to 0.00010481773157528104
Epoch 34: reducing lr to 7.750343062764846e-05
Epoch 37: reducing lr to 5.2277615959188285e-05
Epoch 40: reducing lr to 3.0725338328472295e-05
Epoch 43: reducing lr to 1.4200795882160846e-05
Epoch 46: reducing lr to 3.7423104579673886e-06
Epoch 49: reducing lr to 7.012550818242397e-09
[I 2024-06-22 09:16:09,317] Trial 960 finished with value: 0.9720026254653931 and parameters: {'hidden_size': 182, 'n_layers': 2, 'rnn_dropout': 0.20625405890837956, 'bidirectional': True, 'fc_dropout': 0.19220189226240808, 'learning_rate_model': 0.002214819855866629}. Best is trial 944 with value: 0.9664796590805054.
Epoch 32: reducing lr to 8.000707430658352e-05
Epoch 36: reducing lr to 5.051638912172186e-05
Epoch 39: reducing lr to 3.131461166249788e-05
Epoch 42: reducing lr to 1.596949349630249e-05
Epoch 45: reducing lr to 5.4452471782500626e-06
Epoch 48: reducing lr to 4.0313652931039687e-07
[I 2024-06-22 09:16:34,307] Trial 961 finished with value: 0.9682702422142029 and parameters: {'hidden_size': 170, 'n_layers': 2, 'rnn_dropout': 0.15767214010047048, 'bidirectional': True, 'fc_dropout': 0.21639687507369168, 'learning_rate_model': 0.001853854435328119}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.00022843551597653695
Epoch 27: reducing lr to 0.00015692997276173687
Epoch 30: reducing lr to 0.0001265206208592563
Epoch 33: reducing lr to 9.587787083376392e-05
Epoch 36: reducing lr to 6.692713023183107e-05
Epoch 39: reducing lr to 4.1487468311428215e-05
Epoch 42: reducing lr to 2.115733902492722e-05
Epoch 45: reducing lr to 7.2141887688197214e-06
Epoch 48: reducing lr to 5.340993579995134e-07
[I 2024-06-22 09:17:00,001] Trial 962 finished with value: 0.9778881669044495 and parameters: {'hidden_size': 172, 'n_layers': 2, 'rnn_dropout': 0.16381530053137067, 'bidirectional': True, 'fc_dropout': 0.22399011760562296, 'learning_rate_model': 0.002456097107912879}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.00019623746835641418
Epoch 23: reducing lr to 0.00016650737112010715
Epoch 27: reducing lr to 0.00013481065075348158
Epoch 31: reducing lr to 9.985278626126677e-05
Epoch 34: reducing lr to 7.383229322625942e-05
Epoch 37: reducing lr to 4.980136026767877e-05
Epoch 40: reducing lr to 2.9269958382132895e-05
Epoch 43: reducing lr to 1.3528140846502408e-05
Epoch 46: reducing lr to 3.5650468739093465e-06
Epoch 49: reducing lr to 6.680384391846705e-09
[I 2024-06-22 09:17:27,749] Trial 963 finished with value: 0.9738048315048218 and parameters: {'hidden_size': 184, 'n_layers': 2, 'rnn_dropout': 0.15287904318772486, 'bidirectional': True, 'fc_dropout': 0.21641669973602398, 'learning_rate_model': 0.002109909557775766}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.00017262530971572475
Epoch 23: reducing lr to 0.00014647246904630243
Epoch 27: reducing lr to 0.00011858963802484003
Epoch 31: reducing lr to 8.783805813792151e-05
Epoch 36: reducing lr to 5.057583333863526e-05
Epoch 39: reducing lr to 3.1351460546604385e-05
Epoch 42: reducing lr to 1.5988285299356822e-05
Epoch 45: reducing lr to 5.451654771113264e-06
Epoch 48: reducing lr to 4.0361091268796384e-07
[I 2024-06-22 09:17:51,782] Trial 964 finished with value: 0.9727725386619568 and parameters: {'hidden_size': 168, 'n_layers': 2, 'rnn_dropout': 0.17344790076473673, 'bidirectional': True, 'fc_dropout': 0.21617624292012547, 'learning_rate_model': 0.0018560359238924341}. Best is trial 944 with value: 0.9664796590805054.
Epoch 12: reducing lr to 0.0003500350393175167
Epoch 23: reducing lr to 0.0002763098655340158
Epoch 27: reducing lr to 0.00022371089358787832
Epoch 30: reducing lr to 0.00018036096388476884
Epoch 33: reducing lr to 0.00013667831442301914
Epoch 36: reducing lr to 9.540770221229422e-05
Epoch 39: reducing lr to 5.9142294141220675e-05
Epoch 42: reducing lr to 3.0160759833906033e-05
Epoch 45: reducing lr to 1.0284157879990406e-05
Epoch 48: reducing lr to 7.613831987608212e-07
[I 2024-06-22 09:18:18,680] Trial 965 finished with value: 0.9895968437194824 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.1570134474104345, 'bidirectional': True, 'fc_dropout': 0.21045010615402432, 'learning_rate_model': 0.0035012793864688907}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.00017615610675701077
Epoch 27: reducing lr to 0.00012101521480587414
Epoch 31: reducing lr to 8.963465654111286e-05
Epoch 34: reducing lr to 6.627689113914705e-05
Epoch 37: reducing lr to 4.4705090263517086e-05
Epoch 40: reducing lr to 2.627470664354264e-05
Epoch 43: reducing lr to 1.2143779896569728e-05
Epoch 46: reducing lr to 3.200228697271593e-06
Epoch 49: reducing lr to 5.9967676711283545e-09
[I 2024-06-22 09:18:47,737] Trial 966 finished with value: 0.9687012434005737 and parameters: {'hidden_size': 183, 'n_layers': 2, 'rnn_dropout': 0.16347540567489605, 'bidirectional': True, 'fc_dropout': 0.2262164495241062, 'learning_rate_model': 0.0018939984113132634}. Best is trial 944 with value: 0.9664796590805054.
Epoch 23: reducing lr to 0.00020417835205368797
Epoch 27: reducing lr to 0.00016531049841797188
Epoch 30: reducing lr to 0.0001332771969963313
Epoch 33: reducing lr to 0.00010099803330016219
Epoch 36: reducing lr to 7.05012373455666e-05
Epoch 39: reducing lr to 4.3703022080267795e-05
Epoch 42: reducing lr to 2.2287203635209608e-05
Epoch 45: reducing lr to 7.5994478305657465e-06
Epoch 48: reducing lr to 5.626218466861683e-07
[I 2024-06-22 09:19:16,425] Trial 967 finished with value: 0.9775557518005371 and parameters: {'hidden_size': 185, 'n_layers': 2, 'rnn_dropout': 0.1976092312614647, 'bidirectional': True, 'fc_dropout': 0.23129080789113382, 'learning_rate_model': 0.0025872599728827203}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.0001822867264599454
Epoch 23: reducing lr to 0.00015467017520736124
Epoch 31: reducing lr to 9.27541396040232e-05
Epoch 34: reducing lr to 6.858347262614245e-05
Epoch 37: reducing lr to 4.626092566563029e-05
Epoch 40: reducing lr to 2.7189124186046114e-05
Epoch 43: reducing lr to 1.2566410128768847e-05
Epoch 46: reducing lr to 3.3116036899787613e-06
Epoch 49: reducing lr to 6.205468366850958e-09
[I 2024-06-22 09:19:45,850] Trial 968 finished with value: 0.9673475027084351 and parameters: {'hidden_size': 187, 'n_layers': 2, 'rnn_dropout': 0.1678474802844957, 'bidirectional': True, 'fc_dropout': 0.23160589937495527, 'learning_rate_model': 0.001959913718999648}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.0001888298842644287
Epoch 27: reducing lr to 0.0001297218099713636
Epoch 31: reducing lr to 9.608353710999879e-05
Epoch 34: reducing lr to 7.104526725533568e-05
Epoch 37: reducing lr to 4.792145544028834e-05
Epoch 40: reducing lr to 2.8165074182899477e-05
Epoch 43: reducing lr to 1.3017479749169642e-05
Epoch 46: reducing lr to 3.430473264029758e-06
Epoch 49: reducing lr to 6.428212828658506e-09
[I 2024-06-22 09:20:14,906] Trial 969 finished with value: 0.9693307876586914 and parameters: {'hidden_size': 183, 'n_layers': 2, 'rnn_dropout': 0.16474587100958732, 'bidirectional': True, 'fc_dropout': 0.2253243915588936, 'learning_rate_model': 0.0020302645612997553}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.0002048198355174917
Epoch 23: reducing lr to 0.00017378950437400292
Epoch 27: reducing lr to 0.0001407065406244658
Epoch 31: reducing lr to 0.00010421980791583848
Epoch 34: reducing lr to 7.706131902912609e-05
Epoch 37: reducing lr to 5.197940283273078e-05
Epoch 40: reducing lr to 3.055006830063568e-05
Epoch 43: reducing lr to 1.4119788673616586e-05
Epoch 46: reducing lr to 3.720962772512217e-06
Epoch 49: reducing lr to 6.972548330288347e-09
[I 2024-06-22 09:20:44,311] Trial 970 finished with value: 0.9696963429450989 and parameters: {'hidden_size': 187, 'n_layers': 2, 'rnn_dropout': 0.17573094647043097, 'bidirectional': True, 'fc_dropout': 0.2340849859774503, 'learning_rate_model': 0.0022021856080793193}. Best is trial 944 with value: 0.9664796590805054.
Epoch 12: reducing lr to 0.00028822540079610716
Epoch 23: reducing lr to 0.00022751871324864478
Epoch 27: reducing lr to 0.00018420773558138696
Epoch 30: reducing lr to 0.00014851259235365997
Epoch 33: reducing lr to 0.00011254348145122871
Epoch 36: reducing lr to 7.856048715234442e-05
Epoch 39: reducing lr to 4.869887159322894e-05
Epoch 42: reducing lr to 2.4834933978015813e-05
Epoch 45: reducing lr to 8.468168022807274e-06
Epoch 48: reducing lr to 6.269371719189436e-07
[I 2024-06-22 09:21:12,998] Trial 971 finished with value: 0.9810224175453186 and parameters: {'hidden_size': 185, 'n_layers': 2, 'rnn_dropout': 0.17054794570255305, 'bidirectional': True, 'fc_dropout': 0.23322053260313588, 'learning_rate_model': 0.0028830189584212952}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.00021123730818160572
Epoch 23: reducing lr to 0.00017923472597967515
Epoch 27: reducing lr to 0.0001451151975098603
Epoch 31: reducing lr to 0.00010748525223508256
Epoch 34: reducing lr to 7.9475825939946e-05
Epoch 37: reducing lr to 5.360803609441274e-05
Epoch 40: reducing lr to 3.1507271628676605e-05
Epoch 43: reducing lr to 1.4562193861605583e-05
Epoch 46: reducing lr to 3.8375490241144766e-06
Epoch 49: reducing lr to 7.1910141746182186e-09
[I 2024-06-22 09:21:42,421] Trial 972 finished with value: 0.9705175757408142 and parameters: {'hidden_size': 187, 'n_layers': 2, 'rnn_dropout': 0.2004840245599136, 'bidirectional': True, 'fc_dropout': 0.2355836709861466, 'learning_rate_model': 0.0022711851066164}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.00019099508689556064
Epoch 22: reducing lr to 0.000168860095349805
Epoch 30: reducing lr to 0.00010578397527981574
Epoch 36: reducing lr to 5.595781811621634e-05
Epoch 39: reducing lr to 3.468769985851031e-05
Epoch 42: reducing lr to 1.7689665235592662e-05
Epoch 45: reducing lr to 6.031788029507756e-06
Epoch 48: reducing lr to 4.465608285817487e-07
[I 2024-06-22 09:22:11,822] Trial 973 finished with value: 0.972736120223999 and parameters: {'hidden_size': 188, 'n_layers': 2, 'rnn_dropout': 0.182744836244362, 'bidirectional': True, 'fc_dropout': 0.23259564082420023, 'learning_rate_model': 0.0020535444260687467}. Best is trial 944 with value: 0.9664796590805054.
Epoch 16: reducing lr to 0.00034538583247711375
Epoch 23: reducing lr to 0.00028199119787018626
Epoch 27: reducing lr to 0.00022831071463023602
Epoch 30: reducing lr to 0.00018406944738144393
Epoch 33: reducing lr to 0.00013948861917230498
Epoch 36: reducing lr to 9.736942320495889e-05
Epoch 39: reducing lr to 6.0358345647345625e-05
Epoch 42: reducing lr to 3.0780908882137344e-05
Epoch 45: reducing lr to 1.0495615109723956e-05
Epoch 48: reducing lr to 7.770383436792992e-07
[I 2024-06-22 09:22:41,287] Trial 974 finished with value: 1.0044617652893066 and parameters: {'hidden_size': 187, 'n_layers': 2, 'rnn_dropout': 0.1674780617422736, 'bidirectional': True, 'fc_dropout': 0.22568551533866957, 'learning_rate_model': 0.0035732707782994656}. Best is trial 944 with value: 0.9664796590805054.
Epoch 27: reducing lr to 0.00015053224770671084
Epoch 30: reducing lr to 0.00012136262502325577
Epoch 33: reducing lr to 9.196911939730614e-05
Epoch 36: reducing lr to 6.41986433124121e-05
Epoch 39: reducing lr to 3.979610616553263e-05
Epoch 42: reducing lr to 2.0294796098325367e-05
Epoch 45: reducing lr to 6.920080540635419e-06
Epoch 48: reducing lr to 5.123251820125189e-07
[I 2024-06-22 09:23:11,442] Trial 975 finished with value: 0.9744412899017334 and parameters: {'hidden_size': 189, 'n_layers': 2, 'rnn_dropout': 0.1907113175053818, 'bidirectional': True, 'fc_dropout': 0.23504334733062174, 'learning_rate_model': 0.002355966879580217}. Best is trial 944 with value: 0.9664796590805054.
Epoch 23: reducing lr to 0.00021549816900070804
Epoch 27: reducing lr to 0.00017447544936742437
Epoch 30: reducing lr to 0.0001406661951836303
Epoch 33: reducing lr to 0.00010659744791717435
Epoch 36: reducing lr to 7.44098843361172e-05
Epoch 39: reducing lr to 4.6125953821092756e-05
Epoch 42: reducing lr to 2.3522824663952183e-05
Epoch 45: reducing lr to 8.020767512477087e-06
Epoch 48: reducing lr to 5.938140678536999e-07
[I 2024-06-22 09:23:40,100] Trial 976 finished with value: 0.9785766005516052 and parameters: {'hidden_size': 185, 'n_layers': 2, 'rnn_dropout': 0.15844074874967495, 'bidirectional': True, 'fc_dropout': 0.22036261362866993, 'learning_rate_model': 0.0027306998086576873}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.00017808949817314305
Epoch 27: reducing lr to 0.00012234341047183433
Epoch 31: reducing lr to 9.061843665941214e-05
Epoch 34: reducing lr to 6.700430942043953e-05
Epoch 37: reducing lr to 4.519574846074965e-05
Epoch 40: reducing lr to 2.656308320465779e-05
Epoch 43: reducing lr to 1.2277063268027374e-05
Epoch 46: reducing lr to 3.2353526268751175e-06
Epoch 49: reducing lr to 6.062584856554067e-09
[I 2024-06-22 09:24:09,126] Trial 977 finished with value: 0.9687298536300659 and parameters: {'hidden_size': 183, 'n_layers': 2, 'rnn_dropout': 0.17364799038517054, 'bidirectional': True, 'fc_dropout': 0.2368754581638094, 'learning_rate_model': 0.0019147858840725949}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.00018511571071475643
Epoch 23: reducing lr to 0.00015707056660638408
Epoch 27: reducing lr to 0.00012717025772481059
Epoch 31: reducing lr to 9.4193630046384e-05
Epoch 36: reducing lr to 5.423527609532048e-05
Epoch 39: reducing lr to 3.361991303933139e-05
Epoch 42: reducing lr to 1.7145126639741668e-05
Epoch 45: reducing lr to 5.846112306405319e-06
Epoch 48: reducing lr to 4.328144064013814e-07
[I 2024-06-22 09:24:36,888] Trial 978 finished with value: 0.9720128774642944 and parameters: {'hidden_size': 184, 'n_layers': 2, 'rnn_dropout': 0.17886073251062978, 'bidirectional': True, 'fc_dropout': 0.23582206061736627, 'learning_rate_model': 0.0019903304430229215}. Best is trial 944 with value: 0.9664796590805054.
Epoch 23: reducing lr to 0.0002065695899117164
Epoch 27: reducing lr to 0.00016724653482031617
Epoch 30: reducing lr to 0.00013483807490461091
Epoch 33: reducing lr to 0.00010218087329463066
Epoch 36: reducing lr to 7.13269136529855e-05
Epoch 39: reducing lr to 4.4214850684316453e-05
Epoch 42: reducing lr to 2.2548220557650497e-05
Epoch 45: reducing lr to 7.688448878765938e-06
Epoch 48: reducing lr to 5.692110009526078e-07
[I 2024-06-22 09:25:06,304] Trial 979 finished with value: 0.9797518253326416 and parameters: {'hidden_size': 188, 'n_layers': 2, 'rnn_dropout': 0.15160303125310268, 'bidirectional': True, 'fc_dropout': 0.2344894377482023, 'learning_rate_model': 0.0026175607071843273}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.00018264081649884358
Epoch 23: reducing lr to 0.00015497062038743087
Epoch 27: reducing lr to 0.0001254700620252449
Epoch 31: reducing lr to 9.293431353954876e-05
Epoch 34: reducing lr to 6.8716695296611e-05
Epoch 37: reducing lr to 4.635078702463608e-05
Epoch 40: reducing lr to 2.7241938772316962e-05
Epoch 43: reducing lr to 1.259082024758398e-05
Epoch 46: reducing lr to 3.318036445134221e-06
Epoch 49: reducing lr to 6.217522423562077e-09
[I 2024-06-22 09:25:35,353] Trial 980 finished with value: 0.9687384366989136 and parameters: {'hidden_size': 183, 'n_layers': 2, 'rnn_dropout': 0.2059897747461791, 'bidirectional': True, 'fc_dropout': 0.23332527255527125, 'learning_rate_model': 0.0019637208306773606}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.00018390165212889606
Epoch 27: reducing lr to 0.0001263362272545671
Epoch 31: reducing lr to 9.357587272665342e-05
Epoch 34: reducing lr to 6.919107150380439e-05
Epoch 37: reducing lr to 4.6670763275738775e-05
Epoch 40: reducing lr to 2.7429999730947523e-05
Epoch 43: reducing lr to 1.2677739234719793e-05
Epoch 46: reducing lr to 3.340942051077234e-06
Epoch 49: reducing lr to 6.2604442301657934e-09
[I 2024-06-22 09:26:03,894] Trial 981 finished with value: 0.9700139760971069 and parameters: {'hidden_size': 182, 'n_layers': 2, 'rnn_dropout': 0.21484544960502608, 'bidirectional': True, 'fc_dropout': 0.2426463099272205, 'learning_rate_model': 0.0019772771059845835}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.00020735601292084797
Epoch 23: reducing lr to 0.0001759414493397833
Epoch 27: reducing lr to 0.0001424488364716164
Epoch 31: reducing lr to 0.00010551030754518567
Epoch 34: reducing lr to 7.801552922805873e-05
Epoch 37: reducing lr to 5.262303671990408e-05
Epoch 40: reducing lr to 3.092835389343141e-05
Epoch 43: reducing lr to 1.429462666664447e-05
Epoch 46: reducing lr to 3.767037517560853e-06
Epoch 49: reducing lr to 7.0588857666639e-09
[I 2024-06-22 09:26:32,978] Trial 982 finished with value: 0.9703819751739502 and parameters: {'hidden_size': 183, 'n_layers': 2, 'rnn_dropout': 0.2013084057055594, 'bidirectional': True, 'fc_dropout': 0.23300010693980172, 'learning_rate_model': 0.0022294541261068624}. Best is trial 944 with value: 0.9664796590805054.
Epoch 12: reducing lr to 0.0003144736645383665
Epoch 23: reducing lr to 0.0002482385081562231
Epoch 27: reducing lr to 0.0002009832633924317
Epoch 30: reducing lr to 0.00016203741592014063
Epoch 33: reducing lr to 0.00012279265093956835
Epoch 36: reducing lr to 8.571487528329837e-05
Epoch 39: reducing lr to 5.3133806272819344e-05
Epoch 42: reducing lr to 2.7096614923817313e-05
Epoch 45: reducing lr to 9.239351641816786e-06
Epoch 48: reducing lr to 6.840314189662279e-07
[I 2024-06-22 09:27:01,534] Trial 983 finished with value: 0.9851455688476562 and parameters: {'hidden_size': 182, 'n_layers': 2, 'rnn_dropout': 0.1898412072597229, 'bidirectional': True, 'fc_dropout': 0.23359432397827207, 'learning_rate_model': 0.00314557125875831}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.00017832320901921148
Epoch 22: reducing lr to 0.00015765679927951673
Epoch 27: reducing lr to 0.00012250396447566713
Epoch 30: reducing lr to 9.87655663887349e-05
Epoch 38: reducing lr to 3.8619897081535206e-05
Epoch 41: reducing lr to 2.1295438169062508e-05
Epoch 44: reducing lr to 8.656527681306795e-06
Epoch 47: reducing lr to 1.497300118470787e-06
[I 2024-06-22 09:27:30,227] Trial 984 finished with value: 0.9725145697593689 and parameters: {'hidden_size': 186, 'n_layers': 2, 'rnn_dropout': 0.16740736862821254, 'bidirectional': True, 'fc_dropout': 0.2231950509308113, 'learning_rate_model': 0.0019172987005699014}. Best is trial 944 with value: 0.9664796590805054.
Epoch 23: reducing lr to 0.00019254315750288507
Epoch 27: reducing lr to 0.00015589020585983848
Epoch 31: reducing lr to 0.00011546618400657283
Epoch 34: reducing lr to 8.537701825349502e-05
Epoch 37: reducing lr to 5.7588508480869604e-05
Epoch 40: reducing lr to 3.3846731042367315e-05
Epoch 43: reducing lr to 1.564345732088003e-05
Epoch 46: reducing lr to 4.122492458626071e-06
Epoch 49: reducing lr to 7.724957132434692e-09
[I 2024-06-22 09:27:58,846] Trial 985 finished with value: 0.9739279747009277 and parameters: {'hidden_size': 182, 'n_layers': 2, 'rnn_dropout': 0.2066181957540807, 'bidirectional': True, 'fc_dropout': 0.24089766161339457, 'learning_rate_model': 0.0024398238082001884}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.00017233384002285457
Epoch 22: reducing lr to 0.00015236155615966083
Epoch 27: reducing lr to 0.00011838940501480454
Epoch 31: reducing lr to 8.768974771999998e-05
Epoch 38: reducing lr to 3.7322764669580225e-05
Epoch 41: reducing lr to 2.0580185018139904e-05
Epoch 44: reducing lr to 8.365779557180461e-06
Epoch 47: reducing lr to 1.4470100695358452e-06
[I 2024-06-22 09:28:27,590] Trial 986 finished with value: 0.9716640710830688 and parameters: {'hidden_size': 186, 'n_layers': 2, 'rnn_dropout': 0.18045326726092834, 'bidirectional': True, 'fc_dropout': 0.22086584603199477, 'learning_rate_model': 0.0018529020947825323}. Best is trial 944 with value: 0.9664796590805054.
Epoch 27: reducing lr to 0.00014540961932536138
Epoch 30: reducing lr to 0.00011723264200067791
Epoch 33: reducing lr to 8.883940049381732e-05
Epoch 36: reducing lr to 6.201395666030659e-05
Epoch 39: reducing lr to 3.8441840444954665e-05
Epoch 42: reducing lr to 1.960416203106866e-05
Epoch 45: reducing lr to 6.684589464678401e-06
Epoch 48: reducing lr to 4.948907016414511e-07
[I 2024-06-22 09:28:57,776] Trial 987 finished with value: 0.9734647274017334 and parameters: {'hidden_size': 189, 'n_layers': 2, 'rnn_dropout': 0.2179823592040503, 'bidirectional': True, 'fc_dropout': 0.22261664206804666, 'learning_rate_model': 0.0022757930763804466}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.0002512259031763912
Epoch 23: reducing lr to 0.0002131650242205553
Epoch 27: reducing lr to 0.00017258644731304928
Epoch 30: reducing lr to 0.00013914323746868308
Epoch 33: reducing lr to 0.00010544334400836228
Epoch 36: reducing lr to 7.360426712815843e-05
Epoch 39: reducing lr to 4.562655965507133e-05
Epoch 42: reducing lr to 2.3268148924322254e-05
Epoch 45: reducing lr to 7.933928668595784e-06
Epoch 48: reducing lr to 5.873849914526334e-07
[I 2024-06-22 09:29:26,858] Trial 988 finished with value: 0.9808768033981323 and parameters: {'hidden_size': 183, 'n_layers': 2, 'rnn_dropout': 0.15821042380368822, 'bidirectional': True, 'fc_dropout': 0.24605126267890903, 'learning_rate_model': 0.0027011352047713647}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.00017313600284160564
Epoch 23: reducing lr to 0.00014690579184929074
Epoch 32: reducing lr to 8.033819185084774e-05
Epoch 36: reducing lr to 5.072545641803348e-05
Epoch 39: reducing lr to 3.144421041864714e-05
Epoch 42: reducing lr to 1.6035584895287746e-05
Epoch 45: reducing lr to 5.467782896362511e-06
Epoch 48: reducing lr to 4.048049514936191e-07
[I 2024-06-22 09:29:55,186] Trial 989 finished with value: 0.9697229266166687 and parameters: {'hidden_size': 181, 'n_layers': 2, 'rnn_dropout': 0.1501241451671409, 'bidirectional': True, 'fc_dropout': 0.21909720146844572, 'learning_rate_model': 0.0018615268034701783}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.00017082110049133843
Epoch 22: reducing lr to 0.0001510241325343562
Epoch 30: reducing lr to 9.46104706951322e-05
Epoch 36: reducing lr to 5.004723538743763e-05
Epoch 39: reducing lr to 3.102378788719356e-05
Epoch 42: reducing lr to 1.5821182272189356e-05
Epoch 45: reducing lr to 5.394676302298879e-06
Epoch 48: reducing lr to 3.9939253629265716e-07
[I 2024-06-22 09:30:24,610] Trial 990 finished with value: 0.97107994556427 and parameters: {'hidden_size': 188, 'n_layers': 2, 'rnn_dropout': 0.19013648080721796, 'bidirectional': True, 'fc_dropout': 0.2357771705168729, 'learning_rate_model': 0.0018366373945563035}. Best is trial 944 with value: 0.9664796590805054.
Epoch 12: reducing lr to 0.00031621060866426725
Epoch 18: reducing lr to 0.00029417771859980153
Epoch 23: reducing lr to 0.0002496096131713196
Epoch 27: reducing lr to 0.00020209336175079922
Epoch 30: reducing lr to 0.0001629324032259037
Epoch 33: reducing lr to 0.0001234708761704994
Epoch 36: reducing lr to 8.618830745239333e-05
Epoch 39: reducing lr to 5.3427282207689104e-05
Epoch 42: reducing lr to 2.7246278668133748e-05
Epoch 45: reducing lr to 9.290383697505618e-06
Epoch 48: reducing lr to 6.878095552271916e-07
[I 2024-06-22 09:30:53,668] Trial 991 finished with value: 0.9891282916069031 and parameters: {'hidden_size': 183, 'n_layers': 2, 'rnn_dropout': 0.1716647295714325, 'bidirectional': True, 'fc_dropout': 0.21425748278410073, 'learning_rate_model': 0.0031629453098685136}. Best is trial 944 with value: 0.9664796590805054.
Epoch 27: reducing lr to 0.00013813704220488887
Epoch 31: reducing lr to 0.00010231660831658833
Epoch 34: reducing lr to 7.565407145856489e-05
Epoch 37: reducing lr to 5.103018616635193e-05
Epoch 40: reducing lr to 2.9992181283670665e-05
Epoch 43: reducing lr to 1.3861941564870944e-05
Epoch 46: reducing lr to 3.6530127829748417e-06
Epoch 49: reducing lr to 6.845219836272565e-09
[I 2024-06-22 09:31:23,849] Trial 992 finished with value: 0.9719477891921997 and parameters: {'hidden_size': 189, 'n_layers': 2, 'rnn_dropout': 0.14871741743786046, 'bidirectional': True, 'fc_dropout': 0.24672891839536004, 'learning_rate_model': 0.0021619706158375798}. Best is trial 944 with value: 0.9664796590805054.
Epoch 23: reducing lr to 0.00020953782100483978
Epoch 27: reducing lr to 0.00016964972671842162
Epoch 30: reducing lr to 0.00013677558451887607
Epoch 33: reducing lr to 0.000103649126416328
Epoch 36: reducing lr to 7.235182135102468e-05
Epoch 39: reducing lr to 4.48501808635315e-05
Epoch 42: reducing lr to 2.28722195033928e-05
Epoch 45: reducing lr to 7.798925416138128e-06
Epoch 48: reducing lr to 5.773900838093507e-07
[I 2024-06-22 09:31:52,190] Trial 993 finished with value: 0.9806252121925354 and parameters: {'hidden_size': 181, 'n_layers': 2, 'rnn_dropout': 0.18779439340634568, 'bidirectional': True, 'fc_dropout': 0.23136384111345698, 'learning_rate_model': 0.002655172850784569}. Best is trial 944 with value: 0.9664796590805054.
Epoch 22: reducing lr to 0.00018026741671508438
Epoch 27: reducing lr to 0.0001400730784482832
Epoch 30: reducing lr to 0.0001129301977121426
Epoch 33: reducing lr to 8.557898970098271e-05
Epoch 36: reducing lr to 5.973804110394585e-05
Epoch 39: reducing lr to 3.703102282589747e-05
Epoch 42: reducing lr to 1.8884688226481014e-05
Epoch 45: reducing lr to 6.439264670553713e-06
Epoch 48: reducing lr to 4.7672818618170116e-07
[I 2024-06-22 09:32:20,856] Trial 994 finished with value: 0.9742460250854492 and parameters: {'hidden_size': 185, 'n_layers': 2, 'rnn_dropout': 0.16808155350840134, 'bidirectional': True, 'fc_dropout': 0.21514118091411497, 'learning_rate_model': 0.002192271347651475}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.00017279512107196108
Epoch 23: reducing lr to 0.00014661655387755335
Epoch 27: reducing lr to 0.00011870629454120913
Epoch 31: reducing lr to 8.79244643538242e-05
Epoch 36: reducing lr to 5.0625584738740844e-05
Epoch 39: reducing lr to 3.1382300949116456e-05
Epoch 42: reducing lr to 1.600401296070061e-05
Epoch 45: reducing lr to 5.45701755882927e-06
Epoch 48: reducing lr to 4.040079443664831e-07
[I 2024-06-22 09:32:51,178] Trial 995 finished with value: 0.9699554443359375 and parameters: {'hidden_size': 190, 'n_layers': 2, 'rnn_dropout': 0.16979726239564646, 'bidirectional': True, 'fc_dropout': 0.22999193841622428, 'learning_rate_model': 0.0018578617047004659}. Best is trial 944 with value: 0.9664796590805054.
Epoch 22: reducing lr to 0.00014860384793337353
Epoch 32: reducing lr to 7.799376522563805e-05
Epoch 35: reducing lr to 5.6122344626200694e-05
Epoch 38: reducing lr to 3.640226960926658e-05
Epoch 41: reducing lr to 2.007261386639779e-05
Epoch 44: reducing lr to 8.159453503196202e-06
Epoch 47: reducing lr to 1.4113223161493033e-06
[I 2024-06-22 09:33:18,640] Trial 996 finished with value: 0.969833493232727 and parameters: {'hidden_size': 179, 'n_layers': 2, 'rnn_dropout': 0.20162749841822428, 'bidirectional': True, 'fc_dropout': 0.21350631457175884, 'learning_rate_model': 0.0018072037859731035}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.0003500140352219853
Epoch 23: reducing lr to 0.0002969866934590855
Epoch 26: reducing lr to 0.0002553932014622451
Epoch 29: reducing lr to 0.0002095755910462233
Epoch 32: reducing lr to 0.00016241275211764306
Epoch 35: reducing lr to 0.00011686811656888437
Epoch 38: reducing lr to 7.580340266257557e-05
Epoch 41: reducing lr to 4.179883418636123e-05
Epoch 44: reducing lr to 1.6991092754609342e-05
Epoch 47: reducing lr to 2.938911088953399e-06
[I 2024-06-22 09:33:47,343] Trial 997 finished with value: 0.9931931495666504 and parameters: {'hidden_size': 185, 'n_layers': 2, 'rnn_dropout': 0.22071965687064082, 'bidirectional': True, 'fc_dropout': 0.24748238125347663, 'learning_rate_model': 0.0037632872277441}. Best is trial 944 with value: 0.9664796590805054.
Epoch 18: reducing lr to 0.00023241662295482376
Epoch 23: reducing lr to 0.0001972053615292981
Epoch 27: reducing lr to 0.00015966490216618327
Epoch 30: reducing lr to 0.0001287255850236365
Epoch 33: reducing lr to 9.754880216424403e-05
Epoch 36: reducing lr to 6.809351657094142e-05
Epoch 39: reducing lr to 4.221049970564912e-05
Epoch 42: reducing lr to 2.1526062906036727e-05
Epoch 45: reducing lr to 7.339915528633981e-06
Epoch 48: reducing lr to 5.434074845057577e-07
[I 2024-06-22 09:34:15,675] Trial 998 finished with value: 0.9772093296051025 and parameters: {'hidden_size': 181, 'n_layers': 2, 'rnn_dropout': 0.16122850845841286, 'bidirectional': True, 'fc_dropout': 0.21382454354450545, 'learning_rate_model': 0.0024989012458502846}. Best is trial 944 with value: 0.9664796590805054.
Epoch 22: reducing lr to 0.00014787367537495965
Epoch 30: reducing lr to 9.26368375429103e-05
Epoch 38: reducing lr to 3.622340520768939e-05
Epoch 41: reducing lr to 1.9973986058136375e-05
Epoch 44: reducing lr to 8.119361613769774e-06
Epoch 47: reducing lr to 1.4043877122298229e-06
[I 2024-06-22 09:34:46,564] Trial 999 finished with value: 0.9704599380493164 and parameters: {'hidden_size': 191, 'n_layers': 2, 'rnn_dropout': 0.14985803266916609, 'bidirectional': True, 'fc_dropout': 0.23535781464477318, 'learning_rate_model': 0.001798323998334153}. Best is trial 944 with value: 0.9664796590805054.

Optuna study saved to optuna/no-name-aed6f3de-e835-4c72-bd67-d0d4ea608746.pkl
To reload the study run: study = joblib.load('optuna/no-name-aed6f3de-e835-4c72-bd67-d0d4ea608746.pkl')

Study statistics    : 
  Study name        : no-name-aed6f3de-e835-4c72-bd67-d0d4ea608746
  # finished trials : 1000
  # pruned trials   : 0
  # complete trials : 1000

Best trial          :
  value             : 0.9664796590805054
  best_params = {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.16676884787168744, 'bidirectional': True, 'fc_dropout': 0.17832748876209184, 'learning_rate_model': 0.0019182689836825613}

O Melhor modelo foi o de número 944
Best hyperparameters:  {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.16676884787168744, 'bidirectional': True, 'fc_dropout': 0.17832748876209184, 'learning_rate_model': 0.0019182689836825613}
Epoch 17: reducing lr to 0.00018221759421877356
Epoch 26: reducing lr to 0.00013018215920289838
Epoch 35: reducing lr to 5.957145166670579e-05
Epoch 40: reducing lr to 2.6611402896963445e-05
Epoch 44: reducing lr to 8.660908471123702e-06
Epoch 47: reducing lr to 1.4980578538299458e-06
Métricas de Treinamento para <class 'tsai.models.RNNPlus.LSTMPlus'>
train_loss    0.418128
valid_loss    0.397018
mae           0.756944
_rmse         0.980420
dtype: float64
Performing stepwise search to minimize aic
 ARIMA(4,1,0)(0,0,0)[0] intercept   : AIC=-25526.550, Time=2.60 sec
 ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=-2584.548, Time=0.33 sec
 ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=-15998.099, Time=0.20 sec
 ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=-10467.897, Time=0.85 sec
 ARIMA(0,1,0)(0,0,0)[0]             : AIC=-2586.547, Time=0.15 sec
 ARIMA(3,1,0)(0,0,0)[0] intercept   : AIC=-24089.355, Time=0.64 sec
 ARIMA(5,1,0)(0,0,0)[0] intercept   : AIC=-25527.492, Time=1.69 sec
 ARIMA(6,1,0)(0,0,0)[0] intercept   : AIC=-25526.195, Time=3.71 sec
 ARIMA(5,1,1)(0,0,0)[0] intercept   : AIC=-25516.935, Time=6.92 sec
 ARIMA(4,1,1)(0,0,0)[0] intercept   : AIC=-25515.609, Time=5.23 sec
 ARIMA(6,1,1)(0,0,0)[0] intercept   : AIC=-25536.981, Time=9.14 sec
 ARIMA(7,1,1)(0,0,0)[0] intercept   : AIC=-25643.024, Time=9.23 sec
 ARIMA(7,1,0)(0,0,0)[0] intercept   : AIC=-26293.256, Time=1.47 sec
 ARIMA(8,1,0)(0,0,0)[0] intercept   : AIC=-26441.423, Time=1.31 sec
 ARIMA(9,1,0)(0,0,0)[0] intercept   : AIC=-27505.551, Time=2.24 sec
 ARIMA(10,1,0)(0,0,0)[0] intercept   : AIC=-27659.901, Time=2.58 sec
 ARIMA(10,1,1)(0,0,0)[0] intercept   : AIC=-27601.280, Time=17.32 sec
 ARIMA(9,1,1)(0,0,0)[0] intercept   : AIC=-27471.086, Time=14.04 sec
 ARIMA(10,1,0)(0,0,0)[0]             : AIC=-27661.901, Time=1.15 sec
 ARIMA(9,1,0)(0,0,0)[0]             : AIC=-27507.551, Time=1.50 sec
 ARIMA(10,1,1)(0,0,0)[0]             : AIC=-27527.078, Time=6.76 sec
 ARIMA(9,1,1)(0,0,0)[0]             : AIC=-27590.714, Time=6.71 sec

Best model:  ARIMA(10,1,0)(0,0,0)[0]          
Total fit time: 95.788 seconds
RMSE: 1.2192199483400497
MAE: 0.9467407105761635
R²: 0.7151905710464928
MAPE: 0.10635530554638706
Correlação Linear: 0.8504385143349134
index
2018-12-23    1.828576
2018-12-24    0.370741
2018-12-25   -1.996285
2018-12-26   -1.584708
2018-12-27   -0.466054
                ...   
2021-12-18    1.211753
2021-12-19    1.816038
2021-12-20    2.212831
2021-12-21    2.713684
2021-12-22    2.901109
Freq: D, Length: 1096, dtype: float64
count    1096.000000
mean        0.017126
std         1.219656
min        -4.168143
25%        -0.735406
50%         0.016673
75%         0.752848
max         4.187692
dtype: float64
Shapiro-Wilk Test p-value: 0.020714748880977654
Interpretação: Rejeitamos a hipótese nula de que os resíduos seguem uma distribuição normal (p <= 0.05).
Kolmogorov-Smirnov Test p-value: 0.009231103710401522
Interpretação: Rejeitamos a hipótese nula de que os resíduos seguem uma distribuição normal (p <= 0.05).
Shapiro-Francia Test p-value: 0.01811345773164433
Interpretação: Rejeitamos a hipótese nula de que os resíduos seguem uma distribuição normal (p <= 0.05).
Durbin-Watson Statistic: 1.6459757251787808
Interpretação: Não há evidência de autocorrelação nos resíduos (Durbin-Watson dentro do intervalo 1.5-2.5).
Breusch-Pagan Test p-value: 5.989851644943445e-07
Interpretação: Rejeitamos a hipótese nula de homocedasticidade (variância constante dos resíduos) (p <= 0.05).
====================================================================================================
Default LSTM
[I 2024-06-22 09:41:18,168] Trial 0 finished with value: 2.1446685791015625 and parameters: {'hidden_size': 93, 'n_layers': 6, 'rnn_dropout': 9.149985387590931e-05, 'bidirectional': True, 'fc_dropout': 0.07387087581503825, 'learning_rate_model': 5.5595654267125665e-05}. Best is trial 0 with value: 2.1446685791015625.
Epoch 13: reducing lr to 0.00196354066941974
Epoch 16: reducing lr to 0.0025108056982405735
Epoch 19: reducing lr to 0.002943111978946899
Epoch 23: reducing lr to 0.0032391887918001598
Epoch 26: reducing lr to 0.003249300514968436
Epoch 33: reducing lr to 0.0031428963833818628
Epoch 36: reducing lr to 0.003056752910061986
Epoch 52: reducing lr to 0.002264743892322208
Epoch 55: reducing lr to 0.002072081930251843
Epoch 60: reducing lr to 0.0017369405862873728
Epoch 68: reducing lr to 0.0011969298187882783
Epoch 71: reducing lr to 0.0010036427556606015
Epoch 75: reducing lr to 0.0007617292173376554
Epoch 78: reducing lr to 0.0005958552995617456
Epoch 81: reducing lr to 0.00044624624977347634
Epoch 84: reducing lr to 0.00031526193284838605
Epoch 87: reducing lr to 0.00020496751404468563
Epoch 90: reducing lr to 0.00011710305545961016
Epoch 93: reducing lr to 5.305379182478781e-05
Epoch 96: reducing lr to 1.3830006995171201e-05
Epoch 99: reducing lr to 5.0167409240686744e-08
[I 2024-06-22 09:41:31,743] Trial 1 finished with value: 2.0360779762268066 and parameters: {'hidden_size': 79, 'n_layers': 3, 'rnn_dropout': 0.4310533872026856, 'bidirectional': False, 'fc_dropout': 0.16356179978521396, 'learning_rate_model': 0.032543911150884876}. Best is trial 1 with value: 2.0360779762268066.
Epoch 10: reducing lr to 0.0006757932211192078
Epoch 16: reducing lr to 0.0012311802347119115
Epoch 19: reducing lr to 0.0014431627662636023
Epoch 22: reducing lr to 0.001569002510534908
Epoch 25: reducing lr to 0.0015952463012699255
Epoch 31: reducing lr to 0.0015628099463552117
Epoch 36: reducing lr to 0.0014988868982986681
Epoch 39: reducing lr to 0.0014455913455321777
Epoch 42: reducing lr to 0.0013820814304337892
Epoch 48: reducing lr to 0.001228570194173721
Epoch 52: reducing lr to 0.0011105231754354819
Epoch 70: reducing lr to 0.0005232685053022125
Epoch 73: reducing lr to 0.0004315425869513978
Epoch 77: reducing lr to 0.00031846622773632936
Epoch 80: reducing lr to 0.00024231133083625378
Epoch 83: reducing lr to 0.0001749186223215349
Epoch 86: reducing lr to 0.00011735064802960646
Epoch 92: reducing lr to 3.515195325218233e-05
Epoch 95: reducing lr to 1.1817427524651174e-05
Epoch 98: reducing lr to 8.799903768647033e-07
[I 2024-06-22 09:41:48,483] Trial 2 finished with value: 2.4684014320373535 and parameters: {'hidden_size': 21, 'n_layers': 5, 'rnn_dropout': 0.3338438418937016, 'bidirectional': True, 'fc_dropout': 0.15848119126790305, 'learning_rate_model': 0.015957993164212966}. Best is trial 1 with value: 2.0360779762268066.
[I 2024-06-22 09:42:29,865] Trial 3 finished with value: 7.427013397216797 and parameters: {'hidden_size': 195, 'n_layers': 3, 'rnn_dropout': 0.5538580925354513, 'bidirectional': False, 'fc_dropout': 0.06803536909582233, 'learning_rate_model': 1.432910723846784e-05}. Best is trial 1 with value: 2.0360779762268066.
Epoch 14: reducing lr to 0.0003875493217696109
Epoch 31: reducing lr to 0.0005733718242223647
Epoch 37: reducing lr to 0.000543838325978682
Epoch 40: reducing lr to 0.0005229985212725998
Epoch 43: reducing lr to 0.0004985274185023969
Epoch 49: reducing lr to 0.00044028615053900034
Epoch 56: reducing lr to 0.00036091241125794135
Epoch 59: reducing lr to 0.00032469426426718587
Epoch 62: reducing lr to 0.0002879721334006325
Epoch 68: reducing lr to 0.00021533141625953577
Epoch 71: reducing lr to 0.00018055846934601988
Epoch 74: reducing lr to 0.00014755464902484056
Epoch 77: reducing lr to 0.00011684054249606821
Epoch 80: reducing lr to 8.890043867160885e-05
Epoch 83: reducing lr to 6.417505199839924e-05
Epoch 86: reducing lr to 4.305421480797177e-05
Epoch 89: reducing lr to 2.587114040935477e-05
Epoch 92: reducing lr to 1.2896731050495921e-05
Epoch 95: reducing lr to 4.335639143599953e-06
Epoch 98: reducing lr to 3.228554366818161e-07
[I 2024-06-22 09:42:48,644] Trial 4 finished with value: 2.567143201828003 and parameters: {'hidden_size': 47, 'n_layers': 7, 'rnn_dropout': 0.07867746706644008, 'bidirectional': False, 'fc_dropout': 0.42653222797841367, 'learning_rate_model': 0.005854751355295724}. Best is trial 1 with value: 2.0360779762268066.
Epoch 16: reducing lr to 0.0007585880160435583
Epoch 23: reducing lr to 0.0009786539041567747
Epoch 27: reducing lr to 0.0009796519054996935
Epoch 30: reducing lr to 0.0009683657407269482
Epoch 36: reducing lr to 0.00092353467542478
Epoch 39: reducing lr to 0.0008906967801295091
Epoch 42: reducing lr to 0.0008515653360604329
Epoch 47: reducing lr to 0.0007740775959208248
Epoch 55: reducing lr to 0.0006260367027408444
Epoch 61: reducing lr to 0.0005042114915107441
Epoch 77: reducing lr to 0.00019622201288173498
Epoch 80: reducing lr to 0.00014929940112867363
Epoch 83: reducing lr to 0.00010777558551938148
Epoch 86: reducing lr to 7.230525049083132e-05
Epoch 92: reducing lr to 2.1658770767928047e-05
Epoch 95: reducing lr to 7.2812725934977285e-06
Epoch 98: reducing lr to 5.422034364283707e-07
[I 2024-06-22 09:43:07,321] Trial 5 finished with value: 2.124279499053955 and parameters: {'hidden_size': 74, 'n_layers': 5, 'rnn_dropout': 0.6677005375178984, 'bidirectional': False, 'fc_dropout': 0.7910888711251958, 'learning_rate_model': 0.009832469717408681}. Best is trial 1 with value: 2.0360779762268066.
[I 2024-06-22 09:43:27,492] Trial 6 finished with value: 2.6967387199401855 and parameters: {'hidden_size': 67, 'n_layers': 6, 'rnn_dropout': 0.08258080526211363, 'bidirectional': False, 'fc_dropout': 0.2348913186989436, 'learning_rate_model': 0.00014161242322273667}. Best is trial 1 with value: 2.0360779762268066.
[I 2024-06-22 09:43:31,068] Trial 7 finished with value: 9.376874923706055 and parameters: {'hidden_size': 40, 'n_layers': 1, 'rnn_dropout': 0.5430684263519128, 'bidirectional': False, 'fc_dropout': 0.39325852742427064, 'learning_rate_model': 1.634745612637677e-05}. Best is trial 1 with value: 2.0360779762268066.
Epoch 23: reducing lr to 0.0005964405285132827
Epoch 26: reducing lr to 0.000598302427247291
Epoch 29: reducing lr to 0.0005929791779537542
Epoch 32: reducing lr to 0.0005830295249875117
Epoch 36: reducing lr to 0.0005628481198215894
Epoch 41: reducing lr to 0.0005273401003006108
Epoch 44: reducing lr to 0.0005011398246968081
Epoch 48: reducing lr to 0.0004613412957604864
Epoch 54: reducing lr to 0.000393534856470155
Epoch 57: reducing lr to 0.00035713465155483103
Epoch 60: reducing lr to 0.0003198275333330205
Epoch 63: reducing lr to 0.00028220172029949625
Epoch 71: reducing lr to 0.00018480343508846887
Epoch 74: reducing lr to 0.00015102368834777127
Epoch 77: reducing lr to 0.00011958748702889134
Epoch 80: reducing lr to 9.099050577295571e-05
Epoch 83: reducing lr to 6.568382031173179e-05
Epoch 86: reducing lr to 4.406642801286793e-05
Epoch 89: reducing lr to 2.6479376096960953e-05
Epoch 92: reducing lr to 1.3199935778012914e-05
Epoch 95: reducing lr to 4.4375708873883346e-06
Epoch 98: reducing lr to 3.304458325986519e-07
[I 2024-06-22 09:44:01,591] Trial 8 finished with value: 1.8718087673187256 and parameters: {'hidden_size': 122, 'n_layers': 2, 'rnn_dropout': 0.47144442952262744, 'bidirectional': True, 'fc_dropout': 0.3312447902556547, 'learning_rate_model': 0.005992397731141758}. Best is trial 8 with value: 1.8718087673187256.
Epoch 35: reducing lr to 0.0002105883368559558
Epoch 41: reducing lr to 0.00019530363593748726
Epoch 44: reducing lr to 0.000185600203399226
Epoch 50: reducing lr to 0.00016283391487888142
Epoch 56: reducing lr to 0.00013680863925685368
Epoch 60: reducing lr to 0.00011845008581226695
Epoch 68: reducing lr to 8.162423105661492e-05
Epoch 71: reducing lr to 6.844308404754474e-05
Epoch 74: reducing lr to 5.593254795187342e-05
Epoch 77: reducing lr to 4.428995825664602e-05
Epoch 80: reducing lr to 3.369889110105399e-05
Epoch 83: reducing lr to 2.432640514505348e-05
Epoch 86: reducing lr to 1.63202715074856e-05
Epoch 89: reducing lr to 9.806799115304579e-06
Epoch 92: reducing lr to 4.888677060814516e-06
Epoch 95: reducing lr to 1.643481556861011e-06
Epoch 98: reducing lr to 1.2238263797899676e-07
[I 2024-06-22 09:44:10,787] Trial 9 finished with value: 1.8782336711883545 and parameters: {'hidden_size': 92, 'n_layers': 1, 'rnn_dropout': 0.4287171247324093, 'bidirectional': True, 'fc_dropout': 0.7556758047926507, 'learning_rate_model': 0.002219321201267233}. Best is trial 8 with value: 1.8718087673187256.
Epoch 15: reducing lr to 0.0036841238907239517
Epoch 20: reducing lr to 0.004807724910313659
Epoch 23: reducing lr to 0.005105059839047496
Epoch 26: reducing lr to 0.005120996221632114
Epoch 29: reducing lr to 0.005075433412127179
Epoch 32: reducing lr to 0.004990272241243915
Epoch 40: reducing lr to 0.0045816965990060085
Epoch 43: reducing lr to 0.004367318997969301
Epoch 46: reducing lr to 0.0041245102411927815
Epoch 49: reducing lr to 0.0038570999275589547
Epoch 52: reducing lr to 0.0035693051049355148
Epoch 58: reducing lr to 0.0029509663707425996
Epoch 61: reducing lr to 0.002630173777230844
Epoch 64: reducing lr to 0.002308346977519673
Epoch 67: reducing lr to 0.0019905592357530562
Epoch 70: reducing lr to 0.001681824376600479
Epoch 73: reducing lr to 0.001387010368332641
Epoch 76: reducing lr to 0.0011107661995227182
Epoch 79: reducing lr to 0.0008574492196222937
Epoch 82: reducing lr to 0.0006310536390586915
Epoch 85: reducing lr to 0.0004351505184014788
Epoch 88: reducing lr to 0.00027282857161450223
Epoch 91: reducing lr to 0.00014664867412751692
Epoch 94: reducing lr to 5.860011727938444e-05
Epoch 97: reducing lr to 1.007173882384832e-05
[I 2024-06-22 09:44:31,037] Trial 10 finished with value: 1.9381461143493652 and parameters: {'hidden_size': 183, 'n_layers': 1, 'rnn_dropout': 0.11142107780060684, 'bidirectional': True, 'fc_dropout': 0.13228335769354624, 'learning_rate_model': 0.051290191619112704}. Best is trial 8 with value: 1.8718087673187256.
Epoch 24: reducing lr to 2.486411739779812e-05
Epoch 29: reducing lr to 2.460549921118729e-05
Epoch 39: reducing lr to 2.252478388090555e-05
[I 2024-06-22 09:45:23,205] Trial 11 finished with value: 1.9576374292373657 and parameters: {'hidden_size': 80, 'n_layers': 6, 'rnn_dropout': 0.5807983882803612, 'bidirectional': True, 'fc_dropout': 0.6007539472218698, 'learning_rate_model': 0.0002486528079375971}. Best is trial 8 with value: 1.8718087673187256.
[I 2024-06-22 09:46:12,701] Trial 12 finished with value: 3.4496657848358154 and parameters: {'hidden_size': 65, 'n_layers': 7, 'rnn_dropout': 0.34247295189703597, 'bidirectional': True, 'fc_dropout': 0.49735657616729745, 'learning_rate_model': 2.877291692654975e-05}. Best is trial 8 with value: 1.8718087673187256.
Epoch 8: reducing lr to 6.102298581695267e-05
Epoch 27: reducing lr to 0.00019639350307533278
Epoch 31: reducing lr to 0.00019303934645426864
Epoch 34: reducing lr to 0.0001887791879328599
Epoch 40: reducing lr to 0.00017607996849154143
Epoch 43: reducing lr to 0.0001678411861059891
Epoch 46: reducing lr to 0.00015850976109370104
Epoch 49: reducing lr to 0.00014823286942672057
Epoch 52: reducing lr to 0.0001371725771955506
Epoch 55: reducing lr to 0.00012550329399123872
Epoch 58: reducing lr to 0.00011340909515760667
Epoch 61: reducing lr to 0.00010108065992902255
Epoch 64: reducing lr to 8.871247894445659e-05
Epoch 71: reducing lr to 6.078932979765917e-05
Epoch 74: reducing lr to 4.967780384513213e-05
Epoch 77: reducing lr to 3.933716483783135e-05
Epoch 80: reducing lr to 2.9930460227863736e-05
Epoch 83: reducing lr to 2.160606708088832e-05
Epoch 86: reducing lr to 1.4495231780711553e-05
Epoch 89: reducing lr to 8.71013856221796e-06
Epoch 92: reducing lr to 4.341993150362256e-06
Epoch 95: reducing lr to 1.459696677417327e-06
Epoch 98: reducing lr to 1.086970092763033e-07
[I 2024-06-22 09:48:32,864] Trial 13 finished with value: 1.8697621822357178 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.4627116915097054, 'bidirectional': True, 'fc_dropout': 0.7227036164498031, 'learning_rate_model': 0.001971142158600765}. Best is trial 13 with value: 1.8697621822357178.
Epoch 11: reducing lr to 0.002080870038743925
Epoch 16: reducing lr to 0.003322645161514056
Epoch 23: reducing lr to 0.004286542353256255
Epoch 26: reducing lr to 0.004299923583067541
Epoch 39: reducing lr to 0.0039012867120003765
Epoch 42: reducing lr to 0.0037298894574308967
Epoch 47: reducing lr to 0.0033904901268240932
Epoch 52: reducing lr to 0.002997022167492266
Epoch 58: reducing lr to 0.002477824497662145
Epoch 70: reducing lr to 0.0014121698174613072
Epoch 73: reducing lr to 0.0011646246813382575
Epoch 77: reducing lr to 0.0008594600862329056
Epoch 80: reducing lr to 0.0006539372126709804
Epoch 83: reducing lr to 0.00047206127724374046
Epoch 86: reducing lr to 0.00031669982337504744
Epoch 89: reducing lr to 0.00019030391413935727
Epoch 92: reducing lr to 9.486626254884834e-05
Epoch 95: reducing lr to 3.1892258565631436e-05
Epoch 98: reducing lr to 2.374872244886167e-06
[I 2024-06-22 09:48:39,108] Trial 14 finished with value: 2.5461912155151367 and parameters: {'hidden_size': 16, 'n_layers': 5, 'rnn_dropout': 0.26131592141767696, 'bidirectional': False, 'fc_dropout': 0.28581580800199985, 'learning_rate_model': 0.04306660168805714}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 09:48:46,499] Trial 15 finished with value: 7.774486541748047 and parameters: {'hidden_size': 131, 'n_layers': 1, 'rnn_dropout': 0.7435497869950091, 'bidirectional': False, 'fc_dropout': 0.13787240667626285, 'learning_rate_model': 3.5362503098265445e-05}. Best is trial 13 with value: 1.8697621822357178.
Epoch 27: reducing lr to 0.0006990499428589374
Epoch 33: reducing lr to 0.0006775780866480502
Epoch 37: reducing lr to 0.0006517190100054323
Epoch 47: reducing lr to 0.0005523583388742824
Epoch 54: reducing lr to 0.0004607672558827729
Epoch 57: reducing lr to 0.00041814835629445533
Epoch 60: reducing lr to 0.00037446760424584625
Epoch 64: reducing lr to 0.0003157663179581328
Epoch 67: reducing lr to 0.0002722950954395419
Epoch 74: reducing lr to 0.00017682492239051508
Epoch 77: reducing lr to 0.00014001808818273707
Epoch 80: reducing lr to 0.00010653553291935644
Epoch 86: reducing lr to 5.1594837860530814e-05
Epoch 90: reducing lr to 2.5246287048631255e-05
Epoch 93: reducing lr to 1.1437884794465465e-05
Epoch 96: reducing lr to 2.9816158520740153e-06
Epoch 99: reducing lr to 1.0815608603859105e-08
[I 2024-06-22 09:51:37,899] Trial 16 finished with value: 1.8791426420211792 and parameters: {'hidden_size': 188, 'n_layers': 5, 'rnn_dropout': 0.052800138177649995, 'bidirectional': True, 'fc_dropout': 0.7384196284371867, 'learning_rate_model': 0.007016152732955534}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 09:51:41,885] Trial 17 finished with value: 2.9665822982788086 and parameters: {'hidden_size': 38, 'n_layers': 1, 'rnn_dropout': 0.020968789502175424, 'bidirectional': False, 'fc_dropout': 0.6880223589463105, 'learning_rate_model': 0.0014299612122349629}. Best is trial 13 with value: 1.8697621822357178.
Epoch 31: reducing lr to 0.00017181013097574611
Epoch 37: reducing lr to 0.00016296045614510565
Epoch 40: reducing lr to 0.00015671583542116793
Epoch 44: reducing lr to 0.00014671656915903432
Epoch 47: reducing lr to 0.00013811560730406748
Epoch 56: reducing lr to 0.00010814694065774532
Epoch 59: reducing lr to 9.729421941246965e-05
Epoch 62: reducing lr to 8.62904800458764e-05
Epoch 65: reducing lr to 7.530929052105039e-05
Epoch 68: reducing lr to 6.452378241801423e-05
Epoch 71: reducing lr to 5.410411352039001e-05
Epoch 79: reducing lr to 2.9328845304485884e-05
Epoch 82: reducing lr to 2.1585038664957962e-05
Epoch 85: reducing lr to 1.4884219317367491e-05
Epoch 88: reducing lr to 9.332035983484077e-06
Epoch 91: reducing lr to 5.016082794370648e-06
Epoch 94: reducing lr to 2.0044029840844518e-06
Epoch 97: reducing lr to 3.445014155380134e-07
[I 2024-06-22 09:52:22,713] Trial 18 finished with value: 2.56270432472229 and parameters: {'hidden_size': 118, 'n_layers': 6, 'rnn_dropout': 0.09933865209592892, 'bidirectional': False, 'fc_dropout': 0.7756765986557397, 'learning_rate_model': 0.0017543687266950109}. Best is trial 13 with value: 1.8697621822357178.
Epoch 13: reducing lr to 0.0005875660937391622
Epoch 19: reducing lr to 0.0008806911085869152
Epoch 23: reducing lr to 0.0009692885586343039
Epoch 26: reducing lr to 0.0009723143710228558
Epoch 31: reducing lr to 0.0009537058856705771
Epoch 37: reducing lr to 0.0009045819665843155
Epoch 47: reducing lr to 0.000766669967834726
Epoch 58: reducing lr to 0.0005602946939421341
Epoch 68: reducing lr to 0.00035816695271872416
Epoch 71: reducing lr to 0.000300328107605419
Epoch 74: reducing lr to 0.0002454319017574735
Epoch 78: reducing lr to 0.00017830258178494208
Epoch 81: reducing lr to 0.00013353386049428582
Epoch 84: reducing lr to 9.433836806809048e-05
Epoch 87: reducing lr to 6.133408054453619e-05
Epoch 90: reducing lr to 3.5041690723756205e-05
Epoch 93: reducing lr to 1.5875713554613403e-05
Epoch 96: reducing lr to 4.138464414357897e-06
Epoch 99: reducing lr to 1.5011998039798913e-08
[I 2024-06-22 09:52:40,114] Trial 19 finished with value: 2.4595658779144287 and parameters: {'hidden_size': 19, 'n_layers': 6, 'rnn_dropout': 0.18637941907281635, 'bidirectional': True, 'fc_dropout': 0.6908334836475429, 'learning_rate_model': 0.009738376723091037}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 09:52:52,096] Trial 20 finished with value: 2.1529083251953125 and parameters: {'hidden_size': 118, 'n_layers': 1, 'rnn_dropout': 0.04793415160976933, 'bidirectional': True, 'fc_dropout': 0.08599530328487433, 'learning_rate_model': 7.995347689531267e-05}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 09:53:28,483] Trial 21 finished with value: 3.378385543823242 and parameters: {'hidden_size': 147, 'n_layers': 4, 'rnn_dropout': 0.010044784127292684, 'bidirectional': False, 'fc_dropout': 0.4544803695359537, 'learning_rate_model': 6.503886274016054e-05}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 09:53:48,324] Trial 22 finished with value: 3.6776247024536133 and parameters: {'hidden_size': 62, 'n_layers': 6, 'rnn_dropout': 0.15634358488745503, 'bidirectional': False, 'fc_dropout': 0.6774630411920283, 'learning_rate_model': 9.107329229641331e-05}. Best is trial 13 with value: 1.8697621822357178.
Epoch 12: reducing lr to 4.7922377543657334e-05
Epoch 35: reducing lr to 8.367478332996436e-05
Epoch 52: reducing lr to 6.136627971297614e-05
Epoch 57: reducing lr to 5.255472926704894e-05
Epoch 60: reducing lr to 4.7064739736923445e-05
Epoch 63: reducing lr to 4.1527852154549274e-05
Epoch 66: reducing lr to 3.603140285782803e-05
Epoch 69: reducing lr to 3.0662050005489765e-05
Epoch 72: reducing lr to 2.550450329986011e-05
Epoch 75: reducing lr to 2.0640076953142113e-05
Epoch 78: reducing lr to 1.6145500206591593e-05
Epoch 81: reducing lr to 1.2091641919116278e-05
Epoch 84: reducing lr to 8.542445801317618e-06
Epoch 87: reducing lr to 5.5538702815718e-06
Epoch 90: reducing lr to 3.1730646811503766e-06
Epoch 93: reducing lr to 1.437563796944665e-06
Epoch 96: reducing lr to 3.747426278862269e-07
Epoch 99: reducing lr to 1.3593533813556837e-09
[I 2024-06-22 09:54:58,424] Trial 23 finished with value: 1.9106500148773193 and parameters: {'hidden_size': 107, 'n_layers': 5, 'rnn_dropout': 0.663184719640143, 'bidirectional': True, 'fc_dropout': 0.056017714975377865, 'learning_rate_model': 0.0008818210135856344}. Best is trial 13 with value: 1.8697621822357178.
Epoch 30: reducing lr to 0.0001574597926944094
Epoch 35: reducing lr to 0.0001517074921548476
Epoch 41: reducing lr to 0.00014069641870559084
Epoch 47: reducing lr to 0.00012586783346091993
Epoch 50: reducing lr to 0.0001173052849594808
Epoch 53: reducing lr to 0.00010815347978194552
Epoch 56: reducing lr to 9.855671912624027e-05
Epoch 63: reducing lr to 7.529253204158418e-05
Epoch 66: reducing lr to 6.532713380118921e-05
Epoch 72: reducing lr to 4.624122203004525e-05
Epoch 75: reducing lr to 3.742171999533508e-05
Epoch 78: reducing lr to 2.9272777872260637e-05
Epoch 81: reducing lr to 2.1922885229947807e-05
Epoch 84: reducing lr to 1.548797592072779e-05
Epoch 87: reducing lr to 1.0069505992600148e-05
Epoch 90: reducing lr to 5.7529600444159196e-06
Epoch 93: reducing lr to 2.606390955170565e-06
Epoch 96: reducing lr to 6.794312697046279e-07
Epoch 99: reducing lr to 2.464590695438508e-09
[I 2024-06-22 09:56:07,656] Trial 24 finished with value: 1.872422456741333 and parameters: {'hidden_size': 128, 'n_layers': 4, 'rnn_dropout': 0.2538899274577286, 'bidirectional': True, 'fc_dropout': 0.3041129380988403, 'learning_rate_model': 0.0015987953500037988}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 09:56:55,584] Trial 25 finished with value: 3.07677960395813 and parameters: {'hidden_size': 153, 'n_layers': 5, 'rnn_dropout': 0.2119356461302475, 'bidirectional': False, 'fc_dropout': 0.5037740056172516, 'learning_rate_model': 6.929406508731719e-05}. Best is trial 13 with value: 1.8697621822357178.
Epoch 35: reducing lr to 0.00011909551018517693
Epoch 38: reducing lr to 0.0001151869108808319
Epoch 41: reducing lr to 0.00011045144527118325
Epoch 44: reducing lr to 0.00010496379450217537
Epoch 47: reducing lr to 9.881050453744613e-05
Epoch 50: reducing lr to 9.208861448587672e-05
Epoch 53: reducing lr to 8.490413802230555e-05
Epoch 57: reducing lr to 7.480189425792816e-05
Epoch 62: reducing lr to 6.173383324695684e-05
Epoch 68: reducing lr to 4.6161528156279705e-05
Epoch 71: reducing lr to 3.870710094863811e-05
Epoch 74: reducing lr to 3.163192906946997e-05
Epoch 77: reducing lr to 2.504761305116062e-05
Epoch 80: reducing lr to 1.9057972004878574e-05
Epoch 83: reducing lr to 1.3757483795045806e-05
Epoch 86: reducing lr to 9.22971846666937e-06
Epoch 89: reducing lr to 5.546108399724094e-06
Epoch 92: reducing lr to 2.764728082194426e-06
Epoch 95: reducing lr to 9.294497378939653e-07
Epoch 98: reducing lr to 6.92119180270307e-08
[I 2024-06-22 09:57:11,574] Trial 26 finished with value: 1.885901689529419 and parameters: {'hidden_size': 155, 'n_layers': 1, 'rnn_dropout': 0.20825207886283278, 'bidirectional': True, 'fc_dropout': 0.5115687047039521, 'learning_rate_model': 0.001255108400948586}. Best is trial 13 with value: 1.8697621822357178.
Epoch 21: reducing lr to 0.005150710445522589
Epoch 31: reducing lr to 0.0052337885008497936
Epoch 37: reducing lr to 0.004964204128253021
Epoch 40: reducing lr to 0.004773976555807207
Epoch 49: reducing lr to 0.004018970752355568
Epoch 55: reducing lr to 0.003402714052731871
Epoch 58: reducing lr to 0.0030748095092016747
Epoch 61: reducing lr to 0.002740554220225543
Epoch 64: reducing lr to 0.0024052213225419844
Epoch 67: reducing lr to 0.002074096989855731
Epoch 70: reducing lr to 0.0017524054619019584
Epoch 73: reducing lr to 0.0014452190008648947
Epoch 76: reducing lr to 0.0011573816993152616
Epoch 79: reducing lr to 0.000893433771489819
Epoch 82: reducing lr to 0.0006575370527539092
Epoch 85: reducing lr to 0.00045341247029467293
Epoch 88: reducing lr to 0.00028427836206451824
Epoch 91: reducing lr to 0.0001528030756940267
Epoch 94: reducing lr to 6.105938706636102e-05
Epoch 97: reducing lr to 1.0494419257638804e-05
[I 2024-06-22 09:57:37,037] Trial 27 finished with value: 2.105393886566162 and parameters: {'hidden_size': 187, 'n_layers': 2, 'rnn_dropout': 0.05276887254721903, 'bidirectional': False, 'fc_dropout': 0.7262526820028192, 'learning_rate_model': 0.05344268592242116}. Best is trial 13 with value: 1.8697621822357178.
Epoch 49: reducing lr to 0.003456622622569963
Epoch 54: reducing lr to 0.0030186167197184905
Epoch 57: reducing lr to 0.0027394082446569957
Epoch 60: reducing lr to 0.0024532432735563684
Epoch 63: reducing lr to 0.0021646337477452494
Epoch 66: reducing lr to 0.0018781320621734832
Epoch 69: reducing lr to 0.0015982552617921665
Epoch 72: reducing lr to 0.001329418828522519
Epoch 75: reducing lr to 0.0010758612548165693
Epoch 78: reducing lr to 0.0008415820421280196
Epoch 81: reducing lr to 0.0006302752202632982
Epoch 84: reducing lr to 0.0004452738465981834
Epoch 87: reducing lr to 0.00028949474673885185
Epoch 90: reducing lr to 0.00016539557276005687
Epoch 93: reducing lr to 7.493282093719716e-05
Epoch 96: reducing lr to 1.9533409433803577e-05
Epoch 99: reducing lr to 7.085611346963061e-08
[I 2024-06-22 09:57:41,325] Trial 28 finished with value: 2.020477533340454 and parameters: {'hidden_size': 18, 'n_layers': 2, 'rnn_dropout': 0.4934226856013261, 'bidirectional': False, 'fc_dropout': 0.44532255055612513, 'learning_rate_model': 0.04596480257092418}. Best is trial 13 with value: 1.8697621822357178.
Epoch 6: reducing lr to 0.0009882737498718573
Epoch 20: reducing lr to 0.0044344425323271305
Epoch 24: reducing lr to 0.004730569444692093
Epoch 27: reducing lr to 0.004713493500562928
Epoch 30: reducing lr to 0.004659191289742971
Epoch 33: reducing lr to 0.004568714925399721
Epoch 36: reducing lr to 0.004443491270441423
Epoch 39: reducing lr to 0.004285495144289374
Epoch 42: reducing lr to 0.004097218261192666
Epoch 45: reducing lr to 0.0038816298610410043
Epoch 48: reducing lr to 0.0036421299960203296
Epoch 51: reducing lr to 0.003382495645074353
Epoch 55: reducing lr to 0.003012110641460636
Epoch 58: reducing lr to 0.0027218468256816892
Epoch 61: reducing lr to 0.00242596127747314
Epoch 64: reducing lr to 0.0021291218211181304
Epoch 67: reducing lr to 0.0018360078213302063
Epoch 70: reducing lr to 0.001551243818360474
Epoch 73: reducing lr to 0.0012793198206741234
Epoch 78: reducing lr to 0.0008661733218061074
Epoch 81: reducing lr to 0.0006486920512314018
Epoch 84: reducing lr to 0.0004582848819422197
Epoch 87: reducing lr to 0.0002979538700637642
Epoch 90: reducing lr to 0.00017022848100151018
Epoch 93: reducing lr to 7.712238043881756e-05
Epoch 96: reducing lr to 2.0104181515915447e-05
Epoch 99: reducing lr to 7.292655035545659e-08
[I 2024-06-22 09:58:38,675] Trial 29 finished with value: 1.8938544988632202 and parameters: {'hidden_size': 134, 'n_layers': 3, 'rnn_dropout': 0.3887925336775278, 'bidirectional': True, 'fc_dropout': 0.740945141365163, 'learning_rate_model': 0.047307907887800385}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 09:59:10,197] Trial 30 finished with value: 9.076017379760742 and parameters: {'hidden_size': 89, 'n_layers': 7, 'rnn_dropout': 0.13916453334437148, 'bidirectional': False, 'fc_dropout': 0.40452973254151736, 'learning_rate_model': 1.2192681272217677e-05}. Best is trial 13 with value: 1.8697621822357178.
Epoch 7: reducing lr to 0.0004448181090263408
Epoch 12: reducing lr to 0.0009399844756701832
Epoch 15: reducing lr to 0.0012424035202654431
Epoch 26: reducing lr to 0.00172696248056184
Epoch 32: reducing lr to 0.001682878204833128
Epoch 36: reducing lr to 0.001624625842918241
Epoch 42: reducing lr to 0.0014980217729895268
Epoch 47: reducing lr to 0.0013617100691737088
Epoch 50: reducing lr to 0.001269075531884894
Epoch 54: reducing lr to 0.0011359137134777915
Epoch 58: reducing lr to 0.0009951595320740144
Epoch 61: reducing lr to 0.0008869780866949426
Epoch 64: reducing lr to 0.0007784478741568538
Epoch 67: reducing lr to 0.0006712797601685739
Epoch 70: reducing lr to 0.0005671645655613578
Epoch 76: reducing lr to 0.0003745856212798835
Epoch 79: reducing lr to 0.00028915909467372924
Epoch 83: reducing lr to 0.0001895922331123052
Epoch 86: reducing lr to 0.0001271949842836722
Epoch 89: reducing lr to 7.643106052323753e-05
Epoch 92: reducing lr to 3.810078782286487e-05
Epoch 95: reducing lr to 1.2808770411665976e-05
Epoch 98: reducing lr to 9.53811197760765e-07
[I 2024-06-22 10:00:33,672] Trial 31 finished with value: 2.4619367122650146 and parameters: {'hidden_size': 191, 'n_layers': 6, 'rnn_dropout': 0.012015184593697903, 'bidirectional': False, 'fc_dropout': 0.10479747584873352, 'learning_rate_model': 0.017296680706943544}. Best is trial 13 with value: 1.8697621822357178.
Epoch 16: reducing lr to 5.3316842665558035e-05
Epoch 32: reducing lr to 6.723741072031034e-05
Epoch 38: reducing lr to 6.342244173591978e-05
Epoch 49: reducing lr to 5.196939154444111e-05
Epoch 63: reducing lr to 3.254468626466524e-05
Epoch 68: reducing lr to 2.5416749242989383e-05
Epoch 80: reducing lr to 1.0493406844938147e-05
Epoch 83: reducing lr to 7.57493371210243e-06
Epoch 86: reducing lr to 5.081925343903735e-06
Epoch 89: reducing lr to 3.0537127365669303e-06
Epoch 92: reducing lr to 1.5222719696862393e-06
Epoch 95: reducing lr to 5.117592910276878e-07
Epoch 98: reducing lr to 3.8108399686494645e-08
[I 2024-06-22 10:01:42,116] Trial 32 finished with value: 2.4705891609191895 and parameters: {'hidden_size': 79, 'n_layers': 7, 'rnn_dropout': 0.46561134395766407, 'bidirectional': True, 'fc_dropout': 0.7243138549669195, 'learning_rate_model': 0.0006910684453876749}. Best is trial 13 with value: 1.8697621822357178.
Epoch 36: reducing lr to 0.00022218576011006266
Epoch 39: reducing lr to 0.000214285555688135
Epoch 42: reducing lr to 0.0002048712371183497
Epoch 47: reducing lr to 0.0001862290865848995
Epoch 81: reducing lr to 3.243623711809084e-05
Epoch 84: reducing lr to 2.291539887084486e-05
Epoch 87: reducing lr to 1.4898444279215537e-05
Epoch 92: reducing lr to 5.210709001162621e-06
Epoch 95: reducing lr to 1.7517426565610406e-06
Epoch 98: reducing lr to 1.3044435240258148e-07
[I 2024-06-22 10:02:22,392] Trial 33 finished with value: 2.0758399963378906 and parameters: {'hidden_size': 117, 'n_layers': 6, 'rnn_dropout': 0.22857508138731808, 'bidirectional': False, 'fc_dropout': 0.012426620440668579, 'learning_rate_model': 0.002365514599564762}. Best is trial 13 with value: 1.8697621822357178.
Epoch 8: reducing lr to 0.0004390282253951266
Epoch 14: reducing lr to 0.0009387186811004279
Epoch 17: reducing lr to 0.0011639606943415078
Epoch 26: reducing lr to 0.0014159146299195
Epoch 34: reducing lr to 0.001358166775357492
Epoch 37: reducing lr to 0.0013172805818972894
Epoch 43: reducing lr to 0.0012075288860063404
Epoch 52: reducing lr to 0.0009868844064707869
Epoch 58: reducing lr to 0.0008159186759570048
Epoch 61: reducing lr to 0.0007272220812584147
Epoch 67: reducing lr to 0.000550373759644341
Epoch 70: reducing lr to 0.00046501103236996926
Epoch 77: reducing lr to 0.0002830101713251614
Epoch 83: reducing lr to 0.00015544426680044987
Epoch 86: reducing lr to 0.00010428555404460252
Epoch 89: reducing lr to 6.266485693418605e-05
Epoch 92: reducing lr to 3.1238352597157995e-05
Epoch 95: reducing lr to 1.050174837108099e-05
Epoch 98: reducing lr to 7.82017701190206e-07
[I 2024-06-22 10:03:30,799] Trial 34 finished with value: 2.4606926441192627 and parameters: {'hidden_size': 96, 'n_layers': 6, 'rnn_dropout': 0.2521958424762984, 'bidirectional': True, 'fc_dropout': 0.14720816130197206, 'learning_rate_model': 0.01418132908946585}. Best is trial 13 with value: 1.8697621822357178.
Epoch 18: reducing lr to 0.006985457060388002
Epoch 22: reducing lr to 0.007937467496970102
Epoch 25: reducing lr to 0.00807023289062487
Epoch 28: reducing lr to 0.008019593775319415
Epoch 34: reducing lr to 0.007731660273018576
Epoch 37: reducing lr to 0.00749890670885633
Epoch 40: reducing lr to 0.00721154970612861
Epoch 43: reducing lr to 0.0068741212683546575
Epoch 46: reducing lr to 0.006491942444257698
Epoch 49: reducing lr to 0.006071040988425777
Epoch 59: reducing lr to 0.004477161465695991
Epoch 68: reducing lr to 0.002969173235649201
Epoch 81: reducing lr to 0.0011069842195740366
Epoch 84: reducing lr to 0.0007820569581766287
Epoch 87: reducing lr to 0.0005084542529779551
Epoch 90: reducing lr to 0.0002904926025114968
Epoch 93: reducing lr to 0.0001316083000550014
Epoch 96: reducing lr to 3.430751408672883e-05
Epoch 99: reducing lr to 1.2444817271833886e-07
[I 2024-06-22 10:03:38,019] Trial 35 finished with value: 2.00732421875 and parameters: {'hidden_size': 129, 'n_layers': 1, 'rnn_dropout': 0.3361549440009058, 'bidirectional': False, 'fc_dropout': 0.0003216199130859465, 'learning_rate_model': 0.08073030553318121}. Best is trial 13 with value: 1.8697621822357178.
Epoch 32: reducing lr to 1.3501904881526192e-05
Epoch 35: reducing lr to 1.316798367616257e-05
Epoch 38: reducing lr to 1.2735823204653191e-05
Epoch 41: reducing lr to 1.2212238950721815e-05
Epoch 44: reducing lr to 1.1605488153530383e-05
[I 2024-06-22 10:04:51,306] Trial 36 finished with value: 2.0693464279174805 and parameters: {'hidden_size': 85, 'n_layers': 7, 'rnn_dropout': 0.48377288077924246, 'bidirectional': True, 'fc_dropout': 0.5024609586458804, 'learning_rate_model': 0.00013877304786559896}. Best is trial 13 with value: 1.8697621822357178.
Epoch 18: reducing lr to 1.6899825433982015e-05
Epoch 60: reducing lr to 1.0424115266677422e-05
Epoch 63: reducing lr to 9.197779910316731e-06
Epoch 66: reducing lr to 7.980401011660662e-06
Epoch 69: reducing lr to 6.7911720187225185e-06
Epoch 72: reducing lr to 5.648854826419475e-06
Epoch 75: reducing lr to 4.571459280881807e-06
Epoch 78: reducing lr to 3.5759797277628933e-06
Epoch 81: reducing lr to 2.678112528249499e-06
Epoch 84: reducing lr to 1.8920202297946531e-06
Epoch 87: reducing lr to 1.2300967627758762e-06
Epoch 90: reducing lr to 7.027849759675872e-07
Epoch 93: reducing lr to 3.1839824901437393e-07
Epoch 96: reducing lr to 8.299972272786339e-08
Epoch 99: reducing lr to 3.010758460494619e-10
[I 2024-06-22 10:06:36,411] Trial 37 finished with value: 1.9476675987243652 and parameters: {'hidden_size': 124, 'n_layers': 6, 'rnn_dropout': 0.6866510691432339, 'bidirectional': True, 'fc_dropout': 0.6915835440436799, 'learning_rate_model': 0.00019530977843660444}. Best is trial 13 with value: 1.8697621822357178.
Epoch 31: reducing lr to 0.00030102693974664517
Epoch 35: reducing lr to 0.0002916699078230493
Epoch 47: reducing lr to 0.00024199107678849304
Epoch 50: reducing lr to 0.00022552888565559887
Epoch 53: reducing lr to 0.00020793380096576918
Epoch 56: reducing lr to 0.0001894832534279298
Epoch 59: reducing lr to 0.00017046830101601456
Epoch 62: reducing lr to 0.00015118875115196806
Epoch 65: reducing lr to 0.00013194871065690006
Epoch 68: reducing lr to 0.00011305152176919733
Epoch 71: reducing lr to 9.479531636610679e-05
Epoch 74: reducing lr to 7.746792319552821e-05
Epoch 77: reducing lr to 6.134265664977782e-05
Epoch 80: reducing lr to 4.6673774093702446e-05
Epoch 83: reducing lr to 3.369265578642717e-05
Epoch 86: reducing lr to 2.260396827907631e-05
Epoch 89: reducing lr to 1.3582652471188626e-05
Epoch 92: reducing lr to 6.770935223633732e-06
Epoch 95: reducing lr to 2.276261455668414e-06
Epoch 98: reducing lr to 1.6950289494378072e-07
[I 2024-06-22 10:07:59,011] Trial 38 finished with value: 1.8786978721618652 and parameters: {'hidden_size': 140, 'n_layers': 4, 'rnn_dropout': 0.3056822016252138, 'bidirectional': True, 'fc_dropout': 0.2539071567666215, 'learning_rate_model': 0.003073813202894127}. Best is trial 13 with value: 1.8697621822357178.
Epoch 14: reducing lr to 0.0010265349524179027
Epoch 17: reducing lr to 0.001272848149332121
Epoch 20: reducing lr to 0.0014536521815160098
Epoch 23: reducing lr to 0.0015435536579643896
Epoch 31: reducing lr to 0.0015187388681479155
Epoch 34: reducing lr to 0.0014852220310378824
Epoch 39: reducing lr to 0.0014048258196962815
Epoch 42: reducing lr to 0.00134310687760889
Epoch 45: reducing lr to 0.0012724349620512096
Epoch 49: reducing lr to 0.0011662234900323201
Epoch 56: reducing lr to 0.0009559794950123757
Epoch 59: reducing lr to 0.0008600453991196163
Epoch 64: reducing lr to 0.0006979462598554634
Epoch 67: reducing lr to 0.0006018607198764401
Epoch 70: reducing lr to 0.0005085123877881338
Epoch 73: reducing lr to 0.00041937313081012555
Epoch 77: reducing lr to 0.00030948551318323665
Epoch 80: reducing lr to 0.00023547817646792907
Epoch 83: reducing lr to 0.00016998593533536395
Epoch 86: reducing lr to 0.00011404137194069297
Epoch 89: reducing lr to 6.852709680369874e-05
Epoch 92: reducing lr to 3.41606718205989e-05
Epoch 95: reducing lr to 1.1484177295560846e-05
Epoch 98: reducing lr to 8.551747396142243e-07
[I 2024-06-22 10:08:35,834] Trial 39 finished with value: 2.559417724609375 and parameters: {'hidden_size': 95, 'n_layers': 7, 'rnn_dropout': 0.5422407131474489, 'bidirectional': False, 'fc_dropout': 0.2746769918195384, 'learning_rate_model': 0.015507979414037055}. Best is trial 13 with value: 1.8697621822357178.
Epoch 39: reducing lr to 0.00011692997317746161
Epoch 42: reducing lr to 0.00011179282795871824
Epoch 47: reducing lr to 0.00010162029834118774
Epoch 50: reducing lr to 9.47072633794213e-05
Epoch 56: reducing lr to 7.957047424866338e-05
Epoch 74: reducing lr to 3.253142046176528e-05
Epoch 77: reducing lr to 2.5759871613943264e-05
Epoch 84: reducing lr to 1.2504328472883104e-05
Epoch 87: reducing lr to 8.129687903415884e-06
Epoch 90: reducing lr to 4.644693564539613e-06
Epoch 93: reducing lr to 2.1042884363338397e-06
Epoch 96: reducing lr to 5.485437099475706e-07
Epoch 99: reducing lr to 1.989804979335777e-09
[I 2024-06-22 10:12:20,525] Trial 40 finished with value: 1.9058396816253662 and parameters: {'hidden_size': 178, 'n_layers': 7, 'rnn_dropout': 0.5301758499002098, 'bidirectional': True, 'fc_dropout': 0.6839183541579219, 'learning_rate_model': 0.001290798895846048}. Best is trial 13 with value: 1.8697621822357178.
Epoch 38: reducing lr to 6.692084394589008e-05
Epoch 49: reducing lr to 5.4836039835702264e-05
Epoch 63: reducing lr to 3.4339861588017895e-05
[I 2024-06-22 10:13:13,971] Trial 41 finished with value: 2.478527545928955 and parameters: {'hidden_size': 164, 'n_layers': 5, 'rnn_dropout': 0.5865140202300089, 'bidirectional': False, 'fc_dropout': 0.4550863925637725, 'learning_rate_model': 0.000729188002289183}. Best is trial 13 with value: 1.8697621822357178.
Epoch 44: reducing lr to 0.0014784038222587438
Epoch 48: reducing lr to 0.0013609948788858425
Epoch 55: reducing lr to 0.001125568599183683
Epoch 58: reducing lr to 0.001017102518282488
Epoch 62: reducing lr to 0.0008695144403634662
Epoch 65: reducing lr to 0.0007588614128321801
Epoch 68: reducing lr to 0.0006501801882375178
Epoch 71: reducing lr to 0.0005451853780861135
Epoch 78: reducing lr to 0.000323672536810537
Epoch 81: reducing lr to 0.0002424039121790211
Epoch 84: reducing lr to 0.0001712523655321728
Epoch 91: reducing lr to 5.0545047628025586e-05
Epoch 96: reducing lr to 7.512551204171685e-06
Epoch 99: reducing lr to 2.725126826280677e-08
[I 2024-06-22 10:13:19,275] Trial 42 finished with value: 1.9978018999099731 and parameters: {'hidden_size': 79, 'n_layers': 1, 'rnn_dropout': 0.3023393434624799, 'bidirectional': False, 'fc_dropout': 0.14529028106461103, 'learning_rate_model': 0.01767806762428918}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 10:14:19,306] Trial 43 finished with value: 2.8011043071746826 and parameters: {'hidden_size': 177, 'n_layers': 5, 'rnn_dropout': 0.4555955301963006, 'bidirectional': False, 'fc_dropout': 0.276137640924172, 'learning_rate_model': 7.946206212265512e-05}. Best is trial 13 with value: 1.8697621822357178.
Epoch 65: reducing lr to 2.53850841406616e-06
[I 2024-06-22 10:15:10,912] Trial 44 finished with value: 2.011934757232666 and parameters: {'hidden_size': 125, 'n_layers': 3, 'rnn_dropout': 0.7330444427746806, 'bidirectional': True, 'fc_dropout': 0.08871304059522336, 'learning_rate_model': 5.91358615023068e-05}. Best is trial 13 with value: 1.8697621822357178.
Epoch 23: reducing lr to 0.0002915457964494177
Epoch 30: reducing lr to 0.00028848090212016555
Epoch 36: reducing lr to 0.00027512550795714106
Epoch 39: reducing lr to 0.00026534293794243174
Epoch 42: reducing lr to 0.00025368548889034365
Epoch 47: reducing lr to 0.00023060151117553288
Epoch 56: reducing lr to 0.00018056502397866448
Epoch 62: reducing lr to 0.0001440728928978561
Epoch 68: reducing lr to 0.00010773063249541362
Epoch 71: reducing lr to 9.033367468129064e-05
Epoch 74: reducing lr to 7.382181357097209e-05
Epoch 77: reducing lr to 5.845549972623315e-05
Epoch 80: reducing lr to 4.447702362050491e-05
Epoch 83: reducing lr to 3.2106875356639706e-05
Epoch 86: reducing lr to 2.1540088638370268e-05
Epoch 89: reducing lr to 1.2943370587031227e-05
Epoch 92: reducing lr to 6.452254006069323e-06
Epoch 95: reducing lr to 2.1691268061365105e-06
Epoch 98: reducing lr to 1.6152506216926196e-07
[I 2024-06-22 10:15:46,489] Trial 45 finished with value: 2.566155433654785 and parameters: {'hidden_size': 108, 'n_layers': 6, 'rnn_dropout': 0.1665555507270332, 'bidirectional': False, 'fc_dropout': 0.3326789746140219, 'learning_rate_model': 0.0029291409380281584}. Best is trial 13 with value: 1.8697621822357178.
Epoch 47: reducing lr to 0.00011807619664012413
Epoch 52: reducing lr to 0.00010437339899146323
Epoch 55: reducing lr to 9.549434476117259e-05
Epoch 58: reducing lr to 8.629197599219307e-05
Epoch 61: reducing lr to 7.69113788250272e-05
Epoch 64: reducing lr to 6.750053946418006e-05
Epoch 67: reducing lr to 5.8207809985789785e-05
Epoch 70: reducing lr to 4.917980434056004e-05
Epoch 73: reducing lr to 4.055887135540762e-05
Epoch 76: reducing lr to 3.248095646648582e-05
Epoch 79: reducing lr to 2.507347701680251e-05
Epoch 82: reducing lr to 1.8453231460491162e-05
Epoch 85: reducing lr to 1.2724644529731304e-05
Epoch 88: reducing lr to 7.978036207108145e-06
Epoch 91: reducing lr to 4.288291453458429e-06
Epoch 94: reducing lr to 1.713581003005447e-06
Epoch 97: reducing lr to 2.945171633957121e-07
[I 2024-06-22 10:15:53,049] Trial 46 finished with value: 1.8821499347686768 and parameters: {'hidden_size': 59, 'n_layers': 1, 'rnn_dropout': 0.41268561357482386, 'bidirectional': True, 'fc_dropout': 0.4974449853923324, 'learning_rate_model': 0.0014998246092237527}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 10:16:09,347] Trial 47 finished with value: 9.126129150390625 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.6012222537081952, 'bidirectional': False, 'fc_dropout': 0.6282368225772952, 'learning_rate_model': 1.2283488171661822e-05}. Best is trial 13 with value: 1.8697621822357178.
Epoch 14: reducing lr to 0.0013379704316993323
Epoch 17: reducing lr to 0.001659011399308193
Epoch 21: reducing lr to 0.0019480802787326533
Epoch 31: reducing lr to 0.0019795017152296466
Epoch 35: reducing lr to 0.0019179714722626696
Epoch 41: reducing lr to 0.001778763286465838
Epoch 44: reducing lr to 0.00169038751471481
Epoch 49: reducing lr to 0.0015200383998042546
Epoch 54: reducing lr to 0.0013274267483824498
Epoch 57: reducing lr to 0.0012046457421849266
Epoch 64: reducing lr to 0.000909692803349949
Epoch 71: reducing lr to 0.0006233578014657957
Epoch 77: reducing lr to 0.00040337882767959714
Epoch 80: reducing lr to 0.0003069187626611902
Epoch 83: reducing lr to 0.00022155714693179035
Epoch 86: reducing lr to 0.00014863983275744918
Epoch 89: reducing lr to 8.931720159901652e-05
Epoch 92: reducing lr to 4.4524512989343315e-05
Epoch 95: reducing lr to 1.4968306356896374e-05
Epoch 98: reducing lr to 1.114622071898136e-06
[I 2024-06-22 10:16:36,285] Trial 48 finished with value: 2.557157278060913 and parameters: {'hidden_size': 76, 'n_layers': 7, 'rnn_dropout': 0.6757676860816557, 'bidirectional': False, 'fc_dropout': 0.7598447930829013, 'learning_rate_model': 0.020212870358199457}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 10:16:55,224] Trial 49 finished with value: 9.109418869018555 and parameters: {'hidden_size': 174, 'n_layers': 1, 'rnn_dropout': 0.5210434658727928, 'bidirectional': True, 'fc_dropout': 0.6396922093888224, 'learning_rate_model': 1.3749435862460687e-05}. Best is trial 13 with value: 1.8697621822357178.
Epoch 14: reducing lr to 0.001016654149128651
Epoch 17: reducing lr to 0.001260596484494976
Epoch 26: reducing lr to 0.0015334684525849071
Epoch 33: reducing lr to 0.0014832522973659468
Epoch 36: reducing lr to 0.0014425979171005365
Epoch 43: reducing lr to 0.0013077818486704922
Epoch 46: reducing lr to 0.001235073423900461
Epoch 52: reducing lr to 0.0010688187491621384
Epoch 55: reducing lr to 0.0009778942441842156
Epoch 58: reducing lr to 0.0008836588894671205
Epoch 61: reducing lr to 0.000787598415941446
Epoch 70: reducing lr to 0.0005036177557426807
Epoch 73: reducing lr to 0.00041533649922678544
Epoch 77: reducing lr to 0.0003065065932064409
Epoch 83: reducing lr to 0.00016834975374697085
Epoch 86: reducing lr to 0.00011294367881263309
Epoch 89: reducing lr to 6.786749650279696e-05
Epoch 92: reducing lr to 3.3831861897767704e-05
Epoch 95: reducing lr to 1.1373637565248625e-05
Epoch 98: reducing lr to 8.469433458753797e-07
[I 2024-06-22 10:17:57,644] Trial 50 finished with value: 2.559791088104248 and parameters: {'hidden_size': 158, 'n_layers': 6, 'rnn_dropout': 0.20775871463868248, 'bidirectional': False, 'fc_dropout': 0.27623796927286526, 'learning_rate_model': 0.015358709003280025}. Best is trial 13 with value: 1.8697621822357178.
Epoch 11: reducing lr to 7.085032862226039e-05
Epoch 16: reducing lr to 0.00011313080452181197
Epoch 39: reducing lr to 0.00013283263272017318
Epoch 48: reducing lr to 0.00011289097287280152
Epoch 62: reducing lr to 7.212395330975728e-05
Epoch 68: reducing lr to 5.3930749579926375e-05
Epoch 71: reducing lr to 4.522170412467061e-05
Epoch 74: reducing lr to 3.695574461053685e-05
Epoch 77: reducing lr to 2.9263254510634628e-05
Epoch 80: reducing lr to 2.2265526223843906e-05
Epoch 83: reducing lr to 1.607293872266612e-05
Epoch 86: reducing lr to 1.078312731835878e-05
Epoch 89: reducing lr to 6.4795468260068176e-06
Epoch 92: reducing lr to 3.2300459671228244e-06
Epoch 95: reducing lr to 1.085880885307467e-06
Epoch 98: reducing lr to 8.086063802794675e-08
[I 2024-06-22 10:19:11,521] Trial 51 finished with value: 2.4582748413085938 and parameters: {'hidden_size': 98, 'n_layers': 6, 'rnn_dropout': 0.7923774268990726, 'bidirectional': True, 'fc_dropout': 0.7210467490794191, 'learning_rate_model': 0.0014663495679358655}. Best is trial 13 with value: 1.8697621822357178.
Epoch 37: reducing lr to 2.4591331174705337e-05
Epoch 40: reducing lr to 2.364899497906998e-05
Epoch 43: reducing lr to 2.254245841537826e-05
Epoch 48: reducing lr to 2.038179288645568e-05
Epoch 54: reducing lr to 1.738614342978735e-05
Epoch 57: reducing lr to 1.57780033295483e-05
Epoch 61: reducing lr to 1.357596800829373e-05
Epoch 64: reducing lr to 1.1914819085392375e-05
Epoch 67: reducing lr to 1.0274518260785362e-05
Epoch 70: reducing lr to 8.680945012057548e-06
Epoch 73: reducing lr to 7.159225960909953e-06
Epoch 76: reducing lr to 5.733357438188365e-06
Epoch 79: reducing lr to 4.4258304432585835e-06
Epoch 82: reducing lr to 3.2572615883951258e-06
Epoch 85: reducing lr to 2.2460833454246807e-06
Epoch 88: reducing lr to 1.4082384943730217e-06
Epoch 91: reducing lr to 7.569453112371809e-07
Epoch 94: reducing lr to 3.0247176987096157e-07
Epoch 97: reducing lr to 5.1986528511595125e-08
[I 2024-06-22 10:22:15,945] Trial 52 finished with value: 1.9979404211044312 and parameters: {'hidden_size': 196, 'n_layers': 5, 'rnn_dropout': 0.7951304196883832, 'bidirectional': True, 'fc_dropout': 0.10834232245773592, 'learning_rate_model': 0.000264740682379336}. Best is trial 13 with value: 1.8697621822357178.
Epoch 39: reducing lr to 0.0005342913397855446
Epoch 53: reducing lr to 0.000398986765957224
Epoch 58: reducing lr to 0.00033934449537817614
Epoch 63: reducing lr to 0.000277760123119193
Epoch 66: reducing lr to 0.00024099697852664388
Epoch 69: reducing lr to 0.0002050839218198937
Epoch 72: reducing lr to 0.00017058753605408793
Epoch 75: reducing lr to 0.00013805169346005618
[I 2024-06-22 10:22:20,621] Trial 53 finished with value: 2.5530643463134766 and parameters: {'hidden_size': 20, 'n_layers': 2, 'rnn_dropout': 0.5965097541659968, 'bidirectional': False, 'fc_dropout': 0.6898770022704354, 'learning_rate_model': 0.005898082867158478}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 10:22:38,599] Trial 54 finished with value: 3.749826431274414 and parameters: {'hidden_size': 143, 'n_layers': 2, 'rnn_dropout': 0.3535234245987387, 'bidirectional': False, 'fc_dropout': 0.1631249801864975, 'learning_rate_model': 9.793357468944974e-05}. Best is trial 13 with value: 1.8697621822357178.
Epoch 16: reducing lr to 0.001207147499407567
Epoch 19: reducing lr to 0.001414992115220935
Epoch 22: reducing lr to 0.0015383754577570825
Epoch 27: reducing lr to 0.0015589283286883727
Epoch 30: reducing lr to 0.001540968559623765
Epoch 39: reducing lr to 0.001417373288430607
Epoch 42: reducing lr to 0.0013551030918849763
Epoch 47: reducing lr to 0.0012317961983327585
Epoch 52: reducing lr to 0.0010888456754463548
Epoch 59: reducing lr to 0.0008677270580158679
Epoch 62: reducing lr to 0.0007695892401125403
Epoch 65: reducing lr to 0.0006716525349574679
Epoch 70: reducing lr to 0.0005130542628002036
Epoch 73: reducing lr to 0.00042311884161147886
Epoch 77: reducing lr to 0.0003122497418484195
Epoch 80: reducing lr to 0.00023758139454338155
Epoch 83: reducing lr to 0.00017150419701520477
Epoch 86: reducing lr to 0.0001150599541227565
Epoch 89: reducing lr to 6.91391595893787e-05
Epoch 92: reducing lr to 3.446578435170634e-05
Epoch 95: reducing lr to 1.1586750407141869e-05
Epoch 98: reducing lr to 8.628128952897079e-07
[I 2024-06-22 10:22:56,154] Trial 55 finished with value: 2.5595874786376953 and parameters: {'hidden_size': 64, 'n_layers': 6, 'rnn_dropout': 0.36558026194855425, 'bidirectional': False, 'fc_dropout': 0.16956813171494078, 'learning_rate_model': 0.015646491878786785}. Best is trial 13 with value: 1.8697621822357178.
Epoch 52: reducing lr to 7.711080992764371e-05
Epoch 55: reducing lr to 7.05509865463511e-05
Epoch 58: reducing lr to 6.375229918074272e-05
Epoch 61: reducing lr to 5.6821937113830596e-05
Epoch 64: reducing lr to 4.9869232188764146e-05
Epoch 67: reducing lr to 4.300378655375352e-05
Epoch 72: reducing lr to 3.204810386833038e-05
Epoch 75: reducing lr to 2.5935628789456112e-05
Epoch 78: reducing lr to 2.0287894319817438e-05
Epoch 81: reducing lr to 1.5193951891806567e-05
Epoch 84: reducing lr to 1.0734151028603299e-05
Epoch 87: reducing lr to 6.978807215430954e-06
Epoch 90: reducing lr to 3.987166708829641e-06
Epoch 93: reducing lr to 1.8063944763075084e-06
Epoch 96: reducing lr to 4.7088902383975085e-07
Epoch 99: reducing lr to 1.708117889904874e-09
[I 2024-06-22 10:23:04,086] Trial 56 finished with value: 1.8833309412002563 and parameters: {'hidden_size': 71, 'n_layers': 1, 'rnn_dropout': 0.47474595957169685, 'bidirectional': True, 'fc_dropout': 0.5998866485809148, 'learning_rate_model': 0.001108066724703625}. Best is trial 13 with value: 1.8697621822357178.
Epoch 6: reducing lr to 0.001288662773729956
Epoch 11: reducing lr to 0.0029805754431131083
Epoch 14: reducing lr to 0.004083328227802914
Epoch 17: reducing lr to 0.005063107462276318
Epoch 20: reducing lr to 0.005782305777519368
Epoch 23: reducing lr to 0.006139913899520635
Epoch 26: reducing lr to 0.006159080769258567
Epoch 29: reducing lr to 0.006104281856767709
Epoch 33: reducing lr to 0.0059573907016253844
Epoch 36: reducing lr to 0.005794104909044041
Epoch 41: reducing lr to 0.0054285761225533824
Epoch 44: reducing lr to 0.005158863672341378
Epoch 50: reducing lr to 0.0045260616783208375
Epoch 53: reducing lr to 0.004172951972174108
Epoch 56: reducing lr to 0.0038026742762049285
Epoch 62: reducing lr to 0.0030341550741623162
Epoch 66: reducing lr to 0.0025205568270366624
Epoch 70: reducing lr to 0.0020227494274324873
Epoch 73: reducing lr to 0.0016681732453294345
Epoch 76: reducing lr to 0.0013359312216876446
Epoch 81: reducing lr to 0.0008458641121903969
Epoch 84: reducing lr to 0.0005975820638752592
Epoch 87: reducing lr to 0.00038851791893666223
Epoch 90: reducing lr to 0.0002219699820254127
Epoch 93: reducing lr to 0.00010056397906534481
Epoch 96: reducing lr to 2.6214912942116446e-05
Epoch 99: reducing lr to 9.50928127678441e-08
[I 2024-06-22 10:23:50,142] Trial 57 finished with value: 2.5383291244506836 and parameters: {'hidden_size': 116, 'n_layers': 7, 'rnn_dropout': 0.6431687131989174, 'bidirectional': False, 'fc_dropout': 0.3720011852508049, 'learning_rate_model': 0.061687300513606574}. Best is trial 13 with value: 1.8697621822357178.
Epoch 26: reducing lr to 0.001626196729536684
Epoch 29: reducing lr to 0.001611728042469031
Epoch 32: reducing lr to 0.00158468470723108
Epoch 35: reducing lr to 0.0015454932129787455
Epoch 43: reducing lr to 0.0013868629391563167
Epoch 46: reducing lr to 0.0013097578625103122
Epoch 51: reducing lr to 0.0011645454773238448
Epoch 54: reducing lr to 0.0010696347990678356
Epoch 57: reducing lr to 0.000970698389165392
Epoch 60: reducing lr to 0.0008692969726278137
Epoch 63: reducing lr to 0.0007670293378752872
Epoch 66: reducing lr to 0.0006655086079073797
Epoch 69: reducing lr to 0.0005663353795925454
Epoch 72: reducing lr to 0.00047107426134454527
Epoch 75: reducing lr to 0.00038122714606440985
Epoch 78: reducing lr to 0.00029821124114579636
Epoch 81: reducing lr to 0.00022333551132209997
Epoch 84: reducing lr to 0.0001577810121851533
Epoch 87: reducing lr to 0.00010258130925879326
Epoch 90: reducing lr to 5.860726175677296e-05
Epoch 93: reducing lr to 2.6552146333510067e-05
Epoch 96: reducing lr to 6.921585750967879e-06
Epoch 99: reducing lr to 2.5107581296673896e-08
[I 2024-06-22 10:24:01,788] Trial 58 finished with value: 1.8866143226623535 and parameters: {'hidden_size': 56, 'n_layers': 2, 'rnn_dropout': 0.0651791719018096, 'bidirectional': True, 'fc_dropout': 0.5070294084895343, 'learning_rate_model': 0.01628744452416877}. Best is trial 13 with value: 1.8697621822357178.
Epoch 21: reducing lr to 0.00030900356774082464
Epoch 30: reducing lr to 0.00031576314091062545
Epoch 35: reducing lr to 0.00030422772317157743
Epoch 41: reducing lr to 0.0002821465869167959
Epoch 47: reducing lr to 0.0002524099756079936
Epoch 50: reducing lr to 0.00023523900667206157
Epoch 59: reducing lr to 0.00017780779470226465
Epoch 62: reducing lr to 0.00015769816596925815
Epoch 65: reducing lr to 0.0001376297476767046
Epoch 68: reducing lr to 0.00011791894242922985
Epoch 71: reducing lr to 9.887671813880211e-05
Epoch 77: reducing lr to 6.398375788968843e-05
Epoch 80: reducing lr to 4.8683308231324986e-05
Epoch 83: reducing lr to 3.514328932324087e-05
Epoch 86: reducing lr to 2.3577179612090583e-05
Epoch 89: reducing lr to 1.4167452058329787e-05
Epoch 92: reducing lr to 7.062457084458735e-06
Epoch 95: reducing lr to 2.374265638748537e-06
Epoch 98: reducing lr to 1.7680082317929336e-07
[I 2024-06-22 10:26:10,557] Trial 59 finished with value: 2.460301399230957 and parameters: {'hidden_size': 144, 'n_layers': 6, 'rnn_dropout': 0.2739632958748159, 'bidirectional': True, 'fc_dropout': 0.6592078963298551, 'learning_rate_model': 0.003206155887492329}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 10:26:14,370] Trial 60 finished with value: 9.151275634765625 and parameters: {'hidden_size': 42, 'n_layers': 1, 'rnn_dropout': 0.014666114330405834, 'bidirectional': False, 'fc_dropout': 0.09067353821407238, 'learning_rate_model': 1.2916159380130333e-05}. Best is trial 13 with value: 1.8697621822357178.
Epoch 30: reducing lr to 7.75162054496227e-05
Epoch 33: reducing lr to 7.601092609734413e-05
Epoch 37: reducing lr to 7.311004662328795e-05
Epoch 41: reducing lr to 6.926372956412476e-05
Epoch 44: reducing lr to 6.582244223761035e-05
Epoch 47: reducing lr to 6.196373481191545e-05
Epoch 50: reducing lr to 5.774846018559821e-05
Epoch 55: reducing lr to 5.011328636247903e-05
Epoch 59: reducing lr to 4.3649760719178624e-05
Epoch 65: reducing lr to 3.378651405012095e-05
Epoch 68: reducing lr to 2.8947738933004868e-05
Epoch 71: reducing lr to 2.427309272174113e-05
Epoch 74: reducing lr to 1.983627625043817e-05
Epoch 77: reducing lr to 1.5707273837321746e-05
Epoch 80: reducing lr to 1.1951190097563751e-05
Epoch 83: reducing lr to 8.62727178194281e-06
Epoch 86: reducing lr to 5.787925384396792e-06
Epoch 89: reducing lr to 3.477945909975645e-06
Epoch 92: reducing lr to 1.7337516746231065e-06
Epoch 95: reducing lr to 5.828548022243836e-07
Epoch 98: reducing lr to 4.3402560836276403e-08
[I 2024-06-22 10:27:25,258] Trial 61 finished with value: 1.8750180006027222 and parameters: {'hidden_size': 155, 'n_layers': 3, 'rnn_dropout': 0.5975507964860202, 'bidirectional': True, 'fc_dropout': 0.3824580053742648, 'learning_rate_model': 0.000787074253700552}. Best is trial 13 with value: 1.8697621822357178.
Epoch 48: reducing lr to 0.0002293379720162878
Epoch 58: reducing lr to 0.00017138949785506908
Epoch 78: reducing lr to 5.454128030978585e-05
Epoch 84: reducing lr to 2.8857324023358636e-05
Epoch 92: reducing lr to 6.561837255614688e-06
Epoch 95: reducing lr to 2.2059666397618997e-06
Epoch 98: reducing lr to 1.642683579506851e-07
[I 2024-06-22 10:27:57,027] Trial 62 finished with value: 2.4563746452331543 and parameters: {'hidden_size': 164, 'n_layers': 3, 'rnn_dropout': 0.7237489282748985, 'bidirectional': False, 'fc_dropout': 0.10051310491019105, 'learning_rate_model': 0.0029788886358192816}. Best is trial 13 with value: 1.8697621822357178.
Epoch 8: reducing lr to 0.0014186419125475508
Epoch 12: reducing lr to 0.002490320205046086
Epoch 16: reducing lr to 0.003535416949623243
Epoch 21: reducing lr to 0.004416478686956946
Epoch 27: reducing lr to 0.004565690306443556
Epoch 30: reducing lr to 0.0045130908751449825
Epoch 35: reducing lr to 0.00434821922993349
Epoch 38: reducing lr to 0.004205514885908831
Epoch 42: reducing lr to 0.003968739894576446
Epoch 45: reducing lr to 0.0037599117995262333
Epoch 48: reducing lr to 0.0035279220424620007
Epoch 56: reducing lr to 0.0028248190992865093
Epoch 73: reducing lr to 0.001239203597797551
Epoch 77: reducing lr to 0.000914497217935805
Epoch 83: reducing lr to 0.0005022906027280442
Epoch 86: reducing lr to 0.0003369802880162566
Epoch 91: reducing lr to 0.0001310210570448146
Epoch 94: reducing lr to 5.235539533224101e-05
Epoch 97: reducing lr to 8.998443898868697e-06
[I 2024-06-22 10:28:07,706] Trial 63 finished with value: 2.467468738555908 and parameters: {'hidden_size': 17, 'n_layers': 4, 'rnn_dropout': 0.0024143652956770904, 'bidirectional': True, 'fc_dropout': 0.6367735535538155, 'learning_rate_model': 0.04582445195599803}. Best is trial 13 with value: 1.8697621822357178.
Epoch 37: reducing lr to 0.0006493086517227393
Epoch 40: reducing lr to 0.0006244272396385129
Epoch 45: reducing lr to 0.0005735485850850228
[I 2024-06-22 10:28:13,290] Trial 64 finished with value: 2.2495110034942627 and parameters: {'hidden_size': 42, 'n_layers': 2, 'rnn_dropout': 0.1501053383219472, 'bidirectional': False, 'fc_dropout': 0.791964142687259, 'learning_rate_model': 0.006990203755569746}. Best is trial 13 with value: 1.8697621822357178.
Epoch 6: reducing lr to 0.00037186548391206404
Epoch 12: reducing lr to 0.0009673870395542465
Epoch 17: reducing lr to 0.0014610454689464873
Epoch 20: reducing lr to 0.0016685823319478872
Epoch 26: reducing lr to 0.0017773071414832187
Epoch 29: reducing lr to 0.0017614939865394428
Epoch 32: reducing lr to 0.001731937714549135
Epoch 35: reducing lr to 0.0016891044451451819
Epoch 40: reducing lr to 0.0015901363197896427
Epoch 43: reducing lr to 0.0015157338354278896
Epoch 48: reducing lr to 0.001370452704644191
Epoch 51: reducing lr to 0.001272758059241296
Epoch 54: reducing lr to 0.0011690280349437557
Epoch 57: reducing lr to 0.0010608981975885775
Epoch 60: reducing lr to 0.0009500742987973784
Epoch 63: reducing lr to 0.0008383036905511993
Epoch 66: reducing lr to 0.0007273493914167036
Epoch 69: reducing lr to 0.0006189607298688974
Epoch 72: reducing lr to 0.0005148477017876731
Epoch 75: reducing lr to 0.00041665175985231684
Epoch 78: reducing lr to 0.00032592180203806136
Epoch 81: reducing lr to 0.00024408842547153832
Epoch 85: reducing lr to 0.00015102454493289667
Epoch 88: reducing lr to 9.468864020692018e-05
Epoch 91: reducing lr to 5.089629527842421e-05
Epoch 94: reducing lr to 2.0337919112779766e-05
Epoch 97: reducing lr to 3.495525589955473e-06
[I 2024-06-22 10:31:08,659] Trial 65 finished with value: 2.4617767333984375 and parameters: {'hidden_size': 151, 'n_layers': 7, 'rnn_dropout': 0.3206989858968967, 'bidirectional': True, 'fc_dropout': 0.0955656402669832, 'learning_rate_model': 0.017800916053720233}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 10:31:14,512] Trial 66 finished with value: 2.586214065551758 and parameters: {'hidden_size': 43, 'n_layers': 2, 'rnn_dropout': 0.6552713429068246, 'bidirectional': False, 'fc_dropout': 0.21331096172130817, 'learning_rate_model': 0.0013633688901489999}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 10:31:39,881] Trial 67 finished with value: 8.95959758758545 and parameters: {'hidden_size': 74, 'n_layers': 7, 'rnn_dropout': 0.29324531508098756, 'bidirectional': False, 'fc_dropout': 0.7511091818957707, 'learning_rate_model': 1.3298296907035845e-05}. Best is trial 13 with value: 1.8697621822357178.
Epoch 39: reducing lr to 0.00015528641669033174
Epoch 42: reducing lr to 0.0001484641379250246
Epoch 47: reducing lr to 0.00013495472173294886
Epoch 52: reducing lr to 0.00011929316338114867
Epoch 56: reducing lr to 0.00010567191186876051
Epoch 59: reducing lr to 9.506756378464426e-05
Epoch 62: reducing lr to 8.431565374908122e-05
Epoch 68: reducing lr to 6.304710431609566e-05
Epoch 71: reducing lr to 5.286589783207707e-05
Epoch 74: reducing lr to 4.3202675721880574e-05
Epoch 77: reducing lr to 3.420986124114903e-05
Epoch 80: reducing lr to 2.602924983282544e-05
Epoch 83: reducing lr to 1.8789878727947895e-05
Epoch 86: reducing lr to 1.2605887331248079e-05
Epoch 93: reducing lr to 2.794556451879578e-06
[I 2024-06-22 10:34:24,633] Trial 68 finished with value: 2.447019100189209 and parameters: {'hidden_size': 148, 'n_layers': 7, 'rnn_dropout': 0.021829778813975055, 'bidirectional': True, 'fc_dropout': 0.6875914566924687, 'learning_rate_model': 0.0017142186024413223}. Best is trial 13 with value: 1.8697621822357178.
Epoch 62: reducing lr to 8.66201366530914e-06
Epoch 65: reducing lr to 7.559699555168341e-06
Epoch 68: reducing lr to 6.477028343626434e-06
Epoch 71: reducing lr to 5.4310808146379145e-06
Epoch 74: reducing lr to 4.438347457928911e-06
Epoch 77: reducing lr to 3.514487196422782e-06
Epoch 80: reducing lr to 2.6740671242454364e-06
Epoch 83: reducing lr to 1.9303436440799653e-06
Epoch 86: reducing lr to 1.2950426578149854e-06
Epoch 89: reducing lr to 7.781870041265269e-07
Epoch 92: reducing lr to 3.879252456757574e-07
Epoch 95: reducing lr to 1.3041319334000906e-07
Epoch 98: reducing lr to 9.71128064175097e-09
[I 2024-06-22 10:35:53,040] Trial 69 finished with value: 1.8853203058242798 and parameters: {'hidden_size': 143, 'n_layers': 4, 'rnn_dropout': 0.5026472300170858, 'bidirectional': True, 'fc_dropout': 0.4614047479896726, 'learning_rate_model': 0.00017610709636270444}. Best is trial 13 with value: 1.8697621822357178.
Epoch 10: reducing lr to 0.002367604090273068
Epoch 15: reducing lr to 0.004015812525710151
Epoch 18: reducing lr to 0.004837620060265631
Epoch 21: reducing lr to 0.005388307654539124
Epoch 24: reducing lr to 0.0055905328663617155
Epoch 27: reducing lr to 0.00557035271088691
Epoch 32: reducing lr to 0.0054395558801778
Epoch 35: reducing lr to 0.0053050279693319016
Epoch 41: reducing lr to 0.004919984015398158
Epoch 44: reducing lr to 0.004675540368696052
Epoch 47: reducing lr to 0.004401446279711991
Epoch 50: reducing lr to 0.004102024289118865
Epoch 53: reducing lr to 0.0037819967034862082
Epoch 56: reducing lr to 0.0034464095616096855
Epoch 66: reducing lr to 0.002284411053462414
Epoch 70: reducing lr to 0.0018332422030111733
Epoch 74: reducing lr to 0.001409022625425111
Epoch 78: reducing lr to 0.001023633725313154
Epoch 81: reducing lr to 0.0007666168470744853
Epoch 85: reducing lr to 0.0004743279417844269
Epoch 88: reducing lr to 0.00029739184342300465
Epoch 91: reducing lr to 0.00015985173134998656
Epoch 94: reducing lr to 6.387599656220988e-05
Epoch 97: reducing lr to 1.0978516500580182e-05
[I 2024-06-22 10:36:37,886] Trial 70 finished with value: 2.5404062271118164 and parameters: {'hidden_size': 111, 'n_layers': 7, 'rnn_dropout': 0.3411798308011168, 'bidirectional': False, 'fc_dropout': 0.7454888933092325, 'learning_rate_model': 0.05590794448273345}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 10:37:59,036] Trial 71 finished with value: 3.007652759552002 and parameters: {'hidden_size': 172, 'n_layers': 7, 'rnn_dropout': 0.18232023196105357, 'bidirectional': False, 'fc_dropout': 0.25150129289124035, 'learning_rate_model': 5.001076586958479e-05}. Best is trial 13 with value: 1.8697621822357178.
Epoch 36: reducing lr to 0.0001860659669055044
Epoch 39: reducing lr to 0.00017945006508628364
Epoch 47: reducing lr to 0.00015595461673233898
Epoch 56: reducing lr to 0.00012211519762514905
Epoch 59: reducing lr to 0.00010986073909329246
Epoch 62: reducing lr to 9.743575694220533e-05
Epoch 65: reducing lr to 8.503623717005439e-05
Epoch 68: reducing lr to 7.285767302871566e-05
Epoch 71: reducing lr to 6.10921998781733e-05
Epoch 74: reducing lr to 4.992531307907865e-05
Epoch 77: reducing lr to 3.953315400766243e-05
Epoch 80: reducing lr to 3.007958246575009e-05
Epoch 83: reducing lr to 2.1713714776596815e-05
Epoch 86: reducing lr to 1.4567451231577481e-05
Epoch 89: reducing lr to 8.753534999987687e-06
Epoch 92: reducing lr to 4.363626220169384e-06
Epoch 95: reducing lr to 1.4669693098297211e-06
Epoch 98: reducing lr to 1.0923856931751609e-07
[I 2024-06-22 10:38:20,555] Trial 72 finished with value: 2.5648727416992188 and parameters: {'hidden_size': 128, 'n_layers': 3, 'rnn_dropout': 0.6530812094617713, 'bidirectional': False, 'fc_dropout': 0.1922844998793111, 'learning_rate_model': 0.0019809629608084447}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 10:38:28,009] Trial 73 finished with value: 7.004927635192871 and parameters: {'hidden_size': 80, 'n_layers': 1, 'rnn_dropout': 0.1830509367683007, 'bidirectional': True, 'fc_dropout': 0.41521278725894306, 'learning_rate_model': 4.9989708054979095e-05}. Best is trial 13 with value: 1.8697621822357178.
Epoch 39: reducing lr to 0.00021852392840959167
Epoch 91: reducing lr to 6.897243421635519e-06
Epoch 94: reducing lr to 2.7561058824224735e-06
Epoch 97: reducing lr to 4.7369834579491685e-07
[I 2024-06-22 10:39:15,711] Trial 74 finished with value: 2.1233468055725098 and parameters: {'hidden_size': 121, 'n_layers': 7, 'rnn_dropout': 0.6534680870332976, 'bidirectional': False, 'fc_dropout': 0.7212500677354571, 'learning_rate_model': 0.002412302319431398}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 10:39:20,509] Trial 75 finished with value: 9.174503326416016 and parameters: {'hidden_size': 21, 'n_layers': 1, 'rnn_dropout': 0.05229737203378111, 'bidirectional': True, 'fc_dropout': 0.7802800274162293, 'learning_rate_model': 4.6982180906249337e-05}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 10:40:45,333] Trial 76 finished with value: 6.848813056945801 and parameters: {'hidden_size': 195, 'n_layers': 6, 'rnn_dropout': 0.6593902716995983, 'bidirectional': False, 'fc_dropout': 0.3815058668169547, 'learning_rate_model': 1.1286139856486e-05}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 10:41:22,687] Trial 77 finished with value: 8.639954566955566 and parameters: {'hidden_size': 81, 'n_layers': 4, 'rnn_dropout': 0.5840729688720835, 'bidirectional': True, 'fc_dropout': 0.11013019275648253, 'learning_rate_model': 1.1054902973283413e-05}. Best is trial 13 with value: 1.8697621822357178.
Epoch 15: reducing lr to 0.0007513905573075124
Epoch 23: reducing lr to 0.0010411956468696067
Epoch 50: reducing lr to 0.0007675214659441602
Epoch 55: reducing lr to 0.0006660441315420844
Epoch 66: reducing lr to 0.00042743153062829383
Epoch 69: reducing lr to 0.0003637362391289862
Epoch 78: reducing lr to 0.0001915300354330662
Epoch 81: reducing lr to 0.00014344012731589383
Epoch 84: reducing lr to 0.000101336900441365
Epoch 92: reducing lr to 2.3042893656612626e-05
Epoch 97: reducing lr to 2.0541680118201996e-06
[I 2024-06-22 10:41:52,345] Trial 78 finished with value: 2.030484199523926 and parameters: {'hidden_size': 156, 'n_layers': 3, 'rnn_dropout': 0.7875067601548086, 'bidirectional': False, 'fc_dropout': 0.4191169085329054, 'learning_rate_model': 0.010460822384971712}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 10:41:58,433] Trial 79 finished with value: 8.770699501037598 and parameters: {'hidden_size': 101, 'n_layers': 1, 'rnn_dropout': 0.25000110279363963, 'bidirectional': False, 'fc_dropout': 0.6161199650496738, 'learning_rate_model': 3.321567816135707e-05}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 10:42:05,484] Trial 80 finished with value: 2.697266101837158 and parameters: {'hidden_size': 20, 'n_layers': 4, 'rnn_dropout': 0.6479909685299331, 'bidirectional': False, 'fc_dropout': 0.5494465813255934, 'learning_rate_model': 0.0006265951288394312}. Best is trial 13 with value: 1.8697621822357178.
Epoch 21: reducing lr to 0.000380711195092364
Epoch 30: reducing lr to 0.0003890394004868972
Epoch 41: reducing lr to 0.0003476217607507364
Epoch 47: reducing lr to 0.0003109844464564668
Epoch 55: reducing lr to 0.0002515092524176432
Epoch 61: reducing lr to 0.00020256616702351432
Epoch 64: reducing lr to 0.000177780268149725
Epoch 70: reducing lr to 0.0001295278359642659
Epoch 73: reducing lr to 0.00010682236146039893
Epoch 77: reducing lr to 7.883188246264857e-05
Epoch 80: reducing lr to 5.9980797611188744e-05
Epoch 83: reducing lr to 4.329867054787438e-05
Epoch 86: reducing lr to 2.9048519706914836e-05
Epoch 89: reducing lr to 1.745516287716283e-05
Epoch 92: reducing lr to 8.701376804710589e-06
Epoch 95: reducing lr to 2.9252397161732767e-06
Epoch 98: reducing lr to 2.1782937063806463e-07
[I 2024-06-22 10:43:09,833] Trial 81 finished with value: 2.4633727073669434 and parameters: {'hidden_size': 185, 'n_layers': 5, 'rnn_dropout': 0.004191870121172681, 'bidirectional': False, 'fc_dropout': 0.6643196560298716, 'learning_rate_model': 0.003950179114447677}. Best is trial 13 with value: 1.8697621822357178.
Epoch 11: reducing lr to 0.0012806127529641238
Epoch 17: reducing lr to 0.0021753785836223236
Epoch 20: reducing lr to 0.002484384194112288
Epoch 26: reducing lr to 0.0026462666455476435
Epoch 29: reducing lr to 0.0026227221362662206
Epoch 33: reducing lr to 0.002559609931873764
Epoch 36: reducing lr to 0.002489453724675358
Epoch 39: reducing lr to 0.002400936830909879
Epoch 42: reducing lr to 0.0022954552266107433
Epoch 45: reducing lr to 0.0021746724202340267
Epoch 51: reducing lr to 0.0018950338528496793
Epoch 54: reducing lr to 0.0017405882328251341
Epoch 57: reducing lr to 0.0015795916468649232
Epoch 61: reducing lr to 0.0013591381124787812
Epoch 70: reducing lr to 0.000869080069355796
Epoch 77: reducing lr to 0.0005289304601443928
Epoch 80: reducing lr to 0.00040244720650107167
Epoch 85: reducing lr to 0.00022486333767916708
Epoch 88: reducing lr to 0.00014098373007308365
Epoch 91: reducing lr to 7.578046890918261e-05
Epoch 94: reducing lr to 3.0281517320118746e-05
Epoch 97: reducing lr to 5.204555004284517e-06
[I 2024-06-22 10:43:52,624] Trial 82 finished with value: 2.552701950073242 and parameters: {'hidden_size': 140, 'n_layers': 5, 'rnn_dropout': 0.21929582013348173, 'bidirectional': False, 'fc_dropout': 0.2819437041731271, 'learning_rate_model': 0.026504124871653878}. Best is trial 13 with value: 1.8697621822357178.
Epoch 35: reducing lr to 2.462371655794856e-05
Epoch 38: reducing lr to 2.381559002774485e-05
[I 2024-06-22 10:44:27,235] Trial 83 finished with value: 2.0255653858184814 and parameters: {'hidden_size': 52, 'n_layers': 6, 'rnn_dropout': 0.23168219694950196, 'bidirectional': True, 'fc_dropout': 0.6461587272897676, 'learning_rate_model': 0.00025950124791777955}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 10:44:45,606] Trial 84 finished with value: 4.863873481750488 and parameters: {'hidden_size': 55, 'n_layers': 6, 'rnn_dropout': 0.2469135581045209, 'bidirectional': False, 'fc_dropout': 0.17711736072656087, 'learning_rate_model': 7.185360253135694e-05}. Best is trial 13 with value: 1.8697621822357178.
Epoch 56: reducing lr to 0.000974988535122958
Epoch 62: reducing lr to 0.0007779436775862273
Epoch 66: reducing lr to 0.0006462594032479813
Epoch 69: reducing lr to 0.0005499546664085137
Epoch 72: reducing lr to 0.0004574488855662983
Epoch 75: reducing lr to 0.0003702005127960787
Epoch 79: reducing lr to 0.0002644114363878732
Epoch 84: reducing lr to 0.0001532173462027265
Epoch 87: reducing lr to 9.961424227770549e-05
Epoch 91: reducing lr to 4.522202094663364e-05
Epoch 94: reducing lr to 1.8070505900238952e-05
Epoch 97: reducing lr to 3.1058199930607267e-06
[I 2024-06-22 10:44:49,793] Trial 85 finished with value: 1.9913687705993652 and parameters: {'hidden_size': 52, 'n_layers': 1, 'rnn_dropout': 0.3016660793658673, 'bidirectional': False, 'fc_dropout': 0.5396512190107284, 'learning_rate_model': 0.015816345654373305}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 10:44:58,933] Trial 86 finished with value: 2.458502769470215 and parameters: {'hidden_size': 30, 'n_layers': 2, 'rnn_dropout': 0.1661005294430515, 'bidirectional': True, 'fc_dropout': 0.44310768884113144, 'learning_rate_model': 0.00016522355094605626}. Best is trial 13 with value: 1.8697621822357178.
Epoch 8: reducing lr to 0.0024828455755276794
Epoch 13: reducing lr to 0.004838874486094971
Epoch 18: reducing lr to 0.006939567958680504
Epoch 21: reducing lr to 0.007729529538311736
Epoch 24: reducing lr to 0.008019621687533639
Epoch 29: reducing lr to 0.00793620750536289
Epoch 34: reducing lr to 0.007680869187829846
Epoch 37: reducing lr to 0.007449644636284283
Epoch 40: reducing lr to 0.007164175348935891
Epoch 43: reducing lr to 0.006828963557512514
Epoch 46: reducing lr to 0.006449295355523433
Epoch 49: reducing lr to 0.006031158899826582
Epoch 52: reducing lr to 0.005581148182347487
Epoch 55: reducing lr to 0.005106359415696277
Epoch 61: reducing lr to 0.0041126743622314
Epoch 64: reducing lr to 0.0036094495032091113
Epoch 67: reducing lr to 0.003112540321956838
Epoch 70: reducing lr to 0.0026297866913960665
Epoch 73: reducing lr to 0.0021688004159164465
Epoch 76: reducing lr to 0.0017368508920425421
Epoch 79: reducing lr to 0.001340751494438773
Epoch 82: reducing lr to 0.00098674777500138
Epoch 85: reducing lr to 0.0006804236268470696
Epoch 88: reducing lr to 0.0004266087212475067
Epoch 91: reducing lr to 0.00022930737412128478
Epoch 94: reducing lr to 9.163014324187202e-05
Epoch 97: reducing lr to 1.574868641856103e-05
[I 2024-06-22 10:46:52,033] Trial 87 finished with value: 2.4607152938842773 and parameters: {'hidden_size': 170, 'n_layers': 4, 'rnn_dropout': 0.7387649720950202, 'bidirectional': True, 'fc_dropout': 0.10096808299948697, 'learning_rate_model': 0.08019996926893054}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 10:47:03,320] Trial 88 finished with value: 5.008953094482422 and parameters: {'hidden_size': 45, 'n_layers': 2, 'rnn_dropout': 0.3449454041282968, 'bidirectional': True, 'fc_dropout': 0.5834551354888987, 'learning_rate_model': 5.6882708517701854e-05}. Best is trial 13 with value: 1.8697621822357178.
Epoch 19: reducing lr to 0.00022123113762292657
Epoch 30: reducing lr to 0.00024092729833586306
Epoch 39: reducing lr to 0.0002216034292084318
Epoch 47: reducing lr to 0.0001925888288318855
Epoch 71: reducing lr to 7.544294277285802e-05
Epoch 79: reducing lr to 4.089623235516942e-05
Epoch 85: reducing lr to 2.075460132538046e-05
Epoch 93: reducing lr to 3.988006846007784e-06
Epoch 96: reducing lr to 1.039589455909722e-06
Epoch 99: reducing lr to 3.771040007040719e-09
[I 2024-06-22 10:47:53,248] Trial 89 finished with value: 2.49680233001709 and parameters: {'hidden_size': 135, 'n_layers': 6, 'rnn_dropout': 0.1685859136666566, 'bidirectional': False, 'fc_dropout': 0.5105749684962667, 'learning_rate_model': 0.002446297163720527}. Best is trial 13 with value: 1.8697621822357178.
Epoch 14: reducing lr to 0.0014517937931955239
Epoch 17: reducing lr to 0.001800146247848847
Epoch 20: reducing lr to 0.002055851298213692
Epoch 23: reducing lr to 0.0021829959270443498
Epoch 31: reducing lr to 0.002147901205963345
Epoch 34: reducing lr to 0.002100499472618291
Epoch 39: reducing lr to 0.0019867978199397804
Epoch 42: reducing lr to 0.001899510799820283
Epoch 45: reducing lr to 0.0017995618910001676
Epoch 49: reducing lr to 0.0016493505850139589
Epoch 54: reducing lr to 0.0014403531412691373
Epoch 57: reducing lr to 0.0013071269514396135
Epoch 64: reducing lr to 0.0009870818773929945
Epoch 71: reducing lr to 0.0006763878824725912
Epoch 77: reducing lr to 0.00043769493290515897
Epoch 80: reducing lr to 0.00033302885033179643
Epoch 83: reducing lr to 0.00024040538051738058
Epoch 86: reducing lr to 0.0001612848696101666
Epoch 89: reducing lr to 9.691556392793648e-05
Epoch 92: reducing lr to 4.831228708162376e-05
Epoch 95: reducing lr to 1.624168497953368e-05
Epoch 98: reducing lr to 1.2094448183622846e-06
[I 2024-06-22 10:48:15,385] Trial 90 finished with value: 2.553349018096924 and parameters: {'hidden_size': 70, 'n_layers': 6, 'rnn_dropout': 0.7562467519999598, 'bidirectional': False, 'fc_dropout': 0.04491283172953225, 'learning_rate_model': 0.021932412730099944}. Best is trial 13 with value: 1.8697621822357178.
Epoch 58: reducing lr to 7.63342938899409e-05
Epoch 63: reducing lr to 6.24811162634531e-05
Epoch 66: reducing lr to 5.4211382344479055e-05
Epoch 71: reducing lr to 4.0916564581512034e-05
Epoch 81: reducing lr to 1.8192592329424975e-05
Epoch 84: reducing lr to 1.285261629472216e-05
Epoch 87: reducing lr to 8.356127195877885e-06
Epoch 90: reducing lr to 4.774063982808103e-06
Epoch 93: reducing lr to 2.1628999833354447e-06
Epoch 96: reducing lr to 5.638225067526549e-07
Epoch 99: reducing lr to 2.045227775028871e-09
[I 2024-06-22 10:48:24,110] Trial 91 finished with value: 2.569105625152588 and parameters: {'hidden_size': 51, 'n_layers': 3, 'rnn_dropout': 0.24006483674028517, 'bidirectional': False, 'fc_dropout': 0.2961233420429906, 'learning_rate_model': 0.0013267520089493556}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 10:48:30,338] Trial 92 finished with value: 3.767477512359619 and parameters: {'hidden_size': 107, 'n_layers': 1, 'rnn_dropout': 0.16516324711154642, 'bidirectional': False, 'fc_dropout': 0.2092396414663897, 'learning_rate_model': 0.0002680689020235863}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 10:48:54,813] Trial 93 finished with value: 8.623632431030273 and parameters: {'hidden_size': 35, 'n_layers': 6, 'rnn_dropout': 0.08526710143104088, 'bidirectional': True, 'fc_dropout': 0.4579240899477195, 'learning_rate_model': 1.5114066835104806e-05}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 10:49:40,794] Trial 94 finished with value: 1.905665397644043 and parameters: {'hidden_size': 161, 'n_layers': 2, 'rnn_dropout': 0.4223231827265013, 'bidirectional': True, 'fc_dropout': 0.4408697393495774, 'learning_rate_model': 7.353072250353887e-05}. Best is trial 13 with value: 1.8697621822357178.
Epoch 64: reducing lr to 7.742514245907871e-05
Epoch 67: reducing lr to 6.676610314755064e-05
Epoch 70: reducing lr to 5.6410710009186024e-05
Epoch 74: reducing lr to 4.33570461059004e-05
Epoch 77: reducing lr to 3.433209879539355e-05
Epoch 80: reducing lr to 2.6122256694676056e-05
Epoch 84: reducing lr to 1.6665449538525844e-05
Epoch 87: reducing lr to 1.0835040347200895e-05
Epoch 91: reducing lr to 4.9187988618410896e-06
Epoch 94: reducing lr to 1.9655287843036015e-06
Epoch 97: reducing lr to 3.3782001615937417e-07
[I 2024-06-22 10:52:13,077] Trial 95 finished with value: 1.8877421617507935 and parameters: {'hidden_size': 156, 'n_layers': 6, 'rnn_dropout': 0.14123922742166642, 'bidirectional': True, 'fc_dropout': 0.6881895979384922, 'learning_rate_model': 0.0017203437921322648}. Best is trial 13 with value: 1.8697621822357178.
Epoch 10: reducing lr to 0.002303806448767065
Epoch 13: reducing lr to 0.0032823175029060895
Epoch 17: reducing lr to 0.004465106406486249
Epoch 22: reducing lr to 0.005348793075958391
Epoch 28: reducing lr to 0.005404135219930219
Epoch 33: reducing lr to 0.005253766305763906
Epoch 36: reducing lr to 0.005109766115371867
Epoch 40: reducing lr to 0.004859621428345329
Epoch 46: reducing lr to 0.004374702234512565
Epoch 49: reducing lr to 0.004091070862985821
Epoch 52: reducing lr to 0.0037858184620976594
Epoch 55: reducing lr to 0.0034637585526206285
Epoch 58: reducing lr to 0.003129971419909898
Epoch 61: reducing lr to 0.002789719609734949
Epoch 66: reducing lr to 0.0022228551379105295
Epoch 69: reducing lr to 0.0018916081525469925
Epoch 72: reducing lr to 0.0015734279462736283
Epoch 75: reducing lr to 0.0012733309686329068
Epoch 78: reducing lr to 0.0009960508123974011
Epoch 84: reducing lr to 0.0005270019492359488
Epoch 87: reducing lr to 0.0003426302644777085
Epoch 90: reducing lr to 0.00019575321996893048
Epoch 93: reducing lr to 8.868641847561004e-05
Epoch 96: reducing lr to 2.3118682863330214e-05
Epoch 99: reducing lr to 8.38614488567202e-08
[I 2024-06-22 10:53:18,014] Trial 96 finished with value: 2.4699811935424805 and parameters: {'hidden_size': 90, 'n_layers': 6, 'rnn_dropout': 0.573543201353976, 'bidirectional': True, 'fc_dropout': 0.0030349379274740686, 'learning_rate_model': 0.054401444720336284}. Best is trial 13 with value: 1.8697621822357178.
Epoch 10: reducing lr to 0.0005106527295766145
Epoch 13: reducing lr to 0.0007275456638699436
Epoch 37: reducing lr to 0.0011200862241985085
Epoch 45: reducing lr to 0.0009893967489233472
Epoch 50: reducing lr to 0.0008847382502141759
Epoch 53: reducing lr to 0.0008157136354931049
Epoch 56: reducing lr to 0.0007433330838991534
Epoch 62: reducing lr to 0.0005931057157375618
Epoch 65: reducing lr to 0.0005176280237684232
Epoch 68: reducing lr to 0.0004434953210688486
Epoch 71: reducing lr to 0.000371877163700988
Epoch 77: reducing lr to 0.0002406440955447786
Epoch 80: reducing lr to 0.00018309882169866912
Epoch 83: reducing lr to 0.0001321745604289163
Epoch 86: reducing lr to 8.867420811747452e-05
Epoch 89: reducing lr to 5.3284048939867404e-05
Epoch 92: reducing lr to 2.6562031575942843e-05
Epoch 95: reducing lr to 8.929656932697531e-06
Epoch 98: reducing lr to 6.649511624325436e-07
[I 2024-06-22 10:54:30,112] Trial 97 finished with value: 2.4616360664367676 and parameters: {'hidden_size': 174, 'n_layers': 6, 'rnn_dropout': 0.41335102633723075, 'bidirectional': False, 'fc_dropout': 0.2999007548294875, 'learning_rate_model': 0.01205841152767771}. Best is trial 13 with value: 1.8697621822357178.
Epoch 6: reducing lr to 0.0008046553946264516
Epoch 10: reducing lr to 0.0016311802940433776
Epoch 13: reducing lr to 0.002324002362437793
Epoch 22: reducing lr to 0.0037871436062210397
Epoch 26: reducing lr to 0.003845798658852604
Epoch 29: reducing lr to 0.0038115816073055935
Epoch 32: reducing lr to 0.0037476267238034933
Epoch 40: reducing lr to 0.0034407919617857247
Epoch 43: reducing lr to 0.0032797972929999226
Epoch 46: reducing lr to 0.0030974512121291196
Epoch 49: reducing lr to 0.0028966296959576894
Epoch 52: reducing lr to 0.0026804996901993172
Epoch 55: reducing lr to 0.002452469345844012
Epoch 58: reducing lr to 0.002216135693086694
Epoch 61: reducing lr to 0.001975224809246174
Epoch 64: reducing lr to 0.001733537250586428
Epoch 67: reducing lr to 0.0014948829696238186
Epoch 70: reducing lr to 0.0012630272806360976
Epoch 73: reducing lr to 0.0010416259617251344
Epoch 76: reducing lr to 0.0008341703402120097
Epoch 79: reducing lr to 0.0006439327263956969
Epoch 82: reducing lr to 0.0004739127180965766
Epoch 85: reducing lr to 0.0003267921333349591
Epoch 88: reducing lr to 0.00020489055437680482
Epoch 91: reducing lr to 0.00011013116391294167
Epoch 94: reducing lr to 4.4007892739635764e-05
Epoch 97: reducing lr to 7.563739160253851e-06
[I 2024-06-22 10:56:32,278] Trial 98 finished with value: 2.4693679809570312 and parameters: {'hidden_size': 154, 'n_layers': 5, 'rnn_dropout': 0.3214927391742699, 'bidirectional': True, 'fc_dropout': 0.6199173173796783, 'learning_rate_model': 0.03851823778112661}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 10:56:36,719] Trial 99 finished with value: 4.622625350952148 and parameters: {'hidden_size': 60, 'n_layers': 1, 'rnn_dropout': 0.1762271902349821, 'bidirectional': False, 'fc_dropout': 0.4345331441904756, 'learning_rate_model': 0.00014023182899556565}. Best is trial 13 with value: 1.8697621822357178.
Epoch 23: reducing lr to 0.002044127394004683
Epoch 27: reducing lr to 0.0020462119326507236
Epoch 39: reducing lr to 0.0018604101820686447
Epoch 42: reducing lr to 0.0017786758156610623
Epoch 47: reducing lr to 0.001616826144754814
Epoch 55: reducing lr to 0.0013076111670218466
Epoch 66: reducing lr to 0.0008391549690449935
Epoch 73: reducing lr to 0.0005553756428066228
Epoch 92: reducing lr to 4.523896186296415e-05
Epoch 97: reducing lr to 4.032845428689549e-06
[I 2024-06-22 10:56:43,935] Trial 100 finished with value: 2.0307247638702393 and parameters: {'hidden_size': 41, 'n_layers': 3, 'rnn_dropout': 0.4910968717930021, 'bidirectional': False, 'fc_dropout': 0.35529368561329155, 'learning_rate_model': 0.020537209952065802}. Best is trial 13 with value: 1.8697621822357178.
Epoch 36: reducing lr to 0.0003021433202927398
Epoch 39: reducing lr to 0.00029140008457029666
Epoch 42: reducing lr to 0.00027859785336718324
Epoch 47: reducing lr to 0.00025324698814169113
Epoch 56: reducing lr to 0.00019829682924983692
Epoch 62: reducing lr to 0.00015822110624188186
Epoch 68: reducing lr to 0.00011830997147844186
Epoch 71: reducing lr to 9.920460158387478e-05
Epoch 74: reducing lr to 8.10712464575977e-05
Epoch 77: reducing lr to 6.419593336800594e-05
Epoch 80: reducing lr to 4.884474614230026e-05
Epoch 83: reducing lr to 3.525982740208685e-05
Epoch 86: reducing lr to 2.365536350635634e-05
Epoch 89: reducing lr to 1.4214432511122166e-05
Epoch 92: reducing lr to 7.085876781260173e-06
Epoch 95: reducing lr to 2.382138901654146e-06
Epoch 98: reducing lr to 1.7738710945663234e-07
[I 2024-06-22 10:56:59,945] Trial 101 finished with value: 2.563401699066162 and parameters: {'hidden_size': 94, 'n_layers': 3, 'rnn_dropout': 0.5399772838472842, 'bidirectional': False, 'fc_dropout': 0.25181254985921314, 'learning_rate_model': 0.0032167877678542485}. Best is trial 13 with value: 1.8697621822357178.
Epoch 38: reducing lr to 4.950486981437437e-05
Epoch 49: reducing lr to 4.0565104280472944e-05
Epoch 63: reducing lr to 2.5403002668839854e-05
Epoch 69: reducing lr to 1.875628277674433e-05
Epoch 81: reducing lr to 7.396578344548887e-06
Epoch 84: reducing lr to 5.225499567897087e-06
Epoch 87: reducing lr to 3.397358020349806e-06
Epoch 90: reducing lr to 1.9409954134802163e-06
Epoch 93: reducing lr to 8.793721581002302e-07
Epoch 96: reducing lr to 2.2923381495615044e-07
Epoch 99: reducing lr to 8.315300643576065e-10
[I 2024-06-22 10:57:47,900] Trial 102 finished with value: 2.5206785202026367 and parameters: {'hidden_size': 178, 'n_layers': 4, 'rnn_dropout': 0.6275659350335385, 'bidirectional': False, 'fc_dropout': 0.10547278002200172, 'learning_rate_model': 0.0005394187370487681}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 11:00:29,616] Trial 103 finished with value: 2.194577217102051 and parameters: {'hidden_size': 184, 'n_layers': 5, 'rnn_dropout': 0.6134196697896818, 'bidirectional': True, 'fc_dropout': 0.67431987215428, 'learning_rate_model': 1.8673586483652047e-05}. Best is trial 13 with value: 1.8697621822357178.
Epoch 13: reducing lr to 0.00038324306191590043
Epoch 18: reducing lr to 0.0005496198094206968
Epoch 30: reducing lr to 0.0006255772473818252
Epoch 41: reducing lr to 0.0005589774813252963
Epoch 47: reducing lr to 0.000500064501819911
Epoch 61: reducing lr to 0.00032572738139289073
Epoch 70: reducing lr to 0.00020828138995803106
Epoch 73: reducing lr to 0.00017177087656825496
Epoch 77: reducing lr to 0.00012676205025812602
Epoch 80: reducing lr to 9.644941416836663e-05
Epoch 83: reducing lr to 6.962447274680098e-05
Epoch 86: reducing lr to 4.671016091435838e-05
Epoch 92: reducing lr to 1.3991856205593321e-05
Epoch 95: reducing lr to 4.7037996852900344e-06
Epoch 98: reducing lr to 3.5027068701048296e-07
[I 2024-06-22 11:01:17,986] Trial 104 finished with value: 2.4865636825561523 and parameters: {'hidden_size': 122, 'n_layers': 7, 'rnn_dropout': 0.41428803853785756, 'bidirectional': False, 'fc_dropout': 0.20131131388468226, 'learning_rate_model': 0.006351907220679005}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 11:03:09,390] Trial 105 finished with value: 2.646353244781494 and parameters: {'hidden_size': 115, 'n_layers': 7, 'rnn_dropout': 0.49946936192214075, 'bidirectional': True, 'fc_dropout': 0.7914720449333105, 'learning_rate_model': 2.0457769512926938e-05}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 11:03:35,790] Trial 106 finished with value: 2.739777088165283 and parameters: {'hidden_size': 75, 'n_layers': 7, 'rnn_dropout': 0.007151281896996054, 'bidirectional': False, 'fc_dropout': 0.351864651366858, 'learning_rate_model': 0.00010543402232135162}. Best is trial 13 with value: 1.8697621822357178.
Epoch 5: reducing lr to 0.00044229977233337916
Epoch 13: reducing lr to 0.001612056063395877
Epoch 19: reducing lr to 0.0024162787075433768
Epoch 25: reducing lr to 0.0026709112521142954
Epoch 29: reducing lr to 0.002643922975508445
Epoch 32: reducing lr to 0.0025995603451601733
Epoch 35: reducing lr to 0.0025352695409004682
Epoch 38: reducing lr to 0.0024520644498900426
Epoch 41: reducing lr to 0.002351257276693868
Epoch 44: reducing lr to 0.0022344378111730234
Epoch 52: reducing lr to 0.0018593422486816614
Epoch 55: reducing lr to 0.0017011678400848378
Epoch 58: reducing lr to 0.001537233799367128
Epoch 61: reducing lr to 0.0013701247390192747
Epoch 73: reducing lr to 0.0007225291482183295
Epoch 79: reducing lr to 0.0004466672120403265
Epoch 84: reducing lr to 0.0002588283086371379
Epoch 87: reducing lr to 0.0001682771989197248
Epoch 90: reducing lr to 9.614096287174738e-05
Epoch 93: reducing lr to 4.355687057022682e-05
Epoch 96: reducing lr to 1.135435948976908e-05
Epoch 99: reducing lr to 4.118716638301656e-08
[I 2024-06-22 11:05:22,311] Trial 107 finished with value: 1.8980700969696045 and parameters: {'hidden_size': 164, 'n_layers': 4, 'rnn_dropout': 0.10747088692455692, 'bidirectional': True, 'fc_dropout': 0.3932361200135483, 'learning_rate_model': 0.026718371620438223}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 11:06:21,636] Trial 108 finished with value: 1.9260743856430054 and parameters: {'hidden_size': 93, 'n_layers': 5, 'rnn_dropout': 0.31839251152633796, 'bidirectional': True, 'fc_dropout': 0.7719911392887311, 'learning_rate_model': 0.0001502667013784738}. Best is trial 13 with value: 1.8697621822357178.
Epoch 48: reducing lr to 8.217531798686016e-05
Epoch 51: reducing lr to 7.631733505582513e-05
Epoch 54: reducing lr to 7.009745770978555e-05
Epoch 57: reducing lr to 6.361375802542744e-05
Epoch 60: reducing lr to 5.696851657138191e-05
Epoch 66: reducing lr to 4.361344782250978e-05
Epoch 69: reducing lr to 3.711422847792568e-05
Epoch 72: reducing lr to 3.087138539391662e-05
Epoch 80: reducing lr to 1.620746682421376e-05
Epoch 83: reducing lr to 1.1699773834056683e-05
[I 2024-06-22 11:06:30,687] Trial 109 finished with value: 1.8846545219421387 and parameters: {'hidden_size': 35, 'n_layers': 2, 'rnn_dropout': 0.011273835353954365, 'bidirectional': True, 'fc_dropout': 0.6356626809478018, 'learning_rate_model': 0.0010673815537119522}. Best is trial 13 with value: 1.8697621822357178.
Epoch 84: reducing lr to 1.877075070451522e-06
Epoch 87: reducing lr to 1.2203801689267996e-06
[I 2024-06-22 11:07:22,818] Trial 110 finished with value: 2.5718979835510254 and parameters: {'hidden_size': 162, 'n_layers': 5, 'rnn_dropout': 0.6222787814873403, 'bidirectional': False, 'fc_dropout': 0.14383543815437952, 'learning_rate_model': 0.0001937670170464033}. Best is trial 13 with value: 1.8697621822357178.
Epoch 10: reducing lr to 0.0036041207781693473
Epoch 14: reducing lr to 0.005633551972739072
Epoch 17: reducing lr to 0.00698530155819582
Epoch 25: reducing lr to 0.008507721239320556
Epoch 28: reducing lr to 0.008454336971150906
Epoch 32: reducing lr to 0.008280445388781591
Epoch 37: reducing lr to 0.007905423392766618
Epoch 40: reducing lr to 0.007602488730470295
Epoch 43: reducing lr to 0.007246768254282411
Epoch 51: reducing lr to 0.006085093888854285
Epoch 54: reducing lr to 0.005589157577659416
Epoch 57: reducing lr to 0.005072185630229728
Epoch 62: reducing lr to 0.004186063267522465
Epoch 73: reducing lr to 0.002301490390389888
Epoch 77: reducing lr to 0.0016984348357753794
Epoch 80: reducing lr to 0.0012922877515795018
Epoch 85: reducing lr to 0.0007220528118171921
Epoch 88: reducing lr to 0.00045270918670161503
Epoch 91: reducing lr to 0.0002433366916165372
Epoch 94: reducing lr to 9.723619222569562e-05
Epoch 97: reducing lr to 1.6712211131825194e-05
[I 2024-06-22 11:07:31,874] Trial 111 finished with value: 2.525477409362793 and parameters: {'hidden_size': 47, 'n_layers': 3, 'rnn_dropout': 0.19313499837247905, 'bidirectional': False, 'fc_dropout': 0.256255460657625, 'learning_rate_model': 0.08510670563663286}. Best is trial 13 with value: 1.8697621822357178.
Epoch 37: reducing lr to 5.008588932927351e-05
Epoch 45: reducing lr to 4.424196548330673e-05
Epoch 48: reducing lr to 4.151219857073985e-05
Epoch 51: reducing lr to 3.855294320532692e-05
Epoch 61: reducing lr to 2.765057435770576e-05
Epoch 65: reducing lr to 2.314630726821712e-05
Epoch 68: reducing lr to 1.9831381807235117e-05
Epoch 71: reducing lr to 1.6628897010620527e-05
Epoch 74: reducing lr to 1.3589343501634082e-05
Epoch 77: reducing lr to 1.0760665810191074e-05
Epoch 80: reducing lr to 8.18746550202607e-06
Epoch 83: reducing lr to 5.910331064490289e-06
Epoch 86: reducing lr to 3.965164893721324e-06
Epoch 89: reducing lr to 2.3826549425937916e-06
Epoch 92: reducing lr to 1.1877505009271253e-06
Epoch 95: reducing lr to 3.992994460756878e-07
Epoch 98: reducing lr to 2.9734023695190785e-08
[I 2024-06-22 11:08:28,444] Trial 112 finished with value: 1.8825068473815918 and parameters: {'hidden_size': 133, 'n_layers': 3, 'rnn_dropout': 0.6859875992668774, 'bidirectional': True, 'fc_dropout': 0.6342845423815101, 'learning_rate_model': 0.0005392051542230778}. Best is trial 13 with value: 1.8697621822357178.
Epoch 38: reducing lr to 0.00013778674135556584
Epoch 49: reducing lr to 0.00011290472134384136
Epoch 53: reducing lr to 0.00010156244677661061
Epoch 56: reducing lr to 9.255052690784612e-05
Epoch 63: reducing lr to 7.070409378937218e-05
Epoch 66: reducing lr to 6.134600165550447e-05
Epoch 69: reducing lr to 5.220430017171202e-05
Epoch 72: reducing lr to 4.3423213573715185e-05
Epoch 80: reducing lr to 2.279717234638331e-05
Epoch 83: reducing lr to 1.6456721053423156e-05
Epoch 86: reducing lr to 1.1040601934948574e-05
Epoch 89: reducing lr to 6.634262502215257e-06
Epoch 92: reducing lr to 3.307171537692347e-06
Epoch 95: reducing lr to 1.111809055897705e-06
Epoch 98: reducing lr to 8.279139161720267e-08
[I 2024-06-22 11:08:41,415] Trial 113 finished with value: 2.5649518966674805 and parameters: {'hidden_size': 82, 'n_layers': 3, 'rnn_dropout': 0.5578951004905116, 'bidirectional': False, 'fc_dropout': 0.23623103551987823, 'learning_rate_model': 0.001501362396927331}. Best is trial 13 with value: 1.8697621822357178.
Epoch 38: reducing lr to 5.587155658872856e-05
Epoch 45: reducing lr to 4.9951582760944295e-05
Epoch 53: reducing lr to 4.118285937052474e-05
Epoch 58: reducing lr to 3.502666710709751e-05
Epoch 65: reducing lr to 2.6133438478334478e-05
Epoch 71: reducing lr to 1.8774928197135486e-05
Epoch 74: reducing lr to 1.5343107142129613e-05
Epoch 81: reducing lr to 8.347832184793101e-06
Epoch 84: reducing lr to 5.897536866713768e-06
Epoch 87: reducing lr to 3.834283002821458e-06
Epoch 90: reducing lr to 2.1906215588356767e-06
Epoch 93: reducing lr to 9.924658216065658e-07
Epoch 96: reducing lr to 2.587149529409221e-07
Epoch 99: reducing lr to 9.384708861913914e-10
[I 2024-06-22 11:09:15,766] Trial 114 finished with value: 2.57090425491333 and parameters: {'hidden_size': 106, 'n_layers': 6, 'rnn_dropout': 0.7107091475163088, 'bidirectional': False, 'fc_dropout': 0.7185640368137097, 'learning_rate_model': 0.0006087919149176257}. Best is trial 13 with value: 1.8697621822357178.
Epoch 39: reducing lr to 4.2069089655949534e-05
Epoch 45: reducing lr to 3.81044964787664e-05
Epoch 48: reducing lr to 3.575341662570235e-05
Epoch 57: reducing lr to 2.767752227223997e-05
Epoch 65: reducing lr to 1.9935334566702516e-05
Epoch 68: reducing lr to 1.7080272315839765e-05
Epoch 72: reducing lr to 1.3431739977906917e-05
Epoch 75: reducing lr to 1.0869929263045662e-05
Epoch 78: reducing lr to 8.502896842902649e-06
Epoch 81: reducing lr to 6.367965227710198e-06
Epoch 86: reducing lr to 3.415097183861359e-06
Epoch 89: reducing lr to 2.0521210095070696e-06
Epoch 92: reducing lr to 1.0229797497877253e-06
Epoch 95: reducing lr to 3.4390660927360937e-07
Epoch 98: reducing lr to 2.5609169683482332e-08
[I 2024-06-22 11:10:13,056] Trial 115 finished with value: 1.8706156015396118 and parameters: {'hidden_size': 186, 'n_layers': 2, 'rnn_dropout': 0.4870649393550366, 'bidirectional': True, 'fc_dropout': 0.01101401004617344, 'learning_rate_model': 0.00046440389064944195}. Best is trial 13 with value: 1.8697621822357178.
Epoch 15: reducing lr to 0.0005289116844569085
Epoch 22: reducing lr to 0.0007239834197975602
Epoch 42: reducing lr to 0.0006377325936228299
Epoch 54: reducing lr to 0.00048357721609230356
Epoch 73: reducing lr to 0.00019912630846131957
Epoch 81: reducing lr to 0.00010096900822019993
Epoch 84: reducing lr to 7.133210577219013e-05
Epoch 87: reducing lr to 4.637656141862753e-05
Epoch 97: reducing lr to 1.4459503818925467e-06
[I 2024-06-22 11:10:52,849] Trial 116 finished with value: 2.088345527648926 and parameters: {'hidden_size': 189, 'n_layers': 3, 'rnn_dropout': 0.62379543638608, 'bidirectional': False, 'fc_dropout': 0.1156781766704774, 'learning_rate_model': 0.007363482458797472}. Best is trial 13 with value: 1.8697621822357178.
Epoch 14: reducing lr to 2.034731048067216e-05
Epoch 55: reducing lr to 1.9571569958901106e-05
Epoch 58: reducing lr to 1.7685544094226878e-05
Epoch 61: reducing lr to 1.576299031187867e-05
Epoch 65: reducing lr to 1.3195205730798655e-05
Epoch 68: reducing lr to 1.1305438912573515e-05
Epoch 71: reducing lr to 9.479772068553462e-06
Epoch 74: reducing lr to 7.746988803556372e-06
Epoch 77: reducing lr to 6.134421250028599e-06
Epoch 80: reducing lr to 4.667495789334707e-06
Epoch 83: reducing lr to 3.3693510342432597e-06
Epoch 86: reducing lr to 2.2604541589680292e-06
Epoch 89: reducing lr to 1.3582996971702723e-06
Epoch 92: reducing lr to 6.77110695670801e-07
Epoch 95: reducing lr to 2.276319189107711e-07
Epoch 98: reducing lr to 1.695071940919059e-08
[I 2024-06-22 11:12:43,958] Trial 117 finished with value: 1.8775651454925537 and parameters: {'hidden_size': 145, 'n_layers': 5, 'rnn_dropout': 0.20271682758903975, 'bidirectional': True, 'fc_dropout': 0.3397192927450718, 'learning_rate_model': 0.0003073891164855588}. Best is trial 13 with value: 1.8697621822357178.
[I 2024-06-22 11:12:51,898] Trial 118 finished with value: 7.368609428405762 and parameters: {'hidden_size': 81, 'n_layers': 1, 'rnn_dropout': 0.5053173014927518, 'bidirectional': True, 'fc_dropout': 0.5186502049304996, 'learning_rate_model': 4.7836881141741806e-05}. Best is trial 13 with value: 1.8697621822357178.
Epoch 58: reducing lr to 5.611599687894062e-05
Epoch 71: reducing lr to 3.007919106009666e-05
Epoch 81: reducing lr to 1.3374007963573781e-05
Epoch 84: reducing lr to 9.448405678851675e-06
Epoch 87: reducing lr to 6.142880005151992e-06
Epoch 90: reducing lr to 3.5095806341692703e-06
Epoch 93: reducing lr to 1.5900230752027193e-06
Epoch 96: reducing lr to 4.1448555317517585e-07
Epoch 99: reducing lr to 1.5035181383325075e-09
[I 2024-06-22 11:13:01,755] Trial 119 finished with value: 2.5702013969421387 and parameters: {'hidden_size': 43, 'n_layers': 4, 'rnn_dropout': 0.7002661810836313, 'bidirectional': False, 'fc_dropout': 0.3431458672239224, 'learning_rate_model': 0.0009753415902513679}. Best is trial 13 with value: 1.8697621822357178.
Epoch 36: reducing lr to 0.0002516821596544335
Epoch 41: reducing lr to 0.00023580445708535594
Epoch 44: reducing lr to 0.00022408878865672792
Epoch 50: reducing lr to 0.0001966013725693657
Epoch 60: reducing lr to 0.00014301350838965226
Epoch 64: reducing lr to 0.00012059480833708449
Epoch 67: reducing lr to 0.00010399264575778334
Epoch 72: reducing lr to 7.749938737655257e-05
Epoch 75: reducing lr to 6.271807376394541e-05
Epoch 78: reducing lr to 4.906060550121417e-05
Epoch 81: reducing lr to 3.674232860332921e-05
Epoch 84: reducing lr to 2.5957545948489432e-05
Epoch 87: reducing lr to 1.6876295896851162e-05
Epoch 90: reducing lr to 9.641848970910099e-06
Epoch 93: reducing lr to 4.368260470241463e-06
Epoch 96: reducing lr to 1.1387135731916448e-06
Epoch 99: reducing lr to 4.130605997087339e-09
[I 2024-06-22 11:13:18,297] Trial 120 finished with value: 1.8680797815322876 and parameters: {'hidden_size': 45, 'n_layers': 3, 'rnn_dropout': 0.20950432328281618, 'bidirectional': True, 'fc_dropout': 0.3413113266145149, 'learning_rate_model': 0.002679549863220907}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:13:37,086] Trial 121 finished with value: 5.440189361572266 and parameters: {'hidden_size': 42, 'n_layers': 4, 'rnn_dropout': 0.23755766862952366, 'bidirectional': True, 'fc_dropout': 0.5067792100979368, 'learning_rate_model': 3.155042401877456e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 10: reducing lr to 7.753253904431983e-05
Epoch 15: reducing lr to 0.00013150684386948355
Epoch 47: reducing lr to 0.00014413529142614696
Epoch 62: reducing lr to 9.005139774922622e-05
Epoch 68: reducing lr to 6.73360119415258e-05
Epoch 71: reducing lr to 5.646220815904194e-05
Epoch 74: reducing lr to 4.614162569194693e-05
Epoch 77: reducing lr to 3.653705669815932e-05
Epoch 80: reducing lr to 2.7799942544302294e-05
Epoch 83: reducing lr to 2.0068098481755515e-05
Epoch 86: reducing lr to 1.3463428480628035e-05
Epoch 89: reducing lr to 8.090131248872324e-06
Epoch 92: reducing lr to 4.03292028217637e-06
Epoch 95: reducing lr to 1.3557921747735842e-06
Epoch 98: reducing lr to 1.0095971093037909e-07
[I 2024-06-22 11:15:14,760] Trial 122 finished with value: 2.4594874382019043 and parameters: {'hidden_size': 103, 'n_layers': 7, 'rnn_dropout': 0.7586393392581321, 'bidirectional': True, 'fc_dropout': 0.511301478975924, 'learning_rate_model': 0.0018308318127610833}. Best is trial 120 with value: 1.8680797815322876.
Epoch 13: reducing lr to 0.003015691288996745
Epoch 17: reducing lr to 0.004102400965952318
Epoch 20: reducing lr to 0.0046851339782668314
Epoch 23: reducing lr to 0.004974887240677505
Epoch 28: reducing lr to 0.004965151449509347
Epoch 41: reducing lr to 0.004398523257670745
Epoch 47: reducing lr to 0.003934944456752435
Epoch 50: reducing lr to 0.0036672576948930356
Epoch 53: reducing lr to 0.0033811492900494592
Epoch 56: reducing lr to 0.00308113045992737
Epoch 59: reducing lr to 0.0027719340111092413
Epoch 62: reducing lr to 0.0024584350223323987
Epoch 65: reducing lr to 0.002145578483576916
Epoch 68: reducing lr to 0.0018382969521720956
Epoch 71: reducing lr to 0.0015414382613245284
Epoch 74: reducing lr to 0.0012596827081388581
Epoch 77: reducing lr to 0.0009974745761286228
Epoch 80: reducing lr to 0.0007589482681886354
Epoch 83: reducing lr to 0.0005478661894460973
Epoch 89: reducing lr to 0.00022086344570553182
Epoch 92: reducing lr to 0.00011010015071156662
Epoch 95: reducing lr to 3.7013606104701476e-05
Epoch 98: reducing lr to 2.756235831975068e-06
[I 2024-06-22 11:15:53,957] Trial 123 finished with value: 1.904584288597107 and parameters: {'hidden_size': 102, 'n_layers': 3, 'rnn_dropout': 0.47941580452642407, 'bidirectional': True, 'fc_dropout': 0.46348843223778646, 'learning_rate_model': 0.0499823563097424}. Best is trial 120 with value: 1.8680797815322876.
Epoch 39: reducing lr to 0.0002829755474673215
Epoch 58: reducing lr to 0.00017972627892154234
Epoch 63: reducing lr to 0.00014710948325644336
Epoch 66: reducing lr to 0.0001276386998223108
Epoch 69: reducing lr to 0.00010861814656592328
Epoch 72: reducing lr to 9.034790162494974e-05
Epoch 81: reducing lr to 4.283378749816427e-05
Epoch 84: reducing lr to 3.0261010921084787e-05
Epoch 87: reducing lr to 1.9674193217475218e-05
Epoch 90: reducing lr to 1.124035752790957e-05
Epoch 93: reducing lr to 5.092468219393192e-06
Epoch 96: reducing lr to 1.3274992922181726e-06
Epoch 99: reducing lr to 4.815413345979e-09
[I 2024-06-22 11:16:05,830] Trial 124 finished with value: 2.5662007331848145 and parameters: {'hidden_size': 27, 'n_layers': 7, 'rnn_dropout': 0.042395634134391624, 'bidirectional': False, 'fc_dropout': 0.08600710094813416, 'learning_rate_model': 0.0031237886599691338}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:16:12,381] Trial 125 finished with value: 7.869717121124268 and parameters: {'hidden_size': 24, 'n_layers': 2, 'rnn_dropout': 0.04882934489573687, 'bidirectional': True, 'fc_dropout': 0.3028644746004818, 'learning_rate_model': 5.659486653989453e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 41: reducing lr to 0.00033221424152377917
Epoch 48: reducing lr to 0.00029063624891658926
Epoch 52: reducing lr to 0.00026271050003828236
Epoch 58: reducing lr to 0.00021719909844130568
Epoch 68: reducing lr to 0.000138843969188134
Epoch 77: reducing lr to 7.533784416620422e-05
Epoch 80: reducing lr to 5.7322289437112486e-05
Epoch 84: reducing lr to 3.657041323851893e-05
Epoch 87: reducing lr to 2.3776250501803854e-05
Epoch 92: reducing lr to 8.315708686207172e-06
Epoch 95: reducing lr to 2.795585326663668e-06
Epoch 98: reducing lr to 2.0817459468547643e-07
[I 2024-06-22 11:16:41,246] Trial 126 finished with value: 2.4820995330810547 and parameters: {'hidden_size': 154, 'n_layers': 3, 'rnn_dropout': 0.6362409339245143, 'bidirectional': False, 'fc_dropout': 0.022792679593253776, 'learning_rate_model': 0.0037750966900208034}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:16:49,589] Trial 127 finished with value: 3.098731517791748 and parameters: {'hidden_size': 80, 'n_layers': 2, 'rnn_dropout': 0.34710670096936047, 'bidirectional': False, 'fc_dropout': 0.7835866833438452, 'learning_rate_model': 0.00028675584665119325}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:18:34,590] Trial 128 finished with value: 2.3219103813171387 and parameters: {'hidden_size': 159, 'n_layers': 4, 'rnn_dropout': 0.7113048712859863, 'bidirectional': True, 'fc_dropout': 0.7288153917255746, 'learning_rate_model': 1.4932655812821317e-05}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:20:54,811] Trial 129 finished with value: 2.2037477493286133 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.30109057334967004, 'bidirectional': True, 'fc_dropout': 0.41336508781408976, 'learning_rate_model': 1.3316445065440956e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 7: reducing lr to 0.00017541356348356108
Epoch 17: reducing lr to 0.000559841204885259
Epoch 23: reducing lr to 0.0006789065452412844
Epoch 26: reducing lr to 0.0006810258768035536
Epoch 29: reducing lr to 0.0006749666158805212
Epoch 32: reducing lr to 0.0006636412880418868
Epoch 36: reducing lr to 0.0006406695290746292
Epoch 39: reducing lr to 0.0006178894002127145
Epoch 42: reducing lr to 0.0005907433444003388
Epoch 45: reducing lr to 0.0005596594712940918
Epoch 51: reducing lr to 0.00048769351848225657
Epoch 54: reducing lr to 0.0004479464038169024
Epoch 57: reducing lr to 0.0004065133754029269
Epoch 60: reducing lr to 0.00036404804058067484
Epoch 63: reducing lr to 0.00032121994705363417
Epoch 71: reducing lr to 0.00021035502395749797
Epoch 74: reducing lr to 0.00017190476770811606
Epoch 77: reducing lr to 0.00013612208391546864
Epoch 80: reducing lr to 0.00010357118098271349
Epoch 83: reducing lr to 7.476550199773027e-05
Epoch 86: reducing lr to 5.015921114199353e-05
Epoch 89: reducing lr to 3.014051005377329e-05
Epoch 92: reducing lr to 1.5025006464258056e-05
Epoch 95: reducing lr to 5.051125428933746e-06
Epoch 98: reducing lr to 3.761344641655187e-07
[I 2024-06-22 11:21:24,911] Trial 130 finished with value: 1.8816814422607422 and parameters: {'hidden_size': 121, 'n_layers': 2, 'rnn_dropout': 0.5047673602716244, 'bidirectional': True, 'fc_dropout': 0.36146899953669653, 'learning_rate_model': 0.006820928234876919}. Best is trial 120 with value: 1.8680797815322876.
Epoch 17: reducing lr to 0.0006634319656800283
Epoch 20: reducing lr to 0.0007576703668102593
Epoch 23: reducing lr to 0.0008045286768678893
Epoch 26: reducing lr to 0.0008070401610030597
Epoch 29: reducing lr to 0.0007998597188532908
Epoch 32: reducing lr to 0.0007864387979843171
Epoch 35: reducing lr to 0.0007669890541391392
Epoch 38: reducing lr to 0.0007418172161849857
Epoch 41: reducing lr to 0.0007113202622426793
Epoch 44: reducing lr to 0.0006759791476513491
Epoch 47: reducing lr to 0.0006363512385676787
Epoch 52: reducing lr to 0.0005625023807649154
Epoch 55: reducing lr to 0.0005146502537695324
Epoch 58: reducing lr to 0.0004650556789904638
Epoch 64: reducing lr to 0.0003637824821113945
Epoch 67: reducing lr to 0.00031370092391833066
Epoch 70: reducing lr to 0.0002650460490357355
Epoch 73: reducing lr to 0.00021858502184470127
Epoch 77: reducing lr to 0.00016130956584914109
Epoch 80: reducing lr to 0.00012273557499442443
Epoch 83: reducing lr to 8.859980923621785e-05
Epoch 86: reducing lr to 5.944046946618122e-05
Epoch 89: reducing lr to 3.5717588589556824e-05
Epoch 92: reducing lr to 1.7805173120440282e-05
Epoch 95: reducing lr to 5.9857653259030166e-06
Epoch 98: reducing lr to 4.457328698635396e-07
[I 2024-06-22 11:22:07,223] Trial 131 finished with value: 2.563866138458252 and parameters: {'hidden_size': 159, 'n_layers': 4, 'rnn_dropout': 0.4228267921420521, 'bidirectional': False, 'fc_dropout': 0.11377700245822933, 'learning_rate_model': 0.008083045312026036}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:22:57,383] Trial 132 finished with value: 3.151193618774414 and parameters: {'hidden_size': 104, 'n_layers': 4, 'rnn_dropout': 0.7087983769322811, 'bidirectional': True, 'fc_dropout': 0.2151136177701167, 'learning_rate_model': 1.941101032854524e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 79: reducing lr to 8.228834062591548e-05
Epoch 82: reducing lr to 6.056143689414e-05
Epoch 85: reducing lr to 4.176085680915069e-05
Epoch 89: reducing lr to 2.175060462764524e-05
Epoch 92: reducing lr to 1.0842649130649896e-05
Epoch 95: reducing lr to 3.645095319666867e-06
Epoch 98: reducing lr to 2.7143376148241335e-07
[I 2024-06-22 11:23:03,139] Trial 133 finished with value: 2.0081119537353516 and parameters: {'hidden_size': 93, 'n_layers': 1, 'rnn_dropout': 0.23292315244194245, 'bidirectional': False, 'fc_dropout': 0.08750633135486768, 'learning_rate_model': 0.004922256224784005}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:23:38,656] Trial 134 finished with value: 3.441946029663086 and parameters: {'hidden_size': 108, 'n_layers': 6, 'rnn_dropout': 0.11488559829936743, 'bidirectional': False, 'fc_dropout': 0.6375698015839202, 'learning_rate_model': 5.8439024207513246e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 30: reducing lr to 0.0004273232705513092
Epoch 39: reducing lr to 0.0003930492841152515
Epoch 42: reducing lr to 0.0003757812458547897
Epoch 47: reducing lr to 0.00034158722891318454
Epoch 61: reducing lr to 0.0002224999238822502
Epoch 64: reducing lr to 0.00019527494009642822
Epoch 70: reducing lr to 0.00014227417177388896
Epoch 73: reducing lr to 0.00011733433891308186
Epoch 77: reducing lr to 8.658942460710979e-05
Epoch 80: reducing lr to 6.588327704960202e-05
Epoch 83: reducing lr to 4.755952606827151e-05
Epoch 86: reducing lr to 3.190707272913108e-05
Epoch 92: reducing lr to 9.5576458061436e-06
Epoch 95: reducing lr to 3.2131012979592573e-06
Epoch 98: reducing lr to 2.3926512061935026e-07
[I 2024-06-22 11:24:09,420] Trial 135 finished with value: 2.519113540649414 and parameters: {'hidden_size': 158, 'n_layers': 3, 'rnn_dropout': 0.17351319008943628, 'bidirectional': False, 'fc_dropout': 0.3048651930347619, 'learning_rate_model': 0.004338901037624098}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:24:14,717] Trial 136 finished with value: 4.967358589172363 and parameters: {'hidden_size': 29, 'n_layers': 2, 'rnn_dropout': 0.013260706320747318, 'bidirectional': False, 'fc_dropout': 0.3221254817399354, 'learning_rate_model': 0.00019235578705946124}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:25:03,461] Trial 137 finished with value: 6.145552158355713 and parameters: {'hidden_size': 119, 'n_layers': 7, 'rnn_dropout': 0.6675892112144237, 'bidirectional': False, 'fc_dropout': 0.03178296534646785, 'learning_rate_model': 1.9113215939952844e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 63: reducing lr to 2.8807078047722626e-05
Epoch 71: reducing lr to 1.8864686481821808e-05
Epoch 81: reducing lr to 8.38774110427812e-06
Epoch 84: reducing lr to 5.925731530761081e-06
Epoch 87: reducing lr to 3.8526137608260495e-06
Epoch 90: reducing lr to 2.2010943783028657e-06
Epoch 93: reducing lr to 9.972105550522444e-07
Epoch 96: reducing lr to 2.5995180509581905e-07
Epoch 99: reducing lr to 9.42957483222786e-10
[I 2024-06-22 11:25:21,299] Trial 138 finished with value: 2.5725646018981934 and parameters: {'hidden_size': 103, 'n_layers': 3, 'rnn_dropout': 0.7498016497027592, 'bidirectional': False, 'fc_dropout': 0.7162082925849766, 'learning_rate_model': 0.0006117023983794077}. Best is trial 120 with value: 1.8680797815322876.
Epoch 38: reducing lr to 0.0004450805122533935
Epoch 43: reducing lr to 0.00041294981661364635
Epoch 50: reducing lr to 0.0003558290381663482
Epoch 56: reducing lr to 0.0002989579078524429
Epoch 59: reducing lr to 0.00026895699596108085
Epoch 62: reducing lr to 0.0002385386144554855
Epoch 65: reducing lr to 0.00020818256900374544
Epoch 68: reducing lr to 0.00017836745895071475
Epoch 73: reducing lr to 0.00013114812027940093
Epoch 76: reducing lr to 0.00010502798137869574
Epoch 79: reducing lr to 8.107571216189637e-05
Epoch 82: reducing lr to 5.966898333825193e-05
Epoch 85: reducing lr to 4.114545487901803e-05
Epoch 88: reducing lr to 2.5797178696485967e-05
Epoch 91: reducing lr to 1.3866297175853421e-05
Epoch 94: reducing lr to 5.5409068344474365e-06
Epoch 97: reducing lr to 9.523285801249851e-07
[I 2024-06-22 11:25:44,575] Trial 139 finished with value: 2.0189852714538574 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.21904776332648687, 'bidirectional': False, 'fc_dropout': 0.176102939221585, 'learning_rate_model': 0.004849720213485595}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:25:48,363] Trial 140 finished with value: 2.9989535808563232 and parameters: {'hidden_size': 42, 'n_layers': 1, 'rnn_dropout': 0.6961533969643021, 'bidirectional': False, 'fc_dropout': 0.45723367418026584, 'learning_rate_model': 0.0012682587865373047}. Best is trial 120 with value: 1.8680797815322876.
Epoch 63: reducing lr to 1.07306473379331e-05
Epoch 66: reducing lr to 9.31038464786077e-06
Epoch 69: reducing lr to 7.922963220984608e-06
Epoch 72: reducing lr to 6.5902717391072415e-06
Epoch 75: reducing lr to 5.333321501620352e-06
Epoch 78: reducing lr to 4.171939067946261e-06
Epoch 81: reducing lr to 3.1244366958282838e-06
Epoch 84: reducing lr to 2.2073372096443666e-06
Epoch 87: reducing lr to 1.4351000656229586e-06
Epoch 90: reducing lr to 8.199084784630699e-07
Epoch 93: reducing lr to 3.7146130441286025e-07
Epoch 96: reducing lr to 9.683214454174113e-08
Epoch 99: reducing lr to 3.5125201487912294e-10
[I 2024-06-22 11:26:16,516] Trial 141 finished with value: 2.527501106262207 and parameters: {'hidden_size': 30, 'n_layers': 7, 'rnn_dropout': 0.7609084947349537, 'bidirectional': True, 'fc_dropout': 0.4222774368943676, 'learning_rate_model': 0.0002278593719884827}. Best is trial 120 with value: 1.8680797815322876.
Epoch 72: reducing lr to 5.81401693936658e-06
Epoch 75: reducing lr to 4.705120332065284e-06
Epoch 79: reducing lr to 3.3605778014263908e-06
Epoch 82: reducing lr to 2.4732716555088287e-06
Epoch 85: reducing lr to 1.705473792446119e-06
Epoch 88: reducing lr to 1.0692897262959134e-06
Epoch 91: reducing lr to 5.747562276616663e-07
Epoch 94: reducing lr to 2.296698729014338e-07
Epoch 97: reducing lr to 3.947389669104731e-08
[I 2024-06-22 11:28:13,463] Trial 142 finished with value: 1.9065203666687012 and parameters: {'hidden_size': 118, 'n_layers': 7, 'rnn_dropout': 0.24936233417518514, 'bidirectional': True, 'fc_dropout': 0.6195818128120637, 'learning_rate_model': 0.00020102027670166017}. Best is trial 120 with value: 1.8680797815322876.
Epoch 48: reducing lr to 1.4918903336537393e-05
Epoch 51: reducing lr to 1.38553883634749e-05
Epoch 57: reducing lr to 1.1549057917938141e-05
Epoch 61: reducing lr to 9.937229543248438e-06
Epoch 64: reducing lr to 8.721314910692864e-06
Epoch 67: reducing lr to 7.520660504013232e-06
Epoch 70: reducing lr to 6.35420937825084e-06
Epoch 73: reducing lr to 5.240353518959641e-06
Epoch 76: reducing lr to 4.196657570345086e-06
Epoch 79: reducing lr to 3.239584315997893e-06
Epoch 82: reducing lr to 2.3842245404905376e-06
Epoch 85: reducing lr to 1.6440702985685369e-06
Epoch 88: reducing lr to 1.030791260090932e-06
Epoch 91: reducing lr to 5.540628340354252e-07
Epoch 94: reducing lr to 2.2140089058283618e-07
Epoch 97: reducing lr to 3.8052687415052946e-08
[I 2024-06-22 11:31:26,606] Trial 143 finished with value: 1.9415661096572876 and parameters: {'hidden_size': 180, 'n_layers': 6, 'rnn_dropout': 0.6101056763560886, 'bidirectional': True, 'fc_dropout': 0.0331800636611991, 'learning_rate_model': 0.0001937827879848057}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:31:35,870] Trial 144 finished with value: 9.317771911621094 and parameters: {'hidden_size': 22, 'n_layers': 5, 'rnn_dropout': 0.5362800255489347, 'bidirectional': False, 'fc_dropout': 0.428806795791651, 'learning_rate_model': 1.4435761775167338e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 58: reducing lr to 0.00016973600915028248
Epoch 63: reducing lr to 0.00013893225156577852
[I 2024-06-22 11:31:40,976] Trial 145 finished with value: 2.529348850250244 and parameters: {'hidden_size': 40, 'n_layers': 2, 'rnn_dropout': 0.2685310409317198, 'bidirectional': False, 'fc_dropout': 0.40964882201913505, 'learning_rate_model': 0.0029501496595471795}. Best is trial 120 with value: 1.8680797815322876.
Epoch 17: reducing lr to 0.0016078056193672859
Epoch 22: reducing lr to 0.001926005515090565
Epoch 25: reducing lr to 0.001958220687056921
Epoch 34: reducing lr to 0.001876066936000014
Epoch 41: reducing lr to 0.0017238613361527078
Epoch 47: reducing lr to 0.0015421763650048464
Epoch 50: reducing lr to 0.001437265050016438
Epoch 53: reducing lr to 0.0013251339577917832
Epoch 57: reducing lr to 0.0011674640659126633
Epoch 62: reducing lr to 0.0009635054390247312
Epoch 69: reducing lr to 0.0006811345442715577
Epoch 72: reducing lr to 0.0005665634955559839
Epoch 77: reducing lr to 0.00039092844458301566
Epoch 80: reducing lr to 0.0002974456423275276
Epoch 83: reducing lr to 0.00021471873309397393
Epoch 86: reducing lr to 0.00014405202909931574
Epoch 89: reducing lr to 8.656040500803209e-05
Epoch 92: reducing lr to 4.315025334588399e-05
Epoch 95: reducing lr to 1.4506306034463983e-05
Epoch 98: reducing lr to 1.0802189975404191e-06
[I 2024-06-22 11:32:12,701] Trial 146 finished with value: 2.557077169418335 and parameters: {'hidden_size': 95, 'n_layers': 6, 'rnn_dropout': 0.36324724484732107, 'bidirectional': False, 'fc_dropout': 0.5249612381952792, 'learning_rate_model': 0.01958899532517217}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:32:23,573] Trial 147 finished with value: 8.648744583129883 and parameters: {'hidden_size': 192, 'n_layers': 1, 'rnn_dropout': 0.18805749127281166, 'bidirectional': False, 'fc_dropout': 0.6910633164011616, 'learning_rate_model': 2.1207815222486866e-05}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:32:34,175] Trial 148 finished with value: 2.1818628311157227 and parameters: {'hidden_size': 103, 'n_layers': 1, 'rnn_dropout': 0.6183084817336942, 'bidirectional': True, 'fc_dropout': 0.38915259453631584, 'learning_rate_model': 9.070770449331516e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 35: reducing lr to 5.195101610989617e-05
Epoch 38: reducing lr to 5.0246034073953463e-05
Epoch 41: reducing lr to 4.8180362162458215e-05
Epoch 44: reducing lr to 4.578657726609278e-05
Epoch 47: reducing lr to 4.310243186389022e-05
Epoch 50: reducing lr to 4.017025568180693e-05
Epoch 53: reducing lr to 3.703629326861589e-05
Epoch 65: reducing lr to 2.3502148864720612e-05
Epoch 78: reducing lr to 1.0024228422884988e-05
Epoch 84: reducing lr to 5.3037333564656284e-06
Epoch 87: reducing lr to 3.448221710146762e-06
Epoch 90: reducing lr to 1.970055108695503e-06
Epoch 93: reducing lr to 8.925377156887223e-07
Epoch 96: reducing lr to 2.3266579874620194e-07
Epoch 99: reducing lr to 8.439793519978734e-10
[I 2024-06-22 11:36:40,549] Trial 149 finished with value: 1.9157965183258057 and parameters: {'hidden_size': 192, 'n_layers': 7, 'rnn_dropout': 0.49114720190776073, 'bidirectional': True, 'fc_dropout': 0.7182805741341455, 'learning_rate_model': 0.0005474946675652392}. Best is trial 120 with value: 1.8680797815322876.
Epoch 32: reducing lr to 0.00025573926298658493
Epoch 38: reducing lr to 0.00024122892795237148
Epoch 44: reducing lr to 0.00021981927831859504
Epoch 53: reducing lr to 0.0001778095621908828
Epoch 56: reducing lr to 0.00016203202258622098
Epoch 62: reducing lr to 0.00012928540490126142
Epoch 87: reducing lr to 1.6554755849118715e-05
[I 2024-06-22 11:37:01,108] Trial 150 finished with value: 2.457779884338379 and parameters: {'hidden_size': 59, 'n_layers': 7, 'rnn_dropout': 0.547374440786582, 'bidirectional': False, 'fc_dropout': 0.561104235706677, 'learning_rate_model': 0.002628497037637164}. Best is trial 120 with value: 1.8680797815322876.
Epoch 13: reducing lr to 0.001413687425991046
Epoch 17: reducing lr to 0.0019231121842944628
Epoch 20: reducing lr to 0.002196284154921792
Epoch 26: reducing lr to 0.0023393940104304363
Epoch 29: reducing lr to 0.002318579825252175
Epoch 32: reducing lr to 0.0022796761579844697
Epoch 37: reducing lr to 0.0021764294528986124
Epoch 43: reducing lr to 0.0019950961616277494
Epoch 46: reducing lr to 0.001884175292582075
Epoch 49: reducing lr to 0.0017620158417705838
Epoch 52: reducing lr to 0.0016305442578951345
Epoch 55: reducing lr to 0.0014918337144938982
Epoch 58: reducing lr to 0.0013480722800644192
Epoch 61: reducing lr to 0.0012015265222913999
Epoch 64: reducing lr to 0.0010545083142989794
Epoch 67: reducing lr to 0.0009093352449386397
Epoch 70: reducing lr to 0.0007682977496829928
Epoch 73: reducing lr to 0.0006336196333002087
Epoch 76: reducing lr to 0.0005074246653757247
Epoch 79: reducing lr to 0.0003917033877430476
Epoch 82: reducing lr to 0.00028828045161176233
Epoch 85: reducing lr to 0.00019878720317814956
Epoch 88: reducing lr to 0.00012463464112961973
Epoch 91: reducing lr to 6.699263483973816e-05
Epoch 94: reducing lr to 2.676994034770501e-05
Epoch 97: reducing lr to 4.601012080344968e-06
[I 2024-06-22 11:37:44,126] Trial 151 finished with value: 1.9076361656188965 and parameters: {'hidden_size': 151, 'n_layers': 2, 'rnn_dropout': 0.4803911642196355, 'bidirectional': True, 'fc_dropout': 0.10440853707141279, 'learning_rate_model': 0.023430590821514066}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:38:14,499] Trial 152 finished with value: 6.640063762664795 and parameters: {'hidden_size': 130, 'n_layers': 4, 'rnn_dropout': 0.19863258319767887, 'bidirectional': False, 'fc_dropout': 0.3373327478797674, 'learning_rate_model': 1.8101769159892324e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 22: reducing lr to 0.00012639072054418664
Epoch 31: reducing lr to 0.00012589187962874342
Epoch 41: reducing lr to 0.00011312536474453446
Epoch 44: reducing lr to 0.00010750486341645807
Epoch 50: reducing lr to 9.431798811646297e-05
Epoch 53: reducing lr to 8.695958263390425e-05
Epoch 56: reducing lr to 7.924341572979588e-05
Epoch 59: reducing lr to 7.129121018233941e-05
Epoch 62: reducing lr to 6.322834785904151e-05
Epoch 65: reducing lr to 5.518200867060931e-05
Epoch 68: reducing lr to 4.727905277312534e-05
Epoch 72: reducing lr to 3.7179731769348924e-05
Epoch 75: reducing lr to 3.0088510871754697e-05
Epoch 79: reducing lr to 2.1490371037803352e-05
Epoch 82: reducing lr to 1.5816186588986804e-05
Epoch 85: reducing lr to 1.0906238974547813e-05
Epoch 88: reducing lr to 6.837941069317536e-06
Epoch 91: reducing lr to 3.6754764563090094e-06
Epoch 94: reducing lr to 1.4687030256409997e-06
Epoch 97: reducing lr to 2.5242941432228586e-07
[I 2024-06-22 11:39:10,750] Trial 153 finished with value: 1.869872808456421 and parameters: {'hidden_size': 181, 'n_layers': 2, 'rnn_dropout': 0.6609820283939132, 'bidirectional': True, 'fc_dropout': 0.4787717544837091, 'learning_rate_model': 0.0012854933251677597}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:41:06,149] Trial 154 finished with value: 1.9388554096221924 and parameters: {'hidden_size': 131, 'n_layers': 6, 'rnn_dropout': 0.22571155872388848, 'bidirectional': True, 'fc_dropout': 0.09208243277352271, 'learning_rate_model': 8.091045824612664e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 25: reducing lr to 0.007146901891385192
Epoch 36: reducing lr to 0.00671519977817559
Epoch 42: reducing lr to 0.006191896750584755
Epoch 45: reducing lr to 0.005866090061932961
Epoch 49: reducing lr to 0.0053764414126464734
Epoch 53: reducing lr to 0.004836330476885394
Epoch 57: reducing lr to 0.004260883972855775
Epoch 60: reducing lr to 0.0038157821004592162
Epoch 63: reducing lr to 0.0033668779601797967
Epoch 66: reducing lr to 0.0029212523610636803
Epoch 69: reducing lr to 0.002485931128660722
Epoch 72: reducing lr to 0.0020677821170730823
Epoch 75: reducing lr to 0.0016733978904405339
Epoch 78: reducing lr to 0.0013089992855721676
Epoch 81: reducing lr to 0.0009803320077414342
Epoch 84: reducing lr to 0.0006925803045977174
Epoch 87: reducing lr to 0.0004502810156212997
Epoch 90: reducing lr to 0.00025725678037552405
Epoch 93: reducing lr to 0.00011655073915868702
Epoch 96: reducing lr to 3.0382324852111507e-05
Epoch 99: reducing lr to 1.1020981587755996e-07
[I 2024-06-22 11:41:21,266] Trial 155 finished with value: 2.043881416320801 and parameters: {'hidden_size': 126, 'n_layers': 2, 'rnn_dropout': 0.10513252842996845, 'bidirectional': False, 'fc_dropout': 0.48174692311416867, 'learning_rate_model': 0.07149379468062945}. Best is trial 120 with value: 1.8680797815322876.
Epoch 7: reducing lr to 0.0015960911288602398
Epoch 10: reducing lr to 0.0026282916123915446
Epoch 13: reducing lr to 0.0037446234108385766
Epoch 17: reducing lr to 0.005094005064046998
Epoch 20: reducing lr to 0.0058175922853727444
Epoch 23: reducing lr to 0.006177382710125015
Epoch 26: reducing lr to 0.006196666545641922
Epoch 29: reducing lr to 0.006141533222912248
Epoch 32: reducing lr to 0.0060384838638108205
Epoch 37: reducing lr to 0.005765000474308779
Epoch 40: reducing lr to 0.00554408650360092
Epoch 43: reducing lr to 0.005284678675321805
Epoch 46: reducing lr to 0.004990867698904715
Epoch 49: reducing lr to 0.004667287584267131
Epoch 52: reducing lr to 0.004319041174354482
Epoch 57: reducing lr to 0.0036988724210283174
Epoch 60: reducing lr to 0.003312479585446773
Epoch 66: reducing lr to 0.0025359385193397402
Epoch 69: reducing lr to 0.0021580362551426335
Epoch 76: reducing lr to 0.0013440837389289376
Epoch 79: reducing lr to 0.0010375572767219953
Epoch 90: reducing lr to 0.0002233245533357721
Epoch 93: reducing lr to 0.00010117767051882262
Epoch 96: reducing lr to 2.637488938871113e-05
Epoch 99: reducing lr to 9.567311644170795e-08
[I 2024-06-22 11:42:10,098] Trial 156 finished with value: 2.4820637702941895 and parameters: {'hidden_size': 79, 'n_layers': 5, 'rnn_dropout': 0.4791882311787358, 'bidirectional': True, 'fc_dropout': 0.6002537476597487, 'learning_rate_model': 0.06206374712466089}. Best is trial 120 with value: 1.8680797815322876.
Epoch 86: reducing lr to 1.659497295841174e-05
Epoch 91: reducing lr to 6.452279186539012e-06
[I 2024-06-22 11:42:23,092] Trial 157 finished with value: 2.131286382675171 and parameters: {'hidden_size': 114, 'n_layers': 2, 'rnn_dropout': 0.6023407319839538, 'bidirectional': False, 'fc_dropout': 0.7340850781155146, 'learning_rate_model': 0.002256676630910648}. Best is trial 120 with value: 1.8680797815322876.
Epoch 11: reducing lr to 0.00037095201129131155
Epoch 27: reducing lr to 0.0007649315020696613
Epoch 40: reducing lr to 0.0006858124768564734
Epoch 43: reducing lr to 0.0006537233084943804
Epoch 55: reducing lr to 0.000488821787300083
Epoch 58: reducing lr to 0.00044171618790257803
Epoch 61: reducing lr to 0.00039369826302268994
Epoch 64: reducing lr to 0.0003455255327121334
Epoch 67: reducing lr to 0.0002979573898667798
Epoch 70: reducing lr to 0.00025174433016891617
Epoch 74: reducing lr to 0.0001934896853497378
Epoch 77: reducing lr to 0.0001532140122528501
Epoch 80: reducing lr to 0.00011657591285468379
Epoch 83: reducing lr to 8.415329981492437e-05
Epoch 86: reducing lr to 5.64573636359777e-05
Epoch 89: reducing lr to 3.392504980715468e-05
Epoch 92: reducing lr to 1.691159478533653e-05
Epoch 95: reducing lr to 5.685361045750133e-06
Epoch 98: reducing lr to 4.233631218660862e-07
[I 2024-06-22 11:44:56,629] Trial 158 finished with value: 1.8789077997207642 and parameters: {'hidden_size': 174, 'n_layers': 5, 'rnn_dropout': 0.6580625220190969, 'bidirectional': True, 'fc_dropout': 0.0983638172417428, 'learning_rate_model': 0.007677386005956413}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:45:02,284] Trial 159 finished with value: 9.213069915771484 and parameters: {'hidden_size': 97, 'n_layers': 1, 'rnn_dropout': 0.4718589184879114, 'bidirectional': False, 'fc_dropout': 0.6306409517343752, 'learning_rate_model': 1.1178590317786991e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 52: reducing lr to 7.430654109296057e-05
Epoch 55: reducing lr to 6.798527710802817e-05
Epoch 63: reducing lr to 5.0284799193585545e-05
Epoch 66: reducing lr to 4.362931775585664e-05
Epoch 72: reducing lr to 3.0882618782996976e-05
Epoch 75: reducing lr to 2.499243450073799e-05
Epoch 78: reducing lr to 1.9550089726456348e-05
Epoch 81: reducing lr to 1.464139738218789e-05
Epoch 84: reducing lr to 1.0343784940832466e-05
Epoch 87: reducing lr to 6.725010742590653e-06
Epoch 90: reducing lr to 3.842166450749749e-06
Epoch 93: reducing lr to 1.7407017966714588e-06
Epoch 96: reducing lr to 4.5376432478150264e-07
Epoch 99: reducing lr to 1.6459992093929496e-09
[I 2024-06-22 11:45:09,186] Trial 160 finished with value: 1.883368730545044 and parameters: {'hidden_size': 72, 'n_layers': 1, 'rnn_dropout': 0.7979879941826636, 'bidirectional': True, 'fc_dropout': 0.7431595695144926, 'learning_rate_model': 0.001067769949377941}. Best is trial 120 with value: 1.8680797815322876.
Epoch 31: reducing lr to 0.00021336194236608354
Epoch 37: reducing lr to 0.00020237199782410401
Epoch 40: reducing lr to 0.0001946171326166097
Epoch 43: reducing lr to 0.00018551099625218023
Epoch 46: reducing lr to 0.0001751971871648867
Epoch 49: reducing lr to 0.00016383837556596626
Epoch 56: reducing lr to 0.00013430198317550588
Epoch 59: reducing lr to 0.00012082456090884942
Epoch 62: reducing lr to 0.00010715959720028966
Epoch 68: reducing lr to 8.012868314184274e-05
Epoch 71: reducing lr to 6.71890457515284e-05
Epoch 74: reducing lr to 5.490773210522493e-05
Epoch 79: reducing lr to 3.642194651724505e-05
Epoch 82: reducing lr to 2.680532137102308e-05
Epoch 85: reducing lr to 1.8483927147489424e-05
Epoch 88: reducing lr to 1.158896342350977e-05
Epoch 91: reducing lr to 6.229208731742915e-06
Epoch 94: reducing lr to 2.489162376745997e-06
Epoch 97: reducing lr to 4.278181429093458e-07
[I 2024-06-22 11:45:36,185] Trial 161 finished with value: 2.561640739440918 and parameters: {'hidden_size': 100, 'n_layers': 5, 'rnn_dropout': 0.2113101153684557, 'bidirectional': False, 'fc_dropout': 0.1955583994972887, 'learning_rate_model': 0.0021786580164286193}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:45:43,933] Trial 162 finished with value: 8.973360061645508 and parameters: {'hidden_size': 144, 'n_layers': 1, 'rnn_dropout': 0.7768666557003711, 'bidirectional': False, 'fc_dropout': 0.6013123232650325, 'learning_rate_model': 2.0904698538829833e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 23: reducing lr to 0.0006117311988387073
Epoch 29: reducing lr to 0.0006081811112337295
Epoch 36: reducing lr to 0.0005772775970821801
Epoch 39: reducing lr to 0.0005567514795538143
Epoch 42: reducing lr to 0.000532291427751036
Epoch 45: reducing lr to 0.0005042831914287942
Epoch 48: reducing lr to 0.00047316848946001835
Epoch 52: reducing lr to 0.0004277041522927007
Epoch 68: reducing lr to 0.0002260440375771475
Epoch 74: reducing lr to 0.00015489541288602756
Epoch 78: reducing lr to 0.00011252918559672255
Epoch 84: reducing lr to 5.953822778646161e-05
Epoch 96: reducing lr to 2.611841205588453e-06
Epoch 99: reducing lr to 9.474276236980107e-09
[I 2024-06-22 11:46:06,738] Trial 163 finished with value: 2.0636074542999268 and parameters: {'hidden_size': 105, 'n_layers': 4, 'rnn_dropout': 0.537978120419944, 'bidirectional': False, 'fc_dropout': 0.5453162700820984, 'learning_rate_model': 0.006146022063804241}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:46:21,980] Trial 164 finished with value: 4.50839900970459 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.6086419650446049, 'bidirectional': False, 'fc_dropout': 0.18701495408328475, 'learning_rate_model': 5.0899740979609974e-05}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:46:35,368] Trial 165 finished with value: 7.613010883331299 and parameters: {'hidden_size': 88, 'n_layers': 3, 'rnn_dropout': 0.652381543204501, 'bidirectional': False, 'fc_dropout': 0.3421320797575714, 'learning_rate_model': 2.4989769577785342e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 72: reducing lr to 2.6648714958898826e-05
Epoch 75: reducing lr to 2.1566055256486334e-05
Epoch 83: reducing lr to 1.0099447563140392e-05
Epoch 86: reducing lr to 6.775589131367431e-06
Epoch 93: reducing lr to 1.5020573978486731e-06
Epoch 96: reducing lr to 3.91554752354013e-07
Epoch 99: reducing lr to 1.4203382187939241e-09
[I 2024-06-22 11:46:49,730] Trial 166 finished with value: 1.8868852853775024 and parameters: {'hidden_size': 37, 'n_layers': 3, 'rnn_dropout': 0.15507821400760574, 'bidirectional': True, 'fc_dropout': 0.551076207214932, 'learning_rate_model': 0.0009213822578516836}. Best is trial 120 with value: 1.8680797815322876.
Epoch 31: reducing lr to 0.0007773111202039263
Epoch 35: reducing lr to 0.0007531494123765946
Epoch 43: reducing lr to 0.0006758457422529112
Epoch 46: reducing lr to 0.0006382709132730642
Epoch 49: reducing lr to 0.0005968889757530497
Epoch 52: reducing lr to 0.0005523525208701058
Epoch 55: reducing lr to 0.0005053638433485033
Epoch 66: reducing lr to 0.00032431550832303887
Epoch 69: reducing lr to 0.00027598643252926857
Epoch 72: reducing lr to 0.00022956380535218943
Epoch 75: reducing lr to 0.00018577952890975598
Epoch 78: reducing lr to 0.00014532423639710941
Epoch 81: reducing lr to 0.00010883581222001729
Epoch 84: reducing lr to 7.688980812953382e-05
Epoch 87: reducing lr to 4.998990104924149e-05
Epoch 90: reducing lr to 2.8560477899505255e-05
Epoch 93: reducing lr to 1.2939386106961427e-05
Epoch 96: reducing lr to 3.3730256446797003e-06
Epoch 99: reducing lr to 1.2235420991137114e-08
[I 2024-06-22 11:46:57,263] Trial 167 finished with value: 1.8967459201812744 and parameters: {'hidden_size': 32, 'n_layers': 2, 'rnn_dropout': 0.36928166471329765, 'bidirectional': True, 'fc_dropout': 0.5835181586049222, 'learning_rate_model': 0.00793719388055494}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:47:52,760] Trial 168 finished with value: 2.274625062942505 and parameters: {'hidden_size': 77, 'n_layers': 6, 'rnn_dropout': 0.3427539807608588, 'bidirectional': True, 'fc_dropout': 0.46482852593447216, 'learning_rate_model': 6.351992141755928e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 39: reducing lr to 0.0004890629128398606
Epoch 48: reducing lr to 0.0004156417507948194
Epoch 52: reducing lr to 0.0003757048633648997
Epoch 70: reducing lr to 0.00017702874342143314
Epoch 73: reducing lr to 0.00014599663676818935
Epoch 77: reducing lr to 0.00010774139002645601
Epoch 80: reducing lr to 8.197716847097962e-05
Epoch 83: reducing lr to 5.9177312597297364e-05
Epoch 86: reducing lr to 3.970129589277293e-05
Epoch 92: reducing lr to 1.1892376571467477e-05
Epoch 95: reducing lr to 3.997994001100208e-06
Epoch 98: reducing lr to 2.9771253010806367e-07
[I 2024-06-22 11:48:35,926] Trial 169 finished with value: 2.4767937660217285 and parameters: {'hidden_size': 113, 'n_layers': 7, 'rnn_dropout': 0.24399518066817283, 'bidirectional': False, 'fc_dropout': 0.2638756501455059, 'learning_rate_model': 0.005398802811105274}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:48:46,812] Trial 170 finished with value: 3.4610447883605957 and parameters: {'hidden_size': 182, 'n_layers': 1, 'rnn_dropout': 0.7028702905638164, 'bidirectional': False, 'fc_dropout': 0.5336950309593632, 'learning_rate_model': 0.0002384277397661184}. Best is trial 120 with value: 1.8680797815322876.
Epoch 14: reducing lr to 0.0001515343302037197
Epoch 31: reducing lr to 0.00022419214912953027
Epoch 37: reducing lr to 0.00021264435734268347
Epoch 40: reducing lr to 0.00020449585682849646
Epoch 49: reducing lr to 0.0001721547766236683
Epoch 56: reducing lr to 0.00014111912324463795
Epoch 68: reducing lr to 8.419599803636078e-05
Epoch 71: reducing lr to 7.059954740734486e-05
Epoch 79: reducing lr to 3.827071676715291e-05
Epoch 82: reducing lr to 2.8165953776172064e-05
Epoch 85: reducing lr to 1.9422167353722302e-05
Epoch 88: reducing lr to 1.2177216739254777e-05
Epoch 91: reducing lr to 6.545402040583799e-06
Epoch 94: reducing lr to 2.6155117289735144e-06
Epoch 97: reducing lr to 4.495341007483295e-07
[I 2024-06-22 11:49:11,876] Trial 171 finished with value: 2.4658892154693604 and parameters: {'hidden_size': 44, 'n_layers': 5, 'rnn_dropout': 0.6782304349717934, 'bidirectional': True, 'fc_dropout': 0.7650412078041857, 'learning_rate_model': 0.0022892462334419333}. Best is trial 120 with value: 1.8680797815322876.
Epoch 16: reducing lr to 9.174034372463259e-05
Epoch 46: reducing lr to 9.562139332943029e-05
Epoch 53: reducing lr to 8.043862229700291e-05
Epoch 56: reducing lr to 7.330107843604363e-05
Epoch 59: reducing lr to 6.594519609294476e-05
Epoch 64: reducing lr to 5.351601556803924e-05
Epoch 67: reducing lr to 4.6148521035658296e-05
Epoch 70: reducing lr to 3.89909057855632e-05
Epoch 73: reducing lr to 3.215602731634347e-05
Epoch 79: reducing lr to 1.9878842406706043e-05
Epoch 82: reducing lr to 1.463015599518767e-05
Epoch 85: reducing lr to 1.0088397517359549e-05
Epoch 88: reducing lr to 6.3251747801000435e-06
Epoch 91: reducing lr to 3.3998583419522474e-06
Epoch 94: reducing lr to 1.3585673294151755e-06
Epoch 97: reducing lr to 2.3350013535377353e-07
[I 2024-06-22 11:49:35,403] Trial 172 finished with value: 1.8855342864990234 and parameters: {'hidden_size': 52, 'n_layers': 4, 'rnn_dropout': 0.2693967989609338, 'bidirectional': True, 'fc_dropout': 0.7941606989318544, 'learning_rate_model': 0.0011890962320255246}. Best is trial 120 with value: 1.8680797815322876.
Epoch 46: reducing lr to 0.007396062880324297
Epoch 49: reducing lr to 0.006916543281917126
Epoch 55: reducing lr to 0.005855981660954726
Epoch 61: reducing lr to 0.004716421951943979
Epoch 65: reducing lr to 0.003948118773013352
Epoch 68: reducing lr to 0.0033826843263008027
Epoch 71: reducing lr to 0.0028364291418651643
Epoch 74: reducing lr to 0.002317965521238895
Epoch 77: reducing lr to 0.0018354714729668722
Epoch 80: reducing lr to 0.0013965547885184614
Epoch 83: reducing lr to 0.0010081387393694394
Epoch 86: reducing lr to 0.0006763472796583375
Epoch 89: reducing lr to 0.00040641492396433496
Epoch 92: reducing lr to 0.00020259732993372498
Epoch 95: reducing lr to 6.810942328022845e-05
Epoch 98: reducing lr to 5.071800688888351e-06
[I 2024-06-22 11:49:39,678] Trial 173 finished with value: 2.000699043273926 and parameters: {'hidden_size': 47, 'n_layers': 1, 'rnn_dropout': 0.2962445550191101, 'bidirectional': False, 'fc_dropout': 0.3311358765835271, 'learning_rate_model': 0.0919734611324751}. Best is trial 120 with value: 1.8680797815322876.
Epoch 86: reducing lr to 6.535560717955361e-06
Epoch 92: reducing lr to 1.9577030778424627e-06
[I 2024-06-22 11:50:05,003] Trial 174 finished with value: 2.0876333713531494 and parameters: {'hidden_size': 188, 'n_layers': 2, 'rnn_dropout': 0.3159058100294404, 'bidirectional': False, 'fc_dropout': 0.6242908186314149, 'learning_rate_model': 0.0008887418605061119}. Best is trial 120 with value: 1.8680797815322876.
Epoch 79: reducing lr to 9.251276996091931e-05
Epoch 85: reducing lr to 4.694969554580033e-05
Epoch 97: reducing lr to 1.0866701322879111e-06
[I 2024-06-22 11:50:15,840] Trial 175 finished with value: 2.096275806427002 and parameters: {'hidden_size': 93, 'n_layers': 2, 'rnn_dropout': 0.7286449112694068, 'bidirectional': False, 'fc_dropout': 0.1507747197331348, 'learning_rate_model': 0.005533852722614425}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:50:43,759] Trial 176 finished with value: 2.606873035430908 and parameters: {'hidden_size': 81, 'n_layers': 7, 'rnn_dropout': 0.09821770355953596, 'bidirectional': False, 'fc_dropout': 0.46672505168658235, 'learning_rate_model': 0.0001725780425093903}. Best is trial 120 with value: 1.8680797815322876.
Epoch 39: reducing lr to 0.0005730928832471711
Epoch 58: reducing lr to 0.00036398852234512086
Epoch 64: reducing lr to 0.00028472429023166924
[I 2024-06-22 11:50:47,620] Trial 177 finished with value: 2.0762112140655518 and parameters: {'hidden_size': 16, 'n_layers': 2, 'rnn_dropout': 0.11632290733698857, 'bidirectional': False, 'fc_dropout': 0.6984755026157125, 'learning_rate_model': 0.006326416066049896}. Best is trial 120 with value: 1.8680797815322876.
Epoch 61: reducing lr to 1.8109524990008516e-05
Epoch 64: reducing lr to 1.5893652212978542e-05
Epoch 67: reducing lr to 1.370558954546157e-05
Epoch 70: reducing lr to 1.157985865440355e-05
Epoch 73: reducing lr to 9.549976942271193e-06
Epoch 80: reducing lr to 5.362315776722847e-06
Epoch 83: reducing lr to 3.87092458648267e-06
Epoch 86: reducing lr to 2.5969533870582812e-06
Epoch 89: reducing lr to 1.5605010104770072e-06
Epoch 92: reducing lr to 7.779077967847153e-07
Epoch 95: reducing lr to 2.61518014188695e-07
Epoch 98: reducing lr to 1.947406365580156e-08
[I 2024-06-22 11:52:11,072] Trial 178 finished with value: 1.9045202732086182 and parameters: {'hidden_size': 103, 'n_layers': 6, 'rnn_dropout': 0.3966611176681699, 'bidirectional': True, 'fc_dropout': 0.7397926508306973, 'learning_rate_model': 0.00035314815123986564}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:52:37,100] Trial 179 finished with value: 2.1372392177581787 and parameters: {'hidden_size': 107, 'n_layers': 2, 'rnn_dropout': 0.7079732150704507, 'bidirectional': True, 'fc_dropout': 0.5740759697558036, 'learning_rate_model': 5.800691551121672e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 13: reducing lr to 0.0002157701871869639
Epoch 19: reducing lr to 0.0003234136335954947
Epoch 23: reducing lr to 0.00035594901741820245
Epoch 31: reducing lr to 0.00035022663776069086
Epoch 37: reducing lr to 0.00033218700387176636
Epoch 40: reducing lr to 0.00031945764671561486
Epoch 68: reducing lr to 0.00013152860802517855
Epoch 71: reducing lr to 0.00011028861720583784
Epoch 79: reducing lr to 5.978543187213762e-05
Epoch 82: reducing lr to 4.4000056775638646e-05
Epoch 85: reducing lr to 3.034076080152817e-05
Epoch 88: reducing lr to 1.902290375658225e-05
Epoch 93: reducing lr to 5.829992101154562e-06
Epoch 96: reducing lr to 1.5197562467738831e-06
Epoch 99: reducing lr to 5.512812365502176e-09
[I 2024-06-22 11:53:04,343] Trial 180 finished with value: 2.4637372493743896 and parameters: {'hidden_size': 37, 'n_layers': 6, 'rnn_dropout': 0.5953087370956281, 'bidirectional': True, 'fc_dropout': 0.49859771615006726, 'learning_rate_model': 0.003576195751981789}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:53:09,311] Trial 181 finished with value: 2.604663133621216 and parameters: {'hidden_size': 76, 'n_layers': 1, 'rnn_dropout': 0.7044308780514614, 'bidirectional': False, 'fc_dropout': 0.20311089675419175, 'learning_rate_model': 0.00232032451794255}. Best is trial 120 with value: 1.8680797815322876.
Epoch 13: reducing lr to 0.0005589711022910414
Epoch 26: reducing lr to 0.0009249948925496373
Epoch 31: reducing lr to 0.0009072920235785297
Epoch 36: reducing lr to 0.0008701813872149788
Epoch 39: reducing lr to 0.0008392405616654497
Epoch 42: reducing lr to 0.0008023697703569456
Epoch 45: reducing lr to 0.0007601504878843159
Epoch 48: reducing lr to 0.0007132485560254991
Epoch 51: reducing lr to 0.00066240363118504
Epoch 54: reducing lr to 0.0006084176090509019
Epoch 57: reducing lr to 0.0005521417156213097
Epoch 60: reducing lr to 0.0004944637049040693
Epoch 63: reducing lr to 0.0004362929817061631
Epoch 66: reducing lr to 0.0003785471044684318
Epoch 69: reducing lr to 0.00032213650665901617
Epoch 72: reducing lr to 0.00026795115119893477
Epoch 75: reducing lr to 0.0002168453278782091
Epoch 78: reducing lr to 0.00016962515663116584
Epoch 81: reducing lr to 0.00012703518802227674
Epoch 84: reducing lr to 8.974721677995258e-05
Epoch 87: reducing lr to 5.8349143994695236e-05
Epoch 90: reducing lr to 3.333632198779524e-05
Epoch 93: reducing lr to 1.5103092570924529e-05
Epoch 96: reducing lr to 3.9370583839591635e-06
Epoch 99: reducing lr to 1.4281411370338504e-08
[I 2024-06-22 11:53:45,701] Trial 182 finished with value: 1.891608715057373 and parameters: {'hidden_size': 95, 'n_layers': 3, 'rnn_dropout': 0.10137070707753822, 'bidirectional': True, 'fc_dropout': 0.24228664377406287, 'learning_rate_model': 0.009264440595593887}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 11:55:17,651] Trial 183 finished with value: 2.054638385772705 and parameters: {'hidden_size': 129, 'n_layers': 5, 'rnn_dropout': 0.006706463620025272, 'bidirectional': True, 'fc_dropout': 0.18472676038734337, 'learning_rate_model': 5.007097358286679e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 15: reducing lr to 0.0001101839322822353
Epoch 32: reducing lr to 0.00014924791755336158
Epoch 37: reducing lr to 0.00014248846811387006
Epoch 40: reducing lr to 0.00013702833096186238
Epoch 43: reducing lr to 0.00013061677484266386
Epoch 46: reducing lr to 0.00012335490623896158
Epoch 49: reducing lr to 0.0001153572599157238
Epoch 54: reducing lr to 0.00010073976581904621
Epoch 60: reducing lr to 8.18716570609423e-05
Epoch 74: reducing lr to 3.8660084989032995e-05
Epoch 84: reducing lr to 1.4860045907328745e-05
Epoch 92: reducing lr to 3.379010568544923e-06
Epoch 95: reducing lr to 1.135959990966693e-06
[I 2024-06-22 11:55:49,925] Trial 184 finished with value: 2.2992866039276123 and parameters: {'hidden_size': 50, 'n_layers': 6, 'rnn_dropout': 0.7860718314440683, 'bidirectional': True, 'fc_dropout': 0.624179663753091, 'learning_rate_model': 0.001533975286317709}. Best is trial 120 with value: 1.8680797815322876.
Epoch 14: reducing lr to 1.9264241413943744e-05
Epoch 37: reducing lr to 2.7033030928735515e-05
Epoch 43: reducing lr to 2.4780723386763007e-05
Epoch 46: reducing lr to 2.3402995622805192e-05
Epoch 49: reducing lr to 2.1885675496657177e-05
Epoch 55: reducing lr to 1.8529792863597417e-05
Epoch 61: reducing lr to 1.4923940491404728e-05
Epoch 65: reducing lr to 1.2492836777922999e-05
Epoch 68: reducing lr to 1.070366055058178e-05
Epoch 71: reducing lr to 8.975172313375047e-06
Epoch 74: reducing lr to 7.334623545681459e-06
Epoch 77: reducing lr to 5.8078915150791744e-06
Epoch 80: reducing lr to 4.419049179483467e-06
Epoch 83: reducing lr to 3.1900035040817323e-06
Epoch 86: reducing lr to 2.1401322137821294e-06
Epoch 89: reducing lr to 1.285998623927736e-06
Epoch 92: reducing lr to 6.410687013281755e-07
Epoch 95: reducing lr to 2.155152762612939e-07
Epoch 98: reducing lr to 1.6048447835345692e-08
[I 2024-06-22 11:57:42,770] Trial 185 finished with value: 1.8847359418869019 and parameters: {'hidden_size': 167, 'n_layers': 4, 'rnn_dropout': 0.471961038639523, 'bidirectional': True, 'fc_dropout': 0.4296056934724326, 'learning_rate_model': 0.00029102706982436836}. Best is trial 120 with value: 1.8680797815322876.
Epoch 39: reducing lr to 3.408813932511487e-05
Epoch 58: reducing lr to 2.165040227360809e-05
Epoch 64: reducing lr to 1.693568627072895e-05
Epoch 81: reducing lr to 5.159895012583741e-06
Epoch 84: reducing lr to 3.6453381418614776e-06
Epoch 87: reducing lr to 2.3700162275822533e-06
Epoch 90: reducing lr to 1.35404941135323e-06
Epoch 93: reducing lr to 6.134550060069846e-07
Epoch 96: reducing lr to 1.5991480971461148e-07
Epoch 99: reducing lr to 5.800800900116876e-10
[I 2024-06-22 11:58:36,278] Trial 186 finished with value: 2.573261260986328 and parameters: {'hidden_size': 127, 'n_layers': 7, 'rnn_dropout': 0.31425835599757, 'bidirectional': False, 'fc_dropout': 0.5160627192683411, 'learning_rate_model': 0.00037630157098835073}. Best is trial 120 with value: 1.8680797815322876.
Epoch 3: reducing lr to 0.0004855201689975794
Epoch 6: reducing lr to 0.001054490627423956
Epoch 9: reducing lr to 0.0018440118167481509
Epoch 12: reducing lr to 0.002743197770252285
Epoch 15: reducing lr to 0.0036257604830290725
Epoch 20: reducing lr to 0.004731561562568497
Epoch 23: reducing lr to 0.005024186150341404
Epoch 26: reducing lr to 0.005039870070842347
Epoch 32: reducing lr to 0.004911217002613781
Epoch 35: reducing lr to 0.004789755659514041
Epoch 38: reducing lr to 0.004632560517483494
Epoch 44: reducing lr to 0.004221409589489609
Epoch 47: reducing lr to 0.003973937998097399
Epoch 50: reducing lr to 0.003703598580036423
Epoch 53: reducing lr to 0.0034146549687405006
Epoch 58: reducing lr to 0.0029042175483636416
Epoch 61: reducing lr to 0.002588506909062971
Epoch 64: reducing lr to 0.0022717784473219142
Epoch 67: reducing lr to 0.001959025057298967
Epoch 70: reducing lr to 0.0016551811353105038
Epoch 74: reducing lr to 0.0012721656009220227
Epoch 92: reducing lr to 0.00011119119400990127
Epoch 95: reducing lr to 3.7380394402689034e-05
Epoch 98: reducing lr to 2.7835488975218315e-06
[I 2024-06-22 11:59:40,654] Trial 187 finished with value: 1.8875072002410889 and parameters: {'hidden_size': 143, 'n_layers': 3, 'rnn_dropout': 0.10889052339849768, 'bidirectional': True, 'fc_dropout': 0.17437514404893434, 'learning_rate_model': 0.05047765912753394}. Best is trial 120 with value: 1.8680797815322876.
Epoch 13: reducing lr to 0.0017023032699755997
Epoch 16: reducing lr to 0.0021767579439296173
Epoch 19: reducing lr to 0.002551548446993067
Epoch 22: reducing lr to 0.0027740363129298787
Epoch 26: reducing lr to 0.00281700042067311
Epoch 29: reducing lr to 0.0027919368494483806
Epoch 32: reducing lr to 0.002745090680495982
Epoch 35: reducing lr to 0.0026772007051993965
Epoch 43: reducing lr to 0.0024024113516280194
Epoch 51: reducing lr to 0.0020172990388739034
Epoch 54: reducing lr to 0.0018528887828960054
Epoch 57: reducing lr to 0.0016815049009505062
Epoch 62: reducing lr to 0.001387742171358408
Epoch 65: reducing lr to 0.0012111403053451648
Epoch 73: reducing lr to 0.0007629782608638107
Epoch 77: reducing lr to 0.0005630563840724453
Epoch 80: reducing lr to 0.00042841259096836753
Epoch 85: reducing lr to 0.00023937123566211243
Epoch 88: reducing lr to 0.0001500798219227644
Epoch 91: reducing lr to 8.066972886316796e-05
Epoch 94: reducing lr to 3.223524249641209e-05
Epoch 97: reducing lr to 5.540346306806901e-06
[I 2024-06-22 12:00:26,267] Trial 188 finished with value: 2.553398609161377 and parameters: {'hidden_size': 143, 'n_layers': 5, 'rnn_dropout': 0.597325977698404, 'bidirectional': False, 'fc_dropout': 0.7444478553998761, 'learning_rate_model': 0.02821413746745477}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:00:37,975] Trial 189 finished with value: 8.955942153930664 and parameters: {'hidden_size': 46, 'n_layers': 2, 'rnn_dropout': 0.05728822355344363, 'bidirectional': True, 'fc_dropout': 0.528478684742879, 'learning_rate_model': 2.283088758720842e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 38: reducing lr to 0.00015357578341264309
Epoch 45: reducing lr to 0.00013730337802618343
Epoch 53: reducing lr to 0.00011320053131072026
Epoch 58: reducing lr to 9.627882539416412e-05
Epoch 63: reducing lr to 7.880610635941427e-05
Epoch 66: reducing lr to 6.837566641601132e-05
Epoch 69: reducing lr to 5.818641342050721e-05
Epoch 72: reducing lr to 4.839909832593226e-05
Epoch 75: reducing lr to 3.916802856989623e-05
Epoch 81: reducing lr to 2.2945930735631423e-05
Epoch 84: reducing lr to 1.6210732254651932e-05
Epoch 87: reducing lr to 1.0539405950663679e-05
Epoch 90: reducing lr to 6.021425616172752e-06
Epoch 93: reducing lr to 2.72801986143786e-06
Epoch 96: reducing lr to 7.11137365850341e-07
Epoch 99: reducing lr to 2.57960240160858e-09
[I 2024-06-22 12:00:50,279] Trial 190 finished with value: 2.566354274749756 and parameters: {'hidden_size': 54, 'n_layers': 4, 'rnn_dropout': 0.08396293466394243, 'bidirectional': False, 'fc_dropout': 0.27136334790808, 'learning_rate_model': 0.0016734041608502312}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:01:10,054] Trial 191 finished with value: 2.6456875801086426 and parameters: {'hidden_size': 115, 'n_layers': 3, 'rnn_dropout': 0.5570160664808824, 'bidirectional': False, 'fc_dropout': 0.4491930070394994, 'learning_rate_model': 0.0002705949305417018}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:03:21,139] Trial 192 finished with value: 2.600551128387451 and parameters: {'hidden_size': 129, 'n_layers': 7, 'rnn_dropout': 0.4775108199567785, 'bidirectional': True, 'fc_dropout': 0.5916895700060374, 'learning_rate_model': 1.413889647537952e-05}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:04:02,107] Trial 193 finished with value: 6.401052474975586 and parameters: {'hidden_size': 157, 'n_layers': 4, 'rnn_dropout': 0.00425322636888259, 'bidirectional': False, 'fc_dropout': 0.6143703026250493, 'learning_rate_model': 1.6306861470410482e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 11: reducing lr to 0.002296436126814288
Epoch 17: reducing lr to 0.00390095909740511
Epoch 20: reducing lr to 0.0044550779328414915
Epoch 28: reducing lr to 0.0047213455919371435
Epoch 31: reducing lr to 0.0046545522397098416
Epoch 37: reducing lr to 0.004414803433456523
Epoch 43: reducing lr to 0.004046975826714486
Epoch 46: reducing lr to 0.0038219771101915866
Epoch 49: reducing lr to 0.003574181362824967
Epoch 52: reducing lr to 0.0033074963117095796
Epoch 55: reducing lr to 0.0030261273096273767
Epoch 62: reducing lr to 0.0023377174843075476
Epoch 66: reducing lr to 0.001942006792906308
Epoch 69: reducing lr to 0.001652611463118687
Epoch 72: reducing lr to 0.0013746319801497548
Epoch 76: reducing lr to 0.0010292914166998436
Epoch 79: reducing lr to 0.0007945552559957554
Epoch 87: reducing lr to 0.0002993403797318335
Epoch 90: reducing lr to 0.00017102062857334357
Epoch 93: reducing lr to 7.748126460461192e-05
Epoch 96: reducing lr to 2.019773506510905e-05
Epoch 99: reducing lr to 7.326590948930623e-08
[I 2024-06-22 12:05:40,568] Trial 194 finished with value: 2.4637792110443115 and parameters: {'hidden_size': 117, 'n_layers': 6, 'rnn_dropout': 0.5196664439027832, 'bidirectional': True, 'fc_dropout': 0.2214451700804693, 'learning_rate_model': 0.04752805227340119}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:06:31,925] Trial 195 finished with value: 3.838385820388794 and parameters: {'hidden_size': 157, 'n_layers': 5, 'rnn_dropout': 0.6663745150696564, 'bidirectional': False, 'fc_dropout': 0.7988164056718448, 'learning_rate_model': 3.619072928538003e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 35: reducing lr to 0.0005222833010084327
Epoch 50: reducing lr to 0.00040384684094466327
Epoch 53: reducing lr to 0.00037233992622069944
Epoch 56: reducing lr to 0.0003393011635132281
Epoch 59: reducing lr to 0.0003052517403542287
Epoch 62: reducing lr to 0.0002707285116121681
Epoch 66: reducing lr to 0.0002249016881267748
Epoch 71: reducing lr to 0.0001697467219753975
Epoch 76: reducing lr to 0.00011920111610102006
Epoch 79: reducing lr to 9.201657740651806e-05
Epoch 82: reducing lr to 6.772109029580513e-05
Epoch 85: reducing lr to 4.6697880695039e-05
Epoch 91: reducing lr to 1.5737502309888517e-05
Epoch 95: reducing lr to 4.076023323414026e-06
Epoch 98: reducing lr to 3.0352302080966996e-07
[I 2024-06-22 12:06:37,101] Trial 196 finished with value: 1.8720263242721558 and parameters: {'hidden_size': 29, 'n_layers': 1, 'rnn_dropout': 0.011795032719957544, 'bidirectional': True, 'fc_dropout': 0.6733894609486798, 'learning_rate_model': 0.005504171884830901}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:06:54,642] Trial 197 finished with value: 9.122628211975098 and parameters: {'hidden_size': 106, 'n_layers': 3, 'rnn_dropout': 0.2829182296536041, 'bidirectional': False, 'fc_dropout': 0.5805140363958952, 'learning_rate_model': 1.269705997902816e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 19: reducing lr to 0.0013885815513881828
Epoch 26: reducing lr to 0.0015330435206939498
Epoch 29: reducing lr to 0.0015194036414842966
Epoch 32: reducing lr to 0.0014939094259865766
Epoch 35: reducing lr to 0.001456962932835669
Epoch 38: reducing lr to 0.0014091468203988225
Epoch 43: reducing lr to 0.001307419455682897
Epoch 46: reducing lr to 0.0012347311787863832
Epoch 49: reducing lr to 0.0011546780736988696
Epoch 52: reducing lr to 0.001068522574062216
Epoch 55: reducing lr to 0.0009776232647260876
Epoch 58: reducing lr to 0.0008834140231040535
Epoch 61: reducing lr to 0.0007873801684231245
Epoch 64: reducing lr to 0.0006910367093128115
Epoch 67: reducing lr to 0.0005959024000131254
Epoch 70: reducing lr to 0.0005034782007065751
Epoch 73: reducing lr to 0.00041522140737491035
Epoch 76: reducing lr to 0.00033252376129296014
Epoch 79: reducing lr to 0.0002566896973899923
Epoch 82: reducing lr to 0.00018891493973040573
Epoch 85: reducing lr to 0.0001302685364117274
Epoch 88: reducing lr to 8.167513817077081e-05
Epoch 91: reducing lr to 4.390137972370873e-05
Epoch 94: reducing lr to 1.7542783907470785e-05
Epoch 97: reducing lr to 3.0151191834118867e-06
[I 2024-06-22 12:07:19,305] Trial 198 finished with value: 1.8917454481124878 and parameters: {'hidden_size': 101, 'n_layers': 2, 'rnn_dropout': 0.023172948942318784, 'bidirectional': True, 'fc_dropout': 0.2637880633954088, 'learning_rate_model': 0.015354453027065824}. Best is trial 120 with value: 1.8680797815322876.
Epoch 10: reducing lr to 0.0015094597328982773
Epoch 13: reducing lr to 0.002150582616814661
Epoch 17: reducing lr to 0.0029255488573287623
Epoch 20: reducing lr to 0.0033411137697918524
Epoch 23: reducing lr to 0.003547745772072509
Epoch 27: reducing lr to 0.003551363654788578
Epoch 33: reducing lr to 0.0034422807909286228
Epoch 36: reducing lr to 0.0033479315069239726
Epoch 41: reducing lr to 0.0031367228111563866
Epoch 45: reducing lr to 0.002924599176427574
Epoch 51: reducing lr to 0.002548528409970877
Epoch 54: reducing lr to 0.0023408228590456387
Epoch 57: reducing lr to 0.002124307268777467
Epoch 71: reducing lr to 0.001099247241185742
Epoch 76: reducing lr to 0.0007719235841225781
Epoch 80: reducing lr to 0.0005412294549456559
Epoch 87: reducing lr to 0.00022449220409907837
Epoch 90: reducing lr to 0.0001282579980998025
Epoch 96: reducing lr to 1.5147418689846064e-05
Epoch 99: reducing lr to 5.494623051272018e-08
[I 2024-06-22 12:07:45,579] Trial 199 finished with value: 1.8935800790786743 and parameters: {'hidden_size': 76, 'n_layers': 3, 'rnn_dropout': 0.742487262800581, 'bidirectional': True, 'fc_dropout': 0.76154813405434, 'learning_rate_model': 0.03564396230455292}. Best is trial 120 with value: 1.8680797815322876.
Epoch 17: reducing lr to 0.001023025312362188
Epoch 22: reducing lr to 0.0012254916701076144
Epoch 25: reducing lr to 0.0012459897551787761
Epoch 30: reducing lr to 0.0012275573717679836
Epoch 35: reducing lr to 0.0011827124065160082
Epoch 38: reducing lr to 0.0011438969307507086
Epoch 41: reducing lr to 0.001096870020009464
Epoch 44: reducing lr to 0.0010423733170099874
Epoch 49: reducing lr to 0.0009373279529066034
Epoch 56: reducing lr to 0.0007683487005186334
Epoch 59: reducing lr to 0.0006912436597733007
Epoch 64: reducing lr to 0.0005609598370986434
Epoch 67: reducing lr to 0.0004837330763085933
Epoch 70: reducing lr to 0.0004087062896151156
Epoch 73: reducing lr to 0.0003370624597823821
Epoch 77: reducing lr to 0.0002487425652164265
[I 2024-06-22 12:08:05,113] Trial 200 finished with value: 2.2601397037506104 and parameters: {'hidden_size': 111, 'n_layers': 3, 'rnn_dropout': 0.5682836626556875, 'bidirectional': False, 'fc_dropout': 0.7923249575119574, 'learning_rate_model': 0.012464216955083157}. Best is trial 120 with value: 1.8680797815322876.
Epoch 5: reducing lr to 0.0008947796173122292
Epoch 21: reducing lr to 0.005209406073685222
Epoch 24: reducing lr to 0.005404917041926511
Epoch 27: reducing lr to 0.005385406904191557
Epoch 30: reducing lr to 0.005323363856709316
Epoch 41: reducing lr to 0.004756631628236452
Epoch 44: reducing lr to 0.004520303953677819
Epoch 47: reducing lr to 0.004255310285264705
Epoch 50: reducing lr to 0.003965829647484722
Epoch 53: reducing lr to 0.00365642755776975
Epoch 56: reducing lr to 0.0033319825172811054
Epoch 59: reducing lr to 0.002997612656846279
Epoch 62: reducing lr to 0.0026585899626191843
Epoch 65: reducing lr to 0.002320262023861628
Epoch 68: reducing lr to 0.0019879629849730374
Epoch 73: reducing lr to 0.0014616881924425494
Epoch 84: reducing lr to 0.0005236138687798097
Epoch 90: reducing lr to 0.00019449472811749677
Epoch 93: reducing lr to 8.811625602820755e-05
Epoch 96: reducing lr to 2.2970053512538085e-05
Epoch 99: reducing lr to 8.332230600066604e-08
[I 2024-06-22 12:08:13,285] Trial 201 finished with value: 1.8733925819396973 and parameters: {'hidden_size': 77, 'n_layers': 1, 'rnn_dropout': 0.1479244722227267, 'bidirectional': True, 'fc_dropout': 0.6405277265033917, 'learning_rate_model': 0.05405169938844537}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:08:45,759] Trial 202 finished with value: 9.026737213134766 and parameters: {'hidden_size': 48, 'n_layers': 7, 'rnn_dropout': 0.7659159410613351, 'bidirectional': True, 'fc_dropout': 0.04606680448653897, 'learning_rate_model': 1.0548472419055645e-05}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:08:50,490] Trial 203 finished with value: 4.491088390350342 and parameters: {'hidden_size': 73, 'n_layers': 1, 'rnn_dropout': 0.23508998837643535, 'bidirectional': False, 'fc_dropout': 0.38156073798942236, 'learning_rate_model': 0.00012970411945941392}. Best is trial 120 with value: 1.8680797815322876.
Epoch 19: reducing lr to 0.0032988636877746384
Epoch 22: reducing lr to 0.0035865153460350836
Epoch 26: reducing lr to 0.0036420630802271635
Epoch 29: reducing lr to 0.0036096587160863794
Epoch 33: reducing lr to 0.0035227972390253826
Epoch 36: reducing lr to 0.0034262410841435663
Epoch 43: reducing lr to 0.003106046283511856
Epoch 46: reducing lr to 0.002933360194645777
Epoch 51: reducing lr to 0.002608139600314586
Epoch 54: reducing lr to 0.0023955757260199833
Epoch 57: reducing lr to 0.002173995741722159
Epoch 60: reducing lr to 0.0019468950787172167
Epoch 63: reducing lr to 0.001717854415881515
Epoch 66: reducing lr to 0.0014904865361052648
Epoch 69: reducing lr to 0.0012683761685021368
Epoch 72: reducing lr to 0.0010550274417149153
Epoch 77: reducing lr to 0.0007279682507205509
Epoch 80: reducing lr to 0.0005538890478040913
Epoch 83: reducing lr to 0.00039983895440015777
Epoch 86: reducing lr to 0.0002682467983316728
Epoch 89: reducing lr to 0.00016118864587245097
Epoch 92: reducing lr to 8.035233782964432e-05
Epoch 95: reducing lr to 2.7012949235735023e-05
Epoch 98: reducing lr to 2.0115321484819036e-06
[I 2024-06-22 12:09:21,414] Trial 204 finished with value: 2.4639573097229004 and parameters: {'hidden_size': 109, 'n_layers': 5, 'rnn_dropout': 0.08899653805394801, 'bidirectional': False, 'fc_dropout': 0.6753998876555427, 'learning_rate_model': 0.03647769011909392}. Best is trial 120 with value: 1.8680797815322876.
Epoch 8: reducing lr to 0.0013193501654400475
Epoch 19: reducing lr to 0.003854087850297662
Epoch 25: reducing lr to 0.00426023975374216
Epoch 28: reducing lr to 0.0042335075918526
Epoch 31: reducing lr to 0.004173615733009972
Epoch 34: reducing lr to 0.004081508787163674
Epoch 38: reducing lr to 0.003911167935621259
Epoch 42: reducing lr to 0.003690964992776105
Epoch 45: reducing lr to 0.003496752923249509
Epoch 53: reducing lr to 0.0028829173357896114
Epoch 56: reducing lr to 0.002627108020014135
Epoch 59: reducing lr to 0.0023634734608760114
Epoch 62: reducing lr to 0.002096170365991357
Epoch 71: reducing lr to 0.0013142983951344178
Epoch 77: reducing lr to 0.0008504909132505463
Epoch 85: reducing lr to 0.0003615678049003132
Epoch 88: reducing lr to 0.00022669403707737402
Epoch 91: reducing lr to 0.00012185080093804964
Epoch 94: reducing lr to 4.869100432062373e-05
Epoch 97: reducing lr to 8.36863646961894e-06
[I 2024-06-22 12:09:40,351] Trial 205 finished with value: 2.545522451400757 and parameters: {'hidden_size': 86, 'n_layers': 4, 'rnn_dropout': 0.08126518075693295, 'bidirectional': False, 'fc_dropout': 0.22769917517498836, 'learning_rate_model': 0.042617166273324125}. Best is trial 120 with value: 1.8680797815322876.
Epoch 64: reducing lr to 1.0807628990267846e-05
Epoch 68: reducing lr to 8.832071986098383e-06
Epoch 71: reducing lr to 7.405818559432642e-06
Epoch 78: reducing lr to 4.396779841574358e-06
Epoch 84: reducing lr to 2.326298536210156e-06
Epoch 87: reducing lr to 1.5124427601669716e-06
Epoch 90: reducing lr to 8.64096289837941e-07
Epoch 93: reducing lr to 3.91480687653327e-07
Epoch 96: reducing lr to 1.0205077643838571e-07
Epoch 99: reducing lr to 3.7018224695718836e-10
[I 2024-06-22 12:13:28,845] Trial 206 finished with value: 1.8940212726593018 and parameters: {'hidden_size': 180, 'n_layers': 7, 'rnn_dropout': 0.1249196515526358, 'bidirectional': True, 'fc_dropout': 0.16850254273504195, 'learning_rate_model': 0.0002401395315598264}. Best is trial 120 with value: 1.8680797815322876.
Epoch 38: reducing lr to 0.00018110622223580446
Epoch 53: reducing lr to 0.00013349318574325876
Epoch 56: reducing lr to 0.00012164796212836855
Epoch 62: reducing lr to 9.706294958337992e-05
[I 2024-06-22 12:13:39,598] Trial 207 finished with value: 2.51222562789917 and parameters: {'hidden_size': 45, 'n_layers': 4, 'rnn_dropout': 0.06701996307636841, 'bidirectional': False, 'fc_dropout': 0.5528218732453634, 'learning_rate_model': 0.0019733834274571717}. Best is trial 120 with value: 1.8680797815322876.
Epoch 23: reducing lr to 0.008599489904786305
Epoch 32: reducing lr to 0.008406129822901202
Epoch 35: reducing lr to 0.0081982343424089
Epoch 38: reducing lr to 0.007929176231000892
Epoch 41: reducing lr to 0.00760319873001908
Epoch 48: reducing lr to 0.0066516268192613
Epoch 51: reducing lr to 0.006177456261417181
Epoch 57: reducing lr to 0.005149173612246823
Epoch 61: reducing lr to 0.004430536283263314
Epoch 64: reducing lr to 0.00388841799230078
Epoch 67: reducing lr to 0.0033531035075842312
Epoch 70: reducing lr to 0.0028330386330785535
Epoch 73: reducing lr to 0.0023364234771704336
Epoch 76: reducing lr to 0.0018710892762337714
Epoch 79: reducing lr to 0.0014443759995934904
Epoch 82: reducing lr to 0.0010630119077069248
Epoch 85: reducing lr to 0.000733012463719572
Epoch 88: reducing lr to 0.0004595806163506078
Epoch 91: reducing lr to 0.00024703016859155545
Epoch 94: reducing lr to 9.871208817356231e-05
Epoch 97: reducing lr to 1.696587681046524e-05
[I 2024-06-22 12:14:02,912] Trial 208 finished with value: 2.024057626724243 and parameters: {'hidden_size': 177, 'n_layers': 2, 'rnn_dropout': 0.5036863653751306, 'bidirectional': False, 'fc_dropout': 0.6905811853710797, 'learning_rate_model': 0.08639849462086026}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:14:09,178] Trial 209 finished with value: 9.111162185668945 and parameters: {'hidden_size': 17, 'n_layers': 2, 'rnn_dropout': 0.17907973626202028, 'bidirectional': True, 'fc_dropout': 0.39215624979538544, 'learning_rate_model': 1.5202975046828169e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 20: reducing lr to 0.0006053372847528165
Epoch 27: reducing lr to 0.0006434299997193707
Epoch 31: reducing lr to 0.0006324410160719727
Epoch 39: reducing lr to 0.000585004761151821
Epoch 42: reducing lr to 0.0005593034432601954
Epoch 47: reducing lr to 0.0005084099204319501
Epoch 66: reducing lr to 0.0002638717294535946
Epoch 81: reducing lr to 8.855171356275982e-05
Epoch 88: reducing lr to 3.435165485234414e-05
Epoch 91: reducing lr to 1.8464432109773203e-05
Epoch 94: reducing lr to 7.37829982826229e-06
Epoch 97: reducing lr to 1.2681255991350143e-06
[I 2024-06-22 12:14:26,341] Trial 210 finished with value: 2.0305755138397217 and parameters: {'hidden_size': 67, 'n_layers': 5, 'rnn_dropout': 0.2357536332487234, 'bidirectional': False, 'fc_dropout': 0.30442726176771207, 'learning_rate_model': 0.006457912195134263}. Best is trial 120 with value: 1.8680797815322876.
Epoch 9: reducing lr to 0.0006053518545891945
Epoch 13: reducing lr to 0.0009998009369991717
Epoch 16: reducing lr to 0.0012784588212606891
Epoch 19: reducing lr to 0.0014985816999217865
Epoch 22: reducing lr to 0.0016292538197243637
Epoch 25: reducing lr to 0.001656505398999719
Epoch 28: reducing lr to 0.0016461111552348948
Epoch 31: reducing lr to 0.0016228234547148016
Epoch 36: reducing lr to 0.0015564456959060908
Epoch 42: reducing lr to 0.0014351547780103216
Epoch 45: reducing lr to 0.0013596394642444414
Epoch 56: reducing lr to 0.0010214961763798094
Epoch 60: reducing lr to 0.0008844200951514797
Epoch 63: reducing lr to 0.0007803733146993051
Epoch 66: reducing lr to 0.0006770864329025747
Epoch 77: reducing lr to 0.0003306956582343085
Epoch 83: reducing lr to 0.00018163567721834038
Epoch 86: reducing lr to 0.00012185703353921571
Epoch 89: reducing lr to 7.322350293975828e-05
Epoch 92: reducing lr to 3.650182438992115e-05
Epoch 95: reducing lr to 1.2271228888786554e-05
Epoch 98: reducing lr to 9.137829118828489e-07
[I 2024-06-22 12:15:36,683] Trial 211 finished with value: 2.472982168197632 and parameters: {'hidden_size': 107, 'n_layers': 5, 'rnn_dropout': 0.28271547760130356, 'bidirectional': True, 'fc_dropout': 0.7496643078391715, 'learning_rate_model': 0.0165707965050135}. Best is trial 120 with value: 1.8680797815322876.
Epoch 22: reducing lr to 0.00015789619282890238
Epoch 25: reducing lr to 0.00016053723044014557
Epoch 30: reducing lr to 0.0001581623443137705
Epoch 41: reducing lr to 0.00014132417576730473
Epoch 44: reducing lr to 0.00013430264952177004
Epoch 50: reducing lr to 0.00011782867582960498
Epoch 53: reducing lr to 0.00010863603727208403
Epoch 56: reducing lr to 9.89964579410591e-05
Epoch 59: reducing lr to 8.906200250690969e-05
Epoch 62: reducing lr to 7.898930683217256e-05
Epoch 65: reducing lr to 6.893725302162974e-05
Epoch 68: reducing lr to 5.9064323720058195e-05
Epoch 72: reducing lr to 4.6447540385115715e-05
Epoch 75: reducing lr to 3.7588687635340954e-05
Epoch 78: reducing lr to 2.940338669083866e-05
Epoch 81: reducing lr to 2.202070041346744e-05
Epoch 84: reducing lr to 1.5557079927392206e-05
Epoch 87: reducing lr to 1.0114433955607168e-05
Epoch 90: reducing lr to 5.778628510798105e-06
Epoch 93: reducing lr to 2.6180201092225983e-06
Epoch 96: reducing lr to 6.824627454268123e-07
Epoch 99: reducing lr to 2.4755871673139487e-09
[I 2024-06-22 12:16:36,845] Trial 212 finished with value: 1.8701753616333008 and parameters: {'hidden_size': 189, 'n_layers': 2, 'rnn_dropout': 0.7465246102585805, 'bidirectional': True, 'fc_dropout': 0.2197073987390904, 'learning_rate_model': 0.001605928829877941}. Best is trial 120 with value: 1.8680797815322876.
Epoch 16: reducing lr to 0.0002931636026819371
Epoch 19: reducing lr to 0.00034364001621035335
Epoch 43: reducing lr to 0.00032355438000414847
Epoch 46: reducing lr to 0.0003055658069699977
Epoch 54: reducing lr to 0.0002495452250341282
Epoch 57: reducing lr to 0.00022646341365823607
Epoch 63: reducing lr to 0.00017894753320916206
Epoch 66: reducing lr to 0.0001552628013478309
Epoch 72: reducing lr to 0.00010990137255954686
Epoch 75: reducing lr to 8.894008874493273e-05
Epoch 78: reducing lr to 6.957252264444594e-05
Epoch 81: reducing lr to 5.21040550284593e-05
Epoch 84: reducing lr to 3.6810225533209785e-05
Epoch 87: reducing lr to 2.3932164441162283e-05
Epoch 90: reducing lr to 1.3673042739889779e-05
Epoch 98: reducing lr to 2.0953970989272263e-07
[I 2024-06-22 12:16:58,784] Trial 213 finished with value: 1.874051570892334 and parameters: {'hidden_size': 50, 'n_layers': 4, 'rnn_dropout': 0.14825411675014752, 'bidirectional': True, 'fc_dropout': 0.7711342139391983, 'learning_rate_model': 0.003799852073396136}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:17:37,971] Trial 214 finished with value: 7.345830917358398 and parameters: {'hidden_size': 105, 'n_layers': 7, 'rnn_dropout': 0.11455182536701028, 'bidirectional': False, 'fc_dropout': 0.3470718617221808, 'learning_rate_model': 1.727543140097415e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 23: reducing lr to 0.0005542939981953602
Epoch 27: reducing lr to 0.0005548592502749984
Epoch 34: reducing lr to 0.0005333469643533531
Epoch 37: reducing lr to 0.0005172911105645406
Epoch 40: reducing lr to 0.0004974685912506298
Epoch 43: reducing lr to 0.00047419203400182427
Epoch 47: reducing lr to 0.00043842523259139053
Epoch 52: reducing lr to 0.0003875457800241877
Epoch 55: reducing lr to 0.0003545772264386472
Epoch 58: reducing lr to 0.0003204081832044024
Epoch 61: reducing lr to 0.00028557736537754096
Epoch 64: reducing lr to 0.0002506342561560032
Epoch 87: reducing lr to 3.5074294881359216e-05
Epoch 96: reducing lr to 2.366607927215946e-06
Epoch 99: reducing lr to 8.584709207826863e-09
[I 2024-06-22 12:18:01,775] Trial 215 finished with value: 2.10941481590271 and parameters: {'hidden_size': 107, 'n_layers': 4, 'rnn_dropout': 0.6453974486198909, 'bidirectional': False, 'fc_dropout': 0.1921377657771018, 'learning_rate_model': 0.005568954385864473}. Best is trial 120 with value: 1.8680797815322876.
Epoch 5: reducing lr to 0.0009110017094835094
Epoch 14: reducing lr to 0.0036427635939699685
Epoch 17: reducing lr to 0.004516831003287167
Epoch 20: reducing lr to 0.005158432484591934
Epoch 23: reducing lr to 0.005477457009454858
Epoch 26: reducing lr to 0.005494555898252556
Epoch 29: reducing lr to 0.0054456694330275324
Epoch 32: reducing lr to 0.00535429603739757
Epoch 37: reducing lr to 0.005111799566142495
Epoch 42: reducing lr to 0.004766151240189389
Epoch 45: reducing lr to 0.004515364766233231
Epoch 48: reducing lr to 0.0042367629183635985
Epoch 51: reducing lr to 0.0039347393245810405
Epoch 54: reducing lr to 0.003614057320032131
Epoch 57: reducing lr to 0.003279773266505419
Epoch 60: reducing lr to 0.0029371605055718425
Epoch 63: reducing lr to 0.002591620986567936
Epoch 66: reducing lr to 0.002248605092175505
Epoch 69: reducing lr to 0.001913520882074264
Epoch 72: reducing lr to 0.0015916548189855695
Epoch 75: reducing lr to 0.0012880814639068788
Epoch 78: reducing lr to 0.0010075892444019873
Epoch 81: reducing lr to 0.0007546008602376863
Epoch 84: reducing lr to 0.0005331068347316066
Epoch 87: reducing lr to 0.0003465993551708568
Epoch 90: reducing lr to 0.0001980208605251922
Epoch 93: reducing lr to 8.971377792010293e-05
Epoch 96: reducing lr to 2.3386493849410218e-05
Epoch 99: reducing lr to 8.483291498370682e-08
[I 2024-06-22 12:19:07,942] Trial 216 finished with value: 1.9384769201278687 and parameters: {'hidden_size': 199, 'n_layers': 2, 'rnn_dropout': 0.6571573921810379, 'bidirectional': True, 'fc_dropout': 0.11105321443653891, 'learning_rate_model': 0.055031640853951194}. Best is trial 120 with value: 1.8680797815322876.
Epoch 26: reducing lr to 0.00033016590262836744
Epoch 29: reducing lr to 0.00032722833238316097
Epoch 32: reducing lr to 0.0003217377376557545
Epoch 35: reducing lr to 0.0003137807083245843
Epoch 40: reducing lr to 0.0002953956471965567
Epoch 43: reducing lr to 0.00028157408375727925
Epoch 46: reducing lr to 0.0002659194788957195
Epoch 49: reducing lr to 0.00024867873827574133
Epoch 52: reducing lr to 0.0002301237475530661
Epoch 55: reducing lr to 0.0002105471001127685
Epoch 58: reducing lr to 0.00019025760482042805
Epoch 61: reducing lr to 0.00016957514937437665
Epoch 64: reducing lr to 0.00014882601556955366
Epoch 67: reducing lr to 0.00012833729187915316
Epoch 70: reducing lr to 0.00010843223453613771
Epoch 73: reducing lr to 8.942469597634259e-05
Epoch 76: reducing lr to 7.161441036127474e-05
Epoch 79: reducing lr to 5.5282309008296195e-05
Epoch 82: reducing lr to 4.068591057861102e-05
Epoch 85: reducing lr to 2.805545199981354e-05
Epoch 88: reducing lr to 1.7590071874960552e-05
Epoch 91: reducing lr to 9.454877482243826e-06
Epoch 94: reducing lr to 3.7781243684475995e-06
Epoch 97: reducing lr to 6.49355046536881e-07
[I 2024-06-22 12:20:37,338] Trial 217 finished with value: 1.8770089149475098 and parameters: {'hidden_size': 180, 'n_layers': 3, 'rnn_dropout': 0.7582112528812855, 'bidirectional': True, 'fc_dropout': 0.3744396438931783, 'learning_rate_model': 0.003306831655210467}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:20:41,438] Trial 218 finished with value: 4.426053047180176 and parameters: {'hidden_size': 45, 'n_layers': 1, 'rnn_dropout': 0.2124096139779087, 'bidirectional': False, 'fc_dropout': 0.4411750686502212, 'learning_rate_model': 0.00019346062318522451}. Best is trial 120 with value: 1.8680797815322876.
Epoch 15: reducing lr to 0.0021687256991964866
Epoch 18: reducing lr to 0.0026125400228416366
Epoch 21: reducing lr to 0.002909936958152608
Epoch 24: reducing lr to 0.0030191479860821437
Epoch 27: reducing lr to 0.0030082497627433305
Epoch 31: reducing lr to 0.0029568726005586433
Epoch 34: reducing lr to 0.0028916177898821367
Epoch 37: reducing lr to 0.0028045686538590844
Epoch 41: reducing lr to 0.0026570203926397236
Epoch 44: reducing lr to 0.0025250094446150977
Epoch 47: reducing lr to 0.0023769858775357405
Epoch 50: reducing lr to 0.002215284064578461
Epoch 53: reducing lr to 0.0020424542711132863
Epoch 56: reducing lr to 0.001861221592982026
Epoch 59: reducing lr to 0.0016744449814434063
Epoch 62: reducing lr to 0.001485069330240619
Epoch 65: reducing lr to 0.0012960817644719663
Epoch 68: reducing lr to 0.001110461899031826
Epoch 71: reducing lr to 0.0009311381694280845
Epoch 74: reducing lr to 0.0007609378074660896
Epoch 77: reducing lr to 0.0006025454759825226
Epoch 80: reducing lr to 0.00045845864791531734
Epoch 83: reducing lr to 0.0003309500831347123
Epoch 86: reducing lr to 0.00022203014296511106
Epoch 89: reducing lr to 0.000133417205014175
Epoch 92: reducing lr to 6.650830938842306e-05
Epoch 95: reducing lr to 2.235884647280549e-05
Epoch 98: reducing lr to 1.6649621664976966e-06
[I 2024-06-22 12:21:25,596] Trial 219 finished with value: 1.9360764026641846 and parameters: {'hidden_size': 160, 'n_layers': 2, 'rnn_dropout': 0.1959045110095069, 'bidirectional': True, 'fc_dropout': 0.07035094701648266, 'learning_rate_model': 0.030192892524910118}. Best is trial 120 with value: 1.8680797815322876.
Epoch 10: reducing lr to 0.0011712840402703416
Epoch 15: reducing lr to 0.0019866738444177696
Epoch 18: reducing lr to 0.0023932325479414365
Epoch 23: reducing lr to 0.0027529174254860003
Epoch 31: reducing lr to 0.0027086604170282356
Epoch 39: reducing lr to 0.0025054972717403062
Epoch 42: reducing lr to 0.0023954219593089834
Epoch 47: reducing lr to 0.0021774517972467824
Epoch 55: reducing lr to 0.0017610182114932704
Epoch 59: reducing lr to 0.0015338851057940196
Epoch 65: reducing lr to 0.0011872832708430046
Epoch 70: reducing lr to 0.0009069283767326891
Epoch 73: reducing lr to 0.0007479491196375691
Epoch 77: reducing lr to 0.0005519653027813726
Epoch 80: reducing lr to 0.00041997372230981116
Epoch 83: reducing lr to 0.000303168756756661
Epoch 86: reducing lr to 0.00020339200935579443
Epoch 89: reducing lr to 0.00012221760995186626
Epoch 92: reducing lr to 6.092532529465532e-05
Epoch 95: reducing lr to 2.0481951910900076e-05
Epoch 98: reducing lr to 1.525198317773105e-06
[I 2024-06-22 12:21:34,129] Trial 220 finished with value: 2.5553836822509766 and parameters: {'hidden_size': 32, 'n_layers': 5, 'rnn_dropout': 0.09632070148617872, 'bidirectional': False, 'fc_dropout': 0.7256872441762966, 'learning_rate_model': 0.02765837555610633}. Best is trial 120 with value: 1.8680797815322876.
Epoch 44: reducing lr to 0.0014238811316750679
Epoch 47: reducing lr to 0.0013404089828254763
Epoch 52: reducing lr to 0.0011848538956804745
Epoch 57: reducing lr to 0.0010147213746661006
Epoch 62: reducing lr to 0.0008374472432443842
Epoch 65: reducing lr to 0.0007308750363193503
Epoch 68: reducing lr to 0.0006262019133621528
Epoch 71: reducing lr to 0.0005250792519840321
Epoch 78: reducing lr to 0.00031173567807867025
Epoch 81: reducing lr to 0.00023346419401743188
Epoch 84: reducing lr to 0.00016493667586940669
Epoch 87: reducing lr to 0.00010723356328594391
Epoch 91: reducing lr to 4.868097507161814e-05
Epoch 96: reducing lr to 7.235492596345885e-06
Epoch 99: reducing lr to 2.624625701670713e-08
[I 2024-06-22 12:21:39,316] Trial 221 finished with value: 1.9973583221435547 and parameters: {'hidden_size': 78, 'n_layers': 1, 'rnn_dropout': 0.6406874841040265, 'bidirectional': False, 'fc_dropout': 0.5357172799004265, 'learning_rate_model': 0.017026110563109636}. Best is trial 120 with value: 1.8680797815322876.
Epoch 39: reducing lr to 0.00010922493835627069
Epoch 49: reducing lr to 9.06736529343932e-05
Epoch 56: reducing lr to 7.432722259842867e-05
Epoch 62: reducing lr to 5.9305715718697784e-05
Epoch 68: reducing lr to 4.4345901136990206e-05
Epoch 71: reducing lr to 3.7184671749960184e-05
Epoch 79: reducing lr to 2.015712696302962e-05
Epoch 82: reducing lr to 1.4834964021066083e-05
Epoch 85: reducing lr to 1.0229625319748466e-05
Epoch 93: reducing lr to 1.965627533272022e-06
Epoch 96: reducing lr to 5.123977306811973e-07
Epoch 99: reducing lr to 1.8586878992952333e-09
[I 2024-06-22 12:22:30,290] Trial 222 finished with value: 2.5224218368530273 and parameters: {'hidden_size': 182, 'n_layers': 4, 'rnn_dropout': 0.4462602429719284, 'bidirectional': False, 'fc_dropout': 0.5463254941038999, 'learning_rate_model': 0.0012057424285486997}. Best is trial 120 with value: 1.8680797815322876.
Epoch 11: reducing lr to 0.00044860329094426716
Epoch 14: reducing lr to 0.0006145774586013375
Epoch 17: reducing lr to 0.0007620430058999986
Epoch 24: reducing lr to 0.0009284056645664198
Epoch 31: reducing lr to 0.0009092556192723162
Epoch 34: reducing lr to 0.0008891893833171535
Epoch 37: reducing lr to 0.0008624212648440051
Epoch 40: reducing lr to 0.0008293734087529911
Epoch 43: reducing lr to 0.000790567023849487
Epoch 46: reducing lr to 0.0007466140640820275
Epoch 49: reducing lr to 0.0006982077590023089
Epoch 52: reducing lr to 0.0006461114737283237
Epoch 55: reducing lr to 0.0005911467138423682
Epoch 58: reducing lr to 0.0005341805126400565
Epoch 61: reducing lr to 0.0004761110091200552
Epoch 64: reducing lr to 0.00041785429479234814
Epoch 67: reducing lr to 0.0003603287260530139
Epoch 70: reducing lr to 0.00030444189963334245
Epoch 73: reducing lr to 0.00025107500950834487
Epoch 76: reducing lr to 0.00020106961020195535
Epoch 79: reducing lr to 0.00015521446406228993
Epoch 82: reducing lr to 0.00011423259843212525
Epoch 85: reducing lr to 7.877044255736251e-05
Epoch 88: reducing lr to 4.938711186031393e-05
Epoch 91: reducing lr to 2.6546173043547407e-05
Epoch 94: reducing lr to 1.0607725319889718e-05
Epoch 97: reducing lr to 1.8231744900388396e-06
[I 2024-06-22 12:24:22,073] Trial 223 finished with value: 1.904542326927185 and parameters: {'hidden_size': 169, 'n_layers': 4, 'rnn_dropout': 0.6598868499081856, 'bidirectional': True, 'fc_dropout': 0.7758964562757713, 'learning_rate_model': 0.00928449104813407}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:24:37,045] Trial 224 finished with value: 4.391448020935059 and parameters: {'hidden_size': 64, 'n_layers': 5, 'rnn_dropout': 0.7642098028305495, 'bidirectional': False, 'fc_dropout': 0.4953078651359258, 'learning_rate_model': 7.906311395751618e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 13: reducing lr to 2.6272124887037e-05
Epoch 37: reducing lr to 4.0447007834347186e-05
Epoch 40: reducing lr to 3.8897084439933436e-05
Epoch 43: reducing lr to 3.7077089713228e-05
Epoch 48: reducing lr to 3.352329854369475e-05
Epoch 51: reducing lr to 3.113354313450539e-05
Epoch 55: reducing lr to 2.7724404233418027e-05
Epoch 61: reducing lr to 2.2329302976290912e-05
Epoch 65: reducing lr to 1.869186878681634e-05
Epoch 68: reducing lr to 1.6014890941636072e-05
Epoch 71: reducing lr to 1.3428714887009412e-05
Epoch 74: reducing lr to 1.0974114474852258e-05
Epoch 77: reducing lr to 8.689807451880651e-06
Epoch 80: reducing lr to 6.611811944214497e-06
Epoch 83: reducing lr to 4.772905304674396e-06
Epoch 86: reducing lr to 3.2020806192831033e-06
Epoch 89: reducing lr to 1.9241200350077756e-06
Epoch 92: reducing lr to 9.591714245187771e-07
Epoch 95: reducing lr to 3.224554468324954e-07
Epoch 98: reducing lr to 2.401179864132126e-08
[I 2024-06-22 12:26:19,252] Trial 225 finished with value: 1.8867475986480713 and parameters: {'hidden_size': 157, 'n_layers': 4, 'rnn_dropout': 0.5329504721787962, 'bidirectional': True, 'fc_dropout': 0.5000968546443917, 'learning_rate_model': 0.0004354367145964692}. Best is trial 120 with value: 1.8680797815322876.
Epoch 14: reducing lr to 0.004646315485510583
Epoch 17: reducing lr to 0.00576118139281604
Epoch 22: reducing lr to 0.006901373525717264
Epoch 25: reducing lr to 0.007016808779247462
Epoch 34: reducing lr to 0.006722430742351839
Epoch 38: reducing lr to 0.0064418716067967535
Epoch 45: reducing lr to 0.00575931121931908
Epoch 50: reducing lr to 0.0051500906346863325
Epoch 53: reducing lr to 0.004748296068042735
Epoch 62: reducing lr to 0.0034524879999926657
Epoch 70: reducing lr to 0.002301635201401412
Epoch 77: reducing lr to 0.0014007972442219047
Epoch 83: reducing lr to 0.000769392490540518
Epoch 86: reducing lr to 0.0005161755001024087
Epoch 89: reducing lr to 0.0003101682123011512
Epoch 92: reducing lr to 0.00015461846486731075
Epoch 95: reducing lr to 5.197982852997995e-05
Epoch 98: reducing lr to 3.870702722910679e-06
[I 2024-06-22 12:26:25,164] Trial 226 finished with value: 2.5266590118408203 and parameters: {'hidden_size': 32, 'n_layers': 3, 'rnn_dropout': 0.5377155277254789, 'bidirectional': False, 'fc_dropout': 0.7598771111040699, 'learning_rate_model': 0.07019241257270516}. Best is trial 120 with value: 1.8680797815322876.
Epoch 47: reducing lr to 0.00018858641902838114
Epoch 71: reducing lr to 7.387507626891406e-05
Epoch 78: reducing lr to 4.385908776015348e-05
Epoch 84: reducing lr to 2.3205467485818908e-05
[I 2024-06-22 12:26:54,327] Trial 227 finished with value: 2.467315196990967 and parameters: {'hidden_size': 151, 'n_layers': 3, 'rnn_dropout': 0.5949993570809757, 'bidirectional': False, 'fc_dropout': 0.4847170889742864, 'learning_rate_model': 0.002395457850714959}. Best is trial 120 with value: 1.8680797815322876.
Epoch 19: reducing lr to 0.0003841478058000917
Epoch 23: reducing lr to 0.0004227930421415786
Epoch 27: reducing lr to 0.0004232241935650203
Epoch 30: reducing lr to 0.00041834840252376196
Epoch 36: reducing lr to 0.0003989807155395847
Epoch 39: reducing lr to 0.0003847942563731273
Epoch 42: reducing lr to 0.0003678888905322657
Epoch 45: reducing lr to 0.0003485312258223721
Epoch 48: reducing lr to 0.0003270265526494483
Epoch 52: reducing lr to 0.0002956042458316643
Epoch 55: reducing lr to 0.0002704571666447712
Epoch 58: reducing lr to 0.0002443944025103815
Epoch 61: reducing lr to 0.0002178268634837229
Epoch 64: reducing lr to 0.00019117367312308613
Epoch 71: reducing lr to 0.00013099982778505548
Epoch 74: reducing lr to 0.00010705470466796743
Epoch 77: reducing lr to 8.477082798018739e-05
Epoch 80: reducing lr to 6.449956182159755e-05
Epoch 83: reducing lr to 4.65606558935564e-05
Epoch 86: reducing lr to 3.123694360997534e-05
Epoch 89: reducing lr to 1.8770179823210648e-05
Epoch 92: reducing lr to 9.356911103225364e-06
Epoch 95: reducing lr to 3.1456180549542348e-06
Epoch 98: reducing lr to 2.3423994874333804e-07
[I 2024-06-22 12:27:47,941] Trial 228 finished with value: 1.869337797164917 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.6129506921622574, 'bidirectional': True, 'fc_dropout': 0.3808712944016839, 'learning_rate_model': 0.004247773156507247}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:28:10,274] Trial 229 finished with value: 6.684633731842041 and parameters: {'hidden_size': 172, 'n_layers': 2, 'rnn_dropout': 0.6849273903898975, 'bidirectional': False, 'fc_dropout': 0.022169392097891996, 'learning_rate_model': 2.195637079696368e-05}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:29:22,525] Trial 230 finished with value: 2.530667543411255 and parameters: {'hidden_size': 197, 'n_layers': 5, 'rnn_dropout': 0.3341058774513466, 'bidirectional': False, 'fc_dropout': 0.029580054687486257, 'learning_rate_model': 0.00017638256496317842}. Best is trial 120 with value: 1.8680797815322876.
Epoch 32: reducing lr to 0.00019988747915198062
Epoch 37: reducing lr to 0.00019083456014939456
Epoch 40: reducing lr to 0.0001835218078575666
Epoch 45: reducing lr to 0.0001685683560023551
Epoch 49: reducing lr to 0.0001544977796972584
Epoch 56: reducing lr to 0.0001266452876981797
Epoch 59: reducing lr to 0.00011393622726561675
Epoch 62: reducing lr to 0.00010105031732343679
[I 2024-06-22 12:29:41,794] Trial 231 finished with value: 2.2780673503875732 and parameters: {'hidden_size': 75, 'n_layers': 5, 'rnn_dropout': 0.5206650053405577, 'bidirectional': False, 'fc_dropout': 0.5283661820888798, 'learning_rate_model': 0.00205445046128604}. Best is trial 120 with value: 1.8680797815322876.
Epoch 13: reducing lr to 0.00032272691692974027
Epoch 16: reducing lr to 0.0004126752221751856
Epoch 19: reducing lr to 0.0004837289443183308
Epoch 22: reducing lr to 0.0005259087511098044
Epoch 30: reducing lr to 0.0005267952284371246
Epoch 34: reducing lr to 0.0005122726864381646
Epoch 39: reducing lr to 0.00048454297175393516
Epoch 42: reducing lr to 0.00046325529381319294
Epoch 45: reducing lr to 0.0004388796171251997
Epoch 48: reducing lr to 0.0004118003713380651
Epoch 51: reducing lr to 0.000382444603628374
Epoch 54: reducing lr to 0.0003512752955742709
Epoch 58: reducing lr to 0.0003077478109693471
Epoch 61: reducing lr to 0.0002742932723452492
Epoch 64: reducing lr to 0.00024073087932568386
Epoch 70: reducing lr to 0.0001753926359396084
Epoch 73: reducing lr to 0.00014464732938950544
Epoch 77: reducing lr to 0.00010674563933129999
Epoch 80: reducing lr to 8.12195318517393e-05
Epoch 83: reducing lr to 5.8630393255141524e-05
Epoch 86: reducing lr to 3.933437475899023e-05
Epoch 89: reducing lr to 2.3635900383801608e-05
Epoch 92: reducing lr to 1.1782466700848652e-05
Epoch 95: reducing lr to 3.961044363594551e-06
Epoch 98: reducing lr to 2.9496105772826645e-07
[I 2024-06-22 12:30:56,584] Trial 232 finished with value: 2.499506950378418 and parameters: {'hidden_size': 89, 'n_layers': 7, 'rnn_dropout': 0.12326333746588594, 'bidirectional': True, 'fc_dropout': 0.43879127173288324, 'learning_rate_model': 0.0053489068365791615}. Best is trial 120 with value: 1.8680797815322876.
Epoch 5: reducing lr to 0.0006635529587927906
Epoch 9: reducing lr to 0.0014643110966505353
Epoch 24: reducing lr to 0.004008192213824691
Epoch 29: reducing lr to 0.00396650195853259
Epoch 32: reducing lr to 0.003899947651999422
Epoch 37: reducing lr to 0.003723318728778911
Epoch 41: reducing lr to 0.003527435057455381
Epoch 45: reducing lr to 0.003288889163952786
Epoch 48: reducing lr to 0.003085961904262084
Epoch 51: reducing lr to 0.0028659747766931684
Epoch 54: reducing lr to 0.0026323972864042947
Epoch 57: reducing lr to 0.0023889123725058833
Epoch 60: reducing lr to 0.0021393610172548245
Epoch 63: reducing lr to 0.001887677877884129
Epoch 66: reducing lr to 0.0016378328893757149
Epoch 69: reducing lr to 0.001393765159597818
Epoch 72: reducing lr to 0.0011593252279553448
Epoch 75: reducing lr to 0.0009382092894492315
Epoch 78: reducing lr to 0.0007339051259846546
Epoch 81: reducing lr to 0.0005496341316441466
Epoch 84: reducing lr to 0.00038830291299823266
Epoch 87: reducing lr to 0.0002524550999686771
Epoch 90: reducing lr to 0.00014423389828618557
Epoch 93: reducing lr to 6.534547867875426e-05
Epoch 96: reducing lr to 1.7034191075627342e-05
Epoch 99: reducing lr to 6.179036894718437e-08
[I 2024-06-22 12:31:10,009] Trial 233 finished with value: 1.9102389812469482 and parameters: {'hidden_size': 59, 'n_layers': 2, 'rnn_dropout': 0.4067856107436928, 'bidirectional': True, 'fc_dropout': 0.09517866973841188, 'learning_rate_model': 0.040083797577684495}. Best is trial 120 with value: 1.8680797815322876.
Epoch 16: reducing lr to 0.000514106183086801
Epoch 19: reducing lr to 0.0006026241166147205
Epoch 23: reducing lr to 0.0006632480511004348
Epoch 27: reducing lr to 0.0006639244111934921
Epoch 30: reducing lr to 0.0006562756124116885
Epoch 39: reducing lr to 0.0006036382229030522
Epoch 42: reducing lr to 0.0005771182714622792
Epoch 47: reducing lr to 0.0005246036977059383
Epoch 52: reducing lr to 0.0004637231940993294
Epoch 56: reducing lr to 0.00041077388770216895
Epoch 61: reducing lr to 0.00034171149541888785
Epoch 93: reducing lr to 1.0863159356550976e-05
Epoch 96: reducing lr to 2.8317970256853084e-06
Epoch 99: reducing lr to 1.0272151006445601e-08
[I 2024-06-22 12:31:46,803] Trial 234 finished with value: 2.370241165161133 and parameters: {'hidden_size': 104, 'n_layers': 7, 'rnn_dropout': 0.4427469649986126, 'bidirectional': False, 'fc_dropout': 0.6833903783331062, 'learning_rate_model': 0.006663608401168412}. Best is trial 120 with value: 1.8680797815322876.
Epoch 31: reducing lr to 4.991295586033983e-05
Epoch 41: reducing lr to 4.485135462136361e-05
Epoch 44: reducing lr to 4.262296756789708e-05
Epoch 47: reducing lr to 4.0124282379862214e-05
Epoch 57: reducing lr to 3.0375032915819958e-05
Epoch 60: reducing lr to 2.720198616987041e-05
Epoch 65: reducing lr to 2.1878275002196053e-05
Epoch 68: reducing lr to 1.8744952264934558e-05
Epoch 71: reducing lr to 1.571791031570346e-05
Epoch 74: reducing lr to 1.2844873732247714e-05
Epoch 77: reducing lr to 1.0171160482491214e-05
Epoch 80: reducing lr to 7.73892870895588e-06
Epoch 83: reducing lr to 5.586543325660295e-06
Epoch 86: reducing lr to 3.7479398751873087e-06
Epoch 89: reducing lr to 2.2521251215301935e-06
Epoch 92: reducing lr to 1.1226815488171488e-06
Epoch 95: reducing lr to 3.7742448452950884e-07
Epoch 98: reducing lr to 2.8105094250540156e-08
[I 2024-06-22 12:33:18,894] Trial 235 finished with value: 1.8686821460723877 and parameters: {'hidden_size': 181, 'n_layers': 3, 'rnn_dropout': 0.42240111731010094, 'bidirectional': True, 'fc_dropout': 0.18517034842739824, 'learning_rate_model': 0.000509665689217419}. Best is trial 120 with value: 1.8680797815322876.
Epoch 31: reducing lr to 0.0003109348528959019
Epoch 37: reducing lr to 0.0002949190782380605
Epoch 40: reducing lr to 0.00028361782251372624
Epoch 43: reducing lr to 0.000270347343535494
Epoch 49: reducing lr to 0.00023876358004793486
Epoch 56: reducing lr to 0.0001957198501251643
Epoch 59: reducing lr to 0.00017607904509954917
Epoch 62: reducing lr to 0.00015616493373821444
Epoch 65: reducing lr to 0.00013629163214573845
Epoch 68: reducing lr to 0.00011677246667872346
Epoch 71: reducing lr to 9.791538184032042e-05
Epoch 74: reducing lr to 8.001767989013028e-05
Epoch 79: reducing lr to 5.307812844659509e-05
Epoch 82: reducing lr to 3.906370819884018e-05
Epoch 85: reducing lr to 2.6936843116482592e-05
[I 2024-06-22 12:33:38,226] Trial 236 finished with value: 2.5608925819396973 and parameters: {'hidden_size': 61, 'n_layers': 6, 'rnn_dropout': 0.7893419559744661, 'bidirectional': False, 'fc_dropout': 0.26409244175925944, 'learning_rate_model': 0.0031749837967185394}. Best is trial 120 with value: 1.8680797815322876.
Epoch 32: reducing lr to 0.00027687900372435557
Epoch 37: reducing lr to 0.000264339132768619
Epoch 40: reducing lr to 0.000254209696059355
Epoch 43: reducing lr to 0.00024231522342812534
Epoch 49: reducing lr to 0.00021400635748513824
Epoch 53: reducing lr to 0.00019250753230902194
Epoch 56: reducing lr to 0.0001754258007205897
Epoch 63: reducing lr to 0.0001340167655617396
Epoch 92: reducing lr to 6.2686106119925075e-06
Epoch 95: reducing lr to 2.1073893406729162e-06
Epoch 98: reducing lr to 1.5692775235825498e-07
[I 2024-06-22 12:33:55,692] Trial 237 finished with value: 2.0767741203308105 and parameters: {'hidden_size': 63, 'n_layers': 5, 'rnn_dropout': 0.29691774757018363, 'bidirectional': False, 'fc_dropout': 0.753999099380966, 'learning_rate_model': 0.00284577202801892}. Best is trial 120 with value: 1.8680797815322876.
Epoch 86: reducing lr to 6.712554644066595e-07
Epoch 89: reducing lr to 4.033552684137192e-07
Epoch 92: reducing lr to 2.0107209547868806e-07
Epoch 95: reducing lr to 6.759666805717746e-08
Epoch 98: reducing lr to 5.033618126650116e-09
[I 2024-06-22 12:34:41,695] Trial 238 finished with value: 2.4800875186920166 and parameters: {'hidden_size': 55, 'n_layers': 7, 'rnn_dropout': 0.6003628373786594, 'bidirectional': True, 'fc_dropout': 0.6641252209298395, 'learning_rate_model': 9.128104780247618e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 31: reducing lr to 0.0002190826416099262
Epoch 36: reducing lr to 0.00021012158383022181
Epoch 39: reducing lr to 0.00020265034235689036
Epoch 42: reducing lr to 0.00019374719965510045
Epoch 47: reducing lr to 0.00017611727506340036
Epoch 50: reducing lr to 0.00016413635294687027
Epoch 53: reducing lr to 0.00015133092883286048
Epoch 56: reducing lr to 0.0001379029123997086
Epoch 62: reducing lr to 0.0001100327798301251
Epoch 68: reducing lr to 8.227710798263813e-05
Epoch 71: reducing lr to 6.899053067879713e-05
Epoch 74: reducing lr to 5.637992821504721e-05
Epoch 77: reducing lr to 4.464421447965704e-05
Epoch 80: reducing lr to 3.3968433957968475e-05
Epoch 83: reducing lr to 2.452098153991446e-05
Epoch 86: reducing lr to 1.645081030161254e-05
Epoch 89: reducing lr to 9.88523946050166e-06
Epoch 92: reducing lr to 4.927779474527558e-06
Epoch 95: reducing lr to 1.6566270551147435e-06
Epoch 98: reducing lr to 1.233615237761134e-07
[I 2024-06-22 12:35:20,606] Trial 239 finished with value: 2.56477689743042 and parameters: {'hidden_size': 132, 'n_layers': 5, 'rnn_dropout': 0.5939067437656814, 'bidirectional': False, 'fc_dropout': 0.722348721697368, 'learning_rate_model': 0.0022370725918161565}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:35:25,145] Trial 240 finished with value: 9.192412376403809 and parameters: {'hidden_size': 32, 'n_layers': 1, 'rnn_dropout': 0.08421002133022953, 'bidirectional': True, 'fc_dropout': 0.6226407120541021, 'learning_rate_model': 1.4376264309585954e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 10: reducing lr to 0.0026371001889076125
Epoch 14: reducing lr to 0.004122015294691939
Epoch 17: reducing lr to 0.005111077345208016
Epoch 23: reducing lr to 0.006198085872595505
Epoch 26: reducing lr to 0.006217434336839252
Epoch 31: reducing lr to 0.006098442949656405
Epoch 34: reducing lr to 0.005963857259347623
Epoch 38: reducing lr to 0.0057149570175474215
Epoch 42: reducing lr to 0.005393198817896602
Epoch 45: reducing lr to 0.005109418206093973
Epoch 53: reducing lr to 0.0042124881698701365
Epoch 56: reducing lr to 0.003838702316537085
Epoch 61: reducing lr to 0.0031933108416153837
Epoch 70: reducing lr to 0.002041913755655233
Epoch 77: reducing lr to 0.0012427282829698973
Epoch 85: reducing lr to 0.0005283190335845401
Epoch 88: reducing lr to 0.0003312429175521221
Epoch 91: reducing lr to 0.0001780470952352675
Epoch 94: reducing lr to 7.114677799928831e-05
Epoch 97: reducing lr to 1.2228162663067976e-05
[I 2024-06-22 12:35:46,309] Trial 241 finished with value: 2.533797025680542 and parameters: {'hidden_size': 60, 'n_layers': 7, 'rnn_dropout': 0.7788150886690349, 'bidirectional': False, 'fc_dropout': 0.03476123337198143, 'learning_rate_model': 0.06227175040056983}. Best is trial 120 with value: 1.8680797815322876.
Epoch 69: reducing lr to 3.7393204232028775e-06
Epoch 94: reducing lr to 1.2286723960658102e-07
Epoch 97: reducing lr to 2.1117479021839306e-08
[I 2024-06-22 12:36:35,390] Trial 242 finished with value: 2.0959103107452393 and parameters: {'hidden_size': 73, 'n_layers': 6, 'rnn_dropout': 0.6485032827575212, 'bidirectional': True, 'fc_dropout': 0.7445155137323587, 'learning_rate_model': 0.00010754047185755203}. Best is trial 120 with value: 1.8680797815322876.
Epoch 37: reducing lr to 8.662785166422026e-05
Epoch 49: reducing lr to 7.013305520545119e-05
Epoch 56: reducing lr to 5.7489634938775105e-05
Epoch 62: reducing lr to 4.5870998905356515e-05
Epoch 71: reducing lr to 2.8761107027677192e-05
Epoch 79: reducing lr to 1.5590867383542068e-05
Epoch 82: reducing lr to 1.147435133569732e-05
Epoch 85: reducing lr to 7.912275000105036e-06
Epoch 88: reducing lr to 4.960799988081672e-06
Epoch 93: reducing lr to 1.5203475303246723e-06
Epoch 96: reducing lr to 3.9632260496897654e-07
Epoch 99: reducing lr to 1.437633279706852e-09
[I 2024-06-22 12:37:41,498] Trial 243 finished with value: 2.497623920440674 and parameters: {'hidden_size': 188, 'n_layers': 5, 'rnn_dropout': 0.7883559319558645, 'bidirectional': False, 'fc_dropout': 0.45244338830430486, 'learning_rate_model': 0.0009326016716911777}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:37:52,621] Trial 244 finished with value: 8.801898002624512 and parameters: {'hidden_size': 63, 'n_layers': 3, 'rnn_dropout': 0.1995697653611102, 'bidirectional': False, 'fc_dropout': 0.4019160047186067, 'learning_rate_model': 2.15159018655157e-05}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:39:37,729] Trial 245 finished with value: 2.635453701019287 and parameters: {'hidden_size': 159, 'n_layers': 4, 'rnn_dropout': 0.5915102298771057, 'bidirectional': True, 'fc_dropout': 0.7889974259490361, 'learning_rate_model': 1.2970911036527296e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 11: reducing lr to 0.0007887459290231923
Epoch 14: reducing lr to 0.00108056601083082
Epoch 26: reducing lr to 0.0016298697939360885
Epoch 36: reducing lr to 0.0015332866913002857
Epoch 42: reducing lr to 0.0014138005115547643
Epoch 48: reducing lr to 0.0012567661577354173
Epoch 54: reducing lr to 0.0010720507659858968
Epoch 58: reducing lr to 0.0009392100174332232
Epoch 61: reducing lr to 0.0008371107118187013
Epoch 67: reducing lr to 0.0006335393019212974
Epoch 76: reducing lr to 0.00035352579817961115
Epoch 79: reducing lr to 0.00027290209217358896
Epoch 83: reducing lr to 0.00017893304422809692
Epoch 86: reducing lr to 0.00012004387191821772
Epoch 89: reducing lr to 7.213397990256162e-05
Epoch 92: reducing lr to 3.595869852218922e-05
Epoch 95: reducing lr to 1.2088640156586103e-05
Epoch 98: reducing lr to 9.001863548550176e-07
[I 2024-06-22 12:41:11,144] Trial 246 finished with value: 2.4618706703186035 and parameters: {'hidden_size': 188, 'n_layers': 7, 'rnn_dropout': 0.3127349692011665, 'bidirectional': False, 'fc_dropout': 0.14237786837613556, 'learning_rate_model': 0.016324232713169764}. Best is trial 120 with value: 1.8680797815322876.
Epoch 82: reducing lr to 3.1634591382250898e-06
Epoch 93: reducing lr to 4.1915722705132183e-07
Epoch 96: reducing lr to 1.0926546779806189e-07
Epoch 99: reducing lr to 3.96353048900391e-10
[I 2024-06-22 12:41:33,259] Trial 247 finished with value: 1.9526898860931396 and parameters: {'hidden_size': 62, 'n_layers': 3, 'rnn_dropout': 0.7327326320079657, 'bidirectional': True, 'fc_dropout': 0.6902711844628926, 'learning_rate_model': 0.00025711669394827373}. Best is trial 120 with value: 1.8680797815322876.
Epoch 27: reducing lr to 0.0008644686063129273
Epoch 30: reducing lr to 0.0008545094207318646
Epoch 33: reducing lr to 0.0008379157887307323
Epoch 36: reducing lr to 0.0008149494011741816
Epoch 39: reducing lr to 0.0007859724457670742
Epoch 42: reducing lr to 0.0007514419102498161
Epoch 45: reducing lr to 0.0007119023619733451
Epoch 48: reducing lr to 0.0006679773805340304
Epoch 51: reducing lr to 0.0006203596750070313
Epoch 54: reducing lr to 0.0005698002433110708
Epoch 57: reducing lr to 0.0005170962825911455
Epoch 60: reducing lr to 0.0004630791995030185
Epoch 63: reducing lr to 0.00040860067728625807
Epoch 66: reducing lr to 0.0003545200352884067
Epoch 69: reducing lr to 0.0003016899201192075
Epoch 72: reducing lr to 0.00025094380714392017
Epoch 75: reducing lr to 0.00020308176283493333
Epoch 78: reducing lr to 0.00015885874123677872
Epoch 81: reducing lr to 0.00011897203494336244
Epoch 84: reducing lr to 8.405079865699637e-05
Epoch 87: reducing lr to 5.464561832297082e-05
Epoch 90: reducing lr to 3.122040535508703e-05
Epoch 93: reducing lr to 1.4144471977211442e-05
Epoch 96: reducing lr to 3.6871661696467537e-06
Epoch 99: reducing lr to 1.3374944368184372e-08
[I 2024-06-22 12:42:29,832] Trial 248 finished with value: 2.562687635421753 and parameters: {'hidden_size': 200, 'n_layers': 4, 'rnn_dropout': 0.2358496094387963, 'bidirectional': False, 'fc_dropout': 0.37191010472945796, 'learning_rate_model': 0.008676409799751065}. Best is trial 120 with value: 1.8680797815322876.
Epoch 5: reducing lr to 0.0011873268377462938
Epoch 11: reducing lr to 0.0034655159988695025
Epoch 14: reducing lr to 0.004747687006139454
Epoch 20: reducing lr to 0.006723088733972532
Epoch 26: reducing lr to 0.007161165134576014
Epoch 29: reducing lr to 0.007097450421903159
Epoch 32: reducing lr to 0.006978361638909576
Epoch 43: reducing lr to 0.006107228200582722
Epoch 46: reducing lr to 0.00576768614115828
Epoch 52: reducing lr to 0.004991291179664654
Epoch 57: reducing lr to 0.004274594416790391
Epoch 76: reducing lr to 0.0015532876488146392
Epoch 79: reducing lr to 0.001199050964007857
Epoch 85: reducing lr to 0.000608511427426163
Epoch 88: reducing lr to 0.00038152155756507463
Epoch 91: reducing lr to 0.00020507247549951811
Epoch 94: reducing lr to 8.194599226035993e-05
Epoch 97: reducing lr to 1.4084248804016597e-05
[I 2024-06-22 12:43:18,745] Trial 249 finished with value: 2.4681553840637207 and parameters: {'hidden_size': 134, 'n_layers': 6, 'rnn_dropout': 0.04088872306089302, 'bidirectional': False, 'fc_dropout': 0.4415960217233577, 'learning_rate_model': 0.07172384357890647}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:43:56,584] Trial 250 finished with value: 4.726553916931152 and parameters: {'hidden_size': 67, 'n_layers': 5, 'rnn_dropout': 0.31717015204209914, 'bidirectional': True, 'fc_dropout': 0.6554867077248162, 'learning_rate_model': 1.901708352739404e-05}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:44:00,292] Trial 251 finished with value: 2.1343934535980225 and parameters: {'hidden_size': 23, 'n_layers': 1, 'rnn_dropout': 0.12036402269413077, 'bidirectional': False, 'fc_dropout': 0.08863635175676494, 'learning_rate_model': 0.0038923505213127522}. Best is trial 120 with value: 1.8680797815322876.
Epoch 22: reducing lr to 0.00040551428773097175
Epoch 25: reducing lr to 0.00041229708892843
Epoch 28: reducing lr to 0.0004097100062370984
Epoch 31: reducing lr to 0.00040391379746043557
Epoch 35: reducing lr to 0.00039135866103178553
Epoch 40: reducing lr to 0.0003684281470927019
Epoch 43: reducing lr to 0.00035118939270960557
Epoch 46: reducing lr to 0.0003316644026924876
Epoch 49: reducing lr to 0.00031016112672546626
Epoch 52: reducing lr to 0.00028701867044299875
Epoch 55: reducing lr to 0.000262601966909393
Epoch 58: reducing lr to 0.00023729617372338484
Epoch 63: reducing lr to 0.00019423157094582318
Epoch 66: reducing lr to 0.00016852390907221766
Epoch 71: reducing lr to 0.00012719504854656212
Epoch 74: reducing lr to 0.0001039453912849602
Epoch 77: reducing lr to 8.230873095470032e-05
Epoch 80: reducing lr to 6.262622658245978e-05
Epoch 83: reducing lr to 4.52083410098674e-05
Epoch 86: reducing lr to 3.032969299346139e-05
Epoch 89: reducing lr to 1.822501582031356e-05
Epoch 92: reducing lr to 9.085147531441226e-06
Epoch 95: reducing lr to 3.0542562381482253e-06
Epoch 98: reducing lr to 2.2743664748053893e-07
[I 2024-06-22 12:44:43,758] Trial 252 finished with value: 1.8774032592773438 and parameters: {'hidden_size': 113, 'n_layers': 3, 'rnn_dropout': 0.1585593834561535, 'bidirectional': True, 'fc_dropout': 0.5705311785278516, 'learning_rate_model': 0.004124400176641758}. Best is trial 120 with value: 1.8680797815322876.
Epoch 34: reducing lr to 3.1373346172981745e-05
Epoch 65: reducing lr to 1.4062170321246158e-05
Epoch 68: reducing lr to 1.2048240155436389e-05
Epoch 72: reducing lr to 9.474604735026552e-06
Epoch 79: reducing lr to 5.476445404592939e-06
Epoch 82: reducing lr to 4.0304786832705555e-06
Epoch 85: reducing lr to 2.779264360233208e-06
Epoch 88: reducing lr to 1.7425297534448309e-06
Epoch 91: reducing lr to 9.366309270991756e-07
Epoch 94: reducing lr to 3.7427329297082364e-07
Epoch 97: reducing lr to 6.432722374209612e-08
[I 2024-06-22 12:45:57,550] Trial 253 finished with value: 1.8870363235473633 and parameters: {'hidden_size': 98, 'n_layers': 6, 'rnn_dropout': 0.08798077795625635, 'bidirectional': True, 'fc_dropout': 0.5272285277327067, 'learning_rate_model': 0.0003275855033338442}. Best is trial 120 with value: 1.8680797815322876.
Epoch 4: reducing lr to 0.00040555135929065103
Epoch 8: reducing lr to 0.0009826321197491787
Epoch 15: reducing lr to 0.0022798976654641754
Epoch 19: reducing lr to 0.0028704665472751166
Epoch 27: reducing lr to 0.00316245692747255
Epoch 33: reducing lr to 0.003065319632614753
Epoch 36: reducing lr to 0.0029813024561703583
Epoch 41: reducing lr to 0.0027932230399235845
Epoch 44: reducing lr to 0.0026544450228011022
Epoch 47: reducing lr to 0.0024988335569790564
Epoch 51: reducing lr to 0.002269441293094726
Epoch 54: reducing lr to 0.002084481395362955
Epoch 57: reducing lr to 0.0018916762379902017
Epoch 68: reducing lr to 0.0011673857565885736
Epoch 73: reducing lr to 0.0008583429315985342
Epoch 76: reducing lr to 0.0006873909076577558
Epoch 79: reducing lr to 0.0005306272351462092
Epoch 82: reducing lr to 0.00039052370689680107
Epoch 88: reducing lr to 0.00016883830238770755
Epoch 91: reducing lr to 9.075264016733219e-05
Epoch 94: reducing lr to 3.626432621269479e-05
Epoch 97: reducing lr to 6.232834321742708e-06
[I 2024-06-22 12:46:44,678] Trial 254 finished with value: 1.9732253551483154 and parameters: {'hidden_size': 93, 'n_layers': 4, 'rnn_dropout': 0.7482270160054405, 'bidirectional': True, 'fc_dropout': 0.4436147986225336, 'learning_rate_model': 0.03174062317178117}. Best is trial 120 with value: 1.8680797815322876.
Epoch 39: reducing lr to 0.00014631565294575193
Epoch 47: reducing lr to 0.00012715850265155917
Epoch 62: reducing lr to 7.944481040119749e-05
Epoch 68: reducing lr to 5.940492691478799e-05
Epoch 71: reducing lr to 4.981188003899226e-05
Epoch 75: reducing lr to 3.7805448385112305e-05
Epoch 78: reducing lr to 2.957294568706524e-05
Epoch 85: reducing lr to 1.3703411790146926e-05
Epoch 88: reducing lr to 8.59169897966599e-06
Epoch 93: reducing lr to 2.6331173110003427e-06
Epoch 96: reducing lr to 6.863982682049847e-07
Epoch 99: reducing lr to 2.489863008391796e-09
[I 2024-06-22 12:47:53,968] Trial 255 finished with value: 2.527953863143921 and parameters: {'hidden_size': 191, 'n_layers': 5, 'rnn_dropout': 0.598816256071803, 'bidirectional': False, 'fc_dropout': 0.669401639883065, 'learning_rate_model': 0.001615189656981585}. Best is trial 120 with value: 1.8680797815322876.
Epoch 10: reducing lr to 0.0005130537645002655
Epoch 14: reducing lr to 0.0008019473333492906
Epoch 37: reducing lr to 0.001125352750716502
Epoch 45: reducing lr to 0.0009940487874025722
Epoch 50: reducing lr to 0.0008888981955430012
Epoch 53: reducing lr to 0.0008195490344111546
Epoch 62: reducing lr to 0.0005958944358489447
Epoch 69: reducing lr to 0.0004212579073832398
Epoch 72: reducing lr to 0.00035039971844753984
Epoch 77: reducing lr to 0.00024177557853528859
Epoch 83: reducing lr to 0.00013279603118042138
Epoch 86: reducing lr to 8.909114483040218e-05
Epoch 89: reducing lr to 5.353458488135589e-05
Epoch 92: reducing lr to 2.668692342859165e-05
Epoch 95: reducing lr to 8.97164323162402e-06
Epoch 98: reducing lr to 6.680776922072044e-07
[I 2024-06-22 12:49:03,176] Trial 256 finished with value: 2.46968412399292 and parameters: {'hidden_size': 176, 'n_layers': 6, 'rnn_dropout': 0.6467290852849135, 'bidirectional': False, 'fc_dropout': 0.7206264917655328, 'learning_rate_model': 0.012115108898561665}. Best is trial 120 with value: 1.8680797815322876.
Epoch 12: reducing lr to 0.002170290805920315
Epoch 17: reducing lr to 0.003277792050787847
Epoch 22: reducing lr to 0.003926498011383837
Epoch 26: reducing lr to 0.003987311376669782
Epoch 32: reducing lr to 0.0038855270379109254
Epoch 35: reducing lr to 0.0037894324584158957
Epoch 38: reducing lr to 0.0036650669550665474
Epoch 41: reducing lr to 0.003514391861949996
Epoch 44: reducing lr to 0.003339783416071592
Epoch 47: reducing lr to 0.0031439953743382437
Epoch 53: reducing lr to 0.002701516589292823
Epoch 56: reducing lr to 0.0024618034689463408
Epoch 61: reducing lr to 0.0020479065733871324
Epoch 70: reducing lr to 0.0013095025225858096
Epoch 77: reducing lr to 0.000796975786529048
Epoch 85: reducing lr to 0.0003388170069832587
Epoch 88: reducing lr to 0.00021242985161437216
Epoch 91: reducing lr to 0.00011418362783634853
Epoch 94: reducing lr to 4.5627238175896535e-05
Epoch 97: reducing lr to 7.842059837016857e-06
[I 2024-06-22 12:49:38,832] Trial 257 finished with value: 2.54396653175354 and parameters: {'hidden_size': 93, 'n_layers': 7, 'rnn_dropout': 0.20061852533840183, 'bidirectional': False, 'fc_dropout': 0.6469722785482318, 'learning_rate_model': 0.03993558200464396}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:49:44,184] Trial 258 finished with value: 2.772587299346924 and parameters: {'hidden_size': 18, 'n_layers': 3, 'rnn_dropout': 0.18841756528789544, 'bidirectional': False, 'fc_dropout': 0.5792678758912593, 'learning_rate_model': 0.0007033923501905388}. Best is trial 120 with value: 1.8680797815322876.
Epoch 18: reducing lr to 0.001056895772901481
Epoch 21: reducing lr to 0.0011772068728486277
Epoch 27: reducing lr to 0.0012169790434892924
Epoch 30: reducing lr to 0.001202958730832633
Epoch 35: reducing lr to 0.0011590123999120233
Epoch 41: reducing lr to 0.0010748901823289623
Epoch 44: reducing lr to 0.0010214855218360723
Epoch 47: reducing lr to 0.0009616030010064747
Epoch 50: reducing lr to 0.0008961869839920575
Epoch 53: reducing lr to 0.0008262691735287757
Epoch 59: reducing lr to 0.00067739204275184
Epoch 62: reducing lr to 0.0006007806517313154
Epoch 65: reducing lr to 0.0005243262595897649
Epoch 70: reducing lr to 0.0004005163512077711
Epoch 73: reducing lr to 0.0003303081698309224
Epoch 77: reducing lr to 0.000243758090203023
Epoch 80: reducing lr to 0.0001854681661507311
Epoch 83: reducing lr to 0.00013388493223005972
Epoch 86: reducing lr to 8.982167450253906e-05
Epoch 89: reducing lr to 5.397355783221223e-05
Epoch 92: reducing lr to 2.6905750894101744e-05
Epoch 95: reducing lr to 9.045208922142494e-06
Epoch 98: reducing lr to 6.735557964383798e-07
[I 2024-06-22 12:50:05,662] Trial 259 finished with value: 2.56439208984375 and parameters: {'hidden_size': 83, 'n_layers': 5, 'rnn_dropout': 0.4269662037074211, 'bidirectional': False, 'fc_dropout': 0.3165186178140391, 'learning_rate_model': 0.012214450382481491}. Best is trial 120 with value: 1.8680797815322876.
Epoch 32: reducing lr to 0.00014562632303418162
Epoch 38: reducing lr to 0.00013736366241512201
Epoch 41: reducing lr to 0.00013171648519326597
Epoch 54: reducing lr to 9.829525208823796e-05
Epoch 57: reducing lr to 8.920338319939751e-05
Epoch 63: reducing lr to 7.048699443139945e-05
Epoch 66: reducing lr to 6.115763664211107e-05
Epoch 71: reducing lr to 4.6159317123084347e-05
Epoch 81: reducing lr to 2.0523659481528418e-05
Epoch 84: reducing lr to 1.4499457554104307e-05
Epoch 87: reducing lr to 9.426820875612783e-06
Epoch 90: reducing lr to 5.385777999746986e-06
Epoch 93: reducing lr to 2.440038337954832e-06
Epoch 96: reducing lr to 6.360666433390636e-07
Epoch 99: reducing lr to 2.30728846425856e-09
[I 2024-06-22 12:50:22,527] Trial 260 finished with value: 2.561927556991577 and parameters: {'hidden_size': 62, 'n_layers': 5, 'rnn_dropout': 0.7683177185760145, 'bidirectional': False, 'fc_dropout': 0.08719602714371498, 'learning_rate_model': 0.0014967524119181418}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:50:56,810] Trial 261 finished with value: 3.1468868255615234 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.7310751001972866, 'bidirectional': True, 'fc_dropout': 0.616105750079846, 'learning_rate_model': 2.6425810722624015e-05}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:51:03,733] Trial 262 finished with value: 3.0080277919769287 and parameters: {'hidden_size': 118, 'n_layers': 1, 'rnn_dropout': 0.46478725814001565, 'bidirectional': False, 'fc_dropout': 0.572661562286544, 'learning_rate_model': 0.0012495178317291505}. Best is trial 120 with value: 1.8680797815322876.
Epoch 5: reducing lr to 0.0005222725607204017
Epoch 11: reducing lr to 0.0015243855839918627
Epoch 14: reducing lr to 0.0020883774975574707
Epoch 17: reducing lr to 0.00258947581532589
Epoch 20: reducing lr to 0.002957302620824447
Epoch 23: reducing lr to 0.003140197728263084
Epoch 26: reducing lr to 0.003150000432632224
Epoch 29: reducing lr to 0.0031219740753687256
Epoch 32: reducing lr to 0.0030695901810021227
Epoch 35: reducing lr to 0.0029936748740727274
Epoch 38: reducing lr to 0.0028954253639774906
Epoch 41: reducing lr to 0.0027763911166696098
Epoch 44: reducing lr to 0.0026384493739513397
Epoch 47: reducing lr to 0.002483775620661633
Epoch 50: reducing lr to 0.002314809105279365
Epoch 53: reducing lr to 0.0021342146677651554
Epoch 56: reducing lr to 0.0019448398330790065
Epoch 59: reducing lr to 0.0017496719952581285
Epoch 62: reducing lr to 0.0015517883519223757
Epoch 65: reducing lr to 0.0013543102293552347
Epoch 68: reducing lr to 0.0011603511062288157
Epoch 71: reducing lr to 0.0009729709825161564
Epoch 74: reducing lr to 0.0007951241077559033
Epoch 77: reducing lr to 0.0006296157573880436
Epoch 80: reducing lr to 0.00047905560715997067
Epoch 83: reducing lr to 0.00034581852417151296
Epoch 86: reducing lr to 0.00023200518831877938
Epoch 89: reducing lr to 0.0001394111779639884
Epoch 92: reducing lr to 6.949629738719453e-05
Epoch 95: reducing lr to 2.3363352008150262e-05
Epoch 98: reducing lr to 1.739763150278989e-06
[I 2024-06-22 12:53:07,014] Trial 263 finished with value: 2.461798667907715 and parameters: {'hidden_size': 137, 'n_layers': 6, 'rnn_dropout': 0.6460098173144257, 'bidirectional': True, 'fc_dropout': 0.326359991237894, 'learning_rate_model': 0.03154935461727455}. Best is trial 120 with value: 1.8680797815322876.
Epoch 17: reducing lr to 0.0015668637924906837
Epoch 20: reducing lr to 0.0017894318118682538
Epoch 23: reducing lr to 0.0019000996620845835
Epoch 27: reducing lr to 0.0019020373256512287
Epoch 30: reducing lr to 0.001880124739618681
Epoch 35: reducing lr to 0.0018114402686874806
Epoch 38: reducing lr to 0.0017519905533871657
Epoch 41: reducing lr to 0.0016799642185323538
Epoch 44: reducing lr to 0.0015964971628220376
Epoch 47: reducing lr to 0.0015029057485891235
Epoch 50: reducing lr to 0.0014006659386905794
Epoch 53: reducing lr to 0.0012913901989476215
Epoch 56: reducing lr to 0.0011768015358975701
Epoch 59: reducing lr to 0.0010587075893426989
Epoch 62: reducing lr to 0.0009389703382612826
Epoch 65: reducing lr to 0.0008194784634083985
Epoch 68: reducing lr to 0.000702115897034408
Epoch 71: reducing lr to 0.0005887342120076101
Epoch 74: reducing lr to 0.00048112099275288785
Epoch 77: reducing lr to 0.0003809736810802261
Epoch 80: reducing lr to 0.0002898713635424058
Epoch 83: reducing lr to 0.0002092510465206702
Epoch 86: reducing lr to 0.0001403838286865515
Epoch 89: reducing lr to 8.43561950752407e-05
Epoch 92: reducing lr to 4.205145745856502e-05
Epoch 95: reducing lr to 1.4136911461433768e-05
Epoch 98: reducing lr to 1.0527118544795266e-06
[I 2024-06-22 12:53:46,585] Trial 264 finished with value: 1.8853777647018433 and parameters: {'hidden_size': 146, 'n_layers': 2, 'rnn_dropout': 0.4525857173034107, 'bidirectional': True, 'fc_dropout': 0.6764141404245652, 'learning_rate_model': 0.019090173051118056}. Best is trial 120 with value: 1.8680797815322876.
Epoch 35: reducing lr to 0.0002840355445679299
Epoch 41: reducing lr to 0.00026341997575841556
Epoch 47: reducing lr to 0.0002356570404852977
Epoch 50: reducing lr to 0.00021962574175409745
Epoch 56: reducing lr to 0.00018452359201401295
Epoch 59: reducing lr to 0.0001660063496849611
Epoch 62: reducing lr to 0.00014723143565447007
Epoch 68: reducing lr to 0.00011009243562221742
Epoch 71: reducing lr to 9.231408035028205e-05
Epoch 74: reducing lr to 7.544022595823503e-05
Epoch 77: reducing lr to 5.973703292467764e-05
Epoch 80: reducing lr to 4.545210351212542e-05
Epoch 86: reducing lr to 2.201231689433914e-05
Epoch 91: reducing lr to 8.558592683506182e-06
Epoch 94: reducing lr to 3.4199731977381866e-06
Epoch 97: reducing lr to 5.877987695478409e-07
[I 2024-06-22 12:53:57,286] Trial 265 finished with value: 1.8768019676208496 and parameters: {'hidden_size': 108, 'n_layers': 1, 'rnn_dropout': 0.11800200431754836, 'bidirectional': True, 'fc_dropout': 0.2979990131242876, 'learning_rate_model': 0.0029933571601558667}. Best is trial 120 with value: 1.8680797815322876.
Epoch 81: reducing lr to 1.0054044210768997e-05
Epoch 84: reducing lr to 7.10293344188134e-06
Epoch 87: reducing lr to 4.617971465357453e-06
Epoch 90: reducing lr to 2.6383623333634975e-06
[I 2024-06-22 12:54:15,964] Trial 266 finished with value: 2.579394578933716 and parameters: {'hidden_size': 52, 'n_layers': 7, 'rnn_dropout': 0.12478018643473794, 'bidirectional': False, 'fc_dropout': 0.18479195893576314, 'learning_rate_model': 0.0007332227927258254}. Best is trial 120 with value: 1.8680797815322876.
Epoch 31: reducing lr to 0.0001241007074502171
Epoch 37: reducing lr to 0.00011770847143392595
Epoch 40: reducing lr to 0.00011319790011198013
Epoch 44: reducing lr to 0.00010597529915087007
Epoch 49: reducing lr to 9.529561874884237e-05
Epoch 53: reducing lr to 8.572233376980397e-05
Epoch 56: reducing lr to 7.811595141672674e-05
Epoch 59: reducing lr to 7.027688874533606e-05
Epoch 63: reducing lr to 5.9676781320907376e-05
Epoch 66: reducing lr to 5.1778217207811136e-05
Epoch 69: reducing lr to 4.40622945347268e-05
Epoch 72: reducing lr to 3.6650743709541396e-05
Epoch 75: reducing lr to 2.9660415717994907e-05
Epoch 80: reducing lr to 1.9241627972815486e-05
Epoch 83: reducing lr to 1.3890060545715205e-05
Epoch 86: reducing lr to 9.318662499032695e-06
Epoch 89: reducing lr to 5.5995545851930156e-06
Epoch 92: reducing lr to 2.791370938626768e-06
Epoch 95: reducing lr to 9.384065666277623e-07
Epoch 98: reducing lr to 6.98788925505801e-08
[I 2024-06-22 12:54:50,287] Trial 267 finished with value: 2.563610792160034 and parameters: {'hidden_size': 106, 'n_layers': 6, 'rnn_dropout': 0.5074246022190988, 'bidirectional': False, 'fc_dropout': 0.3627019412177587, 'learning_rate_model': 0.0012672035046764618}. Best is trial 120 with value: 1.8680797815322876.
Epoch 84: reducing lr to 7.1001526337919544e-06
[I 2024-06-22 12:55:06,529] Trial 268 finished with value: 2.586808681488037 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.6304396409378458, 'bidirectional': False, 'fc_dropout': 0.1779065548249406, 'learning_rate_model': 0.0007329357350066418}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:56:09,862] Trial 269 finished with value: 3.0835418701171875 and parameters: {'hidden_size': 181, 'n_layers': 5, 'rnn_dropout': 0.580767076679052, 'bidirectional': False, 'fc_dropout': 0.7568440238780506, 'learning_rate_model': 6.320395612953416e-05}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 12:59:30,888] Trial 270 finished with value: 2.163940906524658 and parameters: {'hidden_size': 165, 'n_layers': 7, 'rnn_dropout': 0.011654985410665387, 'bidirectional': True, 'fc_dropout': 0.6139388141931627, 'learning_rate_model': 2.1128387034312285e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 41: reducing lr to 0.0002116261295891509
Epoch 86: reducing lr to 1.768423755346142e-05
Epoch 91: reducing lr to 6.875795349710031e-06
Epoch 94: reducing lr to 2.747535334221259e-06
Epoch 97: reducing lr to 4.7222530568736057e-07
[I 2024-06-22 13:00:12,706] Trial 271 finished with value: 2.0471465587615967 and parameters: {'hidden_size': 200, 'n_layers': 3, 'rnn_dropout': 0.7241622127464323, 'bidirectional': False, 'fc_dropout': 0.6758494552049629, 'learning_rate_model': 0.0024048008829167943}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:01:05,061] Trial 272 finished with value: 2.204921245574951 and parameters: {'hidden_size': 101, 'n_layers': 4, 'rnn_dropout': 0.45910842151387793, 'bidirectional': True, 'fc_dropout': 0.0911726091673491, 'learning_rate_model': 4.026351717100039e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 18: reducing lr to 0.00022140610158927833
Epoch 21: reducing lr to 0.00024660973311113404
Epoch 24: reducing lr to 0.0002558650890991873
Epoch 53: reducing lr to 0.0001730927885842288
Epoch 56: reducing lr to 0.0001577337814896764
Epoch 59: reducing lr to 0.0001419049401830307
Epoch 69: reducing lr to 8.897174280061518e-05
Epoch 72: reducing lr to 7.4006144646112e-05
Epoch 75: reducing lr to 5.989109070434224e-05
Epoch 78: reducing lr to 4.684922539461502e-05
Epoch 81: reducing lr to 3.508618812741275e-05
Epoch 84: reducing lr to 2.4787523684389935e-05
Epoch 87: reducing lr to 1.6115606039110745e-05
Epoch 90: reducing lr to 9.207247873200967e-06
Epoch 93: reducing lr to 4.1713635056474504e-06
Epoch 96: reducing lr to 1.0873866782798535e-06
Epoch 99: reducing lr to 3.944421178613336e-09
[I 2024-06-22 13:01:17,902] Trial 273 finished with value: 1.913801670074463 and parameters: {'hidden_size': 25, 'n_layers': 4, 'rnn_dropout': 0.6781768112288086, 'bidirectional': True, 'fc_dropout': 0.340579529588093, 'learning_rate_model': 0.0025587706106692735}. Best is trial 120 with value: 1.8680797815322876.
Epoch 11: reducing lr to 0.0002599540123098652
Epoch 21: reducing lr to 0.0005185265888452592
Epoch 30: reducing lr to 0.0005298695595540199
Epoch 41: reducing lr to 0.00047345895821826716
Epoch 47: reducing lr to 0.0004235591342825661
Epoch 52: reducing lr to 0.00037440489935234206
Epoch 59: reducing lr to 0.00029837218363250144
Epoch 62: reducing lr to 0.00026462701600836453
Epoch 65: reducing lr to 0.00023095100198419768
Epoch 70: reducing lr to 0.00017641621210210163
Epoch 73: reducing lr to 0.00014549147861811035
Epoch 77: reducing lr to 0.0001073685975945377
Epoch 80: reducing lr to 8.169352197274372e-05
Epoch 83: reducing lr to 5.8972555128769014e-05
Epoch 86: reducing lr to 3.956392674761382e-05
Epoch 89: reducing lr to 2.377383744189025e-05
Epoch 92: reducing lr to 1.1851228151326613e-05
Epoch 95: reducing lr to 3.984160673851499e-06
Epoch 98: reducing lr to 2.9668242479668344e-07
[I 2024-06-22 13:02:31,317] Trial 274 finished with value: 2.4606258869171143 and parameters: {'hidden_size': 94, 'n_layers': 6, 'rnn_dropout': 0.7535089969493642, 'bidirectional': True, 'fc_dropout': 0.37623913982574025, 'learning_rate_model': 0.005380122591470972}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:02:49,778] Trial 275 finished with value: 4.730783462524414 and parameters: {'hidden_size': 18, 'n_layers': 7, 'rnn_dropout': 0.13766186546311676, 'bidirectional': True, 'fc_dropout': 0.6401117475095902, 'learning_rate_model': 0.00011905406569348063}. Best is trial 120 with value: 1.8680797815322876.
Epoch 32: reducing lr to 0.00029218544638624517
Epoch 38: reducing lr to 0.0002756071991915309
Epoch 41: reducing lr to 0.00026427667210678844
Epoch 45: reducing lr to 0.0002464047300716363
Epoch 48: reducing lr to 0.0002312013485784854
Epoch 53: reducing lr to 0.00020314974593170376
Epoch 56: reducing lr to 0.00018512370097314532
Epoch 59: reducing lr to 0.00016654623673480374
Epoch 63: reducing lr to 0.00014142548890374082
Epoch 80: reducing lr to 4.559992317155937e-05
Epoch 83: reducing lr to 3.291746907422631e-05
Epoch 97: reducing lr to 5.897104129530019e-07
[I 2024-06-22 13:03:00,647] Trial 276 finished with value: 2.5412206649780273 and parameters: {'hidden_size': 56, 'n_layers': 4, 'rnn_dropout': 0.679496880584524, 'bidirectional': False, 'fc_dropout': 0.28214346180139044, 'learning_rate_model': 0.003003092177939155}. Best is trial 120 with value: 1.8680797815322876.
Epoch 16: reducing lr to 1.0624981792497814e-05
Epoch 43: reducing lr to 1.1726419531543058e-05
Epoch 47: reducing lr to 1.084193289202579e-05
Epoch 57: reducing lr to 8.207600209485049e-06
Epoch 65: reducing lr to 5.911701725191317e-06
Epoch 68: reducing lr to 5.065050449915241e-06
Epoch 71: reducing lr to 4.2471171753906394e-06
Epoch 74: reducing lr to 3.470797500953405e-06
Epoch 80: reducing lr to 2.091126388861775e-06
Epoch 86: reducing lr to 1.0127262146505667e-06
Epoch 89: reducing lr to 6.085439535320026e-07
Epoch 97: reducing lr to 2.7043006227734398e-08
[I 2024-06-22 13:06:09,705] Trial 277 finished with value: 1.875829815864563 and parameters: {'hidden_size': 197, 'n_layers': 5, 'rnn_dropout': 0.036398572832483024, 'bidirectional': True, 'fc_dropout': 0.4257452388173397, 'learning_rate_model': 0.000137716137762918}. Best is trial 120 with value: 1.8680797815322876.
Epoch 7: reducing lr to 0.0005215484326991784
Epoch 11: reducing lr to 0.0009798944706336684
Epoch 15: reducing lr to 0.0014567158926886514
Epoch 21: reducing lr to 0.0019545816306937793
Epoch 27: reducing lr to 0.002020617563663615
Epoch 31: reducing lr to 0.0019861079303322053
Epoch 41: reducing lr to 0.0017846995747734056
Epoch 44: reducing lr to 0.0016960288654865575
Epoch 49: reducing lr to 0.001525111242407016
Epoch 56: reducing lr to 0.0012501678175883342
Epoch 64: reducing lr to 0.0009127287321849399
Epoch 71: reducing lr to 0.0006254381410233008
Epoch 77: reducing lr to 0.0004047250287376548
Epoch 83: reducing lr to 0.00022229655228763295
Epoch 86: reducing lr to 0.00014913589027558578
Epoch 91: reducing lr to 5.798541541481503e-05
Epoch 94: reducing lr to 2.3170698023818048e-05
Epoch 97: reducing lr to 3.982401907995023e-06
[I 2024-06-22 13:06:33,654] Trial 278 finished with value: 2.4738945960998535 and parameters: {'hidden_size': 38, 'n_layers': 5, 'rnn_dropout': 0.545087803913045, 'bidirectional': True, 'fc_dropout': 0.6388962466224248, 'learning_rate_model': 0.020280327015801246}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:06:50,234] Trial 279 finished with value: 2.4947543144226074 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.20913657152467036, 'bidirectional': False, 'fc_dropout': 0.3819503644737736, 'learning_rate_model': 0.0007410364460054091}. Best is trial 120 with value: 1.8680797815322876.
Epoch 30: reducing lr to 0.00026275693777238194
Epoch 36: reducing lr to 0.0002505924497690905
Epoch 39: reducing lr to 0.00024168219567005748
Epoch 42: reducing lr to 0.00023106424629229134
Epoch 47: reducing lr to 0.00021003867665710267
Epoch 50: reducing lr to 0.0001957501463264926
Epoch 53: reducing lr to 0.00018047830922833554
Epoch 56: reducing lr to 0.0001644639642372849
Epoch 61: reducing lr to 0.0001368130468964675
Epoch 64: reducing lr to 0.00012007266821024199
Epoch 67: reducing lr to 0.0001035423881223561
Epoch 70: reducing lr to 8.748300941153694e-05
Epoch 73: reducing lr to 7.214774788317172e-05
Epoch 76: reducing lr to 5.777842873420716e-05
Epoch 79: reducing lr to 4.4601707047293816e-05
Epoch 82: reducing lr to 3.282534859944571e-05
Epoch 85: reducing lr to 2.2635108294541624e-05
Epoch 88: reducing lr to 1.4191650941897115e-05
Epoch 91: reducing lr to 7.628184914776435e-06
Epoch 94: reducing lr to 3.0481866494479204e-06
Epoch 97: reducing lr to 5.238989484135544e-07
[I 2024-06-22 13:10:20,140] Trial 280 finished with value: 2.460679054260254 and parameters: {'hidden_size': 171, 'n_layers': 7, 'rnn_dropout': 0.6534031715014135, 'bidirectional': True, 'fc_dropout': 0.7712289066712654, 'learning_rate_model': 0.002667948198731734}. Best is trial 120 with value: 1.8680797815322876.
Epoch 39: reducing lr to 0.0003783314228423194
Epoch 48: reducing lr to 0.0003215339598289413
Epoch 52: reducing lr to 0.00029063940812899976
Epoch 58: reducing lr to 0.00024028966260554712
Epoch 77: reducing lr to 8.334705478079368e-05
Epoch 80: reducing lr to 6.341625581076453e-05
Epoch 84: reducing lr to 4.045823542312638e-05
Epoch 87: reducing lr to 2.630391770547481e-05
Epoch 92: reducing lr to 9.19975657760249e-06
Epoch 95: reducing lr to 3.0927856503536537e-06
Epoch 98: reducing lr to 2.3030575853674045e-07
[I 2024-06-22 13:10:43,300] Trial 281 finished with value: 2.475581169128418 and parameters: {'hidden_size': 127, 'n_layers': 3, 'rnn_dropout': 0.5449633472936152, 'bidirectional': False, 'fc_dropout': 0.10419373405821207, 'learning_rate_model': 0.00417642944403634}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:10:50,781] Trial 282 finished with value: 7.837672233581543 and parameters: {'hidden_size': 48, 'n_layers': 3, 'rnn_dropout': 0.03544932466648367, 'bidirectional': False, 'fc_dropout': 0.4073115220130942, 'learning_rate_model': 4.020666708053822e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 26: reducing lr to 9.146552531976883e-06
Epoch 32: reducing lr to 8.913067932093952e-06
Epoch 58: reducing lr to 5.270687140146336e-06
Epoch 75: reducing lr to 2.144214199156086e-06
Epoch 82: reducing lr to 1.1271176564754267e-06
Epoch 98: reducing lr to 5.051692971967952e-09
[I 2024-06-22 13:12:31,348] Trial 283 finished with value: 2.0582964420318604 and parameters: {'hidden_size': 112, 'n_layers': 7, 'rnn_dropout': 0.6925488112050673, 'bidirectional': True, 'fc_dropout': 0.6093838901541935, 'learning_rate_model': 9.160882213458435e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 45: reducing lr to 0.0012590293263739186
Epoch 55: reducing lr to 0.000976995686771855
Epoch 62: reducing lr to 0.0007547401894802832
Epoch 66: reducing lr to 0.0006269836217117828
Epoch 72: reducing lr to 0.0004438046975856865
Epoch 79: reducing lr to 0.00025652491735562424
Epoch 82: reducing lr to 0.00018879366719559836
Epoch 85: reducing lr to 0.000130184911497579
Epoch 89: reducing lr to 6.780513511514853e-05
Epoch 92: reducing lr to 3.380077482422889e-05
Epoch 95: reducing lr to 1.1363186674060408e-05
Epoch 98: reducing lr to 8.461651152787478e-07
[I 2024-06-22 13:12:36,092] Trial 284 finished with value: 2.0032315254211426 and parameters: {'hidden_size': 63, 'n_layers': 1, 'rnn_dropout': 0.6913152758112215, 'bidirectional': False, 'fc_dropout': 0.5893687661548159, 'learning_rate_model': 0.015344596350606936}. Best is trial 120 with value: 1.8680797815322876.
Epoch 19: reducing lr to 0.002870066731734917
Epoch 26: reducing lr to 0.003168655958770878
Epoch 36: reducing lr to 0.0029808872027499195
Epoch 61: reducing lr to 0.0016274403360464213
Epoch 72: reducing lr to 0.0009178915675585306
Epoch 75: reducing lr to 0.0007428238208094108
Epoch 78: reducing lr to 0.0005810667363095139
Epoch 81: reducing lr to 0.00043517084120414765
Epoch 84: reducing lr to 0.0003074374307614227
Epoch 87: reducing lr to 0.00019988041479704655
Epoch 90: reducing lr to 0.00011419666871778297
Epoch 93: reducing lr to 5.1737047043381305e-05
Epoch 96: reducing lr to 1.3486759341962279e-05
Epoch 99: reducing lr to 4.8922301736781246e-08
[I 2024-06-22 13:12:46,893] Trial 285 finished with value: 2.00874400138855 and parameters: {'hidden_size': 98, 'n_layers': 2, 'rnn_dropout': 0.6629862812226415, 'bidirectional': False, 'fc_dropout': 0.4615822771820013, 'learning_rate_model': 0.03173620215025359}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:13:01,151] Trial 286 finished with value: 2.3142871856689453 and parameters: {'hidden_size': 128, 'n_layers': 2, 'rnn_dropout': 0.23690393879672353, 'bidirectional': False, 'fc_dropout': 0.14759648359214214, 'learning_rate_model': 0.0010954524035037083}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:13:59,695] Trial 287 finished with value: 2.2304601669311523 and parameters: {'hidden_size': 79, 'n_layers': 6, 'rnn_dropout': 0.6422931891459392, 'bidirectional': True, 'fc_dropout': 0.05367156826308426, 'learning_rate_model': 6.231506576301107e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 15: reducing lr to 0.0006670476330732967
Epoch 23: reducing lr to 0.0009243223581346533
Epoch 31: reducing lr to 0.0009094625799070357
Epoch 47: reducing lr to 0.0007311034327883513
Epoch 50: reducing lr to 0.0006813678615094305
Epoch 59: reducing lr to 0.0005150188251087303
Epoch 62: reducing lr to 0.00045677144972910717
Epoch 66: reducing lr to 0.0003794527200716647
Epoch 71: reducing lr to 0.0002863956065127686
Epoch 74: reducing lr to 0.00023404608686764674
Epoch 79: reducing lr to 0.0001552497932737009
Epoch 82: reducing lr to 0.00011425859953739861
Epoch 85: reducing lr to 7.878837192776527e-05
Epoch 88: reducing lr to 4.9398353130426265e-05
Epoch 91: reducing lr to 2.6552215362897308e-05
Epoch 94: reducing lr to 1.0610139802152453e-05
Epoch 97: reducing lr to 1.8235894727363504e-06
[I 2024-06-22 13:14:16,199] Trial 288 finished with value: 2.0370731353759766 and parameters: {'hidden_size': 66, 'n_layers': 5, 'rnn_dropout': 0.6984667978195485, 'bidirectional': False, 'fc_dropout': 0.5465366750833173, 'learning_rate_model': 0.009286604341821391}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:16:02,197] Trial 289 finished with value: 2.0246357917785645 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.4252667727167214, 'bidirectional': True, 'fc_dropout': 0.23928987572866847, 'learning_rate_model': 2.639603471883599e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 10: reducing lr to 0.00013762054211240735
Epoch 15: reducing lr to 0.00023342513179485424
Epoch 23: reducing lr to 0.0003234552640183099
Epoch 31: reducing lr to 0.00031825526701773164
Epoch 39: reducing lr to 0.00029438452240711325
Epoch 42: reducing lr to 0.00028145117434706895
Epoch 47: reducing lr to 0.00025584067267882645
Epoch 71: reducing lr to 0.00010022062725521192
Epoch 78: reducing lr to 5.95002470138036e-05
Epoch 81: reducing lr to 4.4560755119631056e-05
Epoch 84: reducing lr to 3.14810708108577e-05
Epoch 87: reducing lr to 2.0467415032532226e-05
Epoch 92: reducing lr to 7.158448341437274e-06
Epoch 95: reducing lr to 2.4065360993458846e-06
Epoch 98: reducing lr to 1.792038584188388e-07
[I 2024-06-22 13:16:48,321] Trial 290 finished with value: 2.463128089904785 and parameters: {'hidden_size': 68, 'n_layers': 6, 'rnn_dropout': 0.5126261635614769, 'bidirectional': True, 'fc_dropout': 0.18568001747706805, 'learning_rate_model': 0.0032497332048521473}. Best is trial 120 with value: 1.8680797815322876.
Epoch 26: reducing lr to 0.006210888762150452
Epoch 42: reducing lr to 0.005387520979778566
Epoch 45: reducing lr to 0.005104039125805799
Epoch 48: reducing lr to 0.004789115569090658
Epoch 55: reducing lr to 0.003960689482419634
Epoch 61: reducing lr to 0.003189948995959129
Epoch 64: reducing lr to 0.002799628369429178
Epoch 68: reducing lr to 0.002287876398735347
Epoch 71: reducing lr to 0.0019184170511869984
Epoch 76: reducing lr to 0.0013471685991194673
Epoch 79: reducing lr to 0.0010399386158050083
Epoch 82: reducing lr to 0.0007653596654860692
Epoch 85: reducing lr to 0.0005277628312176964
Epoch 88: reducing lr to 0.00033089419247687226
Epoch 91: reducing lr to 0.00017785965126773244
Epoch 94: reducing lr to 7.107187627551775e-05
Epoch 97: reducing lr to 1.2215289129118012e-05
[I 2024-06-22 13:16:52,873] Trial 291 finished with value: 1.997513771057129 and parameters: {'hidden_size': 72, 'n_layers': 1, 'rnn_dropout': 0.7226649414535005, 'bidirectional': False, 'fc_dropout': 0.5930013882654873, 'learning_rate_model': 0.06220619210575453}. Best is trial 120 with value: 1.8680797815322876.
Epoch 31: reducing lr to 0.0025516291308169225
Epoch 36: reducing lr to 0.002447260770523281
Epoch 39: reducing lr to 0.002360244121250525
Epoch 42: reducing lr to 0.0022565502908914975
Epoch 47: reducing lr to 0.0020512166833007014
Epoch 50: reducing lr to 0.001911676326922226
Epoch 61: reducing lr to 0.0013361025157541885
Epoch 64: reducing lr to 0.0011726176538588919
Epoch 70: reducing lr to 0.0008543503095063357
Epoch 73: reducing lr to 0.0007045876810685487
Epoch 77: reducing lr to 0.0005199657871186189
Epoch 80: reducing lr to 0.00039562625764621995
Epoch 83: reducing lr to 0.00028559291760262804
Epoch 86: reducing lr to 0.00019160060551888028
Epoch 89: reducing lr to 0.000115132192980524
Epoch 92: reducing lr to 5.7393253819870704e-05
Epoch 95: reducing lr to 1.9294535713406878e-05
Epoch 98: reducing lr to 1.4367768042963803e-06
[I 2024-06-22 13:16:58,178] Trial 292 finished with value: 2.5528311729431152 and parameters: {'hidden_size': 29, 'n_layers': 2, 'rnn_dropout': 0.25788599130616857, 'bidirectional': False, 'fc_dropout': 0.6595676886381469, 'learning_rate_model': 0.026054914944806804}. Best is trial 120 with value: 1.8680797815322876.
Epoch 16: reducing lr to 0.0007664194339830828
Epoch 19: reducing lr to 0.0008983802365248523
Epoch 22: reducing lr to 0.0009767164726483617
Epoch 25: reducing lr to 0.0009930534399530788
Epoch 28: reducing lr to 0.0009868222260176443
Epoch 31: reducing lr to 0.0009728615524671443
Epoch 36: reducing lr to 0.0009330689494601336
Epoch 43: reducing lr to 0.0008458702325832949
Epoch 46: reducing lr to 0.0007988425939648852
Epoch 49: reducing lr to 0.0007470500813744855
Epoch 52: reducing lr to 0.0006913094602607198
Epoch 55: reducing lr to 0.0006324997036859634
Epoch 58: reducing lr to 0.0005715484972648361
Epoch 61: reducing lr to 0.0005094168082787654
Epoch 64: reducing lr to 0.0004470848123678179
Epoch 67: reducing lr to 0.00038553510849566276
Epoch 71: reducing lr to 0.0003063603500874792
Epoch 74: reducing lr to 0.0002503615260808132
Epoch 77: reducing lr to 0.0001982477456369482
Epoch 80: reducing lr to 0.00015084071997847233
Epoch 83: reducing lr to 0.00010888822589337697
Epoch 86: reducing lr to 7.305170656954577e-05
Epoch 89: reducing lr to 4.389653756857667e-05
Epoch 92: reducing lr to 2.1882368929713668e-05
Epoch 95: reducing lr to 7.3564421026453206e-06
Epoch 98: reducing lr to 5.47800969778571e-07
[I 2024-06-22 13:17:31,175] Trial 293 finished with value: 1.8854764699935913 and parameters: {'hidden_size': 126, 'n_layers': 2, 'rnn_dropout': 0.14919415061814822, 'bidirectional': True, 'fc_dropout': 0.4315927127412398, 'learning_rate_model': 0.009933976962588156}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:17:38,002] Trial 294 finished with value: 5.402259826660156 and parameters: {'hidden_size': 58, 'n_layers': 2, 'rnn_dropout': 0.031788549260681, 'bidirectional': False, 'fc_dropout': 0.08370872939596685, 'learning_rate_model': 7.844769404538512e-05}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:17:55,652] Trial 295 finished with value: 2.1505813598632812 and parameters: {'hidden_size': 22, 'n_layers': 5, 'rnn_dropout': 0.18536251317177568, 'bidirectional': True, 'fc_dropout': 0.3017738300892052, 'learning_rate_model': 0.0004985972340693427}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:18:35,965] Trial 296 finished with value: 1.889866590499878 and parameters: {'hidden_size': 103, 'n_layers': 3, 'rnn_dropout': 0.08220525109933147, 'bidirectional': True, 'fc_dropout': 0.3394338994606496, 'learning_rate_model': 0.00012241212858594047}. Best is trial 120 with value: 1.8680797815322876.
Epoch 23: reducing lr to 0.0006545688114820083
Epoch 27: reducing lr to 0.0006552363207517481
Epoch 31: reducing lr to 0.0006440456998340685
Epoch 36: reducing lr to 0.0006177025323125674
Epoch 39: reducing lr to 0.0005957390353054042
Epoch 42: reducing lr to 0.0005695661229744208
Epoch 72: reducing lr to 0.00019020644093684734
Epoch 75: reducing lr to 0.0001539287212051427
Epoch 81: reducing lr to 9.017655225344122e-05
Epoch 84: reducing lr to 6.370750269711978e-05
Epoch 87: reducing lr to 4.141942649353589e-05
Epoch 90: reducing lr to 2.3663951921280327e-05
Epoch 93: reducing lr to 1.0721004452496353e-05
Epoch 96: reducing lr to 2.794740233892472e-06
Epoch 99: reducing lr to 1.0137730015961215e-08
[I 2024-06-22 13:18:55,261] Trial 297 finished with value: 2.0257296562194824 and parameters: {'hidden_size': 71, 'n_layers': 5, 'rnn_dropout': 0.41422937905073165, 'bidirectional': False, 'fc_dropout': 0.27163269503704285, 'learning_rate_model': 0.006576408666557594}. Best is trial 120 with value: 1.8680797815322876.
Epoch 19: reducing lr to 6.976109546610743e-05
Epoch 32: reducing lr to 7.505267370128928e-05
Epoch 38: reducing lr to 7.079427619164939e-05
Epoch 51: reducing lr to 5.515434794131172e-05
Epoch 77: reducing lr to 1.5394350128201136e-05
Epoch 81: reducing lr to 1.0577452524582177e-05
Epoch 84: reducing lr to 7.472708463554729e-06
Epoch 87: reducing lr to 4.858380658638215e-06
Epoch 90: reducing lr to 2.775714104569639e-06
Epoch 93: reducing lr to 1.2575432612837069e-06
Epoch 96: reducing lr to 3.278150628275808e-07
Epoch 99: reducing lr to 1.1891268325266504e-09
[I 2024-06-22 13:19:31,760] Trial 298 finished with value: 1.9015599489212036 and parameters: {'hidden_size': 57, 'n_layers': 6, 'rnn_dropout': 0.4209837319133043, 'bidirectional': True, 'fc_dropout': 0.14892453578933118, 'learning_rate_model': 0.0007713939900614159}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:19:53,683] Trial 299 finished with value: 1.9377214908599854 and parameters: {'hidden_size': 197, 'n_layers': 1, 'rnn_dropout': 0.2267699459424457, 'bidirectional': True, 'fc_dropout': 0.22300672695474966, 'learning_rate_model': 0.00018138042998014145}. Best is trial 120 with value: 1.8680797815322876.
Epoch 7: reducing lr to 0.0012974834662564598
Epoch 10: reducing lr to 0.002136572812113643
Epoch 13: reducing lr to 0.0030440536101403026
Epoch 17: reducing lr to 0.004140983699563188
Epoch 20: reducing lr to 0.004729197266500954
Epoch 23: reducing lr to 0.00502167563380246
Epoch 26: reducing lr to 0.005037351717264601
Epoch 31: reducing lr to 0.004940945155314494
Epoch 39: reducing lr to 0.004570349434958314
Epoch 42: reducing lr to 0.004369557900420375
Epoch 47: reducing lr to 0.003971952276077868
Epoch 52: reducing lr to 0.0035110053633388275
Epoch 55: reducing lr to 0.0032123238283389317
Epoch 58: reducing lr to 0.0029027663509020663
Epoch 61: reducing lr to 0.002587213467854403
Epoch 64: reducing lr to 0.00227064327095819
Epoch 67: reducing lr to 0.001958046159491564
Epoch 70: reducing lr to 0.001654354064121101
Epoch 73: reducing lr to 0.0013643554414803799
Epoch 76: reducing lr to 0.0010926233452408125
Epoch 79: reducing lr to 0.0008434439534803946
Epoch 82: reducing lr to 0.0006207462366346464
Epoch 85: reducing lr to 0.0004280429268584109
Epoch 88: reducing lr to 0.0002683722881762149
Epoch 91: reducing lr to 0.00014425336760263974
Epoch 94: reducing lr to 5.764296410965472e-05
Epoch 97: reducing lr to 9.907230676296785e-06
[I 2024-06-22 13:20:53,036] Trial 300 finished with value: 2.4817018508911133 and parameters: {'hidden_size': 93, 'n_layers': 5, 'rnn_dropout': 0.5754710137768604, 'bidirectional': True, 'fc_dropout': 0.18958565157735868, 'learning_rate_model': 0.05045243613732324}. Best is trial 120 with value: 1.8680797815322876.
Epoch 56: reducing lr to 7.956907166632883e-06
Epoch 68: reducing lr to 4.747334909500175e-06
Epoch 72: reducing lr to 3.7332524279084693e-06
Epoch 80: reducing lr to 1.9599562539770287e-06
Epoch 83: reducing lr to 1.4148444753820185e-06
Epoch 86: reducing lr to 9.492009132223064e-07
[I 2024-06-22 13:22:04,501] Trial 301 finished with value: 1.879934310913086 and parameters: {'hidden_size': 160, 'n_layers': 3, 'rnn_dropout': 0.24397251331365055, 'bidirectional': True, 'fc_dropout': 0.5067686673885805, 'learning_rate_model': 0.00012907761430379762}. Best is trial 120 with value: 1.8680797815322876.
Epoch 81: reducing lr to 5.309549579093431e-06
Epoch 84: reducing lr to 3.7510653898134355e-06
Epoch 87: reducing lr to 2.4387547872418557e-06
[I 2024-06-22 13:22:37,533] Trial 302 finished with value: 2.5623974800109863 and parameters: {'hidden_size': 169, 'n_layers': 3, 'rnn_dropout': 0.5866928755750672, 'bidirectional': False, 'fc_dropout': 0.6428757011448218, 'learning_rate_model': 0.00038721560089512944}. Best is trial 120 with value: 1.8680797815322876.
Epoch 21: reducing lr to 0.0029642947446695098
Epoch 30: reducing lr to 0.0030291398445819345
Epoch 35: reducing lr to 0.0029184797042106497
Epoch 42: reducing lr to 0.0026637771053208002
Epoch 45: reducing lr to 0.0025236138511597764
Epoch 56: reducing lr to 0.0018959893173234477
Epoch 59: reducing lr to 0.0017057237081459196
Epoch 64: reducing lr to 0.001384233301713948
Epoch 67: reducing lr to 0.0011936673342429357
Epoch 71: reducing lr to 0.000948531882955007
Epoch 77: reducing lr to 0.0006138010594612029
Epoch 80: reducing lr to 0.0004670226813183055
Epoch 83: reducing lr to 0.00033713224935531946
Epoch 86: reducing lr to 0.00022617767855958557
Epoch 89: reducing lr to 0.00013590944592940316
Epoch 92: reducing lr to 6.775068835928252e-05
Epoch 95: reducing lr to 2.277651098609619e-05
Epoch 98: reducing lr to 1.6960637536821837e-06
[I 2024-06-22 13:22:50,608] Trial 303 finished with value: 2.549154281616211 and parameters: {'hidden_size': 45, 'n_layers': 5, 'rnn_dropout': 0.602660644715514, 'bidirectional': False, 'fc_dropout': 0.7203591299088523, 'learning_rate_model': 0.03075689746034307}. Best is trial 120 with value: 1.8680797815322876.
Epoch 31: reducing lr to 0.00017241165739840362
Epoch 42: reducing lr to 0.0001524734025633631
Epoch 47: reducing lr to 0.0001385991654429441
Epoch 50: reducing lr to 0.00012917052872351838
Epoch 53: reducing lr to 0.00011909303294858207
Epoch 56: reducing lr to 0.00010852557515366101
Epoch 63: reducing lr to 8.290825239547607e-05
Epoch 66: reducing lr to 7.193486990808967e-05
Epoch 69: reducing lr to 6.121522903127965e-05
Epoch 72: reducing lr to 5.091844839306028e-05
Epoch 75: reducing lr to 4.120686769748321e-05
Epoch 78: reducing lr to 3.2233672986448146e-05
Epoch 81: reducing lr to 2.4140350345472674e-05
Epoch 84: reducing lr to 1.7054560152414e-05
Epoch 87: reducing lr to 1.1088020573822208e-05
Epoch 90: reducing lr to 6.33486284031589e-06
Epoch 93: reducing lr to 2.8700232718062914e-06
Epoch 96: reducing lr to 7.481546664274549e-07
Epoch 99: reducing lr to 2.7138801404064107e-09
[I 2024-06-22 13:24:42,202] Trial 304 finished with value: 1.8783400058746338 and parameters: {'hidden_size': 146, 'n_layers': 5, 'rnn_dropout': 0.6539058163177005, 'bidirectional': True, 'fc_dropout': 0.3177822129870094, 'learning_rate_model': 0.0017605109671915277}. Best is trial 120 with value: 1.8680797815322876.
Epoch 62: reducing lr to 0.0002147350324167783
Epoch 66: reducing lr to 0.00017838635097168885
Epoch 69: reducing lr to 0.00015180344865763253
Epoch 72: reducing lr to 0.0001262691684517358
Epoch 75: reducing lr to 0.00010218608545367693
Epoch 84: reducing lr to 4.2292434210326834e-05
Epoch 87: reducing lr to 2.749642186314306e-05
Epoch 90: reducing lr to 1.5709391946269573e-05
Epoch 97: reducing lr to 8.572964548795095e-07
[I 2024-06-22 13:24:51,064] Trial 305 finished with value: 1.9940989017486572 and parameters: {'hidden_size': 151, 'n_layers': 1, 'rnn_dropout': 0.31980399014469163, 'bidirectional': False, 'fc_dropout': 0.3983829061386307, 'learning_rate_model': 0.004365770420996029}. Best is trial 120 with value: 1.8680797815322876.
Epoch 80: reducing lr to 2.8988254486116968e-06
Epoch 83: reducing lr to 2.0925911804116892e-06
Epoch 86: reducing lr to 1.403892437655668e-06
Epoch 89: reducing lr to 8.4359448978956e-07
Epoch 92: reducing lr to 4.205307952548679e-07
Epoch 95: reducing lr to 1.413745676991611e-07
Epoch 98: reducing lr to 1.052752461135802e-08
[I 2024-06-22 13:26:01,333] Trial 306 finished with value: 1.8977662324905396 and parameters: {'hidden_size': 107, 'n_layers': 5, 'rnn_dropout': 0.4659518895113781, 'bidirectional': True, 'fc_dropout': 0.1556131310876815, 'learning_rate_model': 0.00019090909423650804}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:26:25,847] Trial 307 finished with value: 3.2478675842285156 and parameters: {'hidden_size': 43, 'n_layers': 5, 'rnn_dropout': 0.48919958132768193, 'bidirectional': True, 'fc_dropout': 0.5226064416195636, 'learning_rate_model': 5.769443580257137e-05}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:26:40,993] Trial 308 finished with value: 2.6744368076324463 and parameters: {'hidden_size': 17, 'n_layers': 6, 'rnn_dropout': 0.3548030735231481, 'bidirectional': True, 'fc_dropout': 0.050095841587132386, 'learning_rate_model': 0.0003254125867309244}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:26:48,763] Trial 309 finished with value: 8.505170822143555 and parameters: {'hidden_size': 144, 'n_layers': 1, 'rnn_dropout': 0.15666738693999394, 'bidirectional': False, 'fc_dropout': 0.6000993984551881, 'learning_rate_model': 2.7475150823638172e-05}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:27:12,105] Trial 310 finished with value: 8.958438873291016 and parameters: {'hidden_size': 131, 'n_layers': 3, 'rnn_dropout': 0.5360722949982456, 'bidirectional': False, 'fc_dropout': 0.13773004396474617, 'learning_rate_model': 1.1877472237951753e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 10: reducing lr to 0.003842767240509655
Epoch 27: reducing lr to 0.009041025483703887
Epoch 33: reducing lr to 0.008763323437994358
Epoch 36: reducing lr to 0.008523129990076098
Epoch 40: reducing lr to 0.008105886688579329
Epoch 45: reducing lr to 0.007445414847349735
Epoch 49: reducing lr to 0.006823938312742786
Epoch 52: reducing lr to 0.006314774911951146
Epoch 55: reducing lr to 0.005777576454901002
Epoch 58: reducing lr to 0.005220816897443
Epoch 61: reducing lr to 0.0046532742072295285
Epoch 64: reducing lr to 0.004083901810905246
Epoch 70: reducing lr to 0.002975464990364408
Epoch 74: reducing lr to 0.002286930491616118
Epoch 77: reducing lr to 0.0018108965122897863
Epoch 80: reducing lr to 0.0013778564434247299
Epoch 83: reducing lr to 0.0009946408614443722
Epoch 86: reducing lr to 0.000667291727422055
Epoch 89: reducing lr to 0.00040097347149715967
Epoch 92: reducing lr to 0.00019988477270266452
Epoch 95: reducing lr to 6.719751240419347e-05
Epoch 98: reducing lr to 5.0038948105762815e-06
[I 2024-06-22 13:27:23,957] Trial 311 finished with value: 2.0469608306884766 and parameters: {'hidden_size': 106, 'n_layers': 2, 'rnn_dropout': 0.51067466400271, 'bidirectional': False, 'fc_dropout': 0.3143679221073626, 'learning_rate_model': 0.09074203682326881}. Best is trial 120 with value: 1.8680797815322876.
Epoch 84: reducing lr to 5.530384008055094e-07
[I 2024-06-22 13:28:24,444] Trial 312 finished with value: 2.0744287967681885 and parameters: {'hidden_size': 111, 'n_layers': 4, 'rnn_dropout': 0.5995539766017357, 'bidirectional': True, 'fc_dropout': 0.39139582836811376, 'learning_rate_model': 5.7089139866110815e-05}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:28:29,585] Trial 313 finished with value: 2.4982943534851074 and parameters: {'hidden_size': 88, 'n_layers': 1, 'rnn_dropout': 0.3700232482737623, 'bidirectional': False, 'fc_dropout': 0.023417697122490023, 'learning_rate_model': 0.0016609136950705152}. Best is trial 120 with value: 1.8680797815322876.
Epoch 15: reducing lr to 0.007090816703992921
Epoch 19: reducing lr to 0.008927572693280216
Epoch 22: reducing lr to 0.009706031984878008
Epoch 25: reducing lr to 0.009868379126178429
Epoch 28: reducing lr to 0.009806456998871664
Epoch 33: reducing lr to 0.009533594416658554
Epoch 36: reducing lr to 0.009272288654045327
Epoch 41: reducing lr to 0.008687334036738704
Epoch 44: reducing lr to 0.008255714014109853
Epoch 51: reducing lr to 0.007058295849665293
Epoch 54: reducing lr to 0.006483043393262498
Epoch 57: reducing lr to 0.0058833910267443905
Epoch 62: reducing lr to 0.004855549236751913
Epoch 66: reducing lr to 0.004033639507066594
Epoch 70: reducing lr to 0.0032369998231622245
Epoch 74: reducing lr to 0.002487945118130612
Epoch 77: reducing lr to 0.0019700691182823237
Epoch 81: reducing lr to 0.0013536337939074633
Epoch 84: reducing lr to 0.0009563087789595706
Epoch 87: reducing lr to 0.0006217440567958365
Epoch 90: reducing lr to 0.00035521789442581055
Epoch 93: reducing lr to 0.0001609322331457566
Epoch 96: reducing lr to 4.195164631219661e-05
Epoch 99: reducing lr to 1.5217674218109823e-07
[I 2024-06-22 13:28:48,451] Trial 314 finished with value: 2.5239076614379883 and parameters: {'hidden_size': 70, 'n_layers': 5, 'rnn_dropout': 0.5862158826090934, 'bidirectional': False, 'fc_dropout': 0.19112796389616965, 'learning_rate_model': 0.09871800142213323}. Best is trial 120 with value: 1.8680797815322876.
Epoch 12: reducing lr to 0.00036097702472315024
Epoch 30: reducing lr to 0.0006541817847817229
Epoch 36: reducing lr to 0.0006238960517372822
Epoch 39: reducing lr to 0.000601712333283334
Epoch 42: reducing lr to 0.0005752769929510983
Epoch 47: reducing lr to 0.0005229299653650361
Epoch 50: reducing lr to 0.00048735603779215665
Epoch 56: reducing lr to 0.0004094633259511714
Epoch 62: reducing lr to 0.00032671092443868643
Epoch 68: reducing lr to 0.0002442983813609862
Epoch 71: reducing lr to 0.00020484768348472024
Epoch 74: reducing lr to 0.0001674040998997082
Epoch 77: reducing lr to 0.000132558248605607
Epoch 80: reducing lr to 0.00010085956636991141
Epoch 83: reducing lr to 7.280805374014617e-05
Epoch 86: reducing lr to 4.8845984348509696e-05
Epoch 89: reducing lr to 2.9351396260499356e-05
Epoch 92: reducing lr to 1.463163422789496e-05
Epoch 95: reducing lr to 4.918881059464992e-06
Epoch 98: reducing lr to 3.6628682411991853e-07
[I 2024-06-22 13:29:09,515] Trial 315 finished with value: 2.564284324645996 and parameters: {'hidden_size': 60, 'n_layers': 7, 'rnn_dropout': 0.17377580031281437, 'bidirectional': False, 'fc_dropout': 0.5732704374357874, 'learning_rate_model': 0.006642348358707951}. Best is trial 120 with value: 1.8680797815322876.
Epoch 58: reducing lr to 2.1038861439797893e-05
Epoch 81: reducing lr to 5.014147766944102e-06
Epoch 84: reducing lr to 3.542371319415334e-06
Epoch 87: reducing lr to 2.303072358288605e-06
Epoch 90: reducing lr to 1.3158027083324446e-06
Epoch 93: reducing lr to 5.961272547191486e-07
Epoch 96: reducing lr to 1.553978296217856e-07
Epoch 99: reducing lr to 5.636950520997251e-10
[I 2024-06-22 13:29:40,012] Trial 316 finished with value: 2.5785329341888428 and parameters: {'hidden_size': 97, 'n_layers': 6, 'rnn_dropout': 0.7359628616118871, 'bidirectional': False, 'fc_dropout': 0.052828467213309874, 'learning_rate_model': 0.00036567249474403436}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:31:14,833] Trial 317 finished with value: 3.7169249057769775 and parameters: {'hidden_size': 102, 'n_layers': 7, 'rnn_dropout': 0.08844595220435299, 'bidirectional': True, 'fc_dropout': 0.4852283444132348, 'learning_rate_model': 1.3156239670034818e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 84: reducing lr to 1.441643832549135e-05
[I 2024-06-22 13:31:20,711] Trial 318 finished with value: 2.5819644927978516 and parameters: {'hidden_size': 43, 'n_layers': 2, 'rnn_dropout': 0.003897638362884859, 'bidirectional': False, 'fc_dropout': 0.4375746474169927, 'learning_rate_model': 0.0014881824892021818}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:31:43,252] Trial 319 finished with value: 2.7669014930725098 and parameters: {'hidden_size': 129, 'n_layers': 3, 'rnn_dropout': 0.29338558532875336, 'bidirectional': False, 'fc_dropout': 0.1612773075265369, 'learning_rate_model': 0.00020421903674544914}. Best is trial 120 with value: 1.8680797815322876.
Epoch 41: reducing lr to 0.00030548508486332154
Epoch 48: reducing lr to 0.0002672523572662271
Epoch 52: reducing lr to 0.00024157344679317697
Epoch 77: reducing lr to 6.927634291946888e-05
Epoch 80: reducing lr to 5.271027627514508e-05
Epoch 83: reducing lr to 3.805025904655879e-05
Epoch 86: reducing lr to 2.5527428112257336e-05
Epoch 92: reducing lr to 7.646646820596388e-06
Epoch 95: reducing lr to 2.5706592734898398e-06
Epoch 98: reducing lr to 1.914253688588795e-07
[I 2024-06-22 13:32:11,807] Trial 320 finished with value: 2.490734815597534 and parameters: {'hidden_size': 153, 'n_layers': 3, 'rnn_dropout': 0.03808874296524403, 'bidirectional': False, 'fc_dropout': 0.39608989004894257, 'learning_rate_model': 0.0034713615148726346}. Best is trial 120 with value: 1.8680797815322876.
Epoch 28: reducing lr to 1.4723871627640037e-05
Epoch 57: reducing lr to 8.833597518753305e-06
Epoch 65: reducing lr to 6.362589838490204e-06
Epoch 68: reducing lr to 5.4513640948331175e-06
Epoch 71: reducing lr to 4.571046686585524e-06
Epoch 74: reducing lr to 3.735516766165779e-06
Epoch 79: reducing lr to 2.477880375951924e-06
Epoch 82: reducing lr to 1.823636190473622e-06
Epoch 85: reducing lr to 1.2575099556418062e-06
Epoch 88: reducing lr to 7.884275221573608e-07
Epoch 91: reducing lr to 4.237893783844252e-07
Epoch 94: reducing lr to 1.693442332352128e-07
Epoch 97: reducing lr to 2.910558831031591e-08
[I 2024-06-22 13:35:55,472] Trial 321 finished with value: 1.8958818912506104 and parameters: {'hidden_size': 178, 'n_layers': 7, 'rnn_dropout': 0.245728614788076, 'bidirectional': True, 'fc_dropout': 0.29486149581222076, 'learning_rate_model': 0.0001482198086876757}. Best is trial 120 with value: 1.8680797815322876.
Epoch 27: reducing lr to 0.0049069530244913485
Epoch 33: reducing lr to 0.004756232191378117
Epoch 37: reducing lr to 0.004574715440481747
Epoch 44: reducing lr to 0.004118708122102301
Epoch 50: reducing lr to 0.003613494788703235
Epoch 56: reducing lr to 0.003035960324186352
Epoch 59: reducing lr to 0.002731296771896105
Epoch 62: reducing lr to 0.002422393755948682
Epoch 65: reducing lr to 0.0021141237715461693
Epoch 70: reducing lr to 0.0016149127065346242
Epoch 73: reducing lr to 0.0013318279239377118
Epoch 78: reducing lr to 0.0009017243368772208
Epoch 81: reducing lr to 0.0006753168159398693
Epoch 84: reducing lr to 0.00047709461936384717
Epoch 87: reducing lr to 0.00031018301896325524
Epoch 90: reducing lr to 0.00017721529892958805
Epoch 93: reducing lr to 8.028777336916496e-05
Epoch 96: reducing lr to 2.092933283617815e-05
Epoch 99: reducing lr to 7.591973061882306e-08
[I 2024-06-22 13:36:05,895] Trial 322 finished with value: 2.0247268676757812 and parameters: {'hidden_size': 92, 'n_layers': 2, 'rnn_dropout': 0.2551888051761512, 'bidirectional': False, 'fc_dropout': 0.4031864075190263, 'learning_rate_model': 0.049249602585571886}. Best is trial 120 with value: 1.8680797815322876.
Epoch 15: reducing lr to 0.0005076316447805157
Epoch 27: reducing lr to 0.0007041382760105519
Epoch 30: reducing lr to 0.0006960261899101341
Epoch 33: reducing lr to 0.0006825101277365573
Epoch 36: reducing lr to 0.0006638032453556767
Epoch 40: reducing lr to 0.0006313072658318445
Epoch 43: reducing lr to 0.0006017683964978438
Epoch 46: reducing lr to 0.0005683120274327536
Epoch 49: reducing lr to 0.0005314658351309691
Epoch 52: reducing lr to 0.0004918108822843792
Epoch 55: reducing lr to 0.0004499724872810004
Epoch 58: reducing lr to 0.00040661062355796303
Epoch 61: reducing lr to 0.0003624089417720175
Epoch 64: reducing lr to 0.00031806475777669356
Epoch 67: reducing lr to 0.0002742771114246668
Epoch 70: reducing lr to 0.0002317368524645083
Epoch 73: reducing lr to 0.00019111473324149526
Epoch 76: reducing lr to 0.00015305133311342535
Epoch 79: reducing lr to 0.00011814704678324537
Epoch 82: reducing lr to 8.695223240094254e-05
Epoch 85: reducing lr to 5.9958942732468625e-05
Epoch 88: reducing lr to 3.759276850067422e-05
Epoch 91: reducing lr to 2.0206570099249536e-05
Epoch 94: reducing lr to 8.074449937409654e-06
Epoch 97: reducing lr to 1.3877745419536674e-06
[I 2024-06-22 13:37:41,454] Trial 323 finished with value: 1.894789695739746 and parameters: {'hidden_size': 154, 'n_layers': 4, 'rnn_dropout': 0.6036977802465054, 'bidirectional': True, 'fc_dropout': 0.4173274507306175, 'learning_rate_model': 0.007067222792988559}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:37:46,006] Trial 324 finished with value: 9.08924674987793 and parameters: {'hidden_size': 72, 'n_layers': 1, 'rnn_dropout': 0.2146102633087491, 'bidirectional': False, 'fc_dropout': 0.5002649239226041, 'learning_rate_model': 3.398051356678928e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 13: reducing lr to 0.0015761697736270098
Epoch 17: reducing lr to 0.002144145332589204
Epoch 20: reducing lr to 0.002448714359086048
Epoch 23: reducing lr to 0.002600155700475291
Epoch 26: reducing lr to 0.0026082725643963374
Epoch 29: reducing lr to 0.0025850660981453894
Epoch 32: reducing lr to 0.0025416910328351686
Epoch 35: reducing lr to 0.0024788314185219705
Epoch 43: reducing lr to 0.002224402797692312
Epoch 51: reducing lr to 0.0018678256838955854
Epoch 54: reducing lr to 0.0017155975348241467
Epoch 57: reducing lr to 0.0015569124760724078
Epoch 62: reducing lr to 0.0012849163264040428
Epoch 65: reducing lr to 0.001121399914207883
Epoch 73: reducing lr to 0.0007064447880225713
Epoch 77: reducing lr to 0.0005213362795428514
Epoch 80: reducing lr to 0.0003966690239250094
Epoch 85: reducing lr to 0.00022163483615453422
Epoch 91: reducing lr to 7.469244201277619e-05
Epoch 94: reducing lr to 2.984674691314526e-05
Epoch 97: reducing lr to 5.129830000467559e-06
[I 2024-06-22 13:38:25,847] Trial 325 finished with value: 2.5529584884643555 and parameters: {'hidden_size': 156, 'n_layers': 4, 'rnn_dropout': 0.6030952572969704, 'bidirectional': False, 'fc_dropout': 0.5236300276982652, 'learning_rate_model': 0.02612358881610851}. Best is trial 120 with value: 1.8680797815322876.
Epoch 67: reducing lr to 8.414453283775767e-07
Epoch 96: reducing lr to 9.213778623865492e-09
Epoch 99: reducing lr to 3.3422354958967455e-11
[I 2024-06-22 13:40:52,678] Trial 326 finished with value: 2.295302629470825 and parameters: {'hidden_size': 153, 'n_layers': 6, 'rnn_dropout': 0.6870326050135719, 'bidirectional': True, 'fc_dropout': 0.31660020925214455, 'learning_rate_model': 2.1681290038658853e-05}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:42:44,407] Trial 327 finished with value: 2.2859907150268555 and parameters: {'hidden_size': 169, 'n_layers': 4, 'rnn_dropout': 0.4336609237912984, 'bidirectional': True, 'fc_dropout': 0.7023683349801254, 'learning_rate_model': 1.375738861706401e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 13: reducing lr to 0.0021079612541469355
Epoch 17: reducing lr to 0.0028675688114214604
Epoch 23: reducing lr to 0.003477434704726324
Epoch 29: reducing lr to 0.0034572539490842964
Epoch 33: reducing lr to 0.0033740598833256314
Epoch 36: reducing lr to 0.0032815804624081097
Epoch 40: reducing lr to 0.0031209332039647086
Epoch 43: reducing lr to 0.002974904727655974
Epoch 47: reducing lr to 0.0027505170977941396
Epoch 50: reducing lr to 0.0025634046687777754
Epoch 53: reducing lr to 0.002363415553812137
Epoch 56: reducing lr to 0.00215370308366661
Epoch 62: reducing lr to 0.0017184404092763044
Epoch 66: reducing lr to 0.0014275561398762408
Epoch 72: reducing lr to 0.0010104827287427988
Epoch 75: reducing lr to 0.000817755242509929
Epoch 78: reducing lr to 0.0006396811148940737
Epoch 84: reducing lr to 0.00033844979617776857
Epoch 87: reducing lr to 0.0002200431010643121
Epoch 90: reducing lr to 0.00012571611451471856
Epoch 93: reducing lr to 5.6955956804948665e-05
Epoch 96: reducing lr to 1.48472192832233e-05
Epoch 99: reducing lr to 5.385727759391057e-08
[I 2024-06-22 13:44:30,090] Trial 328 finished with value: 2.46232533454895 and parameters: {'hidden_size': 144, 'n_layers': 5, 'rnn_dropout': 0.5133993446668773, 'bidirectional': True, 'fc_dropout': 0.4075268651510099, 'learning_rate_model': 0.03493755175681622}. Best is trial 120 with value: 1.8680797815322876.
Epoch 21: reducing lr to 0.00039637202001458334
Epoch 30: reducing lr to 0.0004050428120424527
Epoch 41: reducing lr to 0.00036192142833195933
Epoch 47: reducing lr to 0.0003237770121395075
Epoch 52: reducing lr to 0.0002862025389867362
Epoch 59: reducing lr to 0.00022808162143806744
Epoch 62: reducing lr to 0.00020228614528573172
Epoch 65: reducing lr to 0.00017654353151827862
Epoch 70: reducing lr to 0.00013485605532776056
Epoch 73: reducing lr to 0.00011121657503271975
Epoch 77: reducing lr to 8.207468783704005e-05
Epoch 80: reducing lr to 6.244815024539753e-05
Epoch 83: reducing lr to 4.50797920582377e-05
Epoch 86: reducing lr to 3.0243451159532803e-05
Epoch 89: reducing lr to 1.8173193377268686e-05
Epoch 92: reducing lr to 9.059314108570987e-06
Epoch 95: reducing lr to 3.045571525799734e-06
Epoch 98: reducing lr to 2.267899362333979e-07
[I 2024-06-22 13:45:48,985] Trial 329 finished with value: 2.4603235721588135 and parameters: {'hidden_size': 120, 'n_layers': 5, 'rnn_dropout': 0.3531033847628007, 'bidirectional': True, 'fc_dropout': 0.28392200868061873, 'learning_rate_model': 0.004112672532871488}. Best is trial 120 with value: 1.8680797815322876.
Epoch 12: reducing lr to 0.001417088887124367
Epoch 25: reducing lr to 0.0026066855914929415
Epoch 34: reducing lr to 0.0024973266205748123
Epoch 37: reducing lr to 0.0024221472087420795
Epoch 40: reducing lr to 0.0023293308837640096
Epoch 43: reducing lr to 0.0022203414829838754
Epoch 48: reducing lr to 0.0020075246190766386
Epoch 54: reducing lr to 0.001712465196783048
Epoch 58: reducing lr to 0.0015002689409444932
Epoch 61: reducing lr to 0.0013371782431641564
Epoch 64: reducing lr to 0.0011735617557798107
Epoch 67: reducing lr to 0.001011998722221632
Epoch 70: reducing lr to 0.0008550381669385415
Epoch 76: reducing lr to 0.0005647126467847662
Epoch 79: reducing lr to 0.000435926496957233
Epoch 83: reducing lr to 0.00028582285514554517
Epoch 86: reducing lr to 0.00019175486765123372
Epoch 89: reducing lr to 0.00011522488860402496
Epoch 92: reducing lr to 5.74394624719414e-05
Epoch 95: reducing lr to 1.9310070195742434e-05
Epoch 98: reducing lr to 1.437933586932727e-06
[I 2024-06-22 13:46:34,899] Trial 330 finished with value: 2.461867570877075 and parameters: {'hidden_size': 116, 'n_layers': 7, 'rnn_dropout': 0.22004324085232937, 'bidirectional': False, 'fc_dropout': 0.3096369860840687, 'learning_rate_model': 0.026075892366703723}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:47:11,244] Trial 331 finished with value: 2.0557634830474854 and parameters: {'hidden_size': 66, 'n_layers': 5, 'rnn_dropout': 0.49884144083644666, 'bidirectional': True, 'fc_dropout': 0.5958670984259942, 'learning_rate_model': 0.000152370936810405}. Best is trial 120 with value: 1.8680797815322876.
Epoch 10: reducing lr to 0.0036223590075950157
Epoch 15: reducing lr to 0.0061440655281353685
Epoch 18: reducing lr to 0.0074014049361624725
Epoch 21: reducing lr to 0.00824393945267336
Epoch 24: reducing lr to 0.008553337599355853
Epoch 29: reducing lr to 0.00846437209842834
Epoch 32: reducing lr to 0.008322347609057356
Epoch 35: reducing lr to 0.008116524181218384
Epoch 38: reducing lr to 0.007850147718167147
Epoch 44: reducing lr to 0.007153428159505709
Epoch 68: reducing lr to 0.0031459721608302106
Epoch 78: reducing lr to 0.001566127065162523
Epoch 81: reducing lr to 0.0011728994103292283
Epoch 84: reducing lr to 0.0008286244093363871
Epoch 87: reducing lr to 0.0005387300766823144
Epoch 90: reducing lr to 0.00030778993608584907
Epoch 93: reducing lr to 0.00013944489433493634
Epoch 96: reducing lr to 3.635034929194425e-05
Epoch 99: reducing lr to 1.3185841840901437e-07
[I 2024-06-22 13:47:25,469] Trial 332 finished with value: 2.0583062171936035 and parameters: {'hidden_size': 89, 'n_layers': 3, 'rnn_dropout': 0.5308821053368749, 'bidirectional': False, 'fc_dropout': 0.012337457874604675, 'learning_rate_model': 0.08553737811366681}. Best is trial 120 with value: 1.8680797815322876.
Epoch 17: reducing lr to 0.0009554103863857553
Epoch 22: reducing lr to 0.0011444951125857555
Epoch 25: reducing lr to 0.0011636384154359919
Epoch 28: reducing lr to 0.0011563368145166956
Epoch 31: reducing lr to 0.0011399780009874957
Epoch 34: reducing lr to 0.0011148199848403787
Epoch 37: reducing lr to 0.001081259492564689
Epoch 40: reducing lr to 0.0010398257877571155
Epoch 43: reducing lr to 0.0009911723352513699
Epoch 49: reducing lr to 0.0008753770320588274
Epoch 56: reducing lr to 0.0007175661442300722
Epoch 59: reducing lr to 0.0006455572155353471
Epoch 64: reducing lr to 0.0005238842560716235
Epoch 67: reducing lr to 0.00045176165218866105
Epoch 70: reducing lr to 0.00038169361926913993
Epoch 73: reducing lr to 0.00031478495306556736
Epoch 77: reducing lr to 0.00023230239513357502
Epoch 80: reducing lr to 0.000176751873884317
Epoch 83: reducing lr to 0.00012759285406049487
Epoch 86: reducing lr to 8.560040039889277e-05
Epoch 89: reducing lr to 5.14370076819216e-05
Epoch 92: reducing lr to 2.564128382512923e-05
Epoch 95: reducing lr to 8.620118804455665e-06
Epoch 98: reducing lr to 6.419012580810222e-07
[I 2024-06-22 13:47:48,180] Trial 333 finished with value: 2.5612595081329346 and parameters: {'hidden_size': 126, 'n_layers': 3, 'rnn_dropout': 0.11739834458198617, 'bidirectional': False, 'fc_dropout': 0.25448700187569345, 'learning_rate_model': 0.011640418074851954}. Best is trial 120 with value: 1.8680797815322876.
Epoch 10: reducing lr to 0.002095552480713592
Epoch 15: reducing lr to 0.003554372090716416
Epoch 18: reducing lr to 0.00428174911493343
Epoch 24: reducing lr to 0.004948147819454045
Epoch 27: reducing lr to 0.004930286482315732
Epoch 30: reducing lr to 0.004873486689140348
Epoch 36: reducing lr to 0.004647865737446947
Epoch 42: reducing lr to 0.004285666206146853
Epoch 45: reducing lr to 0.00406016200742782
Epoch 51: reducing lr to 0.0035380705528521975
Epoch 55: reducing lr to 0.0031506500172449924
Epoch 60: reducing lr to 0.0026410596068827774
Epoch 63: reducing lr to 0.0023303546030221706
Epoch 66: reducing lr to 0.0020219188122371375
Epoch 77: reducing lr to 0.0009875249894503516
Epoch 83: reducing lr to 0.0005424013462606749
Epoch 86: reducing lr to 0.0003638900685989731
Epoch 89: reducing lr to 0.00021866038203884885
Epoch 92: reducing lr to 0.00010900192623646555
Epoch 95: reducing lr to 3.664440362974308e-05
Epoch 98: reducing lr to 2.7287429935886343e-06
[I 2024-06-22 13:48:13,311] Trial 334 finished with value: 2.5408549308776855 and parameters: {'hidden_size': 69, 'n_layers': 7, 'rnn_dropout': 0.561852603835846, 'bidirectional': False, 'fc_dropout': 0.21523390980153484, 'learning_rate_model': 0.04948379344068349}. Best is trial 120 with value: 1.8680797815322876.
Epoch 11: reducing lr to 0.00013612844009034156
Epoch 39: reducing lr to 0.0002552182810851379
Epoch 42: reducing lr to 0.00024400564383923168
Epoch 47: reducing lr to 0.0002218024785367512
Epoch 50: reducing lr to 0.00020671367921456363
Epoch 53: reducing lr to 0.00019058650028689066
Epoch 79: reducing lr to 4.7099750043703765e-05
Epoch 82: reducing lr to 3.466382379696698e-05
Epoch 85: reducing lr to 2.390285066342008e-05
Epoch 88: reducing lr to 1.4986493933115104e-05
Epoch 91: reducing lr to 8.055422685776308e-06
Epoch 94: reducing lr to 3.2189088440788724e-06
Epoch 97: reducing lr to 5.532413701626275e-07
[I 2024-06-22 13:50:25,536] Trial 335 finished with value: 1.9207723140716553 and parameters: {'hidden_size': 141, 'n_layers': 6, 'rnn_dropout': 0.4573082691479611, 'bidirectional': True, 'fc_dropout': 0.1172869614215272, 'learning_rate_model': 0.0028173740784533195}. Best is trial 120 with value: 1.8680797815322876.
Epoch 13: reducing lr to 0.00010254663551960846
Epoch 23: reducing lr to 0.00016916782910847127
Epoch 31: reducing lr to 0.00016644821900526827
Epoch 37: reducing lr to 0.00015787472798951558
Epoch 40: reducing lr to 0.00015182499162088694
Epoch 49: reducing lr to 0.0001278138243177446
Epoch 56: reducing lr to 0.00010477185228321181
Epoch 62: reducing lr to 8.359749590536853e-05
Epoch 68: reducing lr to 6.251010115624685e-05
Epoch 71: reducing lr to 5.241561300945085e-05
Epoch 79: reducing lr to 2.8413540218427745e-05
Epoch 82: reducing lr to 2.091140506405463e-05
Epoch 85: reducing lr to 1.4419707281460372e-05
Epoch 88: reducing lr to 9.040798469347977e-06
Epoch 91: reducing lr to 4.859539089832979e-06
Epoch 94: reducing lr to 1.941848859406321e-06
Epoch 97: reducing lr to 3.3375009224100633e-07
[I 2024-06-22 13:51:18,533] Trial 336 finished with value: 2.465757369995117 and parameters: {'hidden_size': 67, 'n_layers': 7, 'rnn_dropout': 0.15858583216812303, 'bidirectional': True, 'fc_dropout': 0.35806331524051055, 'learning_rate_model': 0.0016996177604808832}. Best is trial 120 with value: 1.8680797815322876.
Epoch 14: reducing lr to 0.0002733948093909228
Epoch 23: reducing lr to 0.0004110912707939045
Epoch 31: reducing lr to 0.00040448240207884413
Epoch 39: reducing lr to 0.00037414418895203477
Epoch 42: reducing lr to 0.0003577067180524345
Epoch 47: reducing lr to 0.0003251573832675437
Epoch 52: reducing lr to 0.00028742271740205157
Epoch 56: reducing lr to 0.0002546039286874077
Epoch 59: reducing lr to 0.00022905401068518643
Epoch 62: reducing lr to 0.00020314855967614506
Epoch 68: reducing lr to 0.0001519045143346952
Epoch 71: reducing lr to 0.00012737410579218489
Epoch 74: reducing lr to 0.00010409171911510248
Epoch 77: reducing lr to 8.242460004570553e-05
Epoch 80: reducing lr to 6.271438787304253e-05
Epoch 83: reducing lr to 4.527198248894145e-05
Epoch 86: reducing lr to 3.0372389241075173e-05
Epoch 89: reducing lr to 1.8250671859377197e-05
Epoch 92: reducing lr to 9.09793703474061e-06
Epoch 95: reducing lr to 3.058555829332614e-06
Epoch 98: reducing lr to 2.2775681858874102e-07
[I 2024-06-22 13:52:07,016] Trial 337 finished with value: 2.567439317703247 and parameters: {'hidden_size': 122, 'n_layers': 7, 'rnn_dropout': 0.27793114624048404, 'bidirectional': False, 'fc_dropout': 0.4614634425856199, 'learning_rate_model': 0.004130206249628986}. Best is trial 120 with value: 1.8680797815322876.
Epoch 16: reducing lr to 6.268846927793406e-05
Epoch 55: reducing lr to 5.173464618029912e-05
Epoch 59: reducing lr to 4.506200033115758e-05
Epoch 66: reducing lr to 3.320053125032488e-05
Epoch 69: reducing lr to 2.8253031207890407e-05
Epoch 72: reducing lr to 2.3500696383434193e-05
Epoch 75: reducing lr to 1.9018452392647493e-05
Epoch 78: reducing lr to 1.4877000106717012e-05
Epoch 81: reducing lr to 1.1141640445901796e-05
Epoch 84: reducing lr to 7.871293268816908e-06
Epoch 87: reducing lr to 5.117520529831804e-06
Epoch 90: reducing lr to 2.923767179466004e-06
Epoch 93: reducing lr to 1.3246190261622932e-06
Epoch 96: reducing lr to 3.4530030310109305e-07
Epoch 99: reducing lr to 1.2525533517500354e-09
[I 2024-06-22 13:52:37,372] Trial 338 finished with value: 1.8925687074661255 and parameters: {'hidden_size': 67, 'n_layers': 4, 'rnn_dropout': 0.4967619100643778, 'bidirectional': True, 'fc_dropout': 0.09072880090466091, 'learning_rate_model': 0.0008125391685209514}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:52:45,715] Trial 339 finished with value: 6.904706954956055 and parameters: {'hidden_size': 20, 'n_layers': 5, 'rnn_dropout': 0.07806609438086759, 'bidirectional': False, 'fc_dropout': 0.09266139736499923, 'learning_rate_model': 0.00012083923722009125}. Best is trial 120 with value: 1.8680797815322876.
Epoch 52: reducing lr to 4.495560607976133e-05
Epoch 55: reducing lr to 4.113122871737963e-05
Epoch 58: reducing lr to 3.7167593640085865e-05
Epoch 61: reducing lr to 3.312719220528613e-05
Epoch 64: reducing lr to 2.907376488305472e-05
Epoch 67: reducing lr to 2.507120973133001e-05
Epoch 70: reducing lr to 2.1182676164400707e-05
Epoch 73: reducing lr to 1.746947652670152e-05
Epoch 76: reducing lr to 1.3990165100598501e-05
Epoch 79: reducing lr to 1.079962295670294e-05
Epoch 92: reducing lr to 1.4230025975996478e-06
Epoch 95: reducing lr to 4.783867896012393e-07
Epoch 98: reducing lr to 3.5623300450985954e-08
[I 2024-06-22 13:53:05,672] Trial 340 finished with value: 2.401160717010498 and parameters: {'hidden_size': 19, 'n_layers': 7, 'rnn_dropout': 0.4892588864971285, 'bidirectional': True, 'fc_dropout': 0.3118709986999874, 'learning_rate_model': 0.0006460029564286756}. Best is trial 120 with value: 1.8680797815322876.
Epoch 17: reducing lr to 0.0011487471080245022
Epoch 20: reducing lr to 0.0013119229819096888
Epoch 23: reducing lr to 0.0013930591811738336
Epoch 26: reducing lr to 0.0013974078714486067
Epoch 29: reducing lr to 0.0013849747772044527
Epoch 35: reducing lr to 0.001328058494928967
Epoch 40: reducing lr to 0.0012502447990480582
Epoch 45: reducing lr to 0.0011483742059668072
Epoch 48: reducing lr to 0.0010775185403911838
Epoch 51: reducing lr to 0.0010007061182172334
Epoch 54: reducing lr to 0.0009191483789409653
Epoch 57: reducing lr to 0.0008341312863226995
Epoch 64: reducing lr to 0.0006298974060544272
Epoch 68: reducing lr to 0.000514756681519974
Epoch 71: reducing lr to 0.000431630832673573
Epoch 74: reducing lr to 0.00035273413788968096
Epoch 77: reducing lr to 0.0002793110776263989
Epoch 80: reducing lr to 0.00021251936011562264
Epoch 83: reducing lr to 0.00015341252742819364
Epoch 86: reducing lr to 0.0001029224863003159
Epoch 89: reducing lr to 6.18457938724816e-05
Epoch 92: reducing lr to 3.08300506880398e-05
Epoch 95: reducing lr to 1.0364484925588606e-05
Epoch 98: reducing lr to 7.717963132546773e-07
[I 2024-06-22 13:54:04,436] Trial 341 finished with value: 1.8773691654205322 and parameters: {'hidden_size': 188, 'n_layers': 2, 'rnn_dropout': 0.4918255128666393, 'bidirectional': True, 'fc_dropout': 0.4046106791891455, 'learning_rate_model': 0.013995971563871303}. Best is trial 120 with value: 1.8680797815322876.
Epoch 21: reducing lr to 0.004861779902973004
Epoch 30: reducing lr to 0.00496813322837272
Epoch 41: reducing lr to 0.0044392193138528425
Epoch 49: reducing lr to 0.0037935254643218716
Epoch 54: reducing lr to 0.003312828921071518
Epoch 57: reducing lr to 0.0030064071401445717
Epoch 69: reducing lr to 0.0017540306524948938
Epoch 72: reducing lr to 0.0014589918337685275
Epoch 77: reducing lr to 0.0010067034193135607
Epoch 83: reducing lr to 0.0005529351618988598
Epoch 86: reducing lr to 0.0003709570696704377
Epoch 91: reducing lr to 0.0001442315444401397
Epoch 94: reducing lr to 5.763424367703218e-05
Epoch 97: reducing lr to 9.905731875204522e-06
[I 2024-06-22 13:54:13,828] Trial 342 finished with value: 2.541278839111328 and parameters: {'hidden_size': 33, 'n_layers': 5, 'rnn_dropout': 0.020847946150714593, 'bidirectional': False, 'fc_dropout': 0.38512036731922916, 'learning_rate_model': 0.05044480351334593}. Best is trial 120 with value: 1.8680797815322876.
Epoch 15: reducing lr to 0.004067681860622786
Epoch 24: reducing lr to 0.005662741720666767
Epoch 27: reducing lr to 0.005642300912774496
Epoch 30: reducing lr to 0.005577298295578031
Epoch 33: reducing lr to 0.005468993304161905
Epoch 41: reducing lr to 0.004983531877014124
Epoch 47: reducing lr to 0.004458296565854763
Epoch 50: reducing lr to 0.004155007158789664
Epoch 53: reducing lr to 0.003830846009173582
Epoch 56: reducing lr to 0.0034909243318218793
Epoch 59: reducing lr to 0.003140604402000547
Epoch 62: reducing lr to 0.0027854096894895973
Epoch 65: reducing lr to 0.0024309428735869573
Epoch 68: reducing lr to 0.0020827925473830286
Epoch 71: reducing lr to 0.0017464513114403648
Epoch 74: reducing lr to 0.0014272219477265743
Epoch 77: reducing lr to 0.001130139834540861
Epoch 80: reducing lr to 0.0008598892550873156
Epoch 83: reducing lr to 0.0006207330186742555
Epoch 86: reducing lr to 0.0004164417774849452
Epoch 89: reducing lr to 0.0002502385363590334
Epoch 92: reducing lr to 0.00012474359656465956
Epoch 95: reducing lr to 4.1936458011069405e-05
Epoch 98: reducing lr to 3.1228183471031234e-06
[I 2024-06-22 13:54:22,909] Trial 343 finished with value: 1.8999005556106567 and parameters: {'hidden_size': 91, 'n_layers': 1, 'rnn_dropout': 0.689081979586423, 'bidirectional': True, 'fc_dropout': 0.7704258710277405, 'learning_rate_model': 0.0566300668124204}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:54:36,120] Trial 344 finished with value: 4.135727882385254 and parameters: {'hidden_size': 64, 'n_layers': 2, 'rnn_dropout': 0.4711269626448618, 'bidirectional': True, 'fc_dropout': 0.4691435371714093, 'learning_rate_model': 4.456357235021669e-05}. Best is trial 120 with value: 1.8680797815322876.
[I 2024-06-22 13:54:51,954] Trial 345 finished with value: 9.070703506469727 and parameters: {'hidden_size': 51, 'n_layers': 6, 'rnn_dropout': 0.12007876908163997, 'bidirectional': False, 'fc_dropout': 0.012071446106383289, 'learning_rate_model': 1.345171370168811e-05}. Best is trial 120 with value: 1.8680797815322876.
Epoch 31: reducing lr to 0.0003499777421030881
Epoch 43: reducing lr to 0.00030429381586822787
Epoch 60: reducing lr to 0.0001907338392678766
Epoch 63: reducing lr to 0.00016829513394231964
Epoch 66: reducing lr to 0.00014602030819028423
Epoch 72: reducing lr to 0.00010335915719908187
Epoch 75: reducing lr to 8.364565791848466e-05
Epoch 78: reducing lr to 6.54310054303258e-05
Epoch 81: reducing lr to 4.900240178054372e-05
Epoch 84: reducing lr to 3.4618984265726404e-05
Epoch 87: reducing lr to 2.2507529150722063e-05
Epoch 90: reducing lr to 1.2859113048622855e-05
Epoch 93: reducing lr to 5.825848899120804e-06
Epoch 96: reducing lr to 1.518676201198643e-06
Epoch 99: reducing lr to 5.508894573673373e-09
[I 2024-06-22 13:55:16,201] Trial 346 finished with value: 1.8583240509033203 and parameters: {'hidden_size': 69, 'n_layers': 3, 'rnn_dropout': 0.28977288442400895, 'bidirectional': True, 'fc_dropout': 0.28671619570026013, 'learning_rate_model': 0.003573654255997655}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 13:55:31,174] Trial 347 finished with value: 2.592957019805908 and parameters: {'hidden_size': 42, 'n_layers': 7, 'rnn_dropout': 0.5164515266187463, 'bidirectional': False, 'fc_dropout': 0.5963986645985317, 'learning_rate_model': 0.0004107962905784676}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 13:57:06,501] Trial 348 finished with value: 2.0355281829833984 and parameters: {'hidden_size': 105, 'n_layers': 7, 'rnn_dropout': 0.10995483969938472, 'bidirectional': True, 'fc_dropout': 0.07955220580390786, 'learning_rate_model': 7.815445226932655e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 7: reducing lr to 0.00023517666756004457
Epoch 11: reducing lr to 0.0004418541054212301
Epoch 14: reducing lr to 0.0006053312105908824
Epoch 27: reducing lr to 0.000911137058885181
Epoch 30: reducing lr to 0.0009006402253472648
Epoch 33: reducing lr to 0.0008831507839177843
Epoch 36: reducing lr to 0.000858944552877489
Epoch 39: reducing lr to 0.0008284032726826107
Epoch 45: reducing lr to 0.0007503345055737131
Epoch 48: reducing lr to 0.0007040382281751606
Epoch 51: reducing lr to 0.0006538498744884042
Epoch 54: reducing lr to 0.0006005609851545929
Epoch 57: reducing lr to 0.0005450117941125182
Epoch 60: reducing lr to 0.0004880785916167222
Epoch 63: reducing lr to 0.00043065903913962276
Epoch 66: reducing lr to 0.00037365884649791594
Epoch 69: reducing lr to 0.0003179766905418614
Epoch 72: reducing lr to 0.00026449104191505487
Epoch 75: reducing lr to 0.00021404515878470026
Epoch 78: reducing lr to 0.00016743475148973274
Epoch 81: reducing lr to 0.0001253947560573886
Epoch 84: reducing lr to 8.858829219017868e-05
Epoch 87: reducing lr to 5.759566928880532e-05
Epoch 90: reducing lr to 3.2905843086382565e-05
Epoch 93: reducing lr to 1.4908063176252705e-05
Epoch 96: reducing lr to 3.8862183252226515e-06
Epoch 99: reducing lr to 1.4096992516886286e-08
[I 2024-06-22 13:58:56,402] Trial 349 finished with value: 1.8961865901947021 and parameters: {'hidden_size': 165, 'n_layers': 4, 'rnn_dropout': 0.5226009138090556, 'bidirectional': True, 'fc_dropout': 0.41667867244303136, 'learning_rate_model': 0.009144806935610207}. Best is trial 346 with value: 1.8583240509033203.
Epoch 13: reducing lr to 0.0006412603394048477
Epoch 16: reducing lr to 0.0008199881669419061
Epoch 19: reducing lr to 0.0009611723433687226
Epoch 23: reducing lr to 0.001057866198737807
Epoch 34: reducing lr to 0.0010178889319129437
Epoch 38: reducing lr to 0.0009754075662025588
Epoch 46: reducing lr to 0.000854677537201963
Epoch 49: reducing lr to 0.0007992649973090158
Epoch 52: reducing lr to 0.0007396283966375762
Epoch 55: reducing lr to 0.0006767081438963093
Epoch 62: reducing lr to 0.0005227646750772374
Epoch 65: reducing lr to 0.00045623847229269777
Epoch 69: reducing lr to 0.0003695600090025682
Epoch 72: reducing lr to 0.0003073977267473894
Epoch 77: reducing lr to 0.00021210423214398105
Epoch 80: reducing lr to 0.0001613837019144207
Epoch 83: reducing lr to 0.00011649894665097596
Epoch 86: reducing lr to 7.815764098077667e-05
Epoch 89: reducing lr to 4.696467727715297e-05
Epoch 92: reducing lr to 2.3411832726854577e-05
Epoch 95: reducing lr to 7.870619151204364e-06
Epoch 98: reducing lr to 5.860894089328508e-07
[I 2024-06-22 14:00:29,764] Trial 350 finished with value: 2.4590020179748535 and parameters: {'hidden_size': 101, 'n_layers': 7, 'rnn_dropout': 0.5283295477365123, 'bidirectional': True, 'fc_dropout': 0.2992373734936557, 'learning_rate_model': 0.01062831029435455}. Best is trial 346 with value: 1.8583240509033203.
Epoch 38: reducing lr to 3.309951853079579e-05
Epoch 58: reducing lr to 2.0750555162038895e-05
Epoch 63: reducing lr to 1.6984736263885192e-05
Epoch 66: reducing lr to 1.4736709052046816e-05
Epoch 71: reducing lr to 1.1122673534046752e-05
Epoch 81: reducing lr to 4.945436335816625e-06
Epoch 84: reducing lr to 3.4938283936271126e-06
Epoch 87: reducing lr to 2.2715121799525356e-06
[I 2024-06-22 14:02:05,143] Trial 351 finished with value: 2.5491762161254883 and parameters: {'hidden_size': 189, 'n_layers': 7, 'rnn_dropout': 0.20928863414633742, 'bidirectional': False, 'fc_dropout': 0.4374675646526467, 'learning_rate_model': 0.00036066149754058905}. Best is trial 346 with value: 1.8583240509033203.
Epoch 61: reducing lr to 4.5253108533891945e-05
Epoch 64: reducing lr to 3.971595991560637e-05
Epoch 67: reducing lr to 3.424830477684747e-05
Epoch 86: reducing lr to 6.489414468178795e-06
[I 2024-06-22 14:02:24,750] Trial 352 finished with value: 1.8879050016403198 and parameters: {'hidden_size': 34, 'n_layers': 5, 'rnn_dropout': 0.07050813114034708, 'bidirectional': True, 'fc_dropout': 0.5667606332122982, 'learning_rate_model': 0.0008824666370552557}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 1.94990334724209e-05
Epoch 48: reducing lr to 1.8295926516873633e-05
Epoch 51: reducing lr to 1.6991675704476967e-05
Epoch 54: reducing lr to 1.5606850897528194e-05
Epoch 62: reducing lr to 1.1688928851278106e-05
Epoch 65: reducing lr to 1.0201414319085795e-05
Epoch 68: reducing lr to 8.740406838605359e-06
Epoch 71: reducing lr to 7.32895602348123e-06
Epoch 74: reducing lr to 5.9893149165483995e-06
Epoch 77: reducing lr to 4.742614405266887e-06
Epoch 80: reducing lr to 3.6085120119388817e-06
Epoch 83: reducing lr to 2.6048965501559703e-06
Epoch 86: reducing lr to 1.7475915037163855e-06
Epoch 89: reducing lr to 1.0501221627776786e-06
Epoch 92: reducing lr to 5.234845812445051e-07
Epoch 95: reducing lr to 1.759856999908072e-07
Epoch 98: reducing lr to 1.3104859085001489e-08
[I 2024-06-22 14:03:28,167] Trial 353 finished with value: 1.8838515281677246 and parameters: {'hidden_size': 145, 'n_layers': 3, 'rnn_dropout': 0.637996787434679, 'bidirectional': True, 'fc_dropout': 0.2542540402804782, 'learning_rate_model': 0.0002376472029630957}. Best is trial 346 with value: 1.8583240509033203.
Epoch 51: reducing lr to 4.548562112821694e-05
Epoch 54: reducing lr to 4.177853434093607e-05
Epoch 66: reducing lr to 2.599389460204323e-05
Epoch 72: reducing lr to 1.8399543677772606e-05
Epoch 75: reducing lr to 1.489023302853379e-05
Epoch 78: reducing lr to 1.1647740509116214e-05
Epoch 86: reducing lr to 4.678189862344787e-06
Epoch 89: reducing lr to 2.8111093729186455e-06
[I 2024-06-22 14:04:07,566] Trial 354 finished with value: 1.8737696409225464 and parameters: {'hidden_size': 83, 'n_layers': 4, 'rnn_dropout': 0.03334995401623022, 'bidirectional': True, 'fc_dropout': 0.3331700449384865, 'learning_rate_model': 0.0006361662512963186}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:04:19,541] Trial 355 finished with value: 5.922945022583008 and parameters: {'hidden_size': 80, 'n_layers': 3, 'rnn_dropout': 0.1442448486782075, 'bidirectional': False, 'fc_dropout': 0.4766935564933209, 'learning_rate_model': 3.8194352935715024e-05}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:04:28,945] Trial 356 finished with value: 2.9286446571350098 and parameters: {'hidden_size': 58, 'n_layers': 3, 'rnn_dropout': 0.6646010935235407, 'bidirectional': False, 'fc_dropout': 0.45451177415375393, 'learning_rate_model': 0.00023641790929480275}. Best is trial 346 with value: 1.8583240509033203.
Epoch 10: reducing lr to 0.00011164485098361576
Epoch 23: reducing lr to 0.000262403520556504
Epoch 26: reducing lr to 0.0002632226613750217
Epoch 31: reducing lr to 0.00025818501595440313
Epoch 39: reducing lr to 0.00023881984209290466
Epoch 47: reducing lr to 0.00020755109185259082
Epoch 71: reducing lr to 8.130411945517156e-05
Epoch 74: reducing lr to 6.64427476259285e-05
Epoch 78: reducing lr to 4.826965589133179e-05
Epoch 81: reducing lr to 3.6149972879667095e-05
Epoch 84: reducing lr to 2.5539061287900523e-05
Epoch 87: reducing lr to 1.6604218136711353e-05
Epoch 90: reducing lr to 9.486404157211343e-06
Epoch 93: reducing lr to 4.297835862157205e-06
Epoch 96: reducing lr to 1.1203553599719011e-06
Epoch 99: reducing lr to 4.0640128279572505e-09
[I 2024-06-22 14:05:29,927] Trial 357 finished with value: 2.461681842803955 and parameters: {'hidden_size': 80, 'n_layers': 7, 'rnn_dropout': 0.4426751595676166, 'bidirectional': True, 'fc_dropout': 0.5173199853374325, 'learning_rate_model': 0.0026363504591914843}. Best is trial 346 with value: 1.8583240509033203.
Epoch 16: reducing lr to 0.0018938857185504154
Epoch 19: reducing lr to 0.0022199717600322718
Epoch 23: reducing lr to 0.0024433007288368453
Epoch 26: reducing lr to 0.0024509279411343773
Epoch 29: reducing lr to 0.002429121410127675
Epoch 39: reducing lr to 0.002223707566913706
Epoch 42: reducing lr to 0.0021260122678827464
Epoch 47: reducing lr to 0.0019325568990798694
Epoch 52: reducing lr to 0.0017082827702872062
Epoch 71: reducing lr to 0.0007570417268067207
Epoch 74: reducing lr to 0.0006186640078458729
Epoch 77: reducing lr to 0.0004898865523873433
Epoch 81: reducing lr to 0.00033660087676035366
Epoch 84: reducing lr to 0.00023780019005156423
Epoch 87: reducing lr to 0.00015460576972882862
Epoch 90: reducing lr to 8.833013422304414e-05
Epoch 93: reducing lr to 4.0018157805808e-05
Epoch 96: reducing lr to 1.0431891545395824e-05
Epoch 99: reducing lr to 3.784097668893538e-08
[I 2024-06-22 14:05:37,372] Trial 358 finished with value: 2.0068061351776123 and parameters: {'hidden_size': 42, 'n_layers': 3, 'rnn_dropout': 0.3683471947671468, 'bidirectional': False, 'fc_dropout': 0.1825133074343378, 'learning_rate_model': 0.02454767750352977}. Best is trial 346 with value: 1.8583240509033203.
Epoch 26: reducing lr to 0.0005977889590531411
Epoch 29: reducing lr to 0.0005924702782170887
Epoch 32: reducing lr to 0.0005825291641270204
Epoch 35: reducing lr to 0.000568122393945214
Epoch 41: reducing lr to 0.0005268875325745362
Epoch 44: reducing lr to 0.0005007097422684489
Epoch 47: reducing lr to 0.0004713566472612104
Epoch 50: reducing lr to 0.0004392911540952938
Epoch 53: reducing lr to 0.0004050189807666715
Epoch 56: reducing lr to 0.00036908051417944775
Epoch 59: reducing lr to 0.0003320426847864838
Epoch 62: reducing lr to 0.0002944894654478839
Epoch 65: reducing lr to 0.0002570132035076485
Epoch 68: reducing lr to 0.0002202047570352416
Epoch 71: reducing lr to 0.00018464483522029825
Epoch 74: reducing lr to 0.00015089407854343437
Epoch 77: reducing lr to 0.00011948485603792214
Epoch 80: reducing lr to 9.091241695272645e-05
Epoch 83: reducing lr to 6.562744990261356e-05
Epoch 86: reducing lr to 4.4028609832322525e-05
Epoch 89: reducing lr to 2.6456651272846032e-05
Epoch 92: reducing lr to 1.318860748168952e-05
Epoch 95: reducing lr to 4.433762526589221e-06
Epoch 98: reducing lr to 3.3016224119542666e-07
[I 2024-06-22 14:05:56,325] Trial 359 finished with value: 1.8842086791992188 and parameters: {'hidden_size': 79, 'n_layers': 2, 'rnn_dropout': 0.3265451222818258, 'bidirectional': True, 'fc_dropout': 0.14458337088239182, 'learning_rate_model': 0.005987255004818895}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:06:45,953] Trial 360 finished with value: 3.704425096511841 and parameters: {'hidden_size': 155, 'n_layers': 5, 'rnn_dropout': 0.7033588484292764, 'bidirectional': False, 'fc_dropout': 0.7338081272640069, 'learning_rate_model': 4.0082853962425134e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 8: reducing lr to 0.00022684475011881515
Epoch 16: reducing lr to 0.0005653229101788762
Epoch 19: reducing lr to 0.0006626592531976778
Epoch 22: reducing lr to 0.0007204412809153876
Epoch 27: reducing lr to 0.0007300664582968676
Epoch 30: reducing lr to 0.0007216556643228692
Epoch 34: reducing lr to 0.0007017612648898889
Epoch 39: reducing lr to 0.0006637743876524089
Epoch 42: reducing lr to 0.0006346124428644993
Epoch 45: reducing lr to 0.0006012202551528129
Epoch 48: reducing lr to 0.0005641244538756202
Epoch 51: reducing lr to 0.0005239100500529147
Epoch 59: reducing lr to 0.0004063678928376447
Epoch 62: reducing lr to 0.00036040867340262
Epoch 77: reducing lr to 0.00014623062455166677
Epoch 92: reducing lr to 1.6140776103058286e-05
Epoch 95: reducing lr to 5.426226258925613e-06
Epoch 98: reducing lr to 4.0406652637272127e-07
[I 2024-06-22 14:07:14,489] Trial 361 finished with value: 2.4404616355895996 and parameters: {'hidden_size': 61, 'n_layers': 4, 'rnn_dropout': 0.27529546958079804, 'bidirectional': True, 'fc_dropout': 0.12861131385876787, 'learning_rate_model': 0.007327456112320097}. Best is trial 346 with value: 1.8583240509033203.
Epoch 39: reducing lr to 0.00013403807973897376
[I 2024-06-22 14:08:53,767] Trial 362 finished with value: 2.149289608001709 and parameters: {'hidden_size': 195, 'n_layers': 7, 'rnn_dropout': 0.7208535947382511, 'bidirectional': False, 'fc_dropout': 0.057811805451898124, 'learning_rate_model': 0.0014796565895539}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:09:42,742] Trial 363 finished with value: 5.779305458068848 and parameters: {'hidden_size': 179, 'n_layers': 4, 'rnn_dropout': 0.625372228316752, 'bidirectional': False, 'fc_dropout': 0.1499398165802812, 'learning_rate_model': 1.655985402893686e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 81: reducing lr to 4.998772601134987e-06
Epoch 84: reducing lr to 3.531509145238398e-06
Epoch 87: reducing lr to 2.2960103168361154e-06
Epoch 90: reducing lr to 1.3117679878269882e-06
Epoch 93: reducing lr to 5.942993158927269e-07
[I 2024-06-22 14:10:03,508] Trial 364 finished with value: 2.575653076171875 and parameters: {'hidden_size': 93, 'n_layers': 4, 'rnn_dropout': 0.7765936076171157, 'bidirectional': False, 'fc_dropout': 0.08408565526563959, 'learning_rate_model': 0.0003645512124245168}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:10:35,821] Trial 365 finished with value: 3.30148983001709 and parameters: {'hidden_size': 69, 'n_layers': 4, 'rnn_dropout': 0.2786053454005976, 'bidirectional': True, 'fc_dropout': 0.6888090948911805, 'learning_rate_model': 3.2961258390009616e-05}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:10:49,887] Trial 366 finished with value: 2.8518569469451904 and parameters: {'hidden_size': 122, 'n_layers': 2, 'rnn_dropout': 0.6945908571527113, 'bidirectional': False, 'fc_dropout': 0.3008456604378525, 'learning_rate_model': 0.0004058594143691851}. Best is trial 346 with value: 1.8583240509033203.
Epoch 56: reducing lr to 0.0006011940109303983
Epoch 60: reducing lr to 0.0005205188983045832
Epoch 66: reducing lr to 0.0003984942065920206
Epoch 69: reducing lr to 0.0003391111175336304
Epoch 72: reducing lr to 0.00028207052739811025
Epoch 75: reducing lr to 0.00022827174178852993
Epoch 79: reducing lr to 0.0001630404525298817
Epoch 82: reducing lr to 0.00011999226138205543
Epoch 85: reducing lr to 8.274208642937708e-05
Epoch 91: reducing lr to 2.7884643947999507e-05
Epoch 94: reducing lr to 1.114257196915252e-05
Epoch 97: reducing lr to 1.915099831015579e-06
[I 2024-06-22 14:10:54,510] Trial 367 finished with value: 1.9944806098937988 and parameters: {'hidden_size': 67, 'n_layers': 1, 'rnn_dropout': 0.09498432409007487, 'bidirectional': False, 'fc_dropout': 0.16592525185545917, 'learning_rate_model': 0.009752619584408855}. Best is trial 346 with value: 1.8583240509033203.
Epoch 72: reducing lr to 0.0002379557228654904
Epoch 79: reducing lr to 0.00013754151876813406
Epoch 82: reducing lr to 0.0001012259081401042
Epoch 85: reducing lr to 6.980152506295915e-05
Epoch 91: reducing lr to 2.3523587057105132e-05
Epoch 94: reducing lr to 9.399914241157991e-06
Epoch 97: reducing lr to 1.6155851830831903e-06
[I 2024-06-22 14:10:59,241] Trial 368 finished with value: 2.002747058868408 and parameters: {'hidden_size': 63, 'n_layers': 1, 'rnn_dropout': 0.012841767333649657, 'bidirectional': False, 'fc_dropout': 0.6047208780302441, 'learning_rate_model': 0.008227345353826203}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:11:36,412] Trial 369 finished with value: 6.980639457702637 and parameters: {'hidden_size': 81, 'n_layers': 4, 'rnn_dropout': 0.6512590027005323, 'bidirectional': True, 'fc_dropout': 0.0034120147256857082, 'learning_rate_model': 1.5407187680771952e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 91: reducing lr to 3.671846539803547e-06
Epoch 98: reducing lr to 7.081746085020082e-08
[I 2024-06-22 14:11:54,452] Trial 370 finished with value: 2.0669426918029785 and parameters: {'hidden_size': 152, 'n_layers': 2, 'rnn_dropout': 0.38518864974932177, 'bidirectional': False, 'fc_dropout': 0.4027020877092425, 'learning_rate_model': 0.0012842237663788143}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:12:09,373] Trial 371 finished with value: 8.120466232299805 and parameters: {'hidden_size': 69, 'n_layers': 4, 'rnn_dropout': 0.0015709163060160236, 'bidirectional': False, 'fc_dropout': 0.22716070903122879, 'learning_rate_model': 2.2070334240387426e-05}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:12:44,036] Trial 372 finished with value: 4.271686553955078 and parameters: {'hidden_size': 112, 'n_layers': 6, 'rnn_dropout': 0.7540787484865374, 'bidirectional': False, 'fc_dropout': 0.2326879502483765, 'learning_rate_model': 3.950372375988253e-05}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:12:55,773] Trial 373 finished with value: 8.382094383239746 and parameters: {'hidden_size': 40, 'n_layers': 6, 'rnn_dropout': 0.5875755482799475, 'bidirectional': False, 'fc_dropout': 0.47531546231323163, 'learning_rate_model': 3.170500198727212e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 15: reducing lr to 0.0020604636229598736
Epoch 19: reducing lr to 0.0025941918320177967
Epoch 22: reducing lr to 0.0028203980814882055
Epoch 27: reducing lr to 0.0028580789203571516
Epoch 33: reducing lr to 0.002770290829900702
Epoch 36: reducing lr to 0.0026943600816088783
Epoch 40: reducing lr to 0.0025624597472034944
Epoch 43: reducing lr to 0.002442562246029412
Epoch 48: reducing lr to 0.002208445808948922
Epoch 51: reducing lr to 0.0020510136484185883
Epoch 54: reducing lr to 0.0018838556453399276
Epoch 57: reducing lr to 0.0017096074678434475
Epoch 62: reducing lr to 0.0014109351559156993
Epoch 65: reducing lr to 0.0012313817875010957
Epoch 68: reducing lr to 0.001055028004918163
Epoch 71: reducing lr to 0.0008846560571338501
Epoch 74: reducing lr to 0.0007229520414682334
Epoch 77: reducing lr to 0.0005724666032689219
Epoch 80: reducing lr to 0.00043557254244318137
Epoch 83: reducing lr to 0.00031442916343328646
Epoch 86: reducing lr to 0.00021094647098509922
Epoch 89: reducing lr to 0.00012675706185920067
Epoch 92: reducing lr to 6.318823637778553e-05
Epoch 95: reducing lr to 2.1242700183628228e-05
Epoch 98: reducing lr to 1.5818478007356695e-06
[I 2024-06-22 14:13:48,166] Trial 374 finished with value: 2.4637584686279297 and parameters: {'hidden_size': 161, 'n_layers': 5, 'rnn_dropout': 0.49217735320676365, 'bidirectional': False, 'fc_dropout': 0.44991556476574257, 'learning_rate_model': 0.028685673223941464}. Best is trial 346 with value: 1.8583240509033203.
Epoch 20: reducing lr to 2.3832992151774316e-05
Epoch 28: reducing lr to 2.525742403044417e-05
Epoch 31: reducing lr to 2.4900104705525275e-05
Epoch 77: reducing lr to 5.074092620348529e-06
Epoch 82: reducing lr to 3.128277242895168e-06
Epoch 85: reducing lr to 2.157140660784265e-06
Epoch 88: reducing lr to 1.3524736392715145e-06
Epoch 91: reducing lr to 7.269710236647504e-07
Epoch 94: reducing lr to 2.904941861828626e-07
Epoch 97: reducing lr to 4.992791326903197e-08
[I 2024-06-22 14:14:47,517] Trial 375 finished with value: 1.9307156801223755 and parameters: {'hidden_size': 93, 'n_layers': 5, 'rnn_dropout': 0.7402089502870131, 'bidirectional': True, 'fc_dropout': 0.4959565550893039, 'learning_rate_model': 0.0002542572125328951}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:15:06,947] Trial 376 finished with value: 4.40562629699707 and parameters: {'hidden_size': 75, 'n_layers': 5, 'rnn_dropout': 0.12825858559070966, 'bidirectional': False, 'fc_dropout': 0.30713139358283037, 'learning_rate_model': 5.699678680247873e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 4: reducing lr to 0.001276243556836168
Epoch 7: reducing lr to 0.0025687559272085875
Epoch 12: reducing lr to 0.0054282652714996715
Epoch 15: reducing lr to 0.0071746885792315735
Epoch 18: reducing lr to 0.008642937680690554
Epoch 21: reducing lr to 0.00962680133669124
Epoch 28: reducing lr to 0.009922450116770117
Epoch 31: reducing lr to 0.009782076214309116
Epoch 37: reducing lr to 0.009278216557293906
Epoch 43: reducing lr to 0.008505184588250664
Epoch 46: reducing lr to 0.008032323939191552
Epoch 49: reducing lr to 0.007511552711050159
Epoch 52: reducing lr to 0.006951083441209015
Epoch 55: reducing lr to 0.00635975416162104
Epoch 58: reducing lr to 0.0057468927066138655
Epoch 61: reducing lr to 0.005122161556077396
Epoch 64: reducing lr to 0.0044954163290256925
Epoch 67: reducing lr to 0.003876537010875321
Epoch 70: reducing lr to 0.003275287830968564
Epoch 73: reducing lr to 0.0027011489689605637
Epoch 76: reducing lr to 0.002163174150027315
Epoch 79: reducing lr to 0.0016698491434516364
Epoch 82: reducing lr to 0.0012289525193321395
Epoch 85: reducing lr to 0.0008474387798094076
Epoch 88: reducing lr to 0.0005313230756922194
Epoch 91: reducing lr to 0.0002855926126890909
Epoch 94: reducing lr to 0.00011412145863081147
Epoch 97: reducing lr to 1.961432124864687e-05
[I 2024-06-22 14:16:28,284] Trial 377 finished with value: 2.464219808578491 and parameters: {'hidden_size': 102, 'n_layers': 6, 'rnn_dropout': 0.6741341780760262, 'bidirectional': True, 'fc_dropout': 0.21363224794263616, 'learning_rate_model': 0.09988566154433381}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:16:38,166] Trial 378 finished with value: 6.497772693634033 and parameters: {'hidden_size': 38, 'n_layers': 4, 'rnn_dropout': 0.6320209204821483, 'bidirectional': False, 'fc_dropout': 0.39046435641705934, 'learning_rate_model': 5.986319998853214e-05}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:17:31,941] Trial 379 finished with value: 2.196939468383789 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.5112422874358202, 'bidirectional': True, 'fc_dropout': 0.5030732548777529, 'learning_rate_model': 2.9913111272232188e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 10: reducing lr to 0.0017375061874398267
Epoch 14: reducing lr to 0.0027158721952902478
Epoch 17: reducing lr to 0.0033675355032533306
Epoch 24: reducing lr to 0.004102706819225368
Epoch 27: reducing lr to 0.004087897271824676
Epoch 34: reducing lr to 0.003929406600746882
Epoch 37: reducing lr to 0.0038111159155549646
Epoch 40: reducing lr to 0.0036650745139132565
Epoch 43: reducing lr to 0.003493585663672942
Epoch 48: reducing lr to 0.0031587299892498386
Epoch 54: reducing lr to 0.002694470155545694
Epoch 58: reducing lr to 0.0023605909739135077
Epoch 61: reducing lr to 0.002103976697231123
Epoch 67: reducing lr to 0.00159232453868201
Epoch 77: reducing lr to 0.0008187963771907773
Epoch 83: reducing lr to 0.00044972660139854583
Epoch 86: reducing lr to 0.0003017157773702317
Epoch 89: reducing lr to 0.00018130004866835013
Epoch 92: reducing lr to 9.037784690280118e-05
Epoch 95: reducing lr to 3.0383337390834513e-05
Epoch 98: reducing lr to 2.2625097099347736e-06
[I 2024-06-22 14:17:48,943] Trial 380 finished with value: 2.5422885417938232 and parameters: {'hidden_size': 72, 'n_layers': 5, 'rnn_dropout': 0.31637097304600914, 'bidirectional': False, 'fc_dropout': 0.15217767144974684, 'learning_rate_model': 0.04102898785522371}. Best is trial 346 with value: 1.8583240509033203.
Epoch 40: reducing lr to 1.8254401572784755e-05
[I 2024-06-22 14:19:03,734] Trial 381 finished with value: 1.954703688621521 and parameters: {'hidden_size': 86, 'n_layers': 7, 'rnn_dropout': 0.49390585041997215, 'bidirectional': True, 'fc_dropout': 0.42491575551464705, 'learning_rate_model': 0.00020435044842635042}. Best is trial 346 with value: 1.8583240509033203.
Epoch 12: reducing lr to 0.0017269476498776332
Epoch 29: reducing lr to 0.0031445613554318174
Epoch 36: reducing lr to 0.0029847767212940165
Epoch 52: reducing lr to 0.00221141683622673
Epoch 55: reducing lr to 0.002023291411507728
Epoch 59: reducing lr to 0.0017623307587268087
Epoch 62: reducing lr to 0.0015630154400587098
Epoch 65: reducing lr to 0.0013641085760756964
Epoch 68: reducing lr to 0.0011687461712662516
Epoch 71: reducing lr to 0.0009800103645048613
Epoch 74: reducing lr to 0.000800876779134085
Epoch 77: reducing lr to 0.000634170986579875
Epoch 80: reducing lr to 0.00048252154342449255
Epoch 83: reducing lr to 0.0003483204987772902
Epoch 86: reducing lr to 0.00023368373081725547
Epoch 89: reducing lr to 0.0001404198087996647
Epoch 92: reducing lr to 6.999909859391091e-05
Epoch 95: reducing lr to 2.3532384345473224e-05
Epoch 98: reducing lr to 1.752350223896415e-06
[I 2024-06-22 14:19:11,913] Trial 382 finished with value: 2.0740554332733154 and parameters: {'hidden_size': 39, 'n_layers': 3, 'rnn_dropout': 0.1719811846333421, 'bidirectional': False, 'fc_dropout': 0.14446084213348556, 'learning_rate_model': 0.03177761215284233}. Best is trial 346 with value: 1.8583240509033203.
Epoch 8: reducing lr to 0.002151757859378438
Epoch 45: reducing lr to 0.00570293299080089
Epoch 51: reducing lr to 0.004969599548670385
Epoch 58: reducing lr to 0.003998967086965846
Epoch 64: reducing lr to 0.003128129035938595
Epoch 67: reducing lr to 0.0026974827457722644
Epoch 70: reducing lr to 0.002279104362138057
Epoch 73: reducing lr to 0.001879590654517295
Epoch 76: reducing lr to 0.0015052416446506977
Epoch 79: reducing lr to 0.001161962142981396
Epoch 82: reducing lr to 0.0008551648564096278
Epoch 85: reducing lr to 0.0005896890653232817
Epoch 88: reducing lr to 0.00036972040382681344
Epoch 91: reducing lr to 0.00019872921189391518
Epoch 94: reducing lr to 7.941125409491879e-05
Epoch 97: reducing lr to 1.3648597444014532e-05
[I 2024-06-22 14:19:21,549] Trial 383 finished with value: 1.8994204998016357 and parameters: {'hidden_size': 31, 'n_layers': 2, 'rnn_dropout': 0.5632245564290674, 'bidirectional': True, 'fc_dropout': 0.01704090516910526, 'learning_rate_model': 0.0695052950120162}. Best is trial 346 with value: 1.8583240509033203.
Epoch 20: reducing lr to 0.0007526544639916506
Epoch 26: reducing lr to 0.0008016974220025261
Epoch 29: reducing lr to 0.0007945645155643221
Epoch 32: reducing lr to 0.000781232443405505
Epoch 35: reducing lr to 0.0007619114600731408
Epoch 38: reducing lr to 0.0007369062638387571
Epoch 41: reducing lr to 0.0007066112047625283
Epoch 44: reducing lr to 0.0006715040541798972
Epoch 47: reducing lr to 0.0006321384883916432
Epoch 50: reducing lr to 0.0005891353134131811
Epoch 54: reducing lr to 0.0005273184020860679
Epoch 57: reducing lr to 0.00047854382068374747
Epoch 64: reducing lr to 0.0003613741820678479
Epoch 67: reducing lr to 0.0003116241720518079
Epoch 71: reducing lr to 0.00024762800674114677
Epoch 74: reducing lr to 0.00020236471740014885
Epoch 77: reducing lr to 0.0001602416699125247
Epoch 80: reducing lr to 0.00012192304524068741
Epoch 83: reducing lr to 8.801326388306235e-05
Epoch 86: reducing lr to 5.90469637526213e-05
Epoch 89: reducing lr to 3.5481132260883724e-05
Epoch 92: reducing lr to 1.7687299937123487e-05
Epoch 95: reducing lr to 5.946138571993886e-06
Epoch 98: reducing lr to 4.4278204473398306e-07
[I 2024-06-22 14:20:24,342] Trial 384 finished with value: 1.879946231842041 and parameters: {'hidden_size': 142, 'n_layers': 3, 'rnn_dropout': 0.37789875377219184, 'bidirectional': True, 'fc_dropout': 0.6995902372969417, 'learning_rate_model': 0.008029534218627702}. Best is trial 346 with value: 1.8583240509033203.
Epoch 26: reducing lr to 0.0017245349876961094
Epoch 31: reducing lr to 0.0016915302466222158
Epoch 42: reducing lr to 0.001495916089045856
Epoch 45: reducing lr to 0.0014172036222355941
Epoch 52: reducing lr to 0.0012019910323741348
Epoch 55: reducing lr to 0.0010997375495528472
Epoch 58: reducing lr to 0.0009937606929610837
Epoch 61: reducing lr to 0.0008857313120824387
Epoch 68: reducing lr to 0.0006352589859904667
Epoch 71: reducing lr to 0.0005326737367969357
Epoch 74: reducing lr to 0.00043530766827224837
Epoch 77: reducing lr to 0.0003446965883471794
Epoch 80: reducing lr to 0.0002622692197248434
Epoch 83: reducing lr to 0.0001893257341012042
Epoch 86: reducing lr to 0.00012701619353379726
Epoch 89: reducing lr to 7.632362573167205e-05
Epoch 92: reducing lr to 3.804723171399738e-05
Epoch 95: reducing lr to 1.2790765852132777e-05
Epoch 98: reducing lr to 9.524704796478461e-07
[I 2024-06-22 14:21:02,983] Trial 385 finished with value: 2.026852607727051 and parameters: {'hidden_size': 192, 'n_layers': 3, 'rnn_dropout': 0.7497392709774249, 'bidirectional': False, 'fc_dropout': 0.58931776380391, 'learning_rate_model': 0.017272367747345682}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:21:21,560] Trial 386 finished with value: 4.596188545227051 and parameters: {'hidden_size': 89, 'n_layers': 4, 'rnn_dropout': 0.6538216208902006, 'bidirectional': False, 'fc_dropout': 0.783678297134273, 'learning_rate_model': 5.1958110052086076e-05}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:21:38,727] Trial 387 finished with value: 2.594438076019287 and parameters: {'hidden_size': 58, 'n_layers': 6, 'rnn_dropout': 0.5979061877233948, 'bidirectional': False, 'fc_dropout': 0.20159236968859082, 'learning_rate_model': 0.00022800773332305063}. Best is trial 346 with value: 1.8583240509033203.
Epoch 4: reducing lr to 0.0011977214924544376
Epoch 7: reducing lr to 0.0024107108446561223
Epoch 10: reducing lr to 0.003969730160354668
Epoch 13: reducing lr to 0.005655820085979698
Epoch 22: reducing lr to 0.009216601163041153
Epoch 30: reducing lr to 0.009232136762988384
Epoch 34: reducing lr to 0.008977627825467429
Epoch 37: reducing lr to 0.008707365708365394
Epoch 40: reducing lr to 0.008373700734422597
Epoch 43: reducing lr to 0.007981895245679783
Epoch 47: reducing lr to 0.00737984619875291
Epoch 51: reducing lr to 0.006702378256991837
Epoch 54: reducing lr to 0.006156133152196737
Epoch 57: reducing lr to 0.005586718513209162
Epoch 64: reducing lr to 0.0042188316845668365
Epoch 67: reducing lr to 0.003638029488454827
Epoch 70: reducing lr to 0.0030737727200366256
Epoch 77: reducing lr to 0.0018707275388254896
Epoch 80: reducing lr to 0.001423380063835647
Epoch 83: reducing lr to 0.0010275032494222055
Epoch 86: reducing lr to 0.0006893386797351727
Epoch 89: reducing lr to 0.00041422141485032563
Epoch 92: reducing lr to 0.0002064888558507033
Epoch 95: reducing lr to 6.941768132080612e-05
Epoch 98: reducing lr to 5.169220747845704e-06
[I 2024-06-22 14:22:19,812] Trial 388 finished with value: 1.931275725364685 and parameters: {'hidden_size': 107, 'n_layers': 3, 'rnn_dropout': 0.43635127147344355, 'bidirectional': True, 'fc_dropout': 0.38826996317470447, 'learning_rate_model': 0.09374010390011783}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:22:27,580] Trial 389 finished with value: 1.914236068725586 and parameters: {'hidden_size': 75, 'n_layers': 1, 'rnn_dropout': 0.6538813834272106, 'bidirectional': True, 'fc_dropout': 0.712244021779393, 'learning_rate_model': 0.0004150090046603508}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:23:06,703] Trial 390 finished with value: 3.739534854888916 and parameters: {'hidden_size': 187, 'n_layers': 3, 'rnn_dropout': 0.06146582781974468, 'bidirectional': False, 'fc_dropout': 0.6005826338841801, 'learning_rate_model': 5.482676347013862e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 7: reducing lr to 0.00212192316912811
Epoch 10: reducing lr to 0.0034941819841710587
Epoch 13: reducing lr to 0.0049782891662282225
Epoch 18: reducing lr to 0.007139506529106861
Epoch 21: reducing lr to 0.007952227996653832
Epoch 24: reducing lr to 0.008250678102733089
Epoch 27: reducing lr to 0.008220895616722217
Epoch 30: reducing lr to 0.008126186075517844
Epoch 33: reducing lr to 0.007968384490142252
Epoch 37: reducing lr to 0.0076642792227069045
Epoch 40: reducing lr to 0.007370585169558549
Epoch 46: reducing lr to 0.006635108492784772
Epoch 49: reducing lr to 0.006204924946304613
Epoch 52: reducing lr to 0.005741948796385769
Epoch 57: reducing lr to 0.004917465518045767
Epoch 60: reducing lr to 0.0044037756068744235
Epoch 63: reducing lr to 0.0038856975167891605
Epoch 66: reducing lr to 0.0033714031751520476
Epoch 69: reducing lr to 0.0028690010531388574
Epoch 72: reducing lr to 0.002386417307844102
Epoch 76: reducing lr to 0.0017868919733413217
Epoch 79: reducing lr to 0.00137938040313903
Epoch 82: reducing lr to 0.0010151773459314243
Epoch 89: reducing lr to 0.0003646003498380207
Epoch 92: reducing lr to 0.0001817528171691049
Epoch 95: reducing lr to 6.11018889587242e-05
Epoch 98: reducing lr to 4.5499813034435845e-06
[I 2024-06-22 14:24:34,335] Trial 391 finished with value: 2.4940576553344727 and parameters: {'hidden_size': 95, 'n_layers': 7, 'rnn_dropout': 0.42700887309822655, 'bidirectional': True, 'fc_dropout': 0.5646478409288389, 'learning_rate_model': 0.08251064153258493}. Best is trial 346 with value: 1.8583240509033203.
Epoch 8: reducing lr to 0.00018722578095284234
Epoch 15: reducing lr to 0.00043440023212150766
Epoch 22: reducing lr to 0.0005946145166656946
Epoch 31: reducing lr to 0.0005922676825899631
Epoch 34: reducing lr to 0.0005791970093760007
Epoch 39: reducing lr to 0.0005478446295963418
Epoch 42: reducing lr to 0.0005237758870569604
Epoch 45: reducing lr to 0.0004962157234703232
Epoch 48: reducing lr to 0.0004655987911386025
Epoch 51: reducing lr to 0.00043240792753115545
Epoch 54: reducing lr to 0.0003971665990606105
Epoch 57: reducing lr to 0.00036043047428375747
Epoch 60: reducing lr to 0.0003227790667367558
Epoch 63: reducing lr to 0.0002848060232979759
Epoch 66: reducing lr to 0.00024711031342518276
Epoch 69: reducing lr to 0.00021028625549252174
Epoch 72: reducing lr to 0.00017491480498414182
Epoch 75: reducing lr to 0.00014155363045773706
Epoch 78: reducing lr to 0.00011072895585552853
Epoch 81: reducing lr to 8.292681348677352e-05
Epoch 84: reducing lr to 5.8585741657367176e-05
Epoch 87: reducing lr to 3.808951406686217e-05
Epoch 90: reducing lr to 2.176148985848043e-05
Epoch 93: reducing lr to 9.859089912024262e-06
Epoch 96: reducing lr to 2.57005725245096e-06
Epoch 99: reducing lr to 9.322707790471173e-09
[I 2024-06-22 14:27:15,003] Trial 392 finished with value: 2.4581098556518555 and parameters: {'hidden_size': 161, 'n_layers': 6, 'rnn_dropout': 0.5403580125424159, 'bidirectional': True, 'fc_dropout': 0.2117453053734832, 'learning_rate_model': 0.006047698667517102}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:29:43,576] Trial 393 finished with value: 1.8967323303222656 and parameters: {'hidden_size': 176, 'n_layers': 5, 'rnn_dropout': 0.1410209023867453, 'bidirectional': True, 'fc_dropout': 0.06934301291208511, 'learning_rate_model': 5.915618388982257e-05}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:30:27,240] Trial 394 finished with value: 3.016000986099243 and parameters: {'hidden_size': 66, 'n_layers': 6, 'rnn_dropout': 0.6874986626406856, 'bidirectional': True, 'fc_dropout': 0.33207191009709, 'learning_rate_model': 3.559891823841408e-05}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:33:50,820] Trial 395 finished with value: 2.034130811691284 and parameters: {'hidden_size': 185, 'n_layers': 6, 'rnn_dropout': 0.40817106704486206, 'bidirectional': True, 'fc_dropout': 0.25494028504026184, 'learning_rate_model': 3.0376041777025878e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 58: reducing lr to 2.0452691413384554e-05
Epoch 63: reducing lr to 1.6740928945287693e-05
Epoch 71: reducing lr to 1.0963013167948665e-05
Epoch 81: reducing lr to 4.874447092675189e-06
Epoch 84: reducing lr to 3.443676250016776e-06
Epoch 87: reducing lr to 2.238905768810705e-06
Epoch 90: reducing lr to 1.2791427345736496e-06
Epoch 93: reducing lr to 5.795183745454624e-07
Epoch 96: reducing lr to 1.510682440995564e-07
Epoch 99: reducing lr to 5.479897752474865e-10
[I 2024-06-22 14:34:30,348] Trial 396 finished with value: 2.5611867904663086 and parameters: {'hidden_size': 160, 'n_layers': 4, 'rnn_dropout': 0.1818219051321271, 'bidirectional': False, 'fc_dropout': 0.2470982018530048, 'learning_rate_model': 0.0003554843837326049}. Best is trial 346 with value: 1.8583240509033203.
Epoch 16: reducing lr to 0.0012539307731200306
Epoch 21: reducing lr to 0.0015664230310923366
Epoch 30: reducing lr to 0.0016006891438461238
Epoch 42: reducing lr to 0.001407620417967641
Epoch 45: reducing lr to 0.001333553913675014
Epoch 48: reducing lr to 0.0012512725025779488
Epoch 54: reducing lr to 0.001067364550349538
Epoch 57: reducing lr to 0.0009686381282466402
Epoch 60: reducing lr to 0.0008674519313673308
Epoch 64: reducing lr to 0.0007314707581318667
Epoch 70: reducing lr to 0.0005329377965172579
Epoch 73: reducing lr to 0.0004395169077879195
Epoch 77: reducing lr to 0.00032435105104775924
Epoch 80: reducing lr to 0.0002467889150952333
Epoch 83: reducing lr to 0.00017815088086762225
Epoch 86: reducing lr to 0.00011951912860616483
Epoch 89: reducing lr to 7.181866331937604e-05
Epoch 92: reducing lr to 3.580151360089261e-05
Epoch 95: reducing lr to 1.2035797533529666e-05
Epoch 98: reducing lr to 8.962514037261921e-07
[I 2024-06-22 14:34:49,687] Trial 397 finished with value: 2.5587520599365234 and parameters: {'hidden_size': 71, 'n_layers': 5, 'rnn_dropout': 0.028058895444063127, 'bidirectional': False, 'fc_dropout': 0.7542463880602278, 'learning_rate_model': 0.016252875201921996}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:35:03,417] Trial 398 finished with value: 2.133563995361328 and parameters: {'hidden_size': 60, 'n_layers': 2, 'rnn_dropout': 0.1522694357737513, 'bidirectional': True, 'fc_dropout': 0.606544498730502, 'learning_rate_model': 0.0001107130781541933}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:35:14,183] Trial 399 finished with value: 8.584210395812988 and parameters: {'hidden_size': 22, 'n_layers': 6, 'rnn_dropout': 0.5809614478361608, 'bidirectional': False, 'fc_dropout': 0.7586922321512182, 'learning_rate_model': 4.485346455877421e-05}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:35:22,758] Trial 400 finished with value: 8.08047866821289 and parameters: {'hidden_size': 32, 'n_layers': 5, 'rnn_dropout': 0.7447918922767776, 'bidirectional': False, 'fc_dropout': 0.5471993712935661, 'learning_rate_model': 4.8676465808827524e-05}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:35:32,318] Trial 401 finished with value: 9.162498474121094 and parameters: {'hidden_size': 18, 'n_layers': 7, 'rnn_dropout': 0.028185016522687612, 'bidirectional': False, 'fc_dropout': 0.2286263466846098, 'learning_rate_model': 1.093529205055685e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 7: reducing lr to 0.0009399753331809252
Epoch 11: reducing lr to 0.0017660423726118119
Epoch 14: reducing lr to 0.002419442422853037
Epoch 17: reducing lr to 0.0029999785229820517
Epoch 23: reducing lr to 0.0036380049147208167
Epoch 29: reducing lr to 0.0036168923146456256
Epoch 32: reducing lr to 0.0035562041409543063
Epoch 35: reducing lr to 0.0034682541825087143
Epoch 38: reducing lr to 0.0033544294391244533
Epoch 41: reducing lr to 0.003216525009467506
Epoch 44: reducing lr to 0.0030567157294857017
Epoch 59: reducing lr to 0.002027042838929526
Epoch 62: reducing lr to 0.0017977892283944582
Epoch 69: reducing lr to 0.0012709179389981038
Epoch 72: reducing lr to 0.0010571416706718923
Epoch 78: reducing lr to 0.0006692183282911941
Epoch 83: reducing lr to 0.00040064021421776577
Epoch 86: reducing lr to 0.00026878435321055563
Epoch 89: reducing lr to 0.00016151166088529748
Epoch 92: reducing lr to 8.051336040847332e-05
Epoch 95: reducing lr to 2.7067082007290604e-05
Epoch 98: reducing lr to 2.0155631711336886e-06
[I 2024-06-22 14:36:20,932] Trial 402 finished with value: 1.910062313079834 and parameters: {'hidden_size': 122, 'n_layers': 3, 'rnn_dropout': 0.12121124948169851, 'bidirectional': True, 'fc_dropout': 0.5817883529146217, 'learning_rate_model': 0.036550789818385214}. Best is trial 346 with value: 1.8583240509033203.
Epoch 47: reducing lr to 0.0001896647208824707
Epoch 60: reducing lr to 0.00012858191619513311
Epoch 68: reducing lr to 8.860609906056238e-05
Epoch 71: reducing lr to 7.429748013087929e-05
Epoch 74: reducing lr to 6.071683396436378e-05
Epoch 77: reducing lr to 4.8078375475961246e-05
Epoch 80: reducing lr to 3.6581383303446255e-05
Epoch 84: reducing lr to 2.3338152006495938e-05
Epoch 87: reducing lr to 1.5173297187989245e-05
Epoch 90: reducing lr to 8.668883312517879e-06
Epoch 97: reducing lr to 4.7308024123432666e-07
[I 2024-06-22 14:36:44,163] Trial 403 finished with value: 2.017507791519165 and parameters: {'hidden_size': 174, 'n_layers': 2, 'rnn_dropout': 0.18944682695539142, 'bidirectional': False, 'fc_dropout': 0.7096969413258452, 'learning_rate_model': 0.002409154630446646}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:36:49,642] Trial 404 finished with value: 8.208477020263672 and parameters: {'hidden_size': 36, 'n_layers': 2, 'rnn_dropout': 0.4615682967460868, 'bidirectional': False, 'fc_dropout': 0.1367898791544147, 'learning_rate_model': 5.7328011282134295e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 8: reducing lr to 0.00046323620578926145
Epoch 14: reducing lr to 0.000990479552300076
Epoch 17: reducing lr to 0.0012281413917051075
Epoch 23: reducing lr to 0.0014893388018504742
Epoch 26: reducing lr to 0.0014939880466571394
Epoch 29: reducing lr to 0.001480695653961174
Epoch 34: reducing lr to 0.0014330559801238342
Epoch 37: reducing lr to 0.0013899153253046051
Epoch 40: reducing lr to 0.001336653974359519
Epoch 43: reducing lr to 0.0012741119298903282
Epoch 46: reducing lr to 0.0012032754432873
Epoch 54: reducing lr to 0.000982674220818994
Epoch 58: reducing lr to 0.0008609083649297331
Epoch 61: reducing lr to 0.0007673210472632576
Epoch 67: reducing lr to 0.0005807213236783513
Epoch 77: reducing lr to 0.000298615328994954
Epoch 83: reducing lr to 0.00016401545094174105
Epoch 86: reducing lr to 0.00011003585095417171
Epoch 89: reducing lr to 6.612019201360786e-05
Epoch 92: reducing lr to 3.296083280110492e-05
Epoch 95: reducing lr to 1.1080813916223252e-05
Epoch 98: reducing lr to 8.251380931904861e-07
[I 2024-06-22 14:37:42,033] Trial 405 finished with value: 2.462552309036255 and parameters: {'hidden_size': 101, 'n_layers': 4, 'rnn_dropout': 0.40926391173178606, 'bidirectional': True, 'fc_dropout': 0.6049359107101884, 'learning_rate_model': 0.014963286414081123}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:38:50,464] Trial 406 finished with value: 7.45658016204834 and parameters: {'hidden_size': 124, 'n_layers': 4, 'rnn_dropout': 0.7147413098802236, 'bidirectional': True, 'fc_dropout': 0.7703870024702832, 'learning_rate_model': 1.0699403522342655e-05}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:40:14,656] Trial 407 finished with value: 3.388932704925537 and parameters: {'hidden_size': 122, 'n_layers': 5, 'rnn_dropout': 0.6675037318324135, 'bidirectional': True, 'fc_dropout': 0.4653498154951399, 'learning_rate_model': 1.2571394105090453e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 13: reducing lr to 0.003114576711034009
Epoch 17: reducing lr to 0.004236919924296768
Epoch 22: reducing lr to 0.005075446336855062
Epoch 26: reducing lr to 0.005154054544774056
Epoch 29: reducing lr to 0.0051081976069365315
Epoch 32: reducing lr to 0.005022486682570866
Epoch 35: reducing lr to 0.004898273482901426
Epoch 43: reducing lr to 0.00439551199723162
Epoch 51: reducing lr to 0.0036909008614886877
Epoch 57: reducing lr to 0.00307652349399826
Epoch 62: reducing lr to 0.0025390478442155716
Epoch 67: reducing lr to 0.002003409186739298
Epoch 73: reducing lr to 0.0013959641411872038
Epoch 77: reducing lr to 0.0010301820667102377
Epoch 80: reducing lr to 0.0007838344095778792
Epoch 85: reducing lr to 0.00043795960980286193
Epoch 88: reducing lr to 0.000274589802182236
Epoch 91: reducing lr to 0.00014759535696965003
Epoch 94: reducing lr to 5.8978407270109114e-05
Epoch 97: reducing lr to 1.0136756406801256e-05
[I 2024-06-22 14:40:43,547] Trial 408 finished with value: 2.5396952629089355 and parameters: {'hidden_size': 82, 'n_layers': 7, 'rnn_dropout': 0.6950230956440161, 'bidirectional': False, 'fc_dropout': 0.6487975355372214, 'learning_rate_model': 0.05162129276724368}. Best is trial 346 with value: 1.8583240509033203.
Epoch 87: reducing lr to 5.666036128853215e-07
[I 2024-06-22 14:41:08,251] Trial 409 finished with value: 2.2213430404663086 and parameters: {'hidden_size': 57, 'n_layers': 4, 'rnn_dropout': 0.6619552731288607, 'bidirectional': True, 'fc_dropout': 0.5215806566165936, 'learning_rate_model': 8.996302522110986e-05}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:41:17,886] Trial 410 finished with value: 7.967759132385254 and parameters: {'hidden_size': 20, 'n_layers': 6, 'rnn_dropout': 0.2614334602647021, 'bidirectional': False, 'fc_dropout': 0.5257320338908137, 'learning_rate_model': 6.480307324345786e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 26: reducing lr to 0.00029867616666880137
Epoch 29: reducing lr to 0.0002960187686359519
Epoch 32: reducing lr to 0.00029105184209127005
Epoch 35: reducing lr to 0.0002838537183607884
Epoch 38: reducing lr to 0.0002745379142268151
Epoch 47: reducing lr to 0.00023550618392288112
Epoch 50: reducing lr to 0.00021948514767572416
Epoch 53: reducing lr to 0.0002023615772280276
Epoch 56: reducing lr to 0.00018440546868224802
Epoch 59: reducing lr to 0.00016590008022150177
Epoch 62: reducing lr to 0.00014713718500863006
Epoch 65: reducing lr to 0.0001284127403900563
Epoch 68: reducing lr to 0.00011002195961881934
Epoch 71: reducing lr to 9.225498521442019e-05
Epoch 74: reducing lr to 7.539193267095388e-05
Epoch 77: reducing lr to 5.9698792083591726e-05
Epoch 80: reducing lr to 4.542300721151705e-05
Epoch 83: reducing lr to 3.27897577703822e-05
Epoch 86: reducing lr to 2.1998225643550877e-05
Epoch 89: reducing lr to 1.3218663652776612e-05
Epoch 92: reducing lr to 6.5894872541136065e-06
Epoch 96: reducing lr to 1.2712562150813106e-06
Epoch 99: reducing lr to 4.611395411011077e-09
[I 2024-06-22 14:42:35,567] Trial 411 finished with value: 1.8707656860351562 and parameters: {'hidden_size': 168, 'n_layers': 3, 'rnn_dropout': 0.5053955268011132, 'bidirectional': True, 'fc_dropout': 0.2871289134310855, 'learning_rate_model': 0.0029914409535772893}. Best is trial 346 with value: 1.8583240509033203.
Epoch 42: reducing lr to 0.00016450139450643208
Epoch 48: reducing lr to 0.0001462298137723927
Epoch 51: reducing lr to 0.00013580561616570894
Epoch 54: reducing lr to 0.00012473743257627724
Epoch 57: reducing lr to 0.00011319978087468716
Epoch 61: reducing lr to 9.740120924062507e-05
Epoch 64: reducing lr to 8.548324407449404e-05
Epoch 67: reducing lr to 7.371485424494273e-05
Epoch 70: reducing lr to 6.228171287743385e-05
Epoch 73: reducing lr to 5.13640917092251e-05
Epoch 76: reducing lr to 4.113415317030234e-05
Epoch 79: reducing lr to 3.1753259642627655e-05
Epoch 82: reducing lr to 2.3369325659054688e-05
Epoch 85: reducing lr to 1.611459556813495e-05
Epoch 88: reducing lr to 1.010345134633006e-05
Epoch 91: reducing lr to 5.430727930301644e-06
Epoch 94: reducing lr to 2.170093221241006e-06
Epoch 97: reducing lr to 3.729789830204337e-07
[I 2024-06-22 14:43:31,337] Trial 412 finished with value: 1.8793336153030396 and parameters: {'hidden_size': 108, 'n_layers': 4, 'rnn_dropout': 0.5858165878050149, 'bidirectional': True, 'fc_dropout': 0.07058412080007663, 'learning_rate_model': 0.0018993903479429634}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:43:35,916] Trial 413 finished with value: 4.113468170166016 and parameters: {'hidden_size': 61, 'n_layers': 1, 'rnn_dropout': 0.6305121653149466, 'bidirectional': False, 'fc_dropout': 0.3905671544705639, 'learning_rate_model': 0.00018695778940416125}. Best is trial 346 with value: 1.8583240509033203.
Epoch 41: reducing lr to 0.0001152162866358777
Epoch 47: reducing lr to 0.00010307315930063545
Epoch 52: reducing lr to 9.111147112728142e-05
Epoch 55: reducing lr to 8.336061026658587e-05
Epoch 58: reducing lr to 7.53275159676635e-05
Epoch 61: reducing lr to 6.713883938712217e-05
Epoch 64: reducing lr to 5.8923763256666384e-05
Epoch 67: reducing lr to 5.0811789691129415e-05
Epoch 72: reducing lr to 3.7866933222766174e-05
Epoch 78: reducing lr to 2.3971475585431796e-05
Epoch 81: reducing lr to 1.795264905658006e-05
Epoch 84: reducing lr to 1.2683102309989613e-05
Epoch 87: reducing lr to 8.245917695693305e-06
Epoch 90: reducing lr to 4.7110985452242975e-06
Epoch 93: reducing lr to 2.1343733560444868e-06
Epoch 96: reducing lr to 5.563862153696404e-07
Epoch 99: reducing lr to 2.018253134061787e-09
[I 2024-06-22 14:43:43,425] Trial 414 finished with value: 1.8857200145721436 and parameters: {'hidden_size': 74, 'n_layers': 1, 'rnn_dropout': 0.4275198695236406, 'bidirectional': True, 'fc_dropout': 0.0868469644869741, 'learning_rate_model': 0.001309253391186895}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:44:24,613] Trial 415 finished with value: 8.770681381225586 and parameters: {'hidden_size': 138, 'n_layers': 5, 'rnn_dropout': 0.2505773561175088, 'bidirectional': False, 'fc_dropout': 0.18083533670023533, 'learning_rate_model': 1.0103208625366834e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 12: reducing lr to 0.0008222949276820244
Epoch 27: reducing lr to 0.001507574798111573
Epoch 33: reducing lr to 0.0014612684796247678
Epoch 37: reducing lr to 0.0014055006583881195
Epoch 45: reducing lr to 0.0012415095837946663
Epoch 50: reducing lr to 0.0011101825612282474
Epoch 53: reducing lr to 0.0010235694600762614
Epoch 72: reducing lr to 0.0004376290320199308
Epoch 77: reducing lr to 0.0003019637483421612
Epoch 83: reducing lr to 0.00016585458127380635
Epoch 86: reducing lr to 0.00011126969977720905
Epoch 90: reducing lr to 5.444627596241352e-05
Epoch 93: reducing lr to 2.466700274563836e-05
Epoch 96: reducing lr to 6.430168491042796e-06
Epoch 99: reducing lr to 2.3324998627179128e-08
[I 2024-06-22 14:45:26,680] Trial 416 finished with value: 2.4846320152282715 and parameters: {'hidden_size': 142, 'n_layers': 7, 'rnn_dropout': 0.3129214084280226, 'bidirectional': False, 'fc_dropout': 0.6815810064519052, 'learning_rate_model': 0.015131072032774386}. Best is trial 346 with value: 1.8583240509033203.
Epoch 7: reducing lr to 0.0013493985414987544
Epoch 13: reducing lr to 0.0031658526744998442
Epoch 17: reducing lr to 0.004306673271670186
Epoch 20: reducing lr to 0.004918422515462641
Epoch 25: reducing lr to 0.005245296192146935
Epoch 28: reducing lr to 0.005212382995925055
Epoch 33: reducing lr to 0.00506734954664613
Epoch 36: reducing lr to 0.0049284588429047965
Epoch 39: reducing lr to 0.004753218844064557
Epoch 42: reducing lr to 0.004544393212835098
Epoch 45: reducing lr to 0.004305275255245501
Epoch 52: reducing lr to 0.003651488161273615
Epoch 55: reducing lr to 0.0033408557423000363
Epoch 58: reducing lr to 0.0030189122112826063
Epoch 62: reducing lr to 0.002580848748985961
Epoch 66: reducing lr to 0.002143982682098598
Epoch 70: reducing lr to 0.00172054829160058
Epoch 73: reducing lr to 0.0014189462067916714
Epoch 76: reducing lr to 0.0011363415309864281
Epoch 81: reducing lr to 0.000719491022179021
Epoch 91: reducing lr to 0.00015002525189080975
Epoch 96: reducing lr to 2.2298374215470178e-05
Epoch 99: reducing lr to 8.088583505899694e-08
[I 2024-06-22 14:46:02,397] Trial 417 finished with value: 2.5340771675109863 and parameters: {'hidden_size': 142, 'n_layers': 4, 'rnn_dropout': 0.552347215883451, 'bidirectional': False, 'fc_dropout': 0.6222564887203075, 'learning_rate_model': 0.05247114549766932}. Best is trial 346 with value: 1.8583240509033203.
Epoch 9: reducing lr to 0.00025178007346472584
Epoch 13: reducing lr to 0.0004158407238028252
Epoch 16: reducing lr to 0.0005317410915625018
Epoch 19: reducing lr to 0.0006232953738206589
Epoch 23: reducing lr to 0.0006859988350097352
Epoch 40: reducing lr to 0.0006156712415485843
Epoch 47: reducing lr to 0.0005425986927077712
Epoch 50: reducing lr to 0.0005056867391499921
Epoch 53: reducing lr to 0.0004662345821635237
Epoch 59: reducing lr to 0.00038222846274719584
Epoch 63: reducing lr to 0.00032457561501688986
Epoch 66: reducing lr to 0.00028161617169549923
Epoch 71: reducing lr to 0.00021255252638933243
Epoch 74: reducing lr to 0.00017370059429678287
Epoch 77: reducing lr to 0.00013754410182026112
Epoch 80: reducing lr to 0.00010465315144291736
Epoch 83: reducing lr to 7.554655000583735e-05
Epoch 86: reducing lr to 5.068320617852614e-05
Epoch 89: reducing lr to 3.0455377000584463e-05
Epoch 92: reducing lr to 1.5181967242386234e-05
Epoch 95: reducing lr to 5.103892699259705e-06
Epoch 98: reducing lr to 3.800638041173698e-07
[I 2024-06-22 14:46:19,932] Trial 418 finished with value: 1.8928618431091309 and parameters: {'hidden_size': 51, 'n_layers': 3, 'rnn_dropout': 0.004968213752412876, 'bidirectional': True, 'fc_dropout': 0.5131398260428972, 'learning_rate_model': 0.0068921839914617405}. Best is trial 346 with value: 1.8583240509033203.
Epoch 72: reducing lr to 0.0003143782377531607
Epoch 79: reducing lr to 0.0001817147314950758
Epoch 82: reducing lr to 0.00013373589940527745
Epoch 86: reducing lr to 7.993247607414027e-05
Epoch 91: reducing lr to 3.10784870209916e-05
Epoch 94: reducing lr to 1.2418816570495227e-05
Epoch 97: reducing lr to 2.1344509671027135e-06
[I 2024-06-22 14:46:23,838] Trial 419 finished with value: 2.011168956756592 and parameters: {'hidden_size': 37, 'n_layers': 1, 'rnn_dropout': 0.7250680556642687, 'bidirectional': False, 'fc_dropout': 0.7705932519309364, 'learning_rate_model': 0.01086966223201368}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:46:31,690] Trial 420 finished with value: 1.970083236694336 and parameters: {'hidden_size': 76, 'n_layers': 1, 'rnn_dropout': 0.42881921335072887, 'bidirectional': True, 'fc_dropout': 0.6169218666030231, 'learning_rate_model': 0.00034644677219478504}. Best is trial 346 with value: 1.8583240509033203.
Epoch 72: reducing lr to 5.713628221294983e-07
[I 2024-06-22 14:48:12,332] Trial 421 finished with value: 2.493285894393921 and parameters: {'hidden_size': 121, 'n_layers': 6, 'rnn_dropout': 0.7526779359208366, 'bidirectional': True, 'fc_dropout': 0.08806120627761437, 'learning_rate_model': 1.9754932570600042e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 12: reducing lr to 8.422793474102656e-05
Epoch 23: reducing lr to 0.00015426405918914364
Epoch 60: reducing lr to 8.272055833527102e-05
Epoch 63: reducing lr to 7.298897509878079e-05
Epoch 66: reducing lr to 6.332846582521965e-05
Epoch 69: reducing lr to 5.389134010589799e-05
Epoch 72: reducing lr to 4.4826482942877984e-05
Epoch 75: reducing lr to 3.6276811455676866e-05
Epoch 79: reducing lr to 2.591029318711731e-05
Epoch 82: reducing lr to 1.9069099872772104e-05
Epoch 85: reducing lr to 1.3149323894976524e-05
Epoch 88: reducing lr to 8.244299625659595e-06
Epoch 91: reducing lr to 4.4314112779993834e-06
Epoch 94: reducing lr to 1.770771008663516e-06
Epoch 97: reducing lr to 3.043465430465318e-07
[I 2024-06-22 14:48:55,754] Trial 422 finished with value: 1.8911212682724 and parameters: {'hidden_size': 75, 'n_layers': 5, 'rnn_dropout': 0.5561216517821522, 'bidirectional': True, 'fc_dropout': 0.2444085290288249, 'learning_rate_model': 0.001549880590083267}. Best is trial 346 with value: 1.8583240509033203.
Epoch 7: reducing lr to 0.0005995311079746026
Epoch 15: reducing lr to 0.0016745261578641792
Epoch 18: reducing lr to 0.002017206052538711
Epoch 26: reducing lr to 0.002327620455174627
Epoch 32: reducing lr to 0.002268203146986018
Epoch 35: reducing lr to 0.002212107274922259
Epoch 41: reducing lr to 0.0020515504340185655
Epoch 44: reducing lr to 0.0019496215521532447
Epoch 47: reducing lr to 0.0018353289354582965
Epoch 50: reducing lr to 0.0017104750105606727
Epoch 53: reducing lr to 0.0015770288997302782
Epoch 56: reducing lr to 0.0014370947161205145
Epoch 59: reducing lr to 0.0012928799259261925
Epoch 66: reducing lr to 0.0009525609175842729
Epoch 69: reducing lr to 0.0008106115269363902
Epoch 74: reducing lr to 0.0005875386931514109
Epoch 78: reducing lr to 0.00042683801550365275
Epoch 81: reducing lr to 0.0003196663079431422
Epoch 96: reducing lr to 9.90705754327012e-06
Epoch 99: reducing lr to 3.59371770612682e-08
[I 2024-06-22 14:50:33,571] Trial 423 finished with value: 2.083752155303955 and parameters: {'hidden_size': 155, 'n_layers': 4, 'rnn_dropout': 0.22958700219765965, 'bidirectional': True, 'fc_dropout': 0.5221543314676185, 'learning_rate_model': 0.02331267081552816}. Best is trial 346 with value: 1.8583240509033203.
Epoch 36: reducing lr to 0.00018508278210387275
Epoch 39: reducing lr to 0.00017850183914481284
Epoch 42: reducing lr to 0.00017065962517194224
Epoch 45: reducing lr to 0.0001616798166248156
Epoch 48: reducing lr to 0.0001517040343775547
Epoch 51: reducing lr to 0.0001408895992682806
Epoch 56: reducing lr to 0.00012146993289269925
Epoch 59: reducing lr to 0.00010928022772536779
Epoch 62: reducing lr to 9.692089999682044e-05
Epoch 68: reducing lr to 7.247268829456241e-05
Epoch 71: reducing lr to 6.076938467764301e-05
Epoch 74: reducing lr to 4.96615044425375e-05
Epoch 77: reducing lr to 3.93242582228646e-05
Epoch 80: reducing lr to 2.9920639974484246e-05
Epoch 83: reducing lr to 2.1598978080196257e-05
Epoch 86: reducing lr to 1.4490475861564378e-05
Epoch 89: reducing lr to 8.707280745565604e-06
Epoch 92: reducing lr to 4.340568532344887e-06
Epoch 95: reducing lr to 1.4592177475538944e-06
Epoch 98: reducing lr to 1.0866134553559267e-07
[I 2024-06-22 14:51:06,231] Trial 424 finished with value: 2.5690603256225586 and parameters: {'hidden_size': 134, 'n_layers': 4, 'rnn_dropout': 0.24571368741668903, 'bidirectional': False, 'fc_dropout': 0.6219835449679528, 'learning_rate_model': 0.0019704954222894244}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:51:13,793] Trial 425 finished with value: 6.0948967933654785 and parameters: {'hidden_size': 42, 'n_layers': 3, 'rnn_dropout': 0.293487454872947, 'bidirectional': False, 'fc_dropout': 0.4885060134941166, 'learning_rate_model': 7.106542572380849e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 26: reducing lr to 0.00838985366695027
Epoch 29: reducing lr to 0.008315206999025234
Epoch 32: reducing lr to 0.008175685364777863
Epoch 37: reducing lr to 0.007805407958149069
Epoch 42: reducing lr to 0.007277623924521318
Epoch 45: reducing lr to 0.0068946881864734525
Epoch 48: reducing lr to 0.00646928005918303
Epoch 51: reducing lr to 0.006008108346177441
Epoch 54: reducing lr to 0.005518446371377023
Epoch 57: reducing lr to 0.005008014892615284
Epoch 60: reducing lr to 0.004484865982695796
Epoch 63: reducing lr to 0.0039572480906814225
Epoch 66: reducing lr to 0.003433483620416261
Epoch 69: reducing lr to 0.0029218303510866843
Epoch 72: reducing lr to 0.0024303603906972843
Epoch 75: reducing lr to 0.0019668222861699723
Epoch 78: reducing lr to 0.0015385276760245814
Epoch 81: reducing lr to 0.0011522297546126409
Epoch 84: reducing lr to 0.0008140218090549739
Epoch 87: reducing lr to 0.0005292361975728785
Epoch 90: reducing lr to 0.0003023658460437714
Epoch 93: reducing lr to 0.00013698749864357734
Epoch 96: reducing lr to 3.570975795186282e-05
Epoch 99: reducing lr to 1.295347169152916e-07
[I 2024-06-22 14:51:20,975] Trial 426 finished with value: 1.9120186567306519 and parameters: {'hidden_size': 62, 'n_layers': 1, 'rnn_dropout': 0.24607087133984198, 'bidirectional': True, 'fc_dropout': 0.796461769477117, 'learning_rate_model': 0.0840299784671679}. Best is trial 346 with value: 1.8583240509033203.
Epoch 7: reducing lr to 0.0005687541565421665
Epoch 13: reducing lr to 0.001334366247070368
Epoch 16: reducing lr to 0.0017062719549752186
Epoch 19: reducing lr to 0.0020000549757982706
Epoch 22: reducing lr to 0.00217445415832058
Epoch 26: reducing lr to 0.0022081319736777017
Epoch 29: reducing lr to 0.0021884856603190937
Epoch 32: reducing lr to 0.002151764855185745
Epoch 35: reducing lr to 0.0020985486667732633
Epoch 43: reducing lr to 0.0018831524768422024
Epoch 51: reducing lr to 0.0015812786095155743
Epoch 57: reducing lr to 0.0013180632521160105
Epoch 62: reducing lr to 0.0010877946049668017
Epoch 65: reducing lr to 0.0009493635901564425
Epoch 73: reducing lr to 0.0005980676043462647
Epoch 77: reducing lr to 0.00044135698224590165
Epoch 80: reducing lr to 0.000335815193033347
Epoch 85: reducing lr to 0.00018763336887182744
Epoch 88: reducing lr to 0.00011764146393429577
Epoch 91: reducing lr to 6.323371707843382e-05
Epoch 94: reducing lr to 2.5267894570840666e-05
Epoch 97: reducing lr to 4.342851969608291e-06
[I 2024-06-22 14:52:41,309] Trial 427 finished with value: 2.4661664962768555 and parameters: {'hidden_size': 96, 'n_layers': 7, 'rnn_dropout': 0.17151832334065126, 'bidirectional': True, 'fc_dropout': 0.774087118010037, 'learning_rate_model': 0.022115914003569243}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:52:54,173] Trial 428 finished with value: 5.530417442321777 and parameters: {'hidden_size': 55, 'n_layers': 4, 'rnn_dropout': 0.13478157731715773, 'bidirectional': False, 'fc_dropout': 0.028703319172813394, 'learning_rate_model': 5.826872431714904e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 8: reducing lr to 0.00014786818160317176
Epoch 15: reducing lr to 0.0003430829455478786
Epoch 22: reducing lr to 0.00046961784262153085
Epoch 31: reducing lr to 0.00046776434741624087
Epoch 34: reducing lr to 0.00045744132101121425
Epoch 39: reducing lr to 0.0004326796703274448
Epoch 42: reducing lr to 0.0004136705297344105
Epoch 45: reducing lr to 0.00039190391589788735
Epoch 48: reducing lr to 0.00036772311084466014
Epoch 51: reducing lr to 0.00034150945254132927
Epoch 56: reducing lr to 0.0002944371372895025
Epoch 59: reducing lr to 0.00026488989207086415
Epoch 62: reducing lr to 0.00023493149011446558
Epoch 65: reducing lr to 0.00020503448158090345
Epoch 68: reducing lr to 0.0001756702285492735
Epoch 71: reducing lr to 0.00014730199674297496
Epoch 74: reducing lr to 0.00012037704190111972
Epoch 77: reducing lr to 9.532006597386816e-05
Epoch 80: reducing lr to 7.25261582859283e-05
Epoch 83: reducing lr to 5.235485953490578e-05
Epoch 86: reducing lr to 3.512420011304775e-05
Epoch 89: reducing lr to 2.1106019862257053e-05
Epoch 92: reducing lr to 1.052132443344206e-05
Epoch 95: reducing lr to 3.5370719818485914e-06
Epoch 98: reducing lr to 2.633897521108798e-07
[I 2024-06-22 14:57:19,375] Trial 429 finished with value: 2.4597928524017334 and parameters: {'hidden_size': 196, 'n_layers': 7, 'rnn_dropout': 0.4452733272816815, 'bidirectional': True, 'fc_dropout': 0.7616728685702719, 'learning_rate_model': 0.004776383894881025}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:57:33,422] Trial 430 finished with value: 4.290194034576416 and parameters: {'hidden_size': 139, 'n_layers': 1, 'rnn_dropout': 0.6823758798797543, 'bidirectional': True, 'fc_dropout': 0.592625450415668, 'learning_rate_model': 4.229506516204067e-05}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:57:41,890] Trial 431 finished with value: 5.177665710449219 and parameters: {'hidden_size': 148, 'n_layers': 1, 'rnn_dropout': 0.5655595584309073, 'bidirectional': False, 'fc_dropout': 0.3932457478252316, 'learning_rate_model': 6.569729148584553e-05}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 14:58:10,377] Trial 432 finished with value: 2.884726047515869 and parameters: {'hidden_size': 150, 'n_layers': 3, 'rnn_dropout': 0.7070540719980095, 'bidirectional': False, 'fc_dropout': 0.7766001184425293, 'learning_rate_model': 0.0001651737299816249}. Best is trial 346 with value: 1.8583240509033203.
Epoch 66: reducing lr to 4.30828907334142e-06
Epoch 69: reducing lr to 3.6662734317101425e-06
Epoch 72: reducing lr to 3.049583534711452e-06
Epoch 75: reducing lr to 2.46794215482035e-06
Epoch 80: reducing lr to 1.6010303177467402e-06
Epoch 83: reducing lr to 1.1557446220478968e-06
Epoch 86: reducing lr to 7.753741628764007e-07
Epoch 89: reducing lr to 4.659198623649521e-07
Epoch 92: reducing lr to 2.3226046710457873e-07
Epoch 95: reducing lr to 7.808161376294723e-08
Epoch 98: reducing lr to 5.814384609354329e-09
[I 2024-06-22 15:01:22,422] Trial 433 finished with value: 1.9017908573150635 and parameters: {'hidden_size': 175, 'n_layers': 6, 'rnn_dropout': 0.33969016807568225, 'bidirectional': True, 'fc_dropout': 0.7945894275724366, 'learning_rate_model': 0.00010543968694375883}. Best is trial 346 with value: 1.8583240509033203.
Epoch 26: reducing lr to 0.0004935511777160136
Epoch 29: reducing lr to 0.0004891599270065875
Epoch 32: reducing lr to 0.0004809522669408445
Epoch 37: reducing lr to 0.00045916990250662645
Epoch 40: reducing lr to 0.0004415745793415504
Epoch 43: reducing lr to 0.00042091330311942515
Epoch 46: reducing lr to 0.0003975118900583122
Epoch 49: reducing lr to 0.0003717394290926369
Epoch 52: reducing lr to 0.00034400235070028066
Epoch 57: reducing lr to 0.00029460724184097493
Epoch 60: reducing lr to 0.0002638318825961851
Epoch 63: reducing lr to 0.00023279362587264526
Epoch 66: reducing lr to 0.0002019820549672446
Epoch 71: reducing lr to 0.00015244790741285042
Epoch 74: reducing lr to 0.0001245823447349155
Epoch 77: reducing lr to 9.865001774230238e-05
Epoch 80: reducing lr to 7.505981797840243e-05
Epoch 83: reducing lr to 5.41838464886305e-05
Epoch 86: reducing lr to 3.635124387436186e-05
Epoch 89: reducing lr to 2.184334654627548e-05
Epoch 92: reducing lr to 1.0888880860784617e-05
Epoch 95: reducing lr to 3.6606375604149136e-06
Epoch 98: reducing lr to 2.7259112185254107e-07
[I 2024-06-22 15:02:56,632] Trial 434 finished with value: 1.8815443515777588 and parameters: {'hidden_size': 186, 'n_layers': 3, 'rnn_dropout': 0.748341609873127, 'bidirectional': True, 'fc_dropout': 0.4594266834807066, 'learning_rate_model': 0.004943244123469622}. Best is trial 346 with value: 1.8583240509033203.
Epoch 23: reducing lr to 0.0006384338442648017
Epoch 27: reducing lr to 0.0006390848995880725
Epoch 30: reducing lr to 0.0006317222665548163
Epoch 33: reducing lr to 0.0006194549157640506
Epoch 36: reducing lr to 0.0006024763102041792
Epoch 40: reducing lr to 0.0005729825438254104
Epoch 43: reducing lr to 0.0005461726884526538
Epoch 46: reducing lr to 0.0005158072602505597
Epoch 51: reducing lr to 0.0004586198940194286
Epoch 55: reducing lr to 0.0004084007809952714
Epoch 58: reducing lr to 0.00036904499922979647
Epoch 62: reducing lr to 0.00031549421047162227
Epoch 68: reducing lr to 0.00023591107361775463
Epoch 72: reducing lr to 0.00018551789691338686
Epoch 78: reducing lr to 0.00011744119098206296
Epoch 81: reducing lr to 8.795372145422274e-05
Epoch 84: reducing lr to 6.213712774268097e-05
Epoch 87: reducing lr to 4.039844737429611e-05
Epoch 90: reducing lr to 2.3080641073311687e-05
Epoch 95: reducing lr to 4.750004913138366e-06
Epoch 98: reducing lr to 3.5371138133941107e-07
[I 2024-06-22 15:03:05,734] Trial 435 finished with value: 1.8715091943740845 and parameters: {'hidden_size': 91, 'n_layers': 1, 'rnn_dropout': 0.26026506381516834, 'bidirectional': True, 'fc_dropout': 0.08433579715145374, 'learning_rate_model': 0.006414301740012138}. Best is trial 346 with value: 1.8583240509033203.
Epoch 15: reducing lr to 0.0017192750316406568
Epoch 18: reducing lr to 0.0020711124657662805
Epoch 26: reducing lr to 0.0023898221672583654
Epoch 34: reducing lr to 0.002292353647597865
Epoch 37: reducing lr to 0.0022233447332175105
Epoch 40: reducing lr to 0.002138146489877144
Epoch 43: reducing lr to 0.002038102607603411
Epoch 49: reducing lr to 0.0017999979904834266
Epoch 52: reducing lr to 0.0016656923950560422
Epoch 58: reducing lr to 0.0013771314295926188
Epoch 61: reducing lr to 0.0012274267202182464
Epoch 67: reducing lr to 0.0009289369452663634
Epoch 70: reducing lr to 0.0007848592349389248
Epoch 77: reducing lr to 0.0004776728530807756
Epoch 83: reducing lr to 0.0002623633845735874
Epoch 86: reducing lr to 0.0001760162113691701
Epoch 89: reducing lr to 0.000105767580223326
Epoch 92: reducing lr to 5.272500610405094e-05
Epoch 95: reducing lr to 1.772515836890934e-05
Epoch 98: reducing lr to 1.3199123718346529e-06
[I 2024-06-22 15:03:31,641] Trial 436 finished with value: 2.553828716278076 and parameters: {'hidden_size': 111, 'n_layers': 4, 'rnn_dropout': 0.29888777514112486, 'bidirectional': False, 'fc_dropout': 0.2215384451751942, 'learning_rate_model': 0.023935662435466326}. Best is trial 346 with value: 1.8583240509033203.
Epoch 37: reducing lr to 0.0002777685341836027
Epoch 40: reducing lr to 0.00026712448478626786
Epoch 43: reducing lr to 0.0002546257291421097
Epoch 46: reducing lr to 0.0002404693653030914
Epoch 49: reducing lr to 0.0002248786685573777
Epoch 56: reducing lr to 0.00018433807743023552
Epoch 59: reducing lr to 0.00016583945179114666
Epoch 62: reducing lr to 0.0001470834135061568
Epoch 68: reducing lr to 0.00010998175193050854
Epoch 71: reducing lr to 9.22212704932548e-05
Epoch 74: reducing lr to 7.536438057734926e-05
[I 2024-06-22 15:03:52,517] Trial 437 finished with value: 2.489217758178711 and parameters: {'hidden_size': 93, 'n_layers': 4, 'rnn_dropout': 0.5836987028545443, 'bidirectional': False, 'fc_dropout': 0.5616555713830786, 'learning_rate_model': 0.0029903477270443474}. Best is trial 346 with value: 1.8583240509033203.
Epoch 23: reducing lr to 0.00016954401771878606
Epoch 31: reducing lr to 0.00016681835985608463
Epoch 37: reducing lr to 0.00015822580345604604
Epoch 40: reducing lr to 0.00015216261392714882
Epoch 49: reducing lr to 0.0001280980515564726
Epoch 56: reducing lr to 0.0001050048397118396
Epoch 62: reducing lr to 8.378339665243241e-05
Epoch 71: reducing lr to 5.2532172740226736e-05
Epoch 79: reducing lr to 2.8476725105679798e-05
Epoch 82: reducing lr to 2.0957907005069256e-05
Epoch 85: reducing lr to 1.4451773246200515e-05
Epoch 88: reducing lr to 9.060903033142656e-06
Epoch 91: reducing lr to 4.870345537291753e-06
Epoch 94: reducing lr to 1.946167064751335e-06
Epoch 97: reducing lr to 3.3449227226456466e-07
[I 2024-06-22 15:04:28,959] Trial 438 finished with value: 2.469001293182373 and parameters: {'hidden_size': 66, 'n_layers': 5, 'rnn_dropout': 0.2057717987627643, 'bidirectional': True, 'fc_dropout': 0.1332301148892321, 'learning_rate_model': 0.0017033973020565553}. Best is trial 346 with value: 1.8583240509033203.
Epoch 31: reducing lr to 9.332896018743814e-05
Epoch 37: reducing lr to 8.852172940744874e-05
Epoch 41: reducing lr to 8.386460424268317e-05
Epoch 44: reducing lr to 7.969788954886493e-05
Epoch 49: reducing lr to 7.16663199669168e-05
Epoch 52: reducing lr to 6.631898745536152e-05
Epoch 55: reducing lr to 6.0677225360766115e-05
Epoch 61: reducing lr to 4.8869585706303617e-05
Epoch 64: reducing lr to 4.2889926678743434e-05
Epoch 67: reducing lr to 3.698531481730699e-05
Epoch 72: reducing lr to 2.7562903312859437e-05
Epoch 75: reducing lr to 2.2305882170720612e-05
Epoch 78: reducing lr to 1.7448560197385895e-05
Epoch 81: reducing lr to 1.3067525887169439e-05
Epoch 84: reducing lr to 9.231883675944827e-06
Epoch 87: reducing lr to 6.002108246662722e-06
Epoch 90: reducing lr to 3.429154215776344e-06
Epoch 93: reducing lr to 1.553585712899192e-06
Epoch 96: reducing lr to 4.0498709965826937e-07
Epoch 99: reducing lr to 1.4690631445699043e-09
[I 2024-06-22 15:05:58,713] Trial 439 finished with value: 1.8874785900115967 and parameters: {'hidden_size': 147, 'n_layers': 4, 'rnn_dropout': 0.7022476011839601, 'bidirectional': True, 'fc_dropout': 0.6143895835862447, 'learning_rate_model': 0.0009529904209834922}. Best is trial 346 with value: 1.8583240509033203.
Epoch 20: reducing lr to 0.00020664210930257046
Epoch 27: reducing lr to 0.00021964570113148715
Epoch 30: reducing lr to 0.00021711525377495324
Epoch 35: reducing lr to 0.00020918362773845396
Epoch 41: reducing lr to 0.00019400088193800863
Epoch 44: reducing lr to 0.00018436217520726862
Epoch 50: reducing lr to 0.0001617477470108796
Epoch 60: reducing lr to 0.00011765997598000603
Epoch 66: reducing lr to 9.007707295259958e-05
Epoch 72: reducing lr to 6.376024306980873e-05
Epoch 75: reducing lr to 5.159937082637154e-05
Epoch 78: reducing lr to 4.0363107861879984e-05
Epoch 81: reducing lr to 3.022862350274319e-05
Epoch 84: reducing lr to 2.1355774480252918e-05
Epoch 87: reducing lr to 1.3884454637983369e-05
Epoch 90: reducing lr to 7.932535402384528e-06
Epoch 93: reducing lr to 3.5938522716516455e-06
Epoch 96: reducing lr to 9.36841653480713e-07
Epoch 99: reducing lr to 3.3983293457485323e-09
[I 2024-06-22 15:06:36,162] Trial 440 finished with value: 1.868295669555664 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.08763337377301479, 'bidirectional': True, 'fc_dropout': 0.2283068259596248, 'learning_rate_model': 0.0022045174340091364}. Best is trial 346 with value: 1.8583240509033203.
Epoch 4: reducing lr to 0.0006167120403959258
Epoch 14: reducing lr to 0.003194997631945311
Epoch 19: reducing lr to 0.004365048324223663
Epoch 22: reducing lr to 0.0047456682914878865
Epoch 26: reducing lr to 0.004819168916854169
Epoch 29: reducing lr to 0.0047762915418615454
Epoch 32: reducing lr to 0.004696149700336669
Epoch 37: reducing lr to 0.004483460726312228
Epoch 41: reducing lr to 0.004247586010425917
Epoch 44: reducing lr to 0.0040365496715232315
Epoch 47: reducing lr to 0.0037999151185926948
Epoch 50: reducing lr to 0.0035414141449153806
Epoch 53: reducing lr to 0.00326512367498107
Epoch 56: reducing lr to 0.0029754001220889618
Epoch 59: reducing lr to 0.0026768138844959514
Epoch 62: reducing lr to 0.0023740727504826353
Epoch 65: reducing lr to 0.002071952020537517
Epoch 68: reducing lr to 0.0017752149891301756
Epoch 71: reducing lr to 0.0014885431339527612
Epoch 74: reducing lr to 0.0012164561456700091
Epoch 77: reducing lr to 0.0009632458002650432
Epoch 80: reducing lr to 0.0007329046267910713
Epoch 83: reducing lr to 0.0005290659217996104
Epoch 86: reducing lr to 0.0003549435042967508
Epoch 89: reducing lr to 0.00021328442007376714
Epoch 92: reducing lr to 0.00010632201593856888
Epoch 95: reducing lr to 3.574346804045281e-05
Epoch 98: reducing lr to 2.66165439523694e-06
[I 2024-06-22 15:06:56,591] Trial 441 finished with value: 1.9547436237335205 and parameters: {'hidden_size': 86, 'n_layers': 2, 'rnn_dropout': 0.33506811444199425, 'bidirectional': True, 'fc_dropout': 0.30989464559904767, 'learning_rate_model': 0.048267189916329334}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 15:07:52,340] Trial 442 finished with value: 2.914415121078491 and parameters: {'hidden_size': 169, 'n_layers': 5, 'rnn_dropout': 0.254917491824306, 'bidirectional': False, 'fc_dropout': 0.4516880792549694, 'learning_rate_model': 7.42029573257119e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 26: reducing lr to 0.000682931519921108
Epoch 36: reducing lr to 0.000642462247266834
Epoch 39: reducing lr to 0.0006196183751651081
Epoch 42: reducing lr to 0.000592396359398508
Epoch 47: reducing lr to 0.0005384915640611555
Epoch 50: reducing lr to 0.0005018590106270721
Epoch 56: reducing lr to 0.00042164833040923973
Epoch 62: reducing lr to 0.00033643334356263935
Epoch 68: reducing lr to 0.0002515683288198156
Epoch 71: reducing lr to 0.00021094363830726257
Epoch 74: reducing lr to 0.00017238579074793848
Epoch 77: reducing lr to 0.00013650298003292337
Epoch 80: reducing lr to 0.00010386099333043672
Epoch 83: reducing lr to 7.497471044217467e-05
Epoch 86: reducing lr to 5.029956638949642e-05
Epoch 89: reducing lr to 3.0224849074506414e-05
Epoch 92: reducing lr to 1.5067049360328616e-05
Epoch 95: reducing lr to 5.065259462217094e-06
Epoch 98: reducing lr to 3.771869616951001e-07
[I 2024-06-22 15:08:15,240] Trial 443 finished with value: 2.565077781677246 and parameters: {'hidden_size': 62, 'n_layers': 7, 'rnn_dropout': 0.19105130663711362, 'bidirectional': False, 'fc_dropout': 0.6129798455343353, 'learning_rate_model': 0.006840014521299888}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 15:09:27,861] Trial 444 finished with value: 1.9630001783370972 and parameters: {'hidden_size': 97, 'n_layers': 6, 'rnn_dropout': 0.41401015371409555, 'bidirectional': True, 'fc_dropout': 0.7071288527490794, 'learning_rate_model': 0.00012658226359631904}. Best is trial 346 with value: 1.8583240509033203.
Epoch 11: reducing lr to 0.000591306887557569
Epoch 19: reducing lr to 0.001106740047177002
Epoch 26: reducing lr to 0.001221880455433839
Epoch 34: reducing lr to 0.0011720462540338824
Epoch 37: reducing lr to 0.0011367630246424687
Epoch 40: reducing lr to 0.001093202432644815
Epoch 43: reducing lr to 0.001042051486724753
Epoch 46: reducing lr to 0.0009841168073238646
Epoch 54: reducing lr to 0.0008036948000782503
Epoch 58: reducing lr to 0.0007041067747368359
Epoch 61: reducing lr to 0.0006275649880813048
Epoch 64: reducing lr to 0.0005507764376541824
Epoch 67: reducing lr to 0.0004749516149369085
Epoch 70: reducing lr to 0.0004012868290274543
Epoch 76: reducing lr to 0.0002650311484355622
Epoch 79: reducing lr to 0.00020458918492417198
Epoch 82: reducing lr to 0.00015057072384452326
Epoch 85: reducing lr to 0.00010382782774973655
Epoch 88: reducing lr to 6.509747027961011e-05
Epoch 91: reducing lr to 3.4990681690952495e-05
Epoch 94: reducing lr to 1.398211107583886e-05
Epoch 97: reducing lr to 2.403138039647375e-06
[I 2024-06-22 15:11:40,249] Trial 445 finished with value: 2.459009885787964 and parameters: {'hidden_size': 130, 'n_layers': 7, 'rnn_dropout': 0.15153441763500305, 'bidirectional': True, 'fc_dropout': 0.10298528367723812, 'learning_rate_model': 0.012237947458371016}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 15:12:32,433] Trial 446 finished with value: 7.190546989440918 and parameters: {'hidden_size': 192, 'n_layers': 4, 'rnn_dropout': 0.5508441598065185, 'bidirectional': False, 'fc_dropout': 0.25476569585094605, 'learning_rate_model': 1.2462785417518953e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 58: reducing lr to 1.267040128714284e-05
Epoch 81: reducing lr to 3.0197101921130994e-06
Epoch 84: reducing lr to 2.1333505262863294e-06
Epoch 87: reducing lr to 1.386997630852945e-06
Epoch 90: reducing lr to 7.924263571480451e-07
Epoch 93: reducing lr to 3.590104700821239e-07
Epoch 96: reducing lr to 9.358647406340018e-08
Epoch 99: reducing lr to 3.3947856609151116e-10
[I 2024-06-22 15:13:49,505] Trial 447 finished with value: 2.5758109092712402 and parameters: {'hidden_size': 165, 'n_layers': 7, 'rnn_dropout': 0.5900708079333449, 'bidirectional': False, 'fc_dropout': 0.4674155241820646, 'learning_rate_model': 0.00022022186235388098}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 15:15:16,233] Trial 448 finished with value: 3.9334335327148438 and parameters: {'hidden_size': 179, 'n_layers': 7, 'rnn_dropout': 0.7900151269635534, 'bidirectional': False, 'fc_dropout': 0.2557200894896421, 'learning_rate_model': 2.784247303202293e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 33: reducing lr to 3.384249435385771e-05
Epoch 43: reducing lr to 2.9838888440156772e-05
Epoch 46: reducing lr to 2.8179943121732236e-05
Epoch 57: reducing lr to 2.088494841528197e-05
Epoch 60: reducing lr to 1.8703258018695758e-05
Epoch 65: reducing lr to 1.5042836203750766e-05
Epoch 68: reducing lr to 1.2888458826860604e-05
Epoch 72: reducing lr to 1.0135343540032988e-05
Epoch 75: reducing lr to 8.202248369759864e-06
Epoch 78: reducing lr to 6.416129312362533e-06
Epoch 81: reducing lr to 4.805149246485407e-06
Epoch 87: reducing lr to 2.207076241348278e-06
Epoch 92: reducing lr to 7.719216732642032e-07
Epoch 98: reducing lr to 1.9324207656197825e-08
[I 2024-06-22 15:16:39,871] Trial 449 finished with value: 1.8686397075653076 and parameters: {'hidden_size': 176, 'n_layers': 3, 'rnn_dropout': 0.335465838533509, 'bidirectional': True, 'fc_dropout': 0.33673846761190396, 'learning_rate_model': 0.00035043062036654225}. Best is trial 346 with value: 1.8583240509033203.
Epoch 12: reducing lr to 0.0049228505502530405
Epoch 15: reducing lr to 0.006506667941525726
Epoch 19: reducing lr to 0.008192110086035142
Epoch 22: reducing lr to 0.008906439101699856
Epoch 25: reducing lr to 0.009055411918766607
Epoch 28: reducing lr to 0.008998590999902478
Epoch 31: reducing lr to 0.008871287025537128
Epoch 34: reducing lr to 0.00867550782449918
Epoch 37: reducing lr to 0.008414340714749636
Epoch 40: reducing lr to 0.00809190441548688
Epoch 43: reducing lr to 0.007713284177563058
Epoch 46: reducing lr to 0.007284450620250457
Epoch 51: reducing lr to 0.006476826188577092
Epoch 54: reducing lr to 0.005948963620327052
Epoch 57: reducing lr to 0.00539871123161613
Epoch 66: reducing lr to 0.0037013441418564186
Epoch 70: reducing lr to 0.0029703324532749224
Epoch 74: reducing lr to 0.002282985644136043
Epoch 77: reducing lr to 0.0018077728010229277
Epoch 80: reducing lr to 0.0013754797058987411
Epoch 83: reducing lr to 0.0009929251527640097
Epoch 86: reducing lr to 0.0006661406805935452
Epoch 89: reducing lr to 0.0004002818111277645
Epoch 92: reducing lr to 0.00019953998087589297
Epoch 95: reducing lr to 6.708159985746449e-05
Epoch 98: reducing lr to 4.9952633274993885e-06
[I 2024-06-22 15:16:59,426] Trial 450 finished with value: 2.493163824081421 and parameters: {'hidden_size': 38, 'n_layers': 4, 'rnn_dropout': 0.1862832777951968, 'bidirectional': True, 'fc_dropout': 0.05754157184737157, 'learning_rate_model': 0.09058551107987103}. Best is trial 346 with value: 1.8583240509033203.
Epoch 26: reducing lr to 0.0004344861684124187
Epoch 29: reducing lr to 0.000430620434155424
Epoch 32: reducing lr to 0.00042339501370338697
Epoch 36: reducing lr to 0.0004087393128330712
Epoch 44: reducing lr to 0.00036392685764818314
Epoch 48: reducing lr to 0.0003350252360626734
Epoch 52: reducing lr to 0.0003028343767150951
Epoch 55: reducing lr to 0.0002770722296581661
Epoch 58: reducing lr to 0.00025037200108091813
Epoch 61: reducing lr to 0.0002231546514134381
Epoch 64: reducing lr to 0.00019584955548146484
Epoch 67: reducing lr to 0.00016888714966961155
Epoch 71: reducing lr to 0.00013420392892348165
Epoch 74: reducing lr to 0.00010967313636288029
Epoch 77: reducing lr to 8.68442223580969e-05
Epoch 80: reducing lr to 6.607714496010114e-05
Epoch 83: reducing lr to 4.769947457047255e-05
Epoch 86: reducing lr to 3.200096237453362e-05
Epoch 89: reducing lr to 1.9229276262930448e-05
Epoch 92: reducing lr to 9.585770102698166e-06
Epoch 95: reducing lr to 3.2225561590827104e-06
Epoch 98: reducing lr to 2.3996918136238996e-07
[I 2024-06-22 15:17:43,631] Trial 451 finished with value: 1.871209979057312 and parameters: {'hidden_size': 160, 'n_layers': 2, 'rnn_dropout': 0.024594254679757645, 'bidirectional': True, 'fc_dropout': 0.43720919949138237, 'learning_rate_model': 0.004351668673292754}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 15:17:53,047] Trial 452 finished with value: 2.1323719024658203 and parameters: {'hidden_size': 163, 'n_layers': 1, 'rnn_dropout': 0.5714364111031028, 'bidirectional': False, 'fc_dropout': 0.6769933844702254, 'learning_rate_model': 0.0017442339055707746}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 15:18:08,156] Trial 453 finished with value: 2.4450440406799316 and parameters: {'hidden_size': 126, 'n_layers': 2, 'rnn_dropout': 0.18100127086965243, 'bidirectional': False, 'fc_dropout': 0.0895643554732649, 'learning_rate_model': 0.0011094087184867307}. Best is trial 346 with value: 1.8583240509033203.
Epoch 71: reducing lr to 8.933013166978403e-05
Epoch 79: reducing lr to 4.8424222157989524e-05
Epoch 86: reducing lr to 2.130079354164655e-05
Epoch 91: reducing lr to 8.281945813949947e-06
Epoch 97: reducing lr to 5.687988363184144e-07
[I 2024-06-22 15:18:15,967] Trial 454 finished with value: 2.0076193809509277 and parameters: {'hidden_size': 134, 'n_layers': 1, 'rnn_dropout': 0.27535708714421453, 'bidirectional': False, 'fc_dropout': 0.5202318153624044, 'learning_rate_model': 0.002896600261160456}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 15:19:05,246] Trial 455 finished with value: 4.290713787078857 and parameters: {'hidden_size': 166, 'n_layers': 2, 'rnn_dropout': 0.49454547347235644, 'bidirectional': True, 'fc_dropout': 0.24420917725265057, 'learning_rate_model': 1.9294161071421722e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 53: reducing lr to 6.489890357168475e-05
Epoch 58: reducing lr to 5.519753425979899e-05
Epoch 68: reducing lr to 3.528488286105628e-05
Epoch 74: reducing lr to 2.4178768694719935e-05
Epoch 78: reducing lr to 1.7565511458692126e-05
Epoch 81: reducing lr to 1.315511269188959e-05
Epoch 84: reducing lr to 9.293761586094457e-06
Epoch 87: reducing lr to 6.042338163744873e-06
Epoch 90: reducing lr to 3.4521385712883103e-06
Epoch 93: reducing lr to 1.5639988247328065e-06
Epoch 96: reducing lr to 4.0770157876611175e-07
Epoch 99: reducing lr to 1.4789097328173594e-09
[I 2024-06-22 15:22:45,171] Trial 456 finished with value: 1.8714618682861328 and parameters: {'hidden_size': 193, 'n_layers': 6, 'rnn_dropout': 0.08116868625920325, 'bidirectional': True, 'fc_dropout': 0.729148112259034, 'learning_rate_model': 0.0009593779641667671}. Best is trial 346 with value: 1.8583240509033203.
Epoch 27: reducing lr to 0.0004337588865269357
Epoch 30: reducing lr to 0.00042876172964141066
Epoch 36: reducing lr to 0.0004089119515129579
Epoch 39: reducing lr to 0.00039437236983174903
Epoch 42: reducing lr to 0.00037704620375959114
Epoch 45: reducing lr to 0.0003572066973750507
Epoch 52: reducing lr to 0.00030296228446798047
Epoch 55: reducing lr to 0.00027718925628726594
Epoch 58: reducing lr to 0.00025047775036998846
Epoch 63: reducing lr to 0.0002050209498870829
Epoch 71: reducing lr to 0.00013426061245843296
Epoch 74: reducing lr to 0.00010971945885960679
Epoch 77: reducing lr to 8.688090263678054e-05
Epoch 80: reducing lr to 6.610505387592701e-05
Epoch 83: reducing lr to 4.771962133410055e-05
Epoch 86: reducing lr to 3.2014478578446406e-05
Epoch 91: reducing lr to 1.2447525784903701e-05
Epoch 94: reducing lr to 4.973972490192845e-06
Epoch 97: reducing lr to 8.548882521751573e-07
[I 2024-06-22 15:23:38,696] Trial 457 finished with value: 1.8667256832122803 and parameters: {'hidden_size': 177, 'n_layers': 2, 'rnn_dropout': 0.29375386072261245, 'bidirectional': True, 'fc_dropout': 0.26481154725263584, 'learning_rate_model': 0.004353506681802027}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 15:25:13,671] Trial 458 finished with value: 2.1104283332824707 and parameters: {'hidden_size': 102, 'n_layers': 7, 'rnn_dropout': 0.21573621311227864, 'bidirectional': True, 'fc_dropout': 0.4413903764973661, 'learning_rate_model': 5.6607980604122834e-05}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 15:25:46,359] Trial 459 finished with value: 5.236345291137695 and parameters: {'hidden_size': 48, 'n_layers': 7, 'rnn_dropout': 0.30131817099706404, 'bidirectional': True, 'fc_dropout': 0.11268639751949348, 'learning_rate_model': 2.4290374320837906e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 50: reducing lr to 0.0002465470230021937
Epoch 60: reducing lr to 0.00017934541494682394
Epoch 64: reducing lr to 0.0001512313499975085
Epoch 68: reducing lr to 0.00012358734472987603
Epoch 74: reducing lr to 8.46875369711468e-05
Epoch 77: reducing lr to 6.705947815103291e-05
Epoch 80: reducing lr to 5.102353085117356e-05
Epoch 83: reducing lr to 3.683263878608641e-05
Epoch 86: reducing lr to 2.4710542381487303e-05
Epoch 89: reducing lr to 1.484848613298598e-05
Epoch 92: reducing lr to 7.4019517166276635e-06
Epoch 95: reducing lr to 2.4883973679836696e-06
Epoch 98: reducing lr to 1.8529969683113955e-07
[I 2024-06-22 15:25:56,095] Trial 460 finished with value: 2.0109145641326904 and parameters: {'hidden_size': 166, 'n_layers': 1, 'rnn_dropout': 0.41305283654363933, 'bidirectional': False, 'fc_dropout': 0.7060816020024561, 'learning_rate_model': 0.0033602768542725305}. Best is trial 346 with value: 1.8583240509033203.
Epoch 31: reducing lr to 8.26851683103171e-05
Epoch 35: reducing lr to 8.011500711431683e-05
Epoch 47: reducing lr to 6.646937623154843e-05
Epoch 50: reducing lr to 6.194759141811752e-05
Epoch 53: reducing lr to 5.711462683283054e-05
Epoch 56: reducing lr to 5.204668630276455e-05
Epoch 59: reducing lr to 4.6823716750886076e-05
Epoch 62: reducing lr to 4.152806837204826e-05
Epoch 65: reducing lr to 3.624327230704841e-05
Epoch 68: reducing lr to 3.10526496834167e-05
Epoch 71: reducing lr to 2.6038090462462168e-05
Epoch 74: reducing lr to 2.127865457312233e-05
Epoch 77: reducing lr to 1.6849415185091914e-05
Epoch 80: reducing lr to 1.2820210941464467e-05
Epoch 83: reducing lr to 9.254596671204937e-06
Epoch 86: reducing lr to 6.20878956285281e-06
Epoch 89: reducing lr to 3.7308418529783532e-06
Epoch 92: reducing lr to 1.8598199850671372e-06
Epoch 95: reducing lr to 6.252366042010715e-07
Epoch 98: reducing lr to 4.6558541934185824e-08
[I 2024-06-22 15:28:56,929] Trial 461 finished with value: 1.8701295852661133 and parameters: {'hidden_size': 194, 'n_layers': 5, 'rnn_dropout': 0.007615772973246404, 'bidirectional': True, 'fc_dropout': 0.25118746243755713, 'learning_rate_model': 0.0008443057031695725}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 15:29:11,037] Trial 462 finished with value: 5.062924385070801 and parameters: {'hidden_size': 122, 'n_layers': 2, 'rnn_dropout': 0.4466292627581215, 'bidirectional': False, 'fc_dropout': 0.15677361163401723, 'learning_rate_model': 4.23649248161928e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 63: reducing lr to 2.7887181759623705e-05
Epoch 81: reducing lr to 8.11989540696102e-06
Epoch 84: reducing lr to 5.736505173600261e-06
Epoch 87: reducing lr to 3.7295882636828874e-06
[I 2024-06-22 15:29:40,557] Trial 463 finished with value: 2.5380654335021973 and parameters: {'hidden_size': 155, 'n_layers': 3, 'rnn_dropout': 0.5153565540305731, 'bidirectional': False, 'fc_dropout': 0.704546894325612, 'learning_rate_model': 0.0005921689085628344}. Best is trial 346 with value: 1.8583240509033203.
Epoch 32: reducing lr to 0.0003084989290883479
Epoch 38: reducing lr to 0.00029099507470755693
Epoch 41: reducing lr to 0.00027903193446603765
Epoch 44: reducing lr to 0.0002651685594238117
Epoch 47: reducing lr to 0.00024962358943294086
Epoch 53: reducing lr to 0.00021449213107505027
Epoch 56: reducing lr to 0.00019545964456967356
Epoch 59: reducing lr to 0.00017584495159441362
Epoch 62: reducing lr to 0.00015595731563864234
Epoch 65: reducing lr to 0.00013611043519597309
Epoch 68: reducing lr to 0.00011661722006199694
Epoch 71: reducing lr to 9.778520533391828e-05
Epoch 74: reducing lr to 7.991129801403763e-05
Epoch 79: reducing lr to 5.300756215560302e-05
Epoch 82: reducing lr to 3.9011773794205655e-05
Epoch 85: reducing lr to 2.6901031132047643e-05
Epoch 96: reducing lr to 1.347461600419977e-06
Epoch 99: reducing lr to 4.887825260507795e-09
[I 2024-06-22 15:29:55,112] Trial 464 finished with value: 2.56252121925354 and parameters: {'hidden_size': 53, 'n_layers': 5, 'rnn_dropout': 0.5094687802772401, 'bidirectional': False, 'fc_dropout': 0.6260647824356789, 'learning_rate_model': 0.0031707627204098725}. Best is trial 346 with value: 1.8583240509033203.
Epoch 80: reducing lr to 2.665573558162857e-05
[I 2024-06-22 15:30:10,541] Trial 465 finished with value: 2.142446517944336 and parameters: {'hidden_size': 127, 'n_layers': 2, 'rnn_dropout': 0.5210922895982945, 'bidirectional': False, 'fc_dropout': 0.7333200896647462, 'learning_rate_model': 0.0017554773222146585}. Best is trial 346 with value: 1.8583240509033203.
Epoch 58: reducing lr to 6.132696719428972e-05
Epoch 63: reducing lr to 5.019732510889618e-05
Epoch 66: reducing lr to 4.3553421367733004e-05
Epoch 69: reducing lr to 3.70631470874708e-05
Epoch 72: reducing lr to 3.082889620969102e-05
Epoch 81: reducing lr to 1.4615927600959385e-05
Epoch 84: reducing lr to 1.0325791170658722e-05
Epoch 87: reducing lr to 6.713312094715597e-06
Epoch 90: reducing lr to 3.835482721294874e-06
Epoch 93: reducing lr to 1.7376737186275503e-06
Epoch 96: reducing lr to 4.52974968562312e-07
Epoch 99: reducing lr to 1.6431358734357234e-09
[I 2024-06-22 15:30:20,151] Trial 466 finished with value: 2.5685997009277344 and parameters: {'hidden_size': 64, 'n_layers': 3, 'rnn_dropout': 0.7982098924550564, 'bidirectional': False, 'fc_dropout': 0.4776129502276577, 'learning_rate_model': 0.0010659124854827172}. Best is trial 346 with value: 1.8583240509033203.
Epoch 8: reducing lr to 0.0005800620969112988
Epoch 19: reducing lr to 0.0016944783414480195
Epoch 23: reducing lr to 0.0018649427173785353
Epoch 26: reducing lr to 0.0018707644788425667
Epoch 29: reducing lr to 0.0018541198101318106
Epoch 32: reducing lr to 0.0018230093608032076
Epoch 35: reducing lr to 0.00177792376077195
Epoch 38: reducing lr to 0.0017195740248018305
Epoch 41: reducing lr to 0.001648880370501833
Epoch 44: reducing lr to 0.0015669576793956158
Epoch 47: reducing lr to 0.0014750979575790552
Epoch 54: reducing lr to 0.001230499822736775
Epoch 58: reducing lr to 0.0010780252173052298
Epoch 61: reducing lr to 0.0009608356387457854
Epoch 64: reducing lr to 0.0008432682516237233
Epoch 67: reducing lr to 0.0007271763832881701
Epoch 70: reducing lr to 0.0006143916470988848
Epoch 77: reducing lr to 0.00037392464522833705
Epoch 83: reducing lr to 0.00020537934041019238
Epoch 86: reducing lr to 0.0001377863509851234
Epoch 89: reducing lr to 8.279537900593054e-05
Epoch 92: reducing lr to 4.1273392605347195e-05
Epoch 95: reducing lr to 1.3875340647817244e-05
Epoch 98: reducing lr to 1.033233859089322e-06
[I 2024-06-22 15:30:54,787] Trial 467 finished with value: 2.498853921890259 and parameters: {'hidden_size': 80, 'n_layers': 4, 'rnn_dropout': 0.005387765595362648, 'bidirectional': True, 'fc_dropout': 0.01942514025974287, 'learning_rate_model': 0.018736953600696844}. Best is trial 346 with value: 1.8583240509033203.
Epoch 24: reducing lr to 0.00330919214408868
Epoch 30: reducing lr to 0.003259260728351361
Epoch 33: reducing lr to 0.003195969473966261
Epoch 42: reducing lr to 0.002866141727983439
Epoch 45: reducing lr to 0.002715330404213736
Epoch 48: reducing lr to 0.0025477922079980577
Epoch 53: reducing lr to 0.0022386692072683865
Epoch 56: reducing lr to 0.0020400258292396153
Epoch 61: reducing lr to 0.0016970413594255957
Epoch 64: reducing lr to 0.001489392194032255
Epoch 68: reducing lr to 0.0012171419915571954
Epoch 71: reducing lr to 0.0010205909513336115
Epoch 74: reducing lr to 0.0008340397443963972
Epoch 77: reducing lr to 0.0006604309443489758
Epoch 80: reducing lr to 0.0005025019518965737
Epoch 86: reducing lr to 0.00024336018248793495
Epoch 89: reducing lr to 0.00014623435775736952
Epoch 95: reducing lr to 2.4506820944112524e-05
Epoch 98: reducing lr to 1.824912109965604e-06
[I 2024-06-22 15:31:00,114] Trial 468 finished with value: 1.8738718032836914 and parameters: {'hidden_size': 41, 'n_layers': 1, 'rnn_dropout': 0.3698683233080926, 'bidirectional': True, 'fc_dropout': 0.05247881290803456, 'learning_rate_model': 0.033093469817092956}. Best is trial 346 with value: 1.8583240509033203.
Epoch 11: reducing lr to 0.0003980952822857682
Epoch 14: reducing lr to 0.0005453825056730697
Epoch 26: reducing lr to 0.0008226267189861533
Epoch 29: reducing lr to 0.0008153075992546239
Epoch 33: reducing lr to 0.0007956883421723104
Epoch 37: reducing lr to 0.0007653217080834503
Epoch 42: reducing lr to 0.0007135723850139936
Epoch 45: reducing lr to 0.0006760254643789208
Epoch 48: reducing lr to 0.0006343141180462056
Epoch 51: reducing lr to 0.0005890961454546866
Epoch 54: reducing lr to 0.0005410846973731648
Epoch 57: reducing lr to 0.0004910367955491926
Epoch 62: reducing lr to 0.00040525155085014444
Epoch 65: reducing lr to 0.00035367988173033905
Epoch 68: reducing lr to 0.00030302720390147024
Epoch 71: reducing lr to 0.0002540926403451859
Epoch 74: reducing lr to 0.00020764769717934815
Epoch 77: reducing lr to 0.00016442497574176688
Epoch 80: reducing lr to 0.00012510599625556858
Epoch 83: reducing lr to 9.031095835949675e-05
Epoch 86: reducing lr to 6.058845734664238e-05
Epoch 89: reducing lr to 3.640741084682264e-05
Epoch 92: reducing lr to 1.814904865061924e-05
Epoch 95: reducing lr to 6.101369830899782e-06
Epoch 98: reducing lr to 4.5434141446483445e-07
[I 2024-06-22 15:33:55,841] Trial 469 finished with value: 2.458998203277588 and parameters: {'hidden_size': 167, 'n_layers': 6, 'rnn_dropout': 0.37425443825229743, 'bidirectional': True, 'fc_dropout': 0.4657720337114457, 'learning_rate_model': 0.008239155082671501}. Best is trial 346 with value: 1.8583240509033203.
Epoch 59: reducing lr to 8.485413969786206e-06
Epoch 62: reducing lr to 7.5257343063383926e-06
Epoch 65: reducing lr to 6.568021303844368e-06
Epoch 68: reducing lr to 5.627374452660361e-06
Epoch 71: reducing lr to 4.718633886588092e-06
Epoch 74: reducing lr to 3.856126879752889e-06
Epoch 77: reducing lr to 3.0534582240655025e-06
Epoch 80: reducing lr to 2.3232841082879215e-06
Epoch 83: reducing lr to 1.677121965699003e-06
Epoch 86: reducing lr to 1.125159499242381e-06
Epoch 89: reducing lr to 6.761047557747872e-07
Epoch 92: reducing lr to 3.3703737288812194e-07
Epoch 95: reducing lr to 1.1330564474271675e-07
Epoch 98: reducing lr to 8.437358876123941e-09
[I 2024-06-22 15:36:07,953] Trial 470 finished with value: 1.9002190828323364 and parameters: {'hidden_size': 161, 'n_layers': 5, 'rnn_dropout': 0.44997293174081054, 'bidirectional': True, 'fc_dropout': 0.22899823426094815, 'learning_rate_model': 0.0001530054405241054}. Best is trial 346 with value: 1.8583240509033203.
Epoch 3: reducing lr to 0.0007904662738729437
Epoch 10: reducing lr to 0.003480253126169989
Epoch 14: reducing lr to 0.005439936137358
Epoch 17: reducing lr to 0.006745228332081387
Epoch 20: reducing lr to 0.007703366568037691
Epoch 27: reducing lr to 0.008188124659684014
Epoch 34: reducing lr to 0.007870665270201192
Epoch 37: reducing lr to 0.0076337271056571235
Epoch 40: reducing lr to 0.007341203805142887
Epoch 43: reducing lr to 0.0069977088515901995
Epoch 46: reducing lr to 0.006608658959120997
Epoch 51: reducing lr to 0.00587595930691309
Epoch 54: reducing lr to 0.005397067503988039
Epoch 57: reducing lr to 0.004897863024747965
Epoch 73: reducing lr to 0.0022223919837872424
Epoch 76: reducing lr to 0.0017797688856851332
Epoch 79: reducing lr to 0.001373881778896842
Epoch 85: reducing lr to 0.0006972370545426085
Epoch 88: reducing lr to 0.00043715032298791005
Epoch 91: reducing lr to 0.00023497361321516834
Epoch 94: reducing lr to 9.389434561131171e-05
Epoch 97: reducing lr to 1.6137840160364292e-05
[I 2024-06-22 15:38:21,617] Trial 471 finished with value: 2.4647817611694336 and parameters: {'hidden_size': 142, 'n_layers': 6, 'rnn_dropout': 0.478095829591981, 'bidirectional': True, 'fc_dropout': 0.6125488950166031, 'learning_rate_model': 0.08218172935380007}. Best is trial 346 with value: 1.8583240509033203.
Epoch 58: reducing lr to 0.00012655319533566684
Epoch 63: reducing lr to 0.0001035862717572249
Epoch 66: reducing lr to 8.987603486774966e-05
Epoch 69: reducing lr to 7.64828248926026e-05
Epoch 72: reducing lr to 6.361794007598183e-05
Epoch 75: reducing lr to 5.14842090171523e-05
Epoch 78: reducing lr to 4.027302365246731e-05
Epoch 81: reducing lr to 3.01611578938214e-05
Epoch 84: reducing lr to 2.130811169702442e-05
Epoch 87: reducing lr to 1.385346668424433e-05
Epoch 90: reducing lr to 7.914831211151135e-06
Epoch 93: reducing lr to 3.5858313496318473e-06
Epoch 96: reducing lr to 9.347507679129466e-07
Epoch 99: reducing lr to 3.3907448006905565e-09
[I 2024-06-22 15:38:28,281] Trial 472 finished with value: 1.909477949142456 and parameters: {'hidden_size': 18, 'n_layers': 2, 'rnn_dropout': 0.6338502974165983, 'bidirectional': True, 'fc_dropout': 0.4439139048606762, 'learning_rate_model': 0.002199597292307988}. Best is trial 346 with value: 1.8583240509033203.
Epoch 26: reducing lr to 0.0003826957890058323
Epoch 29: reducing lr to 0.00037929084696366534
Epoch 32: reducing lr to 0.0003729266904454158
Epoch 37: reducing lr to 0.0003560368125159538
Epoch 40: reducing lr to 0.000342393534198531
Epoch 43: reducing lr to 0.0003263729394503136
Epoch 46: reducing lr to 0.00030822766366206086
Epoch 49: reducing lr to 0.00028824389555613965
Epoch 52: reducing lr to 0.0002667367782006481
Epoch 55: reducing lr to 0.00024404545702358957
Epoch 58: reducing lr to 0.00022052787284054857
Epoch 61: reducing lr to 0.00019655480795863746
Epoch 64: reducing lr to 0.0001725044560918603
Epoch 67: reducing lr to 0.0001487559459761866
Epoch 70: reducing lr to 0.00012568396439223143
Epoch 73: reducing lr to 0.00010365229816536713
Epoch 76: reducing lr to 8.300836960818485e-05
Epoch 79: reducing lr to 6.407780662864982e-05
Epoch 82: reducing lr to 4.715909949013766e-05
Epoch 85: reducing lr to 3.2519116158003346e-05
Epoch 88: reducing lr to 2.0388678483357566e-05
Epoch 91: reducing lr to 1.0959162558023304e-05
Epoch 94: reducing lr to 4.37922957711558e-06
Epoch 97: reducing lr to 7.526684006466372e-07
[I 2024-06-22 15:40:01,474] Trial 473 finished with value: 1.8894295692443848 and parameters: {'hidden_size': 149, 'n_layers': 4, 'rnn_dropout': 0.3238787772181782, 'bidirectional': True, 'fc_dropout': 0.07400268652048317, 'learning_rate_model': 0.0038329534919439657}. Best is trial 346 with value: 1.8583240509033203.
Epoch 6: reducing lr to 0.0017334128073688538
Epoch 9: reducing lr to 0.003031258521376721
Epoch 12: reducing lr to 0.004509375450512387
Epoch 15: reducing lr to 0.005960166448409432
Epoch 18: reducing lr to 0.00717987221483872
Epoch 21: reducing lr to 0.007997188686145797
Epoch 24: reducing lr to 0.008297326183803102
Epoch 29: reducing lr to 0.008211023524551513
Epoch 32: reducing lr to 0.008073249994545183
Epoch 35: reducing lr to 0.007873587103046894
Epoch 38: reducing lr to 0.007615183599624986
Epoch 41: reducing lr to 0.007302114694734461
Epoch 44: reducing lr to 0.006939317673640181
Epoch 47: reducing lr to 0.006532514222928484
Epoch 50: reducing lr to 0.0060881197471346895
Epoch 53: reducing lr to 0.0056131429731340315
Epoch 56: reducing lr to 0.005115073102908614
Epoch 61: reducing lr to 0.004255088581577831
Epoch 64: reducing lr to 0.00373443798709943
Epoch 70: reducing lr to 0.002720851284825147
Epoch 73: reducing lr to 0.0022439019170193376
Epoch 76: reducing lr to 0.0017969947892066212
Epoch 79: reducing lr to 0.0013871792104698717
Epoch 82: reducing lr to 0.0010209170044835805
Epoch 85: reducing lr to 0.0007039854241369756
Epoch 88: reducing lr to 0.0004413813831827151
Epoch 92: reducing lr to 0.00018278041999694877
Epoch 95: reducing lr to 6.14473497601506e-05
Epoch 98: reducing lr to 4.575706206785824e-06
[I 2024-06-22 15:41:46,062] Trial 474 finished with value: 2.4573774337768555 and parameters: {'hidden_size': 139, 'n_layers': 5, 'rnn_dropout': 0.0025165763714979587, 'bidirectional': True, 'fc_dropout': 0.42128746856504595, 'learning_rate_model': 0.08297714416999552}. Best is trial 346 with value: 1.8583240509033203.
Epoch 14: reducing lr to 0.006388284879377024
Epoch 20: reducing lr to 0.009046301082275936
Epoch 23: reducing lr to 0.00960577179613337
Epoch 26: reducing lr to 0.009635757978311458
Epoch 29: reducing lr to 0.009550026181957674
Epoch 32: reducing lr to 0.009389785401401264
Epoch 35: reducing lr to 0.009157562727130192
Epoch 40: reducing lr to 0.008621002173675528
Epoch 43: reducing lr to 0.008217625449663346
Epoch 46: reducing lr to 0.00776075215508248
Epoch 49: reducing lr to 0.007257588131606848
Epoch 52: reducing lr to 0.006716068251842766
Epoch 55: reducing lr to 0.006144731735080266
Epoch 58: reducing lr to 0.0055525910428321605
Epoch 61: reducing lr to 0.004948981967852416
Epoch 64: reducing lr to 0.004343426912011538
Epoch 67: reducing lr to 0.0037454718197577645
Epoch 70: reducing lr to 0.0031645507931622437
Epoch 73: reducing lr to 0.0026098234882904166
Epoch 76: reducing lr to 0.0020900375250967385
Epoch 79: reducing lr to 0.0016133917701542863
Epoch 82: reducing lr to 0.0011874018011605355
Epoch 85: reducing lr to 0.000818786989480938
Epoch 88: reducing lr to 0.0005133591144904026
Epoch 91: reducing lr to 0.00027593676514814093
Epoch 94: reducing lr to 0.0001102630275764696
Epoch 97: reducing lr to 1.895116370471267e-05
[I 2024-06-22 15:42:27,495] Trial 475 finished with value: 1.964763879776001 and parameters: {'hidden_size': 108, 'n_layers': 3, 'rnn_dropout': 0.6510719939156764, 'bidirectional': True, 'fc_dropout': 0.6424000317972007, 'learning_rate_model': 0.09650854086072257}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 15:42:36,511] Trial 476 finished with value: 8.966525077819824 and parameters: {'hidden_size': 36, 'n_layers': 4, 'rnn_dropout': 0.6508212635880003, 'bidirectional': False, 'fc_dropout': 0.6007220477316534, 'learning_rate_model': 2.4739192264457364e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 44: reducing lr to 0.00010335002496565768
Epoch 54: reducing lr to 8.115866118932712e-05
Epoch 57: reducing lr to 7.365184991359225e-05
Epoch 61: reducing lr to 6.337273083889032e-05
Epoch 64: reducing lr to 5.561847394096388e-05
Epoch 67: reducing lr to 4.796153613814013e-05
Epoch 70: reducing lr to 4.0522723045623404e-05
Epoch 73: reducing lr to 3.341932594113152e-05
Epoch 76: reducing lr to 2.6763359895330695e-05
Epoch 79: reducing lr to 2.0659813079100206e-05
Epoch 82: reducing lr to 1.5204924008889475e-05
Epoch 86: reducing lr to 9.087815836693108e-06
Epoch 96: reducing lr to 5.251761006141737e-07
Epoch 99: reducing lr to 1.9050405666593688e-09
[I 2024-06-22 15:43:14,518] Trial 477 finished with value: 1.870408535003662 and parameters: {'hidden_size': 82, 'n_layers': 4, 'rnn_dropout': 0.19086847675406704, 'bidirectional': True, 'fc_dropout': 0.772434063673871, 'learning_rate_model': 0.0012358116928590523}. Best is trial 346 with value: 1.8583240509033203.
Epoch 23: reducing lr to 0.0012655842601028463
Epoch 26: reducing lr to 0.0012695350139819296
Epoch 30: reducing lr to 0.0012522797224651275
Epoch 36: reducing lr to 0.0011943046912829315
Epoch 39: reducing lr to 0.0011518390931342089
Epoch 42: reducing lr to 0.001101234748249294
Epoch 47: reducing lr to 0.0010010284711833236
Epoch 50: reducing lr to 0.000932930414673199
Epoch 56: reducing lr to 0.0007838228335154143
Epoch 62: reducing lr to 0.0006254125004702121
Epoch 68: reducing lr to 0.00046765274779317443
Epoch 71: reducing lr to 0.00039213351118827543
Epoch 74: reducing lr to 0.0003204564306722165
Epoch 77: reducing lr to 0.000253752107802392
Epoch 80: reducing lr to 0.00019307231219195287
Epoch 83: reducing lr to 0.00013937417924493045
Epoch 86: reducing lr to 9.350433953751493e-05
Epoch 92: reducing lr to 2.8008879605587525e-05
Epoch 95: reducing lr to 9.416060109409923e-06
Epoch 98: reducing lr to 7.01171406973164e-07
[I 2024-06-22 15:43:28,266] Trial 478 finished with value: 2.5570590496063232 and parameters: {'hidden_size': 35, 'n_layers': 7, 'rnn_dropout': 0.6673887915970793, 'bidirectional': False, 'fc_dropout': 0.7053527354831539, 'learning_rate_model': 0.012715239636234955}. Best is trial 346 with value: 1.8583240509033203.
Epoch 10: reducing lr to 5.6870959648655526e-05
Epoch 15: reducing lr to 9.6461698577268e-05
Epoch 59: reducing lr to 7.447682625057472e-05
Epoch 64: reducing lr to 6.0439626071721194e-05
Epoch 67: reducing lr to 5.211896523970496e-05
Epoch 70: reducing lr to 4.4035336727956114e-05
Epoch 73: reducing lr to 3.63161989233097e-05
Epoch 76: reducing lr to 2.9083276650374283e-05
Epoch 82: reducing lr to 1.652292586311634e-05
Epoch 85: reducing lr to 1.1393579419919285e-05
Epoch 88: reducing lr to 7.143491429429955e-06
Epoch 91: reducing lr to 3.839713489565836e-06
Epoch 94: reducing lr to 1.5343313681250156e-06
Epoch 97: reducing lr to 2.6370911060326525e-07
[I 2024-06-22 15:44:51,320] Trial 479 finished with value: 1.9093585014343262 and parameters: {'hidden_size': 121, 'n_layers': 5, 'rnn_dropout': 0.675512546114687, 'bidirectional': True, 'fc_dropout': 0.5531907388431758, 'learning_rate_model': 0.0013429350235452983}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 15:44:55,603] Trial 480 finished with value: 9.226088523864746 and parameters: {'hidden_size': 64, 'n_layers': 1, 'rnn_dropout': 0.1883982090399309, 'bidirectional': False, 'fc_dropout': 0.6645620553924543, 'learning_rate_model': 2.7066340582671316e-05}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 15:45:19,065] Trial 481 finished with value: 6.178983211517334 and parameters: {'hidden_size': 29, 'n_layers': 6, 'rnn_dropout': 0.2278666976946454, 'bidirectional': True, 'fc_dropout': 0.07883120851244244, 'learning_rate_model': 3.4864881188637084e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 31: reducing lr to 0.0006695298184074838
Epoch 37: reducing lr to 0.0006350433702063563
Epoch 40: reducing lr to 0.0006107086016128103
Epoch 43: reducing lr to 0.0005821335438548085
Epoch 47: reducing lr to 0.0005382250566503886
Epoch 56: reducing lr to 0.00042143965043671785
Epoch 62: reducing lr to 0.00033626683774291524
Epoch 68: reducing lr to 0.0002514438239465375
Epoch 71: reducing lr to 0.0002108392391920025
Epoch 74: reducing lr to 0.00017230047447965979
Epoch 77: reducing lr to 0.00013643542269647005
Epoch 80: reducing lr to 0.00010380959099424522
Epoch 83: reducing lr to 7.493760435307974e-05
Epoch 86: reducing lr to 5.0274672392829065e-05
Epoch 89: reducing lr to 3.0209890351277912e-05
Epoch 92: reducing lr to 1.505959245555813e-05
Epoch 95: reducing lr to 5.062752590662828e-06
Epoch 98: reducing lr to 3.77000286309187e-07
[I 2024-06-22 15:45:29,750] Trial 482 finished with value: 2.5598151683807373 and parameters: {'hidden_size': 45, 'n_layers': 4, 'rnn_dropout': 0.6513086517790563, 'bidirectional': False, 'fc_dropout': 0.354873630896267, 'learning_rate_model': 0.006836629297312474}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 15:45:35,870] Trial 483 finished with value: 4.518667697906494 and parameters: {'hidden_size': 106, 'n_layers': 1, 'rnn_dropout': 0.06434514717563769, 'bidirectional': False, 'fc_dropout': 0.5535337834214279, 'learning_rate_model': 0.0001180403062074297}. Best is trial 346 with value: 1.8583240509033203.
Epoch 15: reducing lr to 0.0024764888208188064
Epoch 19: reducing lr to 0.003117981312294512
Epoch 22: reducing lr to 0.0033898605348978645
Epoch 27: reducing lr to 0.0034351494568561035
Epoch 33: reducing lr to 0.0033296362013956055
Epoch 36: reducing lr to 0.0032383743867215936
Epoch 41: reducing lr to 0.0030340772470662343
Epoch 51: reducing lr to 0.002465130815733088
Epoch 54: reducing lr to 0.002264222184626058
Epoch 57: reducing lr to 0.002054791812349844
Epoch 64: reducing lr to 0.0015516838341923962
Epoch 67: reducing lr to 0.001338065125044255
Epoch 71: reducing lr to 0.0010632756683248118
Epoch 76: reducing lr to 0.0007466632928896582
Epoch 79: reducing lr to 0.0005763821928358514
Epoch 85: reducing lr to 0.00029251062834996234
Epoch 88: reducing lr to 0.00018339690185351158
Epoch 91: reducing lr to 9.857806437484698e-05
Epoch 94: reducing lr to 3.9391328751582634e-05
Epoch 97: reducing lr to 6.770279540888741e-06
[I 2024-06-22 15:46:20,795] Trial 484 finished with value: 2.55094838142395 and parameters: {'hidden_size': 146, 'n_layers': 5, 'rnn_dropout': 0.03463212869912598, 'bidirectional': False, 'fc_dropout': 0.06281833186728285, 'learning_rate_model': 0.03447755556815082}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 15:47:40,282] Trial 485 finished with value: 4.122429847717285 and parameters: {'hidden_size': 188, 'n_layers': 6, 'rnn_dropout': 0.0939020832807584, 'bidirectional': False, 'fc_dropout': 0.7103840719521407, 'learning_rate_model': 2.416039983331099e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 64: reducing lr to 0.00012099653242057294
Epoch 67: reducing lr to 0.00010433906490204545
Epoch 71: reducing lr to 8.291165122657168e-05
Epoch 79: reducing lr to 4.49448819052766e-05
Epoch 84: reducing lr to 2.6044015436683466e-05
Epoch 87: reducing lr to 1.6932514025934244e-05
Epoch 90: reducing lr to 9.67396779090234e-06
Epoch 93: reducing lr to 4.382811970907608e-06
Epoch 96: reducing lr to 1.142506843174445e-06
Epoch 99: reducing lr to 4.144365825823823e-09
[I 2024-06-22 15:48:01,837] Trial 486 finished with value: 2.0326106548309326 and parameters: {'hidden_size': 166, 'n_layers': 2, 'rnn_dropout': 0.47382990861273105, 'bidirectional': False, 'fc_dropout': 0.23573187919909558, 'learning_rate_model': 0.0026884759499057934}. Best is trial 346 with value: 1.8583240509033203.
Epoch 10: reducing lr to 0.0021969657135894947
Epoch 14: reducing lr to 0.003434047106523093
Epoch 17: reducing lr to 0.004258033780497909
Epoch 23: reducing lr to 0.0051636195732163
Epoch 26: reducing lr to 0.0051797387607742575
Epoch 31: reducing lr to 0.005080607146832256
Epoch 36: reducing lr to 0.004872796916573099
Epoch 42: reducing lr to 0.004493068916884195
Epoch 45: reducing lr to 0.004256651553245809
Epoch 53: reducing lr to 0.0035094199746501463
Epoch 59: reducing lr to 0.002877092891351323
Epoch 62: reducing lr to 0.0025517006892134576
Epoch 65: reducing lr to 0.0022269753097279694
Epoch 71: reducing lr to 0.0015999158155785432
Epoch 77: reducing lr to 0.0010353157761987697
Epoch 85: reducing lr to 0.0004401420952848879
Epoch 88: reducing lr to 0.0002759581663951932
Epoch 91: reducing lr to 0.00014833086936985872
Epoch 94: reducing lr to 5.927231455000112e-05
Epoch 97: reducing lr to 1.0187270936445892e-05
[I 2024-06-22 15:48:27,090] Trial 487 finished with value: 2.539828300476074 and parameters: {'hidden_size': 69, 'n_layers': 7, 'rnn_dropout': 0.6516044863538806, 'bidirectional': False, 'fc_dropout': 0.586000892086763, 'learning_rate_model': 0.05187853731561344}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 15:48:36,155] Trial 488 finished with value: 2.583651542663574 and parameters: {'hidden_size': 47, 'n_layers': 3, 'rnn_dropout': 0.07498934085971376, 'bidirectional': False, 'fc_dropout': 0.5061700316950029, 'learning_rate_model': 0.0007692716955833606}. Best is trial 346 with value: 1.8583240509033203.
Epoch 19: reducing lr to 0.004079583132231824
Epoch 23: reducing lr to 0.004489988845708387
Epoch 29: reducing lr to 0.004463931888375006
Epoch 32: reducing lr to 0.00438903115862672
Epoch 44: reducing lr to 0.0037725676164858655
Epoch 47: reducing lr to 0.0035514084771284945
Epoch 50: reducing lr to 0.003309812936014518
Epoch 54: reducing lr to 0.002962520203569796
Epoch 59: reducing lr to 0.0025017557562219735
Epoch 62: reducing lr to 0.002218813270362297
Epoch 65: reducing lr to 0.001936450615419438
Epoch 68: reducing lr to 0.0016591195761913127
Epoch 71: reducing lr to 0.0013911954713475515
Epoch 74: reducing lr to 0.0011369024130695564
Epoch 77: reducing lr to 0.0009002515040090218
Epoch 80: reducing lr to 0.0006849741700221117
Epoch 83: reducing lr to 0.0004944660975308317
Epoch 86: reducing lr to 0.0003317309283813741
Epoch 89: reducing lr to 0.0001993360572143346
Epoch 92: reducing lr to 9.936877454501322e-05
Epoch 95: reducing lr to 3.340591866900638e-05
Epoch 98: reducing lr to 2.487587666413077e-06
[I 2024-06-22 15:48:54,297] Trial 489 finished with value: 2.050323963165283 and parameters: {'hidden_size': 148, 'n_layers': 2, 'rnn_dropout': 0.13844537534574758, 'bidirectional': False, 'fc_dropout': 0.3377012542526669, 'learning_rate_model': 0.04511061486539399}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 15:49:04,535] Trial 490 finished with value: 3.4148664474487305 and parameters: {'hidden_size': 65, 'n_layers': 3, 'rnn_dropout': 0.1721677281306616, 'bidirectional': False, 'fc_dropout': 0.4603416456894996, 'learning_rate_model': 0.00012985537014035867}. Best is trial 346 with value: 1.8583240509033203.
Epoch 23: reducing lr to 0.0005400043355574908
Epoch 37: reducing lr to 0.0005039553799241183
Epoch 40: reducing lr to 0.0004846438838479668
Epoch 43: reducing lr to 0.0004619673947065907
Epoch 46: reducing lr to 0.00043628350744482865
Epoch 49: reducing lr to 0.0004079973103604087
Epoch 80: reducing lr to 8.238083306384624e-05
Epoch 87: reducing lr to 3.417008187752458e-05
Epoch 93: reducing lr to 8.844584074776517e-06
Epoch 96: reducing lr to 2.305596931271355e-06
Epoch 99: reducing lr to 8.363395971947019e-09
[I 2024-06-22 15:49:22,134] Trial 491 finished with value: 2.107400894165039 and parameters: {'hidden_size': 64, 'n_layers': 6, 'rnn_dropout': 0.7281429660580356, 'bidirectional': False, 'fc_dropout': 0.10648771590072253, 'learning_rate_model': 0.005425387109872358}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 15:49:55,303] Trial 492 finished with value: 3.26916241645813 and parameters: {'hidden_size': 55, 'n_layers': 5, 'rnn_dropout': 0.16134298784238643, 'bidirectional': True, 'fc_dropout': 0.535693411748701, 'learning_rate_model': 3.9349049956044136e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 23: reducing lr to 0.0010956846380008667
Epoch 27: reducing lr to 0.0010968019837095954
Epoch 30: reducing lr to 0.0010841661812967922
Epoch 39: reducing lr to 0.0009972093044942286
Epoch 42: reducing lr to 0.0009533983903935797
Epoch 47: reducing lr to 0.0008666444050022635
Epoch 55: reducing lr to 0.0007009002826273658
Epoch 59: reducing lr to 0.0006104993674411206
Epoch 62: reducing lr to 0.0005414533751575219
Epoch 65: reducing lr to 0.00047254887806467483
Epoch 70: reducing lr to 0.0003609652367170587
Epoch 73: reducing lr to 0.00029769013512945426
Epoch 77: reducing lr to 0.00021968690283555425
Epoch 80: reducing lr to 0.000167153126947738
Epoch 83: reducing lr to 0.00012066375345110552
Epoch 86: reducing lr to 8.095175615517418e-05
Epoch 89: reducing lr to 4.8643652177035766e-05
Epoch 92: reducing lr to 2.4248799608936334e-05
Epoch 95: reducing lr to 8.151991722411996e-06
Epoch 98: reducing lr to 6.070419516465351e-07
[I 2024-06-22 15:50:06,068] Trial 493 finished with value: 2.562098979949951 and parameters: {'hidden_size': 72, 'n_layers': 3, 'rnn_dropout': 0.5080639460360391, 'bidirectional': False, 'fc_dropout': 0.14667752541513865, 'learning_rate_model': 0.011008269600943211}. Best is trial 346 with value: 1.8583240509033203.
Epoch 21: reducing lr to 0.0006752453660987729
Epoch 27: reducing lr to 0.0006980586664151497
Epoch 30: reducing lr to 0.0006900166209845479
Epoch 33: reducing lr to 0.0006766172580220239
Epoch 36: reducing lr to 0.0006580718929815544
Epoch 41: reducing lr to 0.0006165565555409616
Epoch 44: reducing lr to 0.0005859236647911478
Epoch 47: reducing lr to 0.0005515750760824762
Epoch 50: reducing lr to 0.0005140524762944408
Epoch 53: reducing lr to 0.0004739476496815451
Epoch 56: reducing lr to 0.00043189294957852455
Epoch 59: reducing lr to 0.00038855179021638094
Epoch 62: reducing lr to 0.00034460752861704996
Epoch 70: reducing lr to 0.00022973601024381014
Epoch 94: reducing lr to 8.004734222486105e-06
Epoch 97: reducing lr to 1.3757923394389256e-06
[I 2024-06-22 15:50:42,500] Trial 494 finished with value: 2.1082558631896973 and parameters: {'hidden_size': 147, 'n_layers': 4, 'rnn_dropout': 0.5201303890525476, 'bidirectional': False, 'fc_dropout': 0.3324573765733087, 'learning_rate_model': 0.0070062035912650976}. Best is trial 346 with value: 1.8583240509033203.
Epoch 10: reducing lr to 0.0036599482098404433
Epoch 13: reducing lr to 0.005214462384771164
Epoch 16: reducing lr to 0.006667802746766446
Epoch 19: reducing lr to 0.0078158537520505
Epoch 22: reducing lr to 0.008497374271018984
Epoch 25: reducing lr to 0.008639504899025255
Epoch 28: reducing lr to 0.008585293714454356
Epoch 31: reducing lr to 0.00846383669846644
Epoch 34: reducing lr to 0.008277049462096854
Epoch 37: reducing lr to 0.00802787752553709
Epoch 40: reducing lr to 0.007720250438874027
Epoch 43: reducing lr to 0.007359019891908639
Epoch 47: reducing lr to 0.006803952357711479
Epoch 50: reducing lr to 0.006341092463626829
Epoch 53: reducing lr to 0.0058463795198758686
Epoch 56: reducing lr to 0.005327613918734003
Epoch 62: reducing lr to 0.004250904923899254
Epoch 66: reducing lr to 0.003531344695681441
Epoch 72: reducing lr to 0.002499630469547029
Epoch 75: reducing lr to 0.0020228806120741972
Epoch 78: reducing lr to 0.0015823787582915042
Epoch 84: reducing lr to 0.0008372230409029263
Epoch 87: reducing lr to 0.0005443204761335141
Epoch 90: reducing lr to 0.00031098387079314395
Epoch 93: reducing lr to 0.00014089191334223372
Epoch 96: reducing lr to 3.6727556694181315e-05
Epoch 99: reducing lr to 1.3322671259113905e-07
[I 2024-06-22 15:51:00,336] Trial 495 finished with value: 2.49501895904541 and parameters: {'hidden_size': 47, 'n_layers': 3, 'rnn_dropout': 0.39835676310994095, 'bidirectional': True, 'fc_dropout': 0.6632059927588773, 'learning_rate_model': 0.08642499908075395}. Best is trial 346 with value: 1.8583240509033203.
[I 2024-06-22 15:51:49,868] Trial 496 finished with value: 2.173394203186035 and parameters: {'hidden_size': 65, 'n_layers': 7, 'rnn_dropout': 0.23224011102032735, 'bidirectional': True, 'fc_dropout': 0.47808691910149603, 'learning_rate_model': 8.844423035919478e-05}. Best is trial 346 with value: 1.8583240509033203.
Epoch 32: reducing lr to 0.0013397191255161076
Epoch 45: reducing lr to 0.0011298068866107131
Epoch 52: reducing lr to 0.0009582375635467175
Epoch 55: reducing lr to 0.0008767202097530677
Epoch 61: reducing lr to 0.0007061126011651741
Epoch 64: reducing lr to 0.0006197130025394238
Epoch 68: reducing lr to 0.0005064339138656641
Epoch 71: reducing lr to 0.00042465207307365685
Epoch 74: reducing lr to 0.00034703100788905527
Epoch 77: reducing lr to 0.00027479507757080903
Epoch 80: reducing lr to 0.00020908327211563367
Epoch 83: reducing lr to 0.00015093209955443568
Epoch 86: reducing lr to 0.00010125839922649291
Epoch 89: reducing lr to 6.08458492553917e-05
Epoch 92: reducing lr to 3.0331579550395977e-05
Epoch 95: reducing lr to 1.0196908276292014e-05
Epoch 98: reducing lr to 7.593176381402722e-07
[I 2024-06-22 15:52:07,810] Trial 497 finished with value: 2.019359827041626 and parameters: {'hidden_size': 143, 'n_layers': 2, 'rnn_dropout': 0.44774919920141837, 'bidirectional': False, 'fc_dropout': 0.3851748122989864, 'learning_rate_model': 0.013769679757267702}. Best is trial 346 with value: 1.8583240509033203.
Epoch 17: reducing lr to 0.0015591371911141255
Epoch 22: reducing lr to 0.0018677051458809689
Epoch 25: reducing lr to 0.001898945161543216
Epoch 31: reducing lr to 0.0018603336573671143
Epoch 34: reducing lr to 0.0018192782123054336
Epoch 41: reducing lr to 0.001671679890369504
Epoch 44: reducing lr to 0.0015886244318067122
Epoch 47: reducing lr to 0.0014954945404920708
Epoch 50: reducing lr to 0.0013937589009366588
Epoch 54: reducing lr to 0.001247514280339369
Epoch 57: reducing lr to 0.0011321248181543598
Epoch 60: reducing lr to 0.0010138604205417246
Epoch 91: reducing lr to 5.4313372543933826e-05
Epoch 94: reducing lr to 2.1703367042690687e-05
Epoch 97: reducing lr to 3.7302083101632687e-06
[I 2024-06-22 15:52:22,272] Trial 498 finished with value: 2.0169873237609863 and parameters: {'hidden_size': 90, 'n_layers': 3, 'rnn_dropout': 0.6297817091020858, 'bidirectional': False, 'fc_dropout': 0.4618580250594632, 'learning_rate_model': 0.01899603458287186}. Best is trial 346 with value: 1.8583240509033203.
Epoch 58: reducing lr to 4.214886089392793e-05
Epoch 63: reducing lr to 3.449966906987606e-05
Epoch 66: reducing lr to 2.993344009442775e-05
Epoch 69: reducing lr to 2.547279773238899e-05
Epoch 81: reducing lr to 1.004524970779771e-05
Epoch 84: reducing lr to 7.0967203431570865e-06
Epoch 87: reducing lr to 4.613932019844572e-06
Epoch 90: reducing lr to 2.6360544973431196e-06
Epoch 93: reducing lr to 1.194270175034623e-06
Epoch 96: reducing lr to 3.113211008442282e-07
Epoch 99: reducing lr to 1.129296107852968e-09
[I 2024-06-22 15:52:43,872] Trial 499 finished with value: 2.570012092590332 and parameters: {'hidden_size': 123, 'n_layers': 3, 'rnn_dropout': 0.1284694628073942, 'bidirectional': False, 'fc_dropout': 0.08670311032332174, 'learning_rate_model': 0.0007325814259390815}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.1489651068024696e-05
Epoch 45: reducing lr to 4.289666482683783e-05
Epoch 53: reducing lr to 3.536639316280998e-05
Epoch 57: reducing lr to 3.115835415411462e-05
Epoch 65: reducing lr to 2.2442479081051417e-05
Epoch 68: reducing lr to 1.9228353196898506e-05
Epoch 72: reducing lr to 1.5120967369155076e-05
Epoch 75: reducing lr to 1.2236973464487092e-05
Epoch 80: reducing lr to 7.93850272213221e-06
Epoch 86: reducing lr to 3.844592968937753e-06
Epoch 89: reducing lr to 2.310203657408013e-06
Epoch 92: reducing lr to 1.1516336261191677e-06
Epoch 95: reducing lr to 3.8715763001789284e-07
Epoch 98: reducing lr to 2.8829877571487013e-08
[I 2024-06-22 15:53:39,323] Trial 500 finished with value: 1.8646125793457031 and parameters: {'hidden_size': 184, 'n_layers': 2, 'rnn_dropout': 0.04937489835106587, 'bidirectional': True, 'fc_dropout': 0.28414416167734113, 'learning_rate_model': 0.0005228091139472129}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.189293963177502e-05
Epoch 45: reducing lr to 4.3232649514806274e-05
Epoch 53: reducing lr to 3.564339760171783e-05
Epoch 57: reducing lr to 3.140239946487106e-05
Epoch 65: reducing lr to 2.2618257999103058e-05
Epoch 68: reducing lr to 1.9378957731658628e-05
Epoch 72: reducing lr to 1.5239401133733622e-05
Epoch 75: reducing lr to 1.233281857803474e-05
Epoch 80: reducing lr to 8.000680408225037e-06
Epoch 86: reducing lr to 3.874705435122442e-06
Epoch 89: reducing lr to 2.328298142331513e-06
Epoch 92: reducing lr to 1.1606537041622379e-06
Epoch 95: reducing lr to 3.9019001111421447e-07
Epoch 98: reducing lr to 2.905568527609634e-08
[I 2024-06-22 15:54:34,596] Trial 501 finished with value: 1.8646453619003296 and parameters: {'hidden_size': 184, 'n_layers': 2, 'rnn_dropout': 0.05003075135177646, 'bidirectional': True, 'fc_dropout': 0.28152922394890423, 'learning_rate_model': 0.000526903974415403}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.1927393072134215e-05
Epoch 45: reducing lr to 4.3261353101887645e-05
Epoch 53: reducing lr to 3.566706243323795e-05
Epoch 57: reducing lr to 3.142324855734445e-05
Epoch 65: reducing lr to 2.2633275009288466e-05
Epoch 68: reducing lr to 1.9391824063170564e-05
Epoch 72: reducing lr to 1.5249519076594383e-05
Epoch 75: reducing lr to 1.2341006744524336e-05
Epoch 80: reducing lr to 8.005992324783127e-06
Epoch 86: reducing lr to 3.877277980319648e-06
Epoch 89: reducing lr to 2.3298439765385397e-06
Epoch 92: reducing lr to 1.1614243005758923e-06
Epoch 95: reducing lr to 3.9044907117850036e-07
Epoch 98: reducing lr to 2.907497630734706e-08
[I 2024-06-22 15:55:31,578] Trial 502 finished with value: 1.866842269897461 and parameters: {'hidden_size': 182, 'n_layers': 2, 'rnn_dropout': 0.060755985636413094, 'bidirectional': True, 'fc_dropout': 0.2617732389677623, 'learning_rate_model': 0.000527253803405365}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.538033341614307e-05
Epoch 41: reducing lr to 4.948447121034179e-05
Epoch 45: reducing lr to 4.6138040388199374e-05
Epoch 53: reducing lr to 3.803876321661695e-05
Epoch 57: reducing lr to 3.351275462079631e-05
Epoch 65: reducing lr to 2.4138286984144522e-05
Epoch 68: reducing lr to 2.0681293988198925e-05
Epoch 71: reducing lr to 1.734156051852409e-05
Epoch 74: reducing lr to 1.41717410715869e-05
Epoch 80: reducing lr to 8.538355154044886e-06
Epoch 86: reducing lr to 4.135099695817379e-06
Epoch 89: reducing lr to 2.484768223374122e-06
Epoch 96: reducing lr to 2.389634179350195e-07
Epoch 99: reducing lr to 8.668235370592678e-10
[I 2024-06-22 15:56:27,887] Trial 503 finished with value: 1.8635815382003784 and parameters: {'hidden_size': 181, 'n_layers': 2, 'rnn_dropout': 0.05070199547022872, 'bidirectional': True, 'fc_dropout': 0.2723808357262937, 'learning_rate_model': 0.000562313832834947}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.774206810409905e-05
Epoch 57: reducing lr to 2.7414269366705924e-05
Epoch 68: reducing lr to 1.691781444586752e-05
Epoch 72: reducing lr to 1.3303984879716342e-05
Epoch 75: reducing lr to 1.0766540656460875e-05
Epoch 80: reducing lr to 6.9845875336172655e-06
Epoch 86: reducing lr to 3.382614714965139e-06
Epoch 89: reducing lr to 2.0326023975104326e-06
Epoch 92: reducing lr to 1.0132497461846175e-06
Epoch 98: reducing lr to 2.5365589775529696e-08
[I 2024-06-22 15:57:25,676] Trial 504 finished with value: 1.8661959171295166 and parameters: {'hidden_size': 183, 'n_layers': 2, 'rnn_dropout': 0.05137703732186733, 'bidirectional': True, 'fc_dropout': 0.28670513984467216, 'learning_rate_model': 0.0004599867440438947}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.934321868369614e-05
Epoch 53: reducing lr to 3.2436734787537076e-05
Epoch 57: reducing lr to 2.85772785893236e-05
Epoch 68: reducing lr to 1.7635527326115713e-05
Epoch 72: reducing lr to 1.3868386465828552e-05
Epoch 75: reducing lr to 1.1223294980701936e-05
Epoch 80: reducing lr to 7.280898174222652e-06
Epoch 86: reducing lr to 3.5261170661474183e-06
Epoch 89: reducing lr to 2.1188325028106426e-06
Epoch 92: reducing lr to 1.0562353455403602e-06
Epoch 98: reducing lr to 2.6441686842035382e-08
[I 2024-06-22 15:58:23,459] Trial 505 finished with value: 1.8661065101623535 and parameters: {'hidden_size': 183, 'n_layers': 2, 'rnn_dropout': 0.05816725814564073, 'bidirectional': True, 'fc_dropout': 0.2712146532064815, 'learning_rate_model': 0.0004795009911116876}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.292966833417026e-05
Epoch 41: reducing lr to 4.729470711513821e-05
Epoch 45: reducing lr to 4.409636101295292e-05
Epoch 53: reducing lr to 3.635548933532931e-05
Epoch 57: reducing lr to 3.202976359340809e-05
Epoch 65: reducing lr to 2.307013059356854e-05
Epoch 68: reducing lr to 1.9766114864121653e-05
Epoch 72: reducing lr to 1.554385728277364e-05
Epoch 75: reducing lr to 1.257920637360015e-05
Epoch 80: reducing lr to 8.160519782844302e-06
Epoch 86: reducing lr to 3.9521151630434815e-06
Epoch 89: reducing lr to 2.3748134010356282e-06
Epoch 92: reducing lr to 1.1838415022940187e-06
Epoch 95: reducing lr to 3.9798531403558706e-07
Epoch 98: reducing lr to 2.9636166226057643e-08
[I 2024-06-22 15:59:20,310] Trial 506 finished with value: 1.8669276237487793 and parameters: {'hidden_size': 182, 'n_layers': 2, 'rnn_dropout': 0.062134729019544616, 'bidirectional': True, 'fc_dropout': 0.27360738995970096, 'learning_rate_model': 0.0005374305793362021}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.955872601706545e-05
Epoch 53: reducing lr to 3.261441100344278e-05
Epoch 57: reducing lr to 2.873381415783542e-05
Epoch 65: reducing lr to 2.0696151663417806e-05
Epoch 68: reducing lr to 1.773212810240624e-05
Epoch 72: reducing lr to 1.3944352263375881e-05
Epoch 75: reducing lr to 1.1284771963364533e-05
Epoch 80: reducing lr to 7.3207801920787686e-06
Epoch 86: reducing lr to 3.5454318073276622e-06
Epoch 89: reducing lr to 2.130438669205114e-06
Epoch 92: reducing lr to 1.0620210049333534e-06
Epoch 95: reducing lr to 3.570315471638336e-07
Epoch 98: reducing lr to 2.6586524443322856e-08
[I 2024-06-22 16:00:17,509] Trial 507 finished with value: 1.8654322624206543 and parameters: {'hidden_size': 185, 'n_layers': 2, 'rnn_dropout': 0.05158356038901311, 'bidirectional': True, 'fc_dropout': 0.2802440689464648, 'learning_rate_model': 0.00048212751693747747}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.955299176054362e-05
Epoch 53: reducing lr to 3.2609683363859015e-05
Epoch 57: reducing lr to 2.872964903226589e-05
Epoch 65: reducing lr to 2.0693151641561653e-05
Epoch 68: reducing lr to 1.772955773218822e-05
Epoch 72: reducing lr to 1.3942330952252904e-05
Epoch 75: reducing lr to 1.128313617314216e-05
Epoch 80: reducing lr to 7.319719004427193e-06
Epoch 86: reducing lr to 3.5449178773427816e-06
Epoch 89: reducing lr to 2.1301298503157457e-06
Epoch 92: reducing lr to 1.0618670591042763e-06
Epoch 95: reducing lr to 3.569797934628398e-07
Epoch 98: reducing lr to 2.6582670579295906e-08
[I 2024-06-22 16:01:12,678] Trial 508 finished with value: 1.8643827438354492 and parameters: {'hidden_size': 184, 'n_layers': 2, 'rnn_dropout': 0.05185993747671741, 'bidirectional': True, 'fc_dropout': 0.2736882286492746, 'learning_rate_model': 0.00048205762988254145}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.421600738758553e-05
Epoch 45: reducing lr to 4.516802598025143e-05
Epoch 53: reducing lr to 3.7239029459609015e-05
Epoch 57: reducing lr to 3.2808176477496775e-05
Epoch 65: reducing lr to 2.363079932405328e-05
Epoch 68: reducing lr to 2.0246486766765844e-05
Epoch 72: reducing lr to 1.5921616511063323e-05
Epoch 75: reducing lr to 1.2884916288825253e-05
Epoch 80: reducing lr to 8.35884324911971e-06
Epoch 86: reducing lr to 4.048162620694652e-06
Epoch 89: reducing lr to 2.43252801211233e-06
Epoch 92: reducing lr to 1.212612163539056e-06
Epoch 98: reducing lr to 3.0356407996112637e-08
[I 2024-06-22 16:02:10,523] Trial 509 finished with value: 1.866182804107666 and parameters: {'hidden_size': 183, 'n_layers': 2, 'rnn_dropout': 0.05230129024556353, 'bidirectional': True, 'fc_dropout': 0.27968355440632864, 'learning_rate_model': 0.0005504916463040722}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 4.041473340178502e-05
Epoch 53: reducing lr to 3.332015103802283e-05
Epoch 57: reducing lr to 2.935558233863214e-05
Epoch 65: reducing lr to 2.1143993655384865e-05
Epoch 68: reducing lr to 1.8115831879820734e-05
Epoch 72: reducing lr to 1.4246092731646555e-05
Epoch 75: reducing lr to 1.1528962034888806e-05
Epoch 80: reducing lr to 7.4791938352184575e-06
Epoch 86: reducing lr to 3.622151057785394e-06
Epoch 89: reducing lr to 2.1765390222029526e-06
Epoch 92: reducing lr to 1.0850019730908712e-06
Epoch 95: reducing lr to 3.647573177262666e-07
Epoch 98: reducing lr to 2.7161827633012844e-08
[I 2024-06-22 16:03:07,567] Trial 510 finished with value: 1.86665940284729 and parameters: {'hidden_size': 182, 'n_layers': 2, 'rnn_dropout': 0.05190789295658292, 'bidirectional': True, 'fc_dropout': 0.2850638631115657, 'learning_rate_model': 0.00049256022689626}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.066612957440003e-05
Epoch 45: reducing lr to 4.221057889001446e-05
Epoch 53: reducing lr to 3.4800745808100274e-05
Epoch 57: reducing lr to 3.066000984958665e-05
Epoch 65: reducing lr to 2.2083535807789557e-05
Epoch 68: reducing lr to 1.8920816404238316e-05
Epoch 72: reducing lr to 1.4879123787491546e-05
Epoch 75: reducing lr to 1.2041256258099224e-05
Epoch 80: reducing lr to 7.811534924074366e-06
Epoch 86: reducing lr to 3.783102846583342e-06
Epoch 89: reducing lr to 2.2732544389327806e-06
Epoch 92: reducing lr to 1.1332144870452296e-06
Epoch 98: reducing lr to 2.8368774741176565e-08
[I 2024-06-22 16:04:05,447] Trial 511 finished with value: 1.8660364151000977 and parameters: {'hidden_size': 183, 'n_layers': 2, 'rnn_dropout': 0.053850111475131746, 'bidirectional': True, 'fc_dropout': 0.280997335829327, 'learning_rate_model': 0.0005144473454467897}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.0348273758223894e-05
Epoch 45: reducing lr to 4.194576928018131e-05
Epoch 53: reducing lr to 3.4582422057000855e-05
Epoch 57: reducing lr to 3.046766315690317e-05
Epoch 65: reducing lr to 2.194499393855259e-05
Epoch 68: reducing lr to 1.8802115970804646e-05
Epoch 72: reducing lr to 1.4785779060448324e-05
Epoch 75: reducing lr to 1.1965714996750576e-05
Epoch 80: reducing lr to 7.762528974148119e-06
Epoch 86: reducing lr to 3.7593694125697342e-06
Epoch 89: reducing lr to 2.258993094102766e-06
Epoch 92: reducing lr to 1.1261052245318355e-06
Epoch 98: reducing lr to 2.8190802195719126e-08
[I 2024-06-22 16:05:03,252] Trial 512 finished with value: 1.866053819656372 and parameters: {'hidden_size': 183, 'n_layers': 2, 'rnn_dropout': 0.052407377439870124, 'bidirectional': True, 'fc_dropout': 0.27308884566735053, 'learning_rate_model': 0.0005112199412175695}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.866731822267971e-05
Epoch 53: reducing lr to 3.187948490483113e-05
Epoch 57: reducing lr to 2.8086332591020637e-05
Epoch 65: reducing lr to 2.0229789048539823e-05
Epoch 68: reducing lr to 1.7332556154747648e-05
Epoch 72: reducing lr to 1.3630133238984863e-05
Epoch 75: reducing lr to 1.1030483347455384e-05
Epoch 80: reducing lr to 7.155815311223188e-06
Epoch 86: reducing lr to 3.465539812167064e-06
Epoch 89: reducing lr to 2.0824318240309065e-06
Epoch 92: reducing lr to 1.0380896528167452e-06
Epoch 95: reducing lr to 3.4898627533566237e-07
Epoch 98: reducing lr to 2.598742943950858e-08
[I 2024-06-22 16:05:58,411] Trial 513 finished with value: 1.864250898361206 and parameters: {'hidden_size': 184, 'n_layers': 2, 'rnn_dropout': 0.05249358028923529, 'bidirectional': True, 'fc_dropout': 0.27572782996143563, 'learning_rate_model': 0.00047126335952501396}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.029807102993732e-05
Epoch 45: reducing lr to 4.190394476663282e-05
Epoch 53: reducing lr to 3.454793960490416e-05
Epoch 57: reducing lr to 3.043728356886927e-05
Epoch 65: reducing lr to 2.192311238262799e-05
Epoch 68: reducing lr to 1.8783368207498447e-05
Epoch 72: reducing lr to 1.477103602373089e-05
Epoch 75: reducing lr to 1.1953783871929477e-05
Epoch 80: reducing lr to 7.754788884889505e-06
Epoch 86: reducing lr to 3.755620910643844e-06
Epoch 89: reducing lr to 2.256740631246759e-06
Epoch 92: reducing lr to 1.124982374622809e-06
Epoch 98: reducing lr to 2.8162692886752294e-08
[I 2024-06-22 16:06:56,295] Trial 514 finished with value: 1.8660073280334473 and parameters: {'hidden_size': 183, 'n_layers': 2, 'rnn_dropout': 0.057005129851085516, 'bidirectional': True, 'fc_dropout': 0.2790513131884594, 'learning_rate_model': 0.0005107101990975742}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.4992207920966216e-05
Epoch 41: reducing lr to 4.913766605935512e-05
Epoch 45: reducing lr to 4.581468824010741e-05
Epoch 53: reducing lr to 3.7772173745253086e-05
Epoch 57: reducing lr to 3.3277885061882724e-05
Epoch 65: reducing lr to 2.396911709999011e-05
Epoch 68: reducing lr to 2.0536351966818322e-05
Epoch 72: reducing lr to 1.6149563344916682e-05
Epoch 75: reducing lr to 1.3069387248194387e-05
Epoch 80: reducing lr to 8.478515259307212e-06
Epoch 86: reducing lr to 4.106119414947916e-06
Epoch 89: reducing lr to 2.467354064996817e-06
Epoch 92: reducing lr to 1.229972907228541e-06
Epoch 95: reducing lr to 4.134938273325068e-07
Epoch 98: reducing lr to 3.079101506539084e-08
[I 2024-06-22 16:07:53,133] Trial 515 finished with value: 1.8672003746032715 and parameters: {'hidden_size': 182, 'n_layers': 2, 'rnn_dropout': 0.04958245695913232, 'bidirectional': True, 'fc_dropout': 0.27472416161603025, 'learning_rate_model': 0.0005583729332167761}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.1564664520254595e-05
Epoch 45: reducing lr to 4.295915946121706e-05
Epoch 53: reducing lr to 3.5417917210633836e-05
Epoch 57: reducing lr to 3.120374765868147e-05
Epoch 65: reducing lr to 2.247517473537315e-05
Epoch 68: reducing lr to 1.9256366304854694e-05
Epoch 72: reducing lr to 1.5142996571915012e-05
Epoch 75: reducing lr to 1.225480107849061e-05
Epoch 80: reducing lr to 7.95006804608317e-06
Epoch 86: reducing lr to 3.850194020509019e-06
Epoch 89: reducing lr to 2.3135693114394847e-06
Epoch 92: reducing lr to 1.1533114004331677e-06
Epoch 98: reducing lr to 2.887187880083698e-08
[I 2024-06-22 16:08:50,860] Trial 516 finished with value: 1.8660694360733032 and parameters: {'hidden_size': 183, 'n_layers': 2, 'rnn_dropout': 0.05198856116780948, 'bidirectional': True, 'fc_dropout': 0.2727449568231566, 'learning_rate_model': 0.0005235707760614624}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.95893954897047e-05
Epoch 53: reducing lr to 3.263969662021123e-05
Epoch 57: reducing lr to 2.8756091187857945e-05
Epoch 65: reducing lr to 2.071219717653431e-05
Epoch 68: reducing lr to 1.7745875638599342e-05
Epoch 72: reducing lr to 1.3955163175992965e-05
Epoch 75: reducing lr to 1.129352093078126e-05
Epoch 80: reducing lr to 7.326455917523067e-06
Epoch 86: reducing lr to 3.5481805440731874e-06
Epoch 89: reducing lr to 2.1320903763517657e-06
Epoch 92: reducing lr to 1.0628443788746467e-06
Epoch 95: reducing lr to 3.573083500432413e-07
Epoch 98: reducing lr to 2.660713670176156e-08
[I 2024-06-22 16:09:46,012] Trial 517 finished with value: 1.8643577098846436 and parameters: {'hidden_size': 184, 'n_layers': 2, 'rnn_dropout': 0.05594250639968676, 'bidirectional': True, 'fc_dropout': 0.2719877650545106, 'learning_rate_model': 0.0004825013054331678}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.91342799531793e-05
Epoch 53: reducing lr to 3.226447409267361e-05
Epoch 57: reducing lr to 2.8425514180872863e-05
Epoch 68: reducing lr to 1.7541870914291934e-05
Epoch 72: reducing lr to 1.3794736084404938e-05
Epoch 75: reducing lr to 1.1163691799164188e-05
Epoch 80: reducing lr to 7.242231749043395e-06
Epoch 86: reducing lr to 3.507391032840949e-06
Epoch 89: reducing lr to 2.1075800891005653e-06
Epoch 92: reducing lr to 1.0506260314169226e-06
Epoch 98: reducing lr to 2.6301263850067287e-08
[I 2024-06-22 16:10:43,819] Trial 518 finished with value: 1.866119384765625 and parameters: {'hidden_size': 183, 'n_layers': 2, 'rnn_dropout': 0.05217314145677068, 'bidirectional': True, 'fc_dropout': 0.2738358433540886, 'learning_rate_model': 0.0004769545210536607}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 4.0298188811100516e-05
Epoch 53: reducing lr to 3.3224065204035834e-05
Epoch 57: reducing lr to 2.927092919261361e-05
Epoch 68: reducing lr to 1.8063590975733766e-05
Epoch 72: reducing lr to 1.4205011054087106e-05
Epoch 75: reducing lr to 1.1495715789070095e-05
Epoch 80: reducing lr to 7.457625968482581e-06
Epoch 86: reducing lr to 3.61170580485671e-06
Epoch 89: reducing lr to 2.1702625030204637e-06
Epoch 92: reducing lr to 1.0818731361494373e-06
Epoch 98: reducing lr to 2.7083500651306664e-08
[I 2024-06-22 16:11:41,619] Trial 519 finished with value: 1.866039752960205 and parameters: {'hidden_size': 183, 'n_layers': 2, 'rnn_dropout': 0.05534131620137129, 'bidirectional': True, 'fc_dropout': 0.27624288078273135, 'learning_rate_model': 0.0004911398233651914}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.88739697615572e-05
Epoch 53: reducing lr to 3.204985991186592e-05
Epoch 57: reducing lr to 2.823643567854108e-05
Epoch 68: reducing lr to 1.74251873370146e-05
Epoch 72: reducing lr to 1.3702977391059765e-05
Epoch 75: reducing lr to 1.1089434070264426e-05
Epoch 80: reducing lr to 7.194058466268788e-06
Epoch 86: reducing lr to 3.4840608570220813e-06
Epoch 89: reducing lr to 2.0935610608340665e-06
Epoch 92: reducing lr to 1.043637563406558e-06
Epoch 98: reducing lr to 2.612631526174706e-08
[I 2024-06-22 16:12:39,337] Trial 520 finished with value: 1.866112470626831 and parameters: {'hidden_size': 183, 'n_layers': 2, 'rnn_dropout': 0.04995010038749977, 'bidirectional': True, 'fc_dropout': 0.27625500758383253, 'learning_rate_model': 0.00047378195411441844}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 4.14616370736175e-05
Epoch 53: reducing lr to 3.418327657496324e-05
Epoch 57: reducing lr to 3.011600964699928e-05
Epoch 65: reducing lr to 2.169170788561278e-05
Epoch 68: reducing lr to 1.8585104576110418e-05
Epoch 72: reducing lr to 1.4615123665037993e-05
Epoch 75: reducing lr to 1.1827608386622727e-05
Epoch 80: reducing lr to 7.672934949643108e-06
Epoch 86: reducing lr to 3.715979296230735e-06
Epoch 89: reducing lr to 2.2329201115343584e-06
Epoch 92: reducing lr to 1.1131078754181724e-06
Epoch 98: reducing lr to 2.786542789680671e-08
[I 2024-06-22 16:13:37,157] Trial 521 finished with value: 1.866028070449829 and parameters: {'hidden_size': 183, 'n_layers': 2, 'rnn_dropout': 0.04973574719785377, 'bidirectional': True, 'fc_dropout': 0.278731048511266, 'learning_rate_model': 0.0005053195121056871}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.956082627945183e-05
Epoch 53: reducing lr to 3.261614257641242e-05
Epoch 57: reducing lr to 2.8735339701126842e-05
Epoch 68: reducing lr to 1.773306954126016e-05
Epoch 72: reducing lr to 1.3945092600629118e-05
Epoch 75: reducing lr to 1.128537109747424e-05
Epoch 80: reducing lr to 7.321168868884732e-06
Epoch 86: reducing lr to 3.5456200423346547e-06
Epoch 89: reducing lr to 2.1305517790206706e-06
Epoch 92: reducing lr to 1.062077390034564e-06
Epoch 98: reducing lr to 2.658793598214488e-08
[I 2024-06-22 16:14:34,995] Trial 522 finished with value: 1.8661060333251953 and parameters: {'hidden_size': 183, 'n_layers': 2, 'rnn_dropout': 0.057841855924798735, 'bidirectional': True, 'fc_dropout': 0.2746753657332842, 'learning_rate_model': 0.00048215311417963393}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 4.0742343722923514e-05
Epoch 53: reducing lr to 3.359025118376492e-05
Epoch 57: reducing lr to 2.9593544857438267e-05
Epoch 68: reducing lr to 1.8262682619644667e-05
Epoch 72: reducing lr to 1.436157455280283e-05
Epoch 75: reducing lr to 1.1622418223677532e-05
Epoch 80: reducing lr to 7.539821752019333e-06
Epoch 86: reducing lr to 3.6515129753676808e-06
Epoch 89: reducing lr to 2.1941825048642236e-06
Epoch 92: reducing lr to 1.0937972270717964e-06
Epoch 98: reducing lr to 2.738200711520063e-08
[I 2024-06-22 16:15:32,870] Trial 523 finished with value: 1.866037368774414 and parameters: {'hidden_size': 183, 'n_layers': 2, 'rnn_dropout': 0.053749929268909535, 'bidirectional': True, 'fc_dropout': 0.27414699635553713, 'learning_rate_model': 0.0004965530235951591}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.855640293551045e-05
Epoch 53: reducing lr to 3.178804018133973e-05
Epoch 57: reducing lr to 2.8005768337070464e-05
Epoch 65: reducing lr to 2.0171760900614733e-05
Epoch 68: reducing lr to 1.7282838575881435e-05
Epoch 72: reducing lr to 1.3591035876875319e-05
Epoch 75: reducing lr to 1.0998842952301703e-05
Epoch 80: reducing lr to 7.135289209423117e-06
Epoch 86: reducing lr to 3.455599082860484e-06
Epoch 89: reducing lr to 2.0764584714843755e-06
Epoch 92: reducing lr to 1.0351119440631507e-06
Epoch 95: reducing lr to 3.4798522548979185e-07
Epoch 98: reducing lr to 2.5912885785289415e-08
[I 2024-06-22 16:16:28,042] Trial 524 finished with value: 1.8642241954803467 and parameters: {'hidden_size': 184, 'n_layers': 2, 'rnn_dropout': 0.05316977673917765, 'bidirectional': True, 'fc_dropout': 0.28307139417318483, 'learning_rate_model': 0.000469911563919406}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.980341859013469e-05
Epoch 53: reducing lr to 3.2816149152040584e-05
Epoch 57: reducing lr to 2.8911548671260875e-05
Epoch 65: reducing lr to 2.0824168794226523e-05
Epoch 68: reducing lr to 1.784181111013251e-05
Epoch 72: reducing lr to 1.4030605785130788e-05
Epoch 75: reducing lr to 1.1354574511783912e-05
Epoch 80: reducing lr to 7.3660632616422645e-06
Epoch 86: reducing lr to 3.56736226158957e-06
Epoch 89: reducing lr to 2.1436166092507316e-06
Epoch 92: reducing lr to 1.0685901915203678e-06
Epoch 95: reducing lr to 3.5923998451099997e-07
Epoch 98: reducing lr to 2.6750976783680496e-08
[I 2024-06-22 16:17:23,259] Trial 525 finished with value: 1.8644160032272339 and parameters: {'hidden_size': 184, 'n_layers': 2, 'rnn_dropout': 0.051746389109116823, 'bidirectional': True, 'fc_dropout': 0.28377476111985694, 'learning_rate_model': 0.00048510974196201495}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.8214067565430136e-05
Epoch 53: reducing lr to 3.1505799887352545e-05
Epoch 65: reducing lr to 1.999265946201234e-05
Epoch 68: reducing lr to 1.7129387359236336e-05
Epoch 72: reducing lr to 1.3470363512690643e-05
Epoch 75: reducing lr to 1.0901186203075656e-05
Epoch 80: reducing lr to 7.071936259299021e-06
Epoch 86: reducing lr to 3.4249174398436587e-06
Epoch 89: reducing lr to 2.0580219700171383e-06
Epoch 92: reducing lr to 1.025921371201931e-06
Epoch 95: reducing lr to 3.4489552723267614e-07
Epoch 98: reducing lr to 2.568280993096297e-08
[I 2024-06-22 16:18:18,460] Trial 526 finished with value: 1.8641693592071533 and parameters: {'hidden_size': 184, 'n_layers': 2, 'rnn_dropout': 0.04936702965290953, 'bidirectional': True, 'fc_dropout': 0.27656699772178095, 'learning_rate_model': 0.0004657393036230179}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.7944391958957934e-05
Epoch 53: reducing lr to 3.128346433834389e-05
Epoch 65: reducing lr to 1.9851571823116557e-05
Epoch 68: reducing lr to 1.700850575152237e-05
Epoch 72: reducing lr to 1.3375303533967727e-05
Epoch 75: reducing lr to 1.0824256836801122e-05
Epoch 80: reducing lr to 7.022029802824744e-06
Epoch 86: reducing lr to 3.400747893219871e-06
Epoch 89: reducing lr to 2.0434985665101134e-06
Epoch 92: reducing lr to 1.018681472766676e-06
Epoch 95: reducing lr to 3.4246160913911465e-07
Epoch 98: reducing lr to 2.550156705928108e-08
[I 2024-06-22 16:19:13,577] Trial 527 finished with value: 1.8640494346618652 and parameters: {'hidden_size': 184, 'n_layers': 2, 'rnn_dropout': 0.048599939422613786, 'bidirectional': True, 'fc_dropout': 0.28506574512482136, 'learning_rate_model': 0.00046245259437785767}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.6953330049267386e-05
Epoch 53: reducing lr to 3.0466377851823556e-05
Epoch 57: reducing lr to 2.6841362830813713e-05
Epoch 65: reducing lr to 1.9333072628224797e-05
Epoch 68: reducing lr to 1.6564264025121298e-05
Epoch 72: reducing lr to 1.302595668246454e-05
Epoch 75: reducing lr to 1.0541540258729073e-05
Epoch 80: reducing lr to 6.838622824691614e-06
Epoch 86: reducing lr to 3.3119244458689153e-06
Epoch 89: reducing lr to 1.990124840190664e-06
Epoch 92: reducing lr to 9.920747371294694e-07
Epoch 95: reducing lr to 3.335169235392987e-07
Epoch 98: reducing lr to 2.4835496779989287e-08
[I 2024-06-22 16:20:10,736] Trial 528 finished with value: 1.8654133081436157 and parameters: {'hidden_size': 185, 'n_layers': 2, 'rnn_dropout': 0.045163022665820035, 'bidirectional': True, 'fc_dropout': 0.283792000559985, 'learning_rate_model': 0.00045037388846998054}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.7091847942672553e-05
Epoch 53: reducing lr to 3.0580579697072526e-05
Epoch 57: reducing lr to 2.694197647051794e-05
Epoch 65: reducing lr to 1.9405541780259017e-05
Epoch 68: reducing lr to 1.662635442280692e-05
Epoch 72: reducing lr to 1.3074783894432619e-05
Epoch 75: reducing lr to 1.058105474762461e-05
Epoch 80: reducing lr to 6.864257094355702e-06
Epoch 86: reducing lr to 3.324339045493555e-06
Epoch 89: reducing lr to 1.9975847335239286e-06
Epoch 92: reducing lr to 9.957934846010607e-07
Epoch 95: reducing lr to 3.3476709670643053e-07
Epoch 98: reducing lr to 2.492859152113916e-08
[I 2024-06-22 16:21:07,890] Trial 529 finished with value: 1.8653912544250488 and parameters: {'hidden_size': 185, 'n_layers': 2, 'rnn_dropout': 0.047901333262550444, 'bidirectional': True, 'fc_dropout': 0.2782207430188983, 'learning_rate_model': 0.00045206209470720967}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.649145525222856e-05
Epoch 53: reducing lr to 3.008558261447801e-05
Epoch 57: reducing lr to 2.6505876177968115e-05
Epoch 65: reducing lr to 1.909143110405382e-05
Epoch 68: reducing lr to 1.635722947439203e-05
Epoch 72: reducing lr to 1.2863146968402814e-05
Epoch 75: reducing lr to 1.0409782937778952e-05
Epoch 80: reducing lr to 6.753147780224223e-06
Epoch 86: reducing lr to 3.270529139161668e-06
Epoch 89: reducing lr to 1.965250532369976e-06
Epoch 92: reducing lr to 9.796749258743495e-07
Epoch 95: reducing lr to 3.2934833951282035e-07
Epoch 98: reducing lr to 2.4525081182276248e-08
[I 2024-06-22 16:22:05,065] Trial 530 finished with value: 1.865410327911377 and parameters: {'hidden_size': 185, 'n_layers': 2, 'rnn_dropout': 0.04451643748189007, 'bidirectional': True, 'fc_dropout': 0.2741583271436722, 'learning_rate_model': 0.0004447447246557499}. Best is trial 346 with value: 1.8583240509033203.
Epoch 39: reducing lr to 3.909748759436544e-05
Epoch 45: reducing lr to 3.54129383486058e-05
Epoch 53: reducing lr to 2.9196393373304298e-05
Epoch 57: reducing lr to 2.5722486332162373e-05
Epoch 67: reducing lr to 1.675029153008616e-05
Epoch 72: reducing lr to 1.2482972449647244e-05
Epoch 75: reducing lr to 1.0102118395933834e-05
Epoch 80: reducing lr to 6.553556287276302e-06
Epoch 86: reducing lr to 3.1738675800104638e-06
Epoch 89: reducing lr to 1.907166909659823e-06
Epoch 92: reducing lr to 9.507203127928014e-07
Epoch 95: reducing lr to 3.1961434154289614e-07
Epoch 98: reducing lr to 2.3800234380880193e-08
[I 2024-06-22 16:23:02,344] Trial 531 finished with value: 1.8690615892410278 and parameters: {'hidden_size': 186, 'n_layers': 2, 'rnn_dropout': 0.04235608099034953, 'bidirectional': True, 'fc_dropout': 0.2866949209608417, 'learning_rate_model': 0.0004316001487537247}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.231912084124783e-05
Epoch 41: reducing lr to 5.568454631628527e-05
Epoch 45: reducing lr to 5.191882997028762e-05
Epoch 48: reducing lr to 4.8715393987192504e-05
Epoch 53: reducing lr to 4.28047672399344e-05
Epoch 57: reducing lr to 3.771167987095776e-05
Epoch 65: reducing lr to 2.7162653791953046e-05
Epoch 68: reducing lr to 2.3272522567158318e-05
Epoch 71: reducing lr to 1.9514342707346252e-05
Epoch 74: reducing lr to 1.5947365967169487e-05
Epoch 80: reducing lr to 9.608154263573097e-06
Epoch 86: reducing lr to 4.653200183860464e-06
Epoch 89: reducing lr to 2.796093154791427e-06
Epoch 92: reducing lr to 1.393848931237633e-06
Epoch 97: reducing lr to 1.2425522291279172e-07
[I 2024-06-22 16:23:59,429] Trial 532 finished with value: 1.8673988580703735 and parameters: {'hidden_size': 185, 'n_layers': 2, 'rnn_dropout': 0.06778214656593687, 'bidirectional': True, 'fc_dropout': 0.2930763662368975, 'learning_rate_model': 0.0006327680159637948}. Best is trial 346 with value: 1.8583240509033203.
Epoch 39: reducing lr to 4.117204908810414e-05
Epoch 45: reducing lr to 3.7291993060263225e-05
Epoch 53: reducing lr to 3.074559044900166e-05
Epoch 57: reducing lr to 2.708735356408176e-05
Epoch 65: reducing lr to 1.951025277895402e-05
Epoch 68: reducing lr to 1.6716069113382593e-05
Epoch 72: reducing lr to 1.3145334549230633e-05
Epoch 75: reducing lr to 1.0638149407614854e-05
Epoch 80: reducing lr to 6.901296164111561e-06
Epoch 86: reducing lr to 3.3422769554677326e-06
Epoch 89: reducing lr to 2.008363566436387e-06
Epoch 92: reducing lr to 1.0011667192907838e-06
Epoch 95: reducing lr to 3.365734774518484e-07
Epoch 98: reducing lr to 2.506310452488579e-08
[I 2024-06-22 16:24:56,667] Trial 533 finished with value: 1.868952989578247 and parameters: {'hidden_size': 186, 'n_layers': 2, 'rnn_dropout': 0.04373429405565961, 'bidirectional': True, 'fc_dropout': 0.2703257758729533, 'learning_rate_model': 0.00045450139137539667}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.692879235805188e-05
Epoch 53: reducing lr to 3.0446147616248407e-05
Epoch 57: reducing lr to 2.682353967192471e-05
Epoch 65: reducing lr to 1.9320235112207543e-05
Epoch 68: reducing lr to 1.6553265048971685e-05
Epoch 72: reducing lr to 1.3017307207506947e-05
Epoch 80: reducing lr to 6.834081853282871e-06
Epoch 86: reducing lr to 3.3097252670865567e-06
Epoch 89: reducing lr to 1.988803360671932e-06
Epoch 92: reducing lr to 9.91415981246575e-07
Epoch 95: reducing lr to 3.332954621642573e-07
Epoch 98: reducing lr to 2.481900555306683e-08
[I 2024-06-22 16:25:57,001] Trial 534 finished with value: 1.8669941425323486 and parameters: {'hidden_size': 190, 'n_layers': 2, 'rnn_dropout': 0.06812044355441037, 'bidirectional': True, 'fc_dropout': 0.2713024339892978, 'learning_rate_model': 0.00045007483191967596}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.031590383695683e-05
Epoch 41: reducing lr to 5.389459439541061e-05
Epoch 45: reducing lr to 5.0249925119971846e-05
Epoch 48: reducing lr to 4.714946198609007e-05
Epoch 53: reducing lr to 4.142882938262431e-05
Epoch 57: reducing lr to 3.649945676257463e-05
Epoch 65: reducing lr to 2.628952385649838e-05
Epoch 68: reducing lr to 2.252443895638278e-05
Epoch 71: reducing lr to 1.888706391054647e-05
Epoch 74: reducing lr to 1.5434745855591452e-05
Epoch 80: reducing lr to 9.299304945084292e-06
Epoch 86: reducing lr to 4.503625388727766e-06
Epoch 89: reducing lr to 2.7062141372820467e-06
Epoch 92: reducing lr to 1.3490443537215103e-06
Epoch 95: reducing lr to 4.535234148519106e-07
Epoch 98: reducing lr to 3.3771837391861295e-08
[I 2024-06-22 16:26:54,275] Trial 535 finished with value: 1.8671183586120605 and parameters: {'hidden_size': 185, 'n_layers': 2, 'rnn_dropout': 0.04496326052057792, 'bidirectional': True, 'fc_dropout': 0.2912701856024442, 'learning_rate_model': 0.0006124280042268004}. Best is trial 346 with value: 1.8583240509033203.
Epoch 46: reducing lr to 2.436261519750788e-05
Epoch 57: reducing lr to 1.8055819327361357e-05
Epoch 68: reducing lr to 1.1142554884916575e-05
Epoch 72: reducing lr to 8.762383710063091e-06
Epoch 75: reducing lr to 7.091150607494996e-06
Epoch 80: reducing lr to 4.600248465359244e-06
Epoch 86: reducing lr to 2.2278864824192544e-06
Epoch 97: reducing lr to 5.949164457991055e-08
[I 2024-06-22 16:27:54,542] Trial 536 finished with value: 1.8676490783691406 and parameters: {'hidden_size': 189, 'n_layers': 2, 'rnn_dropout': 0.06704966048477666, 'bidirectional': True, 'fc_dropout': 0.2808589655626925, 'learning_rate_model': 0.0003029603828699722}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 4.105951199110353e-05
Epoch 53: reducing lr to 3.3851742321048026e-05
Epoch 57: reducing lr to 2.9823922703042186e-05
Epoch 65: reducing lr to 2.148132594126545e-05
Epoch 68: reducing lr to 1.8404852727927734e-05
Epoch 72: reducing lr to 1.4473375576333248e-05
Epoch 80: reducing lr to 7.598517251319453e-06
Epoch 86: reducing lr to 3.6799390289720205e-06
Epoch 89: reducing lr to 2.2112636298449497e-06
Epoch 92: reducing lr to 1.1023121464541962e-06
Epoch 95: reducing lr to 3.705766734159134e-07
Epoch 98: reducing lr to 2.75951687299489e-08
[I 2024-06-22 16:28:51,682] Trial 537 finished with value: 1.8686649799346924 and parameters: {'hidden_size': 186, 'n_layers': 2, 'rnn_dropout': 0.028895628561292894, 'bidirectional': True, 'fc_dropout': 0.2674895840959603, 'learning_rate_model': 0.0005004185563103184}. Best is trial 346 with value: 1.8583240509033203.
Epoch 39: reducing lr to 3.66637092339455e-05
Epoch 45: reducing lr to 3.3208519386295214e-05
Epoch 57: reducing lr to 2.4121288033670267e-05
Epoch 68: reducing lr to 1.488565935098611e-05
Epoch 72: reducing lr to 1.1705920263152075e-05
Epoch 86: reducing lr to 2.9762975899587018e-06
Epoch 89: reducing lr to 1.788447732545544e-06
Epoch 92: reducing lr to 8.915389518804925e-07
Epoch 98: reducing lr to 2.2318694287821148e-08
[I 2024-06-22 16:29:46,377] Trial 538 finished with value: 1.8686046600341797 and parameters: {'hidden_size': 180, 'n_layers': 2, 'rnn_dropout': 0.05703315871696491, 'bidirectional': True, 'fc_dropout': 0.2946621116472274, 'learning_rate_model': 0.0004047334837319492}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 4.231989389551e-05
Epoch 53: reducing lr to 3.4890871170492807e-05
Epoch 57: reducing lr to 3.073941172545132e-05
Epoch 65: reducing lr to 2.214072672773594e-05
Epoch 68: reducing lr to 1.8969816659709784e-05
Epoch 72: reducing lr to 1.4917657054302136e-05
Epoch 75: reducing lr to 1.207244014679824e-05
Epoch 80: reducing lr to 7.831764876034481e-06
Epoch 86: reducing lr to 3.7929001514140127e-06
Epoch 89: reducing lr to 2.2791416081690132e-06
Epoch 92: reducing lr to 1.1361492335267162e-06
Epoch 95: reducing lr to 3.8195206758693484e-07
Epoch 98: reducing lr to 2.8442242882306766e-08
[I 2024-06-22 16:30:41,592] Trial 539 finished with value: 1.864590048789978 and parameters: {'hidden_size': 184, 'n_layers': 2, 'rnn_dropout': 0.03845125644845025, 'bidirectional': True, 'fc_dropout': 0.2648517869138288, 'learning_rate_model': 0.000515779637395242}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.190858396870952e-05
Epoch 41: reducing lr to 5.5317715732271796e-05
Epoch 45: reducing lr to 5.157680662666333e-05
Epoch 48: reducing lr to 4.839447377487244e-05
Epoch 53: reducing lr to 4.2522784197888955e-05
Epoch 57: reducing lr to 3.746324833175445e-05
Epoch 65: reducing lr to 2.698371559791151e-05
Epoch 68: reducing lr to 2.3119211215813414e-05
Epoch 71: reducing lr to 1.9385788948619093e-05
Epoch 74: reducing lr to 1.5842310220859087e-05
Epoch 77: reducing lr to 1.254466824887751e-05
Epoch 80: reducing lr to 9.544859057399035e-06
Epoch 85: reducing lr to 5.3330942062788544e-06
Epoch 88: reducing lr to 3.343717663326345e-06
Epoch 91: reducing lr to 1.7972888949344362e-06
Epoch 97: reducing lr to 1.234366723632495e-07
[I 2024-06-22 16:31:42,115] Trial 540 finished with value: 1.8681520223617554 and parameters: {'hidden_size': 190, 'n_layers': 2, 'rnn_dropout': 0.03401710078635936, 'bidirectional': True, 'fc_dropout': 0.26052414500786564, 'learning_rate_model': 0.0006285995585335657}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.55595851405757e-05
Epoch 45: reducing lr to 4.6287377215758384e-05
Epoch 53: reducing lr to 3.8161884792116096e-05
Epoch 57: reducing lr to 3.362122668453644e-05
Epoch 65: reducing lr to 2.4216416336206104e-05
Epoch 68: reducing lr to 2.074823395374638e-05
Epoch 72: reducing lr to 1.6316185029969127e-05
Epoch 75: reducing lr to 1.320422949001776e-05
Epoch 80: reducing lr to 8.565991587247262e-06
Epoch 86: reducing lr to 4.148483937215972e-06
Epoch 89: reducing lr to 2.4928107713578633e-06
Epoch 92: reducing lr to 1.2426630434256724e-06
Epoch 97: reducing lr to 1.1077769620932701e-07
[I 2024-06-22 16:32:36,687] Trial 541 finished with value: 1.8635129928588867 and parameters: {'hidden_size': 179, 'n_layers': 2, 'rnn_dropout': 0.06981051480486727, 'bidirectional': True, 'fc_dropout': 0.2947596357521874, 'learning_rate_model': 0.0005641338963482989}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.71454338196814e-05
Epoch 41: reducing lr to 5.106165673296985e-05
Epoch 45: reducing lr to 4.760856735479831e-05
Epoch 53: reducing lr to 3.9251147327763854e-05
Epoch 57: reducing lr to 3.458088428083924e-05
Epoch 65: reducing lr to 2.4907630494164233e-05
Epoch 68: reducing lr to 2.1340455067818283e-05
Epoch 72: reducing lr to 1.678190125899338e-05
Epoch 75: reducing lr to 1.3581120531273226e-05
Epoch 80: reducing lr to 8.810492448970689e-06
Epoch 86: reducing lr to 4.266894968462504e-06
Epoch 89: reducing lr to 2.563963582506766e-06
Epoch 92: reducing lr to 1.2781326305545964e-06
Epoch 97: reducing lr to 1.1393964680279402e-07
[I 2024-06-22 16:33:31,372] Trial 542 finished with value: 1.863572120666504 and parameters: {'hidden_size': 179, 'n_layers': 2, 'rnn_dropout': 0.04033827583380906, 'bidirectional': True, 'fc_dropout': 0.29924132698140277, 'learning_rate_model': 0.000580236086314245}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.838685131321558e-05
Epoch 45: reducing lr to 4.864280761523346e-05
Epoch 53: reducing lr to 4.010383244496374e-05
Epoch 57: reducing lr to 3.533211341357407e-05
Epoch 65: reducing lr to 2.544871953927516e-05
Epoch 68: reducing lr to 2.18040514126245e-05
Epoch 72: reducing lr to 1.714646837144922e-05
Epoch 75: reducing lr to 1.3876154438313264e-05
Epoch 80: reducing lr to 9.00189005892349e-06
Epoch 86: reducing lr to 4.359588254747402e-06
Epoch 89: reducing lr to 2.619662682703497e-06
Epoch 92: reducing lr to 1.3058985621535054e-06
Epoch 98: reducing lr to 3.269173009000977e-08
[I 2024-06-22 16:34:24,440] Trial 543 finished with value: 1.8686022758483887 and parameters: {'hidden_size': 173, 'n_layers': 2, 'rnn_dropout': 0.023419221174811045, 'bidirectional': True, 'fc_dropout': 0.3006575727180363, 'learning_rate_model': 0.0005928410344226663}. Best is trial 346 with value: 1.8583240509033203.
Epoch 39: reducing lr to 3.6284074554720674e-05
Epoch 57: reducing lr to 2.387152395806294e-05
Epoch 68: reducing lr to 1.473152566863818e-05
Epoch 72: reducing lr to 1.158471121537742e-05
Epoch 80: reducing lr to 6.081969445022881e-06
Epoch 86: reducing lr to 2.9454794920507124e-06
Epoch 89: reducing lr to 1.769929235769256e-06
Epoch 92: reducing lr to 8.823075044605384e-07
Epoch 98: reducing lr to 2.208759518399742e-08
[I 2024-06-22 16:35:18,102] Trial 544 finished with value: 1.869644045829773 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.04025009717072052, 'bidirectional': True, 'fc_dropout': 0.2923168949821291, 'learning_rate_model': 0.0004005426675412658}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.607111694581776e-05
Epoch 41: reducing lr to 5.903710004366649e-05
Epoch 45: reducing lr to 5.504466430769075e-05
Epoch 53: reducing lr to 4.538187869101501e-05
Epoch 65: reducing lr to 2.879801337086633e-05
Epoch 68: reducing lr to 2.4673672211710092e-05
Epoch 72: reducing lr to 1.9403106889604847e-05
Epoch 75: reducing lr to 1.5702388500688132e-05
Epoch 80: reducing lr to 1.0186624512870525e-05
Epoch 85: reducing lr to 5.691674213776374e-06
Epoch 88: reducing lr to 3.5685384256098004e-06
Epoch 96: reducing lr to 2.8509362364284325e-07
Epoch 99: reducing lr to 1.03415771909518e-09
[I 2024-06-22 16:36:16,852] Trial 545 finished with value: 1.8623104095458984 and parameters: {'hidden_size': 187, 'n_layers': 2, 'rnn_dropout': 0.07389265655517946, 'bidirectional': True, 'fc_dropout': 0.31159080671282247, 'learning_rate_model': 0.0006708645599930424}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.221262120931228e-05
Epoch 41: reducing lr to 5.558938477345335e-05
Epoch 45: reducing lr to 5.183010381035919e-05
Epoch 49: reducing lr to 4.7503791044073726e-05
Epoch 53: reducing lr to 4.2731616465427284e-05
Epoch 65: reducing lr to 2.7116234449186555e-05
Epoch 68: reducing lr to 2.323275122484545e-05
Epoch 71: reducing lr to 1.948099386853495e-05
Epoch 74: reducing lr to 1.5920112877220225e-05
Epoch 80: reducing lr to 9.591734505418005e-06
Epoch 85: reducing lr to 5.359285392418254e-06
Epoch 96: reducing lr to 2.6844440410212684e-07
Epoch 99: reducing lr to 9.737638081834011e-10
[I 2024-06-22 16:37:15,443] Trial 546 finished with value: 1.8622374534606934 and parameters: {'hidden_size': 187, 'n_layers': 2, 'rnn_dropout': 0.035094416416793395, 'bidirectional': True, 'fc_dropout': 0.31629967320458807, 'learning_rate_model': 0.0006316866534559314}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.686540694192328e-05
Epoch 41: reducing lr to 5.974682889541604e-05
Epoch 45: reducing lr to 5.570639712256717e-05
Epoch 48: reducing lr to 5.226926502361304e-05
Epoch 53: reducing lr to 4.592744797930641e-05
Epoch 65: reducing lr to 2.914421569020883e-05
Epoch 68: reducing lr to 2.497029276106481e-05
Epoch 72: reducing lr to 1.9636366056517637e-05
Epoch 75: reducing lr to 1.589115858174014e-05
Epoch 79: reducing lr to 1.1350076299812984e-05
Epoch 85: reducing lr to 5.76009805914289e-06
Epoch 96: reducing lr to 2.8852094595371683e-07
Epoch 99: reducing lr to 1.046590097540049e-09
[I 2024-06-22 16:38:14,214] Trial 547 finished with value: 1.8645012378692627 and parameters: {'hidden_size': 188, 'n_layers': 2, 'rnn_dropout': 0.07547512968221945, 'bidirectional': True, 'fc_dropout': 0.318695925271645, 'learning_rate_model': 0.0006789295214069865}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.913699535669019e-05
Epoch 41: reducing lr to 6.177658105793213e-05
Epoch 45: reducing lr to 5.759888551259429e-05
Epoch 48: reducing lr to 5.404498526979576e-05
Epoch 53: reducing lr to 4.748772052562049e-05
Epoch 65: reducing lr to 3.013431902984525e-05
Epoch 68: reducing lr to 2.581859729316224e-05
Epoch 72: reducing lr to 2.030346349422336e-05
Epoch 75: reducing lr to 1.643102176933515e-05
Epoch 79: reducing lr to 1.1735667340211044e-05
Epoch 86: reducing lr to 5.162272432000965e-06
Epoch 96: reducing lr to 2.983227383635814e-07
Epoch 99: reducing lr to 1.0821454324966804e-09
[I 2024-06-22 16:39:15,753] Trial 548 finished with value: 1.8658829927444458 and parameters: {'hidden_size': 191, 'n_layers': 2, 'rnn_dropout': 0.06972016944928672, 'bidirectional': True, 'fc_dropout': 0.3196247938190473, 'learning_rate_model': 0.0007019944888663318}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.648603763002276e-05
Epoch 41: reducing lr to 5.940784773306435e-05
Epoch 45: reducing lr to 5.539033985900785e-05
Epoch 48: reducing lr to 5.1972708762125294e-05
Epoch 53: reducing lr to 4.566687281594295e-05
Epoch 65: reducing lr to 2.897886231006897e-05
Epoch 68: reducing lr to 2.4828620658613575e-05
Epoch 72: reducing lr to 1.9524956659345225e-05
Epoch 75: reducing lr to 1.580099809110418e-05
Epoch 78: reducing lr to 1.2360177654543048e-05
Epoch 81: reducing lr to 9.256748960579146e-06
Epoch 86: reducing lr to 4.964332589226894e-06
Epoch 89: reducing lr to 2.9830516251998727e-06
Epoch 97: reducing lr to 1.3256344625514443e-07
[I 2024-06-22 16:40:14,481] Trial 549 finished with value: 1.8682745695114136 and parameters: {'hidden_size': 192, 'n_layers': 2, 'rnn_dropout': 0.07829408223446675, 'bidirectional': True, 'fc_dropout': 0.3144381174288726, 'learning_rate_model': 0.0006750775292163336}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.513893618052573e-05
Epoch 41: reducing lr to 5.8204160453066605e-05
Epoch 45: reducing lr to 5.426805298838061e-05
Epoch 48: reducing lr to 5.0919667946287424e-05
Epoch 53: reducing lr to 4.474159718278393e-05
Epoch 57: reducing lr to 3.941805781619475e-05
Epoch 65: reducing lr to 2.8391709445885634e-05
Epoch 68: reducing lr to 2.4325557578446998e-05
Epoch 71: reducing lr to 2.039732761084443e-05
Epoch 74: reducing lr to 1.6668952320896394e-05
Epoch 80: reducing lr to 1.0042903990604534e-05
Epoch 86: reducing lr to 4.8637481678187925e-06
Epoch 97: reducing lr to 1.2987752275952797e-07
[I 2024-06-22 16:41:13,271] Trial 550 finished with value: 1.8651803731918335 and parameters: {'hidden_size': 188, 'n_layers': 2, 'rnn_dropout': 0.026715652390856874, 'bidirectional': True, 'fc_dropout': 0.3216879396063762, 'learning_rate_model': 0.0006613995007076901}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.88957535252314e-05
Epoch 41: reducing lr to 6.156102214509842e-05
Epoch 45: reducing lr to 5.739790396054165e-05
Epoch 48: reducing lr to 5.3856404450505246e-05
Epoch 53: reducing lr to 4.732202017066147e-05
Epoch 65: reducing lr to 3.0029170429229648e-05
Epoch 68: reducing lr to 2.572850767233673e-05
Epoch 72: reducing lr to 2.023261799836354e-05
Epoch 75: reducing lr to 1.637368850276892e-05
Epoch 78: reducing lr to 1.2808159180040138e-05
Epoch 81: reducing lr to 9.592250005661581e-06
Epoch 85: reducing lr to 5.935001584729126e-06
Epoch 88: reducing lr to 3.7210986461413615e-06
Epoch 91: reducing lr to 2.0001357611671896e-06
Epoch 96: reducing lr to 2.972817917126997e-07
Epoch 99: reducing lr to 1.0783694693607647e-09
[I 2024-06-22 16:42:13,471] Trial 551 finished with value: 1.8663297891616821 and parameters: {'hidden_size': 189, 'n_layers': 2, 'rnn_dropout': 0.022742759096411595, 'bidirectional': True, 'fc_dropout': 0.3208260202890027, 'learning_rate_model': 0.000699544998035925}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.257927268474417e-05
Epoch 41: reducing lr to 5.591700205672072e-05
Epoch 45: reducing lr to 5.213556568713688e-05
Epoch 48: reducing lr to 4.8918756925907615e-05
Epoch 53: reducing lr to 4.298345620341107e-05
Epoch 57: reducing lr to 3.78691076861666e-05
Epoch 65: reducing lr to 2.7276044583781282e-05
Epoch 68: reducing lr to 2.336967396414415e-05
Epoch 72: reducing lr to 1.83776568810175e-05
Epoch 75: reducing lr to 1.4872520659703391e-05
Epoch 79: reducing lr to 1.0622525940439884e-05
Epoch 86: reducing lr to 4.672625018319737e-06
Epoch 97: reducing lr to 1.2477392768377823e-07
[I 2024-06-22 16:43:12,161] Trial 552 finished with value: 1.8641453981399536 and parameters: {'hidden_size': 188, 'n_layers': 2, 'rnn_dropout': 0.07293577899441761, 'bidirectional': True, 'fc_dropout': 0.30626444096573374, 'learning_rate_model': 0.0006354095128853871}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.419717834615815e-05
Epoch 41: reducing lr to 5.7362663380600585e-05
Epoch 45: reducing lr to 5.34834628944306e-05
Epoch 48: reducing lr to 5.018348772868435e-05
Epoch 53: reducing lr to 4.409473752956161e-05
Epoch 65: reducing lr to 2.798123122242111e-05
Epoch 68: reducing lr to 2.3973866473738546e-05
Epoch 72: reducing lr to 1.8852787284995017e-05
Epoch 75: reducing lr to 1.5257030327881372e-05
Epoch 80: reducing lr to 9.897706907760842e-06
Epoch 86: reducing lr to 4.793429657723078e-06
Epoch 97: reducing lr to 1.2799979521684653e-07
[I 2024-06-22 16:44:10,862] Trial 553 finished with value: 1.865846872329712 and parameters: {'hidden_size': 188, 'n_layers': 2, 'rnn_dropout': 0.07466358211274932, 'bidirectional': True, 'fc_dropout': 0.30771602049220403, 'learning_rate_model': 0.0006518371989883001}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.399509546454581e-05
Epoch 41: reducing lr to 5.718209450496551e-05
Epoch 45: reducing lr to 5.331510514758331e-05
Epoch 48: reducing lr to 5.002551779806081e-05
Epoch 53: reducing lr to 4.395593405169121e-05
Epoch 57: reducing lr to 3.872587611783146e-05
Epoch 65: reducing lr to 2.7893150593612195e-05
Epoch 68: reducing lr to 2.389840041517941e-05
Epoch 71: reducing lr to 2.0039150225911323e-05
Epoch 74: reducing lr to 1.6376245263100303e-05
Epoch 79: reducing lr to 1.0862854943323539e-05
Epoch 86: reducing lr to 4.778340675574749e-06
Epoch 89: reducing lr to 2.871289677280008e-06
Epoch 97: reducing lr to 1.2759687147269413e-07
[I 2024-06-22 16:45:09,447] Trial 554 finished with value: 1.8682018518447876 and parameters: {'hidden_size': 192, 'n_layers': 2, 'rnn_dropout': 0.07384830988019456, 'bidirectional': True, 'fc_dropout': 0.3178882483477741, 'learning_rate_model': 0.0006497853153556053}. Best is trial 346 with value: 1.8583240509033203.
Epoch 41: reducing lr to 6.821722867369822e-05
Epoch 45: reducing lr to 6.36039786772611e-05
Epoch 53: reducing lr to 5.24386528812762e-05
Epoch 60: reducing lr to 4.1373201023563855e-05
Epoch 65: reducing lr to 3.327603595053888e-05
Epoch 68: reducing lr to 2.8510369551369435e-05
Epoch 72: reducing lr to 2.24202438583428e-05
Epoch 75: reducing lr to 1.8144072562548012e-05
Epoch 78: reducing lr to 1.4193024956839402e-05
Epoch 81: reducing lr to 1.0629399729413045e-05
Epoch 85: reducing lr to 6.576716015695074e-06
Epoch 88: reducing lr to 4.123437662599387e-06
Epoch 91: reducing lr to 2.216397873907748e-06
Epoch 96: reducing lr to 3.294250039902343e-07
Epoch 99: reducing lr to 1.1949667845403793e-09
[I 2024-06-22 16:46:09,566] Trial 555 finished with value: 1.8674266338348389 and parameters: {'hidden_size': 189, 'n_layers': 2, 'rnn_dropout': 0.0732621615524501, 'bidirectional': True, 'fc_dropout': 0.3050391706807305, 'learning_rate_model': 0.000775182403340879}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.328189281436201e-05
Epoch 41: reducing lr to 5.654482033500047e-05
Epoch 45: reducing lr to 5.2720927203007064e-05
Epoch 53: reducing lr to 4.346606075078585e-05
Epoch 65: reducing lr to 2.7582291319461082e-05
Epoch 68: reducing lr to 2.3632061215471487e-05
Epoch 71: reducing lr to 1.9815821001307397e-05
Epoch 74: reducing lr to 1.6193737815663592e-05
Epoch 80: reducing lr to 9.756591236262271e-06
Epoch 85: reducing lr to 5.45139743627815e-06
Epoch 96: reducing lr to 2.730582585461585e-07
Epoch 99: reducing lr to 9.905002512199807e-10
[I 2024-06-22 16:47:08,378] Trial 556 finished with value: 1.8622896671295166 and parameters: {'hidden_size': 187, 'n_layers': 2, 'rnn_dropout': 0.03092210808359128, 'bidirectional': True, 'fc_dropout': 0.32339373507096864, 'learning_rate_model': 0.0006425436883903188}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.0175548574878705e-05
Epoch 41: reducing lr to 5.376918153678198e-05
Epoch 45: reducing lr to 5.013299341604373e-05
Epoch 53: reducing lr to 4.1332424390977225e-05
Epoch 65: reducing lr to 2.6228348067426854e-05
Epoch 68: reducing lr to 2.2472024529477534e-05
Epoch 71: reducing lr to 1.884311366465084e-05
Epoch 74: reducing lr to 1.539882916261563e-05
Epoch 80: reducing lr to 9.277665438757065e-06
Epoch 86: reducing lr to 4.493145440960663e-06
Epoch 96: reducing lr to 2.5965453576298236e-07
Epoch 99: reducing lr to 9.418791589439584e-10
[I 2024-06-22 16:48:07,132] Trial 557 finished with value: 1.8620553016662598 and parameters: {'hidden_size': 187, 'n_layers': 2, 'rnn_dropout': 0.03238342796501005, 'bidirectional': True, 'fc_dropout': 0.2968367656401107, 'learning_rate_model': 0.0006110028826988267}. Best is trial 346 with value: 1.8583240509033203.
Epoch 46: reducing lr to 2.653756130427325e-05
Epoch 55: reducing lr to 2.1011648337618234e-05
Epoch 58: reducing lr to 1.8986848471925075e-05
Epoch 68: reducing lr to 1.2137294413899809e-05
Epoch 72: reducing lr to 9.544636033210053e-06
Epoch 86: reducing lr to 2.426778637139521e-06
Epoch 89: reducing lr to 1.4582435458150405e-06
Epoch 97: reducing lr to 6.48026967684824e-08
[I 2024-06-22 16:49:05,868] Trial 558 finished with value: 1.865069031715393 and parameters: {'hidden_size': 187, 'n_layers': 2, 'rnn_dropout': 0.02551481832533882, 'bidirectional': True, 'fc_dropout': 0.29797680522789266, 'learning_rate_model': 0.000330006843189823}. Best is trial 346 with value: 1.8583240509033203.
Epoch 39: reducing lr to 2.7461865064881653e-05
Epoch 46: reducing lr to 2.437815470539808e-05
Epoch 57: reducing lr to 1.8067336093711227e-05
Epoch 67: reducing lr to 1.1765314706895604e-05
Epoch 72: reducing lr to 8.767972729538037e-06
Epoch 80: reducing lr to 4.603182698681272e-06
Epoch 87: reducing lr to 1.909317056665848e-06
Epoch 90: reducing lr to 1.0908404788866987e-06
Epoch 93: reducing lr to 4.942076315068327e-07
Epoch 96: reducing lr to 1.2882952878050638e-07
Epoch 99: reducing lr to 4.673203487810112e-10
[I 2024-06-22 16:50:08,260] Trial 559 finished with value: 1.8697881698608398 and parameters: {'hidden_size': 195, 'n_layers': 2, 'rnn_dropout': 0.030450674755108948, 'bidirectional': True, 'fc_dropout': 0.2960519177179436, 'learning_rate_model': 0.0003031536238345344}. Best is trial 346 with value: 1.8583240509033203.
Epoch 39: reducing lr to 3.4456769380491456e-05
Epoch 45: reducing lr to 3.1209561658364056e-05
Epoch 68: reducing lr to 1.3989630128819575e-05
Epoch 72: reducing lr to 1.100129264936479e-05
Epoch 75: reducing lr to 8.903036620523853e-06
Epoch 86: reducing lr to 2.7971419642933746e-06
Epoch 89: reducing lr to 1.6807936882809753e-06
Epoch 97: reducing lr to 7.469257383283705e-08
[I 2024-06-22 16:51:07,083] Trial 560 finished with value: 1.8637570142745972 and parameters: {'hidden_size': 187, 'n_layers': 2, 'rnn_dropout': 0.021846042193063697, 'bidirectional': True, 'fc_dropout': 0.30054999445220637, 'learning_rate_model': 0.00038037090629670337}. Best is trial 346 with value: 1.8583240509033203.
Epoch 39: reducing lr to 3.318103614776113e-05
Epoch 55: reducing lr to 2.3321681325095594e-05
Epoch 68: reducing lr to 1.3471675706805623e-05
Epoch 72: reducing lr to 1.0593978937484055e-05
Epoch 86: reducing lr to 2.6935801091145202e-06
Epoch 89: reducing lr to 1.6185636996878634e-06
Epoch 97: reducing lr to 7.192714339957678e-08
[I 2024-06-22 16:52:05,860] Trial 561 finished with value: 1.8644652366638184 and parameters: {'hidden_size': 187, 'n_layers': 2, 'rnn_dropout': 0.020315276077431825, 'bidirectional': True, 'fc_dropout': 0.3215288103717976, 'learning_rate_model': 0.00036628798980015}. Best is trial 346 with value: 1.8583240509033203.
Epoch 46: reducing lr to 2.5944879868281086e-05
Epoch 57: reducing lr to 1.9228480176450755e-05
Epoch 72: reducing lr to 9.331469174156029e-06
Epoch 87: reducing lr to 2.0320242554981576e-06
Epoch 90: reducing lr to 1.160946163571055e-06
Epoch 97: reducing lr to 6.335541399307654e-08
[I 2024-06-22 16:53:04,508] Trial 562 finished with value: 1.8695067167282104 and parameters: {'hidden_size': 188, 'n_layers': 2, 'rnn_dropout': 0.019317269195093724, 'bidirectional': True, 'fc_dropout': 0.32560268003794224, 'learning_rate_model': 0.0003226365755353686}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.259827524204857e-05
Epoch 53: reducing lr to 2.6875828768825302e-05
Epoch 68: reducing lr to 1.4612118506044421e-05
Epoch 72: reducing lr to 1.1490810724225902e-05
Epoch 75: reducing lr to 9.299189825951799e-06
Epoch 80: reducing lr to 6.032671719128941e-06
Epoch 86: reducing lr to 2.9216047518143647e-06
Epoch 89: reducing lr to 1.7555829804805155e-06
Epoch 92: reducing lr to 8.751559141899432e-07
Epoch 97: reducing lr to 7.801612553847803e-08
[I 2024-06-22 16:54:06,078] Trial 563 finished with value: 1.8664307594299316 and parameters: {'hidden_size': 194, 'n_layers': 2, 'rnn_dropout': 0.026076733146678483, 'bidirectional': True, 'fc_dropout': 0.3058062169383718, 'learning_rate_model': 0.00039729604770672567}. Best is trial 346 with value: 1.8583240509033203.
Epoch 35: reducing lr to 7.948100582769513e-05
Epoch 45: reducing lr to 6.872734695447617e-05
Epoch 50: reducing lr to 6.145736051036561e-05
Epoch 56: reducing lr to 5.16348075890391e-05
Epoch 60: reducing lr to 4.470585646523909e-05
Epoch 66: reducing lr to 3.422551008264774e-05
Epoch 72: reducing lr to 2.422621839861674e-05
Epoch 75: reducing lr to 1.9605596947023055e-05
Epoch 78: reducing lr to 1.5336288245297646e-05
Epoch 81: reducing lr to 1.148560920737425e-05
Epoch 85: reducing lr to 7.1064775008065555e-06
Epoch 96: reducing lr to 3.5596053919204975e-07
Epoch 99: reducing lr to 1.2912226327434565e-09
[I 2024-06-22 16:55:04,753] Trial 564 finished with value: 1.865654706954956 and parameters: {'hidden_size': 188, 'n_layers': 2, 'rnn_dropout': 0.020837072732482777, 'bidirectional': True, 'fc_dropout': 0.323064139644085, 'learning_rate_model': 0.0008376241721881455}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.04122993240869e-05
Epoch 41: reducing lr to 5.398072749381308e-05
Epoch 45: reducing lr to 5.033023339195402e-05
Epoch 53: reducing lr to 4.149503998274002e-05
Epoch 57: reducing lr to 3.655778935299923e-05
Epoch 65: reducing lr to 2.6331539167508474e-05
Epoch 68: reducing lr to 2.2560436995496993e-05
Epoch 71: reducing lr to 1.8917248780709987e-05
Epoch 74: reducing lr to 1.5459413310621227e-05
Epoch 80: reducing lr to 9.314166879883036e-06
Epoch 86: reducing lr to 4.510822979007952e-06
Epoch 89: reducing lr to 2.710539146333498e-06
Epoch 92: reducing lr to 1.3512003653099012e-06
Epoch 97: reducing lr to 1.2045329937035181e-07
[I 2024-06-22 16:55:59,120] Trial 565 finished with value: 1.8638665676116943 and parameters: {'hidden_size': 179, 'n_layers': 2, 'rnn_dropout': 0.026538477973299068, 'bidirectional': True, 'fc_dropout': 0.30008969516372813, 'learning_rate_model': 0.000613406772545669}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.87414433295244e-05
Epoch 45: reducing lr to 4.893822260753505e-05
Epoch 53: reducing lr to 4.034738897333492e-05
Epoch 57: reducing lr to 3.554669058384417e-05
Epoch 65: reducing lr to 2.560327339122893e-05
Epoch 68: reducing lr to 2.1936470653946966e-05
Epoch 72: reducing lr to 1.7250601419484153e-05
Epoch 75: reducing lr to 1.396042638431184e-05
Epoch 80: reducing lr to 9.056559873698407e-06
Epoch 86: reducing lr to 4.386064681455781e-06
Epoch 89: reducing lr to 2.6355722830983805e-06
Epoch 92: reducing lr to 1.3138294779990018e-06
Epoch 98: reducing lr to 3.2890271820353384e-08
[I 2024-06-22 16:56:52,579] Trial 566 finished with value: 1.869510531425476 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.03207257609254088, 'bidirectional': True, 'fc_dropout': 0.30629441274044283, 'learning_rate_model': 0.0005964414460396388}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.082374829906966e-05
Epoch 41: reducing lr to 5.4348373076660435e-05
Epoch 45: reducing lr to 5.0673016619400315e-05
Epoch 53: reducing lr to 4.17776495152158e-05
Epoch 57: reducing lr to 3.6806772840223166e-05
Epoch 65: reducing lr to 2.651087491406017e-05
Epoch 68: reducing lr to 2.2714088963404453e-05
Epoch 71: reducing lr to 1.9046088151291825e-05
Epoch 74: reducing lr to 1.5564702462526653e-05
Epoch 80: reducing lr to 9.377602710970798e-06
Epoch 86: reducing lr to 4.541544761025966e-06
Epoch 89: reducing lr to 2.7289997671985718e-06
Epoch 92: reducing lr to 1.3604029616606977e-06
Epoch 97: reducing lr to 1.2127366851891977e-07
[I 2024-06-22 16:57:47,046] Trial 567 finished with value: 1.8640334606170654 and parameters: {'hidden_size': 179, 'n_layers': 2, 'rnn_dropout': 0.020719205868150348, 'bidirectional': True, 'fc_dropout': 0.30151050491670395, 'learning_rate_model': 0.0006175844911664659}. Best is trial 346 with value: 1.8583240509033203.
Epoch 41: reducing lr to 7.304683302801012e-05
Epoch 45: reducing lr to 6.810697679582434e-05
Epoch 60: reducing lr to 4.430231725563774e-05
Epoch 66: reducing lr to 3.3916572140754294e-05
Epoch 72: reducing lr to 2.400753946486655e-05
Epoch 75: reducing lr to 1.942862623845568e-05
Epoch 78: reducing lr to 1.5197854623261176e-05
Epoch 81: reducing lr to 1.1381933894388405e-05
Epoch 85: reducing lr to 7.042330596117351e-06
Epoch 96: reducing lr to 3.5274744708303775e-07
Epoch 99: reducing lr to 1.2795673597727416e-09
[I 2024-06-22 16:58:41,402] Trial 568 finished with value: 1.8662837743759155 and parameters: {'hidden_size': 175, 'n_layers': 2, 'rnn_dropout': 0.017217606438146217, 'bidirectional': True, 'fc_dropout': 0.2993187685133233, 'learning_rate_model': 0.0008300633239433388}. Best is trial 346 with value: 1.8583240509033203.
Epoch 57: reducing lr to 2.2002942380875816e-05
Epoch 68: reducing lr to 1.3578392022179937e-05
Epoch 72: reducing lr to 1.0677899484709687e-05
Epoch 75: reducing lr to 8.641323631012774e-06
Epoch 86: reducing lr to 2.714917391176988e-06
Epoch 89: reducing lr to 1.6313852044500388e-06
Epoch 97: reducing lr to 7.249691659528016e-08
[I 2024-06-22 16:59:35,874] Trial 569 finished with value: 1.8666744232177734 and parameters: {'hidden_size': 179, 'n_layers': 2, 'rnn_dropout': 0.029434090626942973, 'bidirectional': True, 'fc_dropout': 0.3069078727053479, 'learning_rate_model': 0.0003691895519731164}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.9859071934636535e-05
Epoch 41: reducing lr to 5.348639741059893e-05
Epoch 45: reducing lr to 4.9869332814730724e-05
Epoch 53: reducing lr to 4.1115047946323075e-05
Epoch 57: reducing lr to 3.6223010332930185e-05
Epoch 65: reducing lr to 2.6090407331163315e-05
Epoch 68: reducing lr to 2.2353839137055578e-05
Epoch 71: reducing lr to 1.8744013524385905e-05
Epoch 74: reducing lr to 1.5317843283259942e-05
Epoch 80: reducing lr to 9.228872125577802e-06
Epoch 86: reducing lr to 4.469514986283468e-06
Epoch 89: reducing lr to 2.685717305206674e-06
Epoch 92: reducing lr to 1.3388267086358809e-06
Epoch 97: reducing lr to 1.193502447754003e-07
[I 2024-06-22 17:00:30,288] Trial 570 finished with value: 1.863889455795288 and parameters: {'hidden_size': 179, 'n_layers': 2, 'rnn_dropout': 0.01415850182380795, 'bidirectional': True, 'fc_dropout': 0.29454124064050663, 'learning_rate_model': 0.0006077894821719633}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.9992807843266976e-05
Epoch 45: reducing lr to 4.998074985347223e-05
Epoch 53: reducing lr to 4.120690634168026e-05
Epoch 57: reducing lr to 3.630393903836508e-05
Epoch 65: reducing lr to 2.614869798315975e-05
Epoch 68: reducing lr to 2.2403781625165608e-05
Epoch 72: reducing lr to 1.76180896736621e-05
Epoch 75: reducing lr to 1.4257824289161463e-05
Epoch 80: reducing lr to 9.249491082061049e-06
Epoch 86: reducing lr to 4.479500684833563e-06
Epoch 89: reducing lr to 2.6917176796282476e-06
Epoch 92: reducing lr to 1.341817887760301e-06
Epoch 98: reducing lr to 3.359093078735345e-08
[I 2024-06-22 17:01:23,350] Trial 571 finished with value: 1.8688938617706299 and parameters: {'hidden_size': 173, 'n_layers': 2, 'rnn_dropout': 0.013923252675722789, 'bidirectional': True, 'fc_dropout': 0.35198758241832456, 'learning_rate_model': 0.0006091473929451717}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.6784009005375524e-05
Epoch 45: reducing lr to 4.730745987401739e-05
Epoch 53: reducing lr to 3.900289759570313e-05
Epoch 57: reducing lr to 3.436217232356942e-05
Epoch 65: reducing lr to 2.4750098472365982e-05
Epoch 68: reducing lr to 2.1205484178728053e-05
Epoch 72: reducing lr to 1.6675761622966285e-05
Epoch 75: reducing lr to 1.3495224710067806e-05
Epoch 80: reducing lr to 8.754769176182858e-06
Epoch 86: reducing lr to 4.239908355209985e-06
Epoch 89: reducing lr to 2.547747412644111e-06
Epoch 92: reducing lr to 1.2700488902138646e-06
Epoch 98: reducing lr to 3.17942730953852e-08
[I 2024-06-22 17:02:16,974] Trial 572 finished with value: 1.8693311214447021 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.035797492550973094, 'bidirectional': True, 'fc_dropout': 0.2931760667972633, 'learning_rate_model': 0.0005765662966962079}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 7.444637407269763e-05
Epoch 41: reducing lr to 6.652071642170529e-05
Epoch 45: reducing lr to 6.20221945561616e-05
Epoch 48: reducing lr to 5.81953723819054e-05
Epoch 53: reducing lr to 5.1134542192221054e-05
Epoch 57: reducing lr to 4.505034391828125e-05
Epoch 65: reducing lr to 3.2448485436021836e-05
Epoch 68: reducing lr to 2.780133765146476e-05
Epoch 72: reducing lr to 2.1862668900551927e-05
Epoch 75: reducing lr to 1.76928428365407e-05
Epoch 80: reducing lr to 1.1477893731464665e-05
Epoch 86: reducing lr to 5.5587093791852165e-06
Epoch 89: reducing lr to 3.3402107432480857e-06
Epoch 97: reducing lr to 1.4843519421617759e-07
[I 2024-06-22 17:03:11,535] Trial 573 finished with value: 1.8706326484680176 and parameters: {'hidden_size': 180, 'n_layers': 2, 'rnn_dropout': 0.0003899727519221831, 'bidirectional': True, 'fc_dropout': 0.32124065893100656, 'learning_rate_model': 0.0007559041876999647}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.967747984480811e-05
Epoch 41: reducing lr to 5.3324137850447555e-05
Epoch 45: reducing lr to 4.9718046199828564e-05
Epoch 53: reducing lr to 4.099031885783802e-05
Epoch 57: reducing lr to 3.6113122024714915e-05
Epoch 65: reducing lr to 2.6011257898360303e-05
Epoch 68: reducing lr to 2.228602518282289e-05
Epoch 71: reducing lr to 1.8687150554786542e-05
Epoch 74: reducing lr to 1.527137415028526e-05
Epoch 80: reducing lr to 9.20087486264206e-06
Epoch 86: reducing lr to 4.455955996131311e-06
Epoch 89: reducing lr to 2.67756975125405e-06
Epoch 97: reducing lr to 1.1898817667661214e-07
[I 2024-06-22 17:04:05,997] Trial 574 finished with value: 1.865809440612793 and parameters: {'hidden_size': 175, 'n_layers': 2, 'rnn_dropout': 0.09916658035973486, 'bidirectional': True, 'fc_dropout': 0.2560448128297916, 'learning_rate_model': 0.0006059456553521313}. Best is trial 346 with value: 1.8583240509033203.
Epoch 35: reducing lr to 8.332949073368543e-05
Epoch 45: reducing lr to 7.205513772195156e-05
Epoch 50: reducing lr to 6.443313719843229e-05
Epoch 56: reducing lr to 5.413497445986258e-05
Epoch 60: reducing lr to 4.6870522249525015e-05
Epoch 66: reducing lr to 3.588271556943327e-05
Epoch 72: reducing lr to 2.5399256344794985e-05
Epoch 75: reducing lr to 2.0554903553523695e-05
Epoch 78: reducing lr to 1.60788741400194e-05
Epoch 81: reducing lr to 1.2041744515557283e-05
Epoch 85: reducing lr to 7.450574447137397e-06
Epoch 96: reducing lr to 3.7319621390379104e-07
Epoch 99: reducing lr to 1.3537438698675574e-09
[I 2024-06-22 17:05:04,464] Trial 575 finished with value: 1.8687889575958252 and parameters: {'hidden_size': 192, 'n_layers': 2, 'rnn_dropout': 0.037699899260820804, 'bidirectional': True, 'fc_dropout': 0.3036793033488269, 'learning_rate_model': 0.0008781820885102795}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 7.186405981839274e-05
Epoch 41: reducing lr to 6.421331869600005e-05
Epoch 45: reducing lr to 5.98708366279803e-05
Epoch 48: reducing lr to 5.617675506832573e-05
Epoch 53: reducing lr to 4.9360843219838803e-05
Epoch 57: reducing lr to 4.348768694928079e-05
Epoch 65: reducing lr to 3.1322947926428415e-05
Epoch 68: reducing lr to 2.6836995312427327e-05
Epoch 71: reducing lr to 2.2503204036041837e-05
Epoch 74: reducing lr to 1.8389900986085882e-05
Epoch 80: reducing lr to 1.1079761129825599e-05
Epoch 86: reducing lr to 5.365894958816119e-06
Epoch 89: reducing lr to 3.2243491727940424e-06
Epoch 92: reducing lr to 1.6073340191597353e-06
Epoch 97: reducing lr to 1.432864368369302e-07
[I 2024-06-22 17:05:59,041] Trial 576 finished with value: 1.870226263999939 and parameters: {'hidden_size': 180, 'n_layers': 2, 'rnn_dropout': 0.08297870330109253, 'bidirectional': True, 'fc_dropout': 0.2957508189378141, 'learning_rate_model': 0.0007296842114674049}. Best is trial 346 with value: 1.8583240509033203.
Epoch 55: reducing lr to 2.5316750780307498e-05
Epoch 68: reducing lr to 1.4624119578176372e-05
Epoch 72: reducing lr to 1.1500248236541379e-05
Epoch 86: reducing lr to 2.9240042936298165e-06
Epoch 89: reducing lr to 1.757024857507015e-06
Epoch 98: reducing lr to 2.192655672133268e-08
[I 2024-06-22 17:06:49,891] Trial 577 finished with value: 1.8678017854690552 and parameters: {'hidden_size': 172, 'n_layers': 2, 'rnn_dropout': 0.013047607500994826, 'bidirectional': True, 'fc_dropout': 0.32888270106830786, 'learning_rate_model': 0.0003976223507355639}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.65649073222096e-05
Epoch 45: reducing lr to 4.7124923553207074e-05
Epoch 53: reducing lr to 3.8852404513913895e-05
Epoch 57: reducing lr to 3.4229585528004866e-05
Epoch 65: reducing lr to 2.465459996268337e-05
Epoch 68: reducing lr to 2.1123662599778472e-05
Epoch 72: reducing lr to 1.661141802511782e-05
Epoch 75: reducing lr to 1.344315324663172e-05
Epoch 80: reducing lr to 8.720988809212842e-06
Epoch 86: reducing lr to 4.2235486251844516e-06
Epoch 89: reducing lr to 2.5379169030311195e-06
Epoch 92: reducing lr to 1.265148393499692e-06
Epoch 97: reducing lr to 1.1278216177450489e-07
[I 2024-06-22 17:07:44,350] Trial 578 finished with value: 1.8634945154190063 and parameters: {'hidden_size': 179, 'n_layers': 2, 'rnn_dropout': 0.03730197062701328, 'bidirectional': True, 'fc_dropout': 0.24703561890123887, 'learning_rate_model': 0.000574341609706409}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.872857145834482e-05
Epoch 41: reducing lr to 5.247625148294678e-05
Epoch 45: reducing lr to 4.8927498892531515e-05
Epoch 53: reducing lr to 4.033854774704118e-05
Epoch 57: reducing lr to 3.553890132304087e-05
Epoch 65: reducing lr to 2.5597663007517058e-05
Epoch 68: reducing lr to 2.1931663767898756e-05
Epoch 71: reducing lr to 1.8390013444998182e-05
Epoch 74: reducing lr to 1.5028549972023905e-05
Epoch 80: reducing lr to 9.054575331518012e-06
Epoch 86: reducing lr to 4.385103573652412e-06
Epoch 89: reducing lr to 2.634994756483611e-06
Epoch 92: reducing lr to 1.3135415817057817e-06
Epoch 97: reducing lr to 1.1709619197766783e-07
[I 2024-06-22 17:08:38,857] Trial 579 finished with value: 1.863699197769165 and parameters: {'hidden_size': 179, 'n_layers': 2, 'rnn_dropout': 0.03462988404633728, 'bidirectional': True, 'fc_dropout': 0.24092384957250468, 'learning_rate_model': 0.0005963107492602538}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.271553790653097e-05
Epoch 41: reducing lr to 5.6038760306058286e-05
Epoch 45: reducing lr to 5.224908992793666e-05
Epoch 53: reducing lr to 4.307705189318887e-05
Epoch 57: reducing lr to 3.7951566975582114e-05
Epoch 65: reducing lr to 2.7335437672023137e-05
Epoch 68: reducing lr to 2.3420561001803595e-05
Epoch 71: reducing lr to 1.9638475050077493e-05
Epoch 74: reducing lr to 1.6048808476793548e-05
Epoch 80: reducing lr to 9.669272524943559e-06
Epoch 86: reducing lr to 4.682799573841581e-06
Epoch 89: reducing lr to 2.8138793338600253e-06
Epoch 97: reducing lr to 1.2504562062937197e-07
[I 2024-06-22 17:09:32,241] Trial 580 finished with value: 1.8644661903381348 and parameters: {'hidden_size': 177, 'n_layers': 2, 'rnn_dropout': 0.01737909050452483, 'bidirectional': True, 'fc_dropout': 0.246738783534603, 'learning_rate_model': 0.0006367931054789758}. Best is trial 346 with value: 1.8583240509033203.
Epoch 35: reducing lr to 8.607289126236643e-05
Epoch 45: reducing lr to 7.442736034302027e-05
Epoch 48: reducing lr to 6.983512888506824e-05
Epoch 51: reducing lr to 6.485683366191164e-05
Epoch 60: reducing lr to 4.8413608789317364e-05
Epoch 66: reducing lr to 3.7064057972908425e-05
Epoch 72: reducing lr to 2.6235458902507806e-05
Epoch 75: reducing lr to 2.123161875697962e-05
Epoch 78: reducing lr to 1.660822805095712e-05
Epoch 81: reducing lr to 1.2438186735224755e-05
Epoch 85: reducing lr to 7.695864676290226e-06
Epoch 96: reducing lr to 3.854827007347011e-07
Epoch 99: reducing lr to 1.3983122647514938e-09
[I 2024-06-22 17:10:25,937] Trial 581 finished with value: 1.8707611560821533 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.017487909213979618, 'bidirectional': True, 'fc_dropout': 0.24401781157251762, 'learning_rate_model': 0.0009070938841385153}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.702044837033132e-05
Epoch 41: reducing lr to 5.988536441203739e-05
Epoch 45: reducing lr to 5.5835564053217135e-05
Epoch 48: reducing lr to 5.2390462244743206e-05
Epoch 53: reducing lr to 4.6033940371464174e-05
Epoch 57: reducing lr to 4.0556632693654984e-05
Epoch 65: reducing lr to 2.9211792648715445e-05
Epoch 68: reducing lr to 2.5028191606439446e-05
Epoch 71: reducing lr to 2.098649628306379e-05
Epoch 74: reducing lr to 1.7150428359991294e-05
Epoch 80: reducing lr to 1.0332989266590646e-05
Epoch 86: reducing lr to 5.004235593657842e-06
Epoch 89: reducing lr to 3.0070292133406103e-06
Epoch 96: reducing lr to 2.8918994210034785e-07
Epoch 99: reducing lr to 1.0490168355498098e-09
[I 2024-06-22 17:11:20,275] Trial 582 finished with value: 1.8657761812210083 and parameters: {'hidden_size': 175, 'n_layers': 2, 'rnn_dropout': 0.0002033064264719102, 'bidirectional': True, 'fc_dropout': 0.24317574764646346, 'learning_rate_model': 0.0006805037614752291}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.890649060146671e-05
Epoch 41: reducing lr to 5.2635229123067273e-05
Epoch 45: reducing lr to 4.9075725530127e-05
Epoch 53: reducing lr to 4.0460754020269286e-05
Epoch 57: reducing lr to 3.56465669909411e-05
Epoch 65: reducing lr to 2.5675211535518466e-05
Epoch 68: reducing lr to 2.1998106092782985e-05
Epoch 71: reducing lr to 1.8445726283790034e-05
Epoch 74: reducing lr to 1.5074079203656644e-05
Epoch 80: reducing lr to 9.08200631177707e-06
Epoch 86: reducing lr to 4.398388314809083e-06
Epoch 89: reducing lr to 2.642977515089227e-06
Epoch 92: reducing lr to 1.3175209768599444e-06
Epoch 97: reducing lr to 1.1745093675729137e-07
[I 2024-06-22 17:12:14,757] Trial 583 finished with value: 1.8637151718139648 and parameters: {'hidden_size': 179, 'n_layers': 2, 'rnn_dropout': 0.01998231022688069, 'bidirectional': True, 'fc_dropout': 0.2565539508824903, 'learning_rate_model': 0.0005981172821778476}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.153571597472294e-05
Epoch 41: reducing lr to 5.4984543749087064e-05
Epoch 45: reducing lr to 5.126616569142826e-05
Epoch 53: reducing lr to 4.2266674556443985e-05
Epoch 57: reducing lr to 3.723761166946633e-05
Epoch 65: reducing lr to 2.6821195907421033e-05
Epoch 68: reducing lr to 2.297996697283492e-05
Epoch 71: reducing lr to 1.926903066125828e-05
Epoch 74: reducing lr to 1.5746893881904356e-05
Epoch 80: reducing lr to 9.487371513322487e-06
Epoch 86: reducing lr to 4.594705461538607e-06
Epoch 96: reducing lr to 2.6552359127025207e-07
Epoch 99: reducing lr to 9.631687584001417e-10
[I 2024-06-22 17:13:05,694] Trial 584 finished with value: 1.866026520729065 and parameters: {'hidden_size': 172, 'n_layers': 2, 'rnn_dropout': 0.01770637121569752, 'bidirectional': True, 'fc_dropout': 0.2508618248442811, 'learning_rate_model': 0.0006248135786033877}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.2210234966219534e-05
Epoch 57: reducing lr to 2.3396175728720334e-05
Epoch 68: reducing lr to 1.4438180147237693e-05
Epoch 72: reducing lr to 1.1354027494750734e-05
Epoch 75: reducing lr to 9.18849500672424e-06
Epoch 80: reducing lr to 5.960860570210935e-06
Epoch 86: reducing lr to 2.8868268285856186e-06
Epoch 89: reducing lr to 1.7346850372940093e-06
Epoch 97: reducing lr to 7.708744453777421e-08
[I 2024-06-22 17:14:00,162] Trial 585 finished with value: 1.8656013011932373 and parameters: {'hidden_size': 179, 'n_layers': 2, 'rnn_dropout': 0.02879053711535888, 'bidirectional': True, 'fc_dropout': 0.2557757304641455, 'learning_rate_model': 0.0003925667524666191}. Best is trial 346 with value: 1.8583240509033203.
Epoch 41: reducing lr to 7.005495963930712e-05
Epoch 45: reducing lr to 6.53174314724521e-05
Epoch 53: reducing lr to 5.385131853874072e-05
Epoch 65: reducing lr to 3.4172472274147535e-05
Epoch 68: reducing lr to 2.927842169866674e-05
Epoch 72: reducing lr to 2.3024231695375314e-05
Epoch 75: reducing lr to 1.8632862925902447e-05
Epoch 80: reducing lr to 1.2087713803389609e-05
Epoch 85: reducing lr to 6.7538887755541945e-06
Epoch 88: reducing lr to 4.234520584386848e-06
Epoch 96: reducing lr to 3.3829951476192735e-07
Epoch 99: reducing lr to 1.2271584684617891e-09
[I 2024-06-22 17:14:53,561] Trial 586 finished with value: 1.8653903007507324 and parameters: {'hidden_size': 177, 'n_layers': 2, 'rnn_dropout': 0.01475679633144859, 'bidirectional': True, 'fc_dropout': 0.2602820564026211, 'learning_rate_model': 0.0007960653493988141}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.625446187043131e-05
Epoch 45: reducing lr to 4.6866287609561704e-05
Epoch 53: reducing lr to 3.863917067613372e-05
Epoch 57: reducing lr to 3.404172312980638e-05
Epoch 60: reducing lr to 3.0485645376643183e-05
Epoch 65: reducing lr to 2.451928800362314e-05
Epoch 68: reducing lr to 2.100772950115876e-05
Epoch 72: reducing lr to 1.652024950000895e-05
Epoch 75: reducing lr to 1.3369373124281255e-05
Epoch 80: reducing lr to 8.673125364561417e-06
Epoch 86: reducing lr to 4.200368503035848e-06
Epoch 89: reducing lr to 2.523988041536673e-06
Epoch 92: reducing lr to 1.2582048735121213e-06
Epoch 98: reducing lr to 3.1497771201276136e-08
[I 2024-06-22 17:15:48,084] Trial 587 finished with value: 1.8687937259674072 and parameters: {'hidden_size': 180, 'n_layers': 2, 'rnn_dropout': 0.03489584824652802, 'bidirectional': True, 'fc_dropout': 0.2411575917888137, 'learning_rate_model': 0.0005711894478990057}. Best is trial 346 with value: 1.8583240509033203.
Epoch 57: reducing lr to 1.703056892525544e-05
Epoch 72: reducing lr to 8.264835675312049e-06
Epoch 78: reducing lr to 5.232013520683942e-06
Epoch 87: reducing lr to 1.7997537415065747e-06
Epoch 90: reducing lr to 1.0282442229325067e-06
Epoch 99: reducing lr to 4.4050386668946334e-10
[I 2024-06-22 17:16:42,534] Trial 588 finished with value: 1.871787667274475 and parameters: {'hidden_size': 175, 'n_layers': 2, 'rnn_dropout': 0.0002616370047615769, 'bidirectional': True, 'fc_dropout': 0.2943494169955097, 'learning_rate_model': 0.00028575760471141417}. Best is trial 346 with value: 1.8583240509033203.
Epoch 39: reducing lr to 3.731557422301945e-05
Epoch 45: reducing lr to 3.379895258520534e-05
Epoch 57: reducing lr to 2.4550154165577067e-05
Epoch 68: reducing lr to 1.5150319975154629e-05
Epoch 72: reducing lr to 1.19140464932547e-05
Epoch 75: reducing lr to 9.641702626117749e-06
Epoch 86: reducing lr to 3.0292148816484052e-06
Epoch 89: reducing lr to 1.8202455644069064e-06
Epoch 92: reducing lr to 9.073901311874811e-07
Epoch 98: reducing lr to 2.2715511077827603e-08
[I 2024-06-22 17:17:37,017] Trial 589 finished with value: 1.8682971000671387 and parameters: {'hidden_size': 180, 'n_layers': 2, 'rnn_dropout': 0.03622782094476347, 'bidirectional': True, 'fc_dropout': 0.3101168863874861, 'learning_rate_model': 0.0004119294710846559}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.6968439166334335e-05
Epoch 45: reducing lr to 4.74611109210629e-05
Epoch 53: reducing lr to 3.912957605337868e-05
Epoch 57: reducing lr to 3.447377805700628e-05
Epoch 65: reducing lr to 2.4830484917862864e-05
Epoch 68: reducing lr to 2.1274357985435193e-05
Epoch 72: reducing lr to 1.67299232338512e-05
Epoch 75: reducing lr to 1.3539056177923797e-05
Epoch 80: reducing lr to 8.783204003462637e-06
Epoch 86: reducing lr to 4.253679256456641e-06
Epoch 89: reducing lr to 2.556022303297782e-06
Epoch 92: reducing lr to 1.2741739128276412e-06
Epoch 98: reducing lr to 3.189753848659941e-08
[I 2024-06-22 17:18:30,144] Trial 590 finished with value: 1.8685122728347778 and parameters: {'hidden_size': 173, 'n_layers': 2, 'rnn_dropout': 0.020293917973433204, 'bidirectional': True, 'fc_dropout': 0.254800511411329, 'learning_rate_model': 0.0005784389403641293}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 7.696096360378275e-05
Epoch 41: reducing lr to 6.876759948616399e-05
Epoch 45: reducing lr to 6.411713018020624e-05
Epoch 53: reducing lr to 5.286172348311679e-05
Epoch 65: reducing lr to 3.354450418499825e-05
Epoch 68: reducing lr to 2.8740388793703998e-05
Epoch 72: reducing lr to 2.2601128483354763e-05
Epoch 75: reducing lr to 1.829045740039379e-05
Epoch 78: reducing lr to 1.4307533077862724e-05
Epoch 81: reducing lr to 1.0715156824487714e-05
Epoch 85: reducing lr to 6.629776402452051e-06
Epoch 88: reducing lr to 4.156705207772945e-06
Epoch 91: reducing lr to 2.234279583885224e-06
Epoch 96: reducing lr to 3.3208277696955775e-07
Epoch 99: reducing lr to 1.2046076751652708e-09
[I 2024-06-22 17:19:24,582] Trial 591 finished with value: 1.8662049770355225 and parameters: {'hidden_size': 179, 'n_layers': 2, 'rnn_dropout': 0.032314089641522374, 'bidirectional': True, 'fc_dropout': 0.3451297433545119, 'learning_rate_model': 0.000781436509194059}. Best is trial 346 with value: 1.8583240509033203.
Epoch 31: reducing lr to 9.721406496501828e-05
Epoch 35: reducing lr to 9.419229186369458e-05
Epoch 43: reducing lr to 8.452434319538458e-05
Epoch 46: reducing lr to 7.982506414153804e-05
Epoch 50: reducing lr to 7.283261677530942e-05
Epoch 56: reducing lr to 6.119198940808777e-05
Epoch 60: reducing lr to 5.2980545934698454e-05
Epoch 63: reducing lr to 4.67476988280757e-05
Epoch 66: reducing lr to 4.0560372900631466e-05
Epoch 72: reducing lr to 2.871029386697795e-05
Epoch 75: reducing lr to 2.323443306441491e-05
Epoch 78: reducing lr to 1.817491013687534e-05
Epoch 81: reducing lr to 1.3611501810113737e-05
Epoch 85: reducing lr to 8.421828535107767e-06
Epoch 88: reducing lr to 5.280277403911515e-06
Epoch 96: reducing lr to 4.218459322497849e-07
Epoch 99: reducing lr to 1.5302174125476215e-09
[I 2024-06-22 17:20:22,898] Trial 592 finished with value: 1.8699860572814941 and parameters: {'hidden_size': 192, 'n_layers': 2, 'rnn_dropout': 0.06776224525264589, 'bidirectional': True, 'fc_dropout': 0.2927165215882177, 'learning_rate_model': 0.0009926615758974137}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.752964827851648e-05
Epoch 45: reducing lr to 4.7928661170165186e-05
Epoch 53: reducing lr to 3.951505044864626e-05
Epoch 57: reducing lr to 3.48133871223073e-05
Epoch 60: reducing lr to 3.117669954965329e-05
Epoch 65: reducing lr to 2.507509569884492e-05
Epoch 68: reducing lr to 2.1483936547389314e-05
Epoch 72: reducing lr to 1.689473353061098e-05
Epoch 75: reducing lr to 1.3672432514165195e-05
Epoch 80: reducing lr to 8.86972935316558e-06
Epoch 86: reducing lr to 4.295583222827448e-06
Epoch 89: reducing lr to 2.5812022630885503e-06
Epoch 92: reducing lr to 1.2867260912065348e-06
Epoch 98: reducing lr to 3.221176842719154e-08
[I 2024-06-22 17:21:17,392] Trial 593 finished with value: 1.8690698146820068 and parameters: {'hidden_size': 180, 'n_layers': 2, 'rnn_dropout': 0.03928008611068018, 'bidirectional': True, 'fc_dropout': 0.24298433589168633, 'learning_rate_model': 0.0005841372745457189}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.362879427569342e-05
Epoch 57: reducing lr to 2.4426558242876776e-05
Epoch 68: reducing lr to 1.5074046817605197e-05
Epoch 72: reducing lr to 1.1854066113518733e-05
Epoch 80: reducing lr to 6.223380674867625e-06
Epoch 86: reducing lr to 3.0139645249366028e-06
Epoch 89: reducing lr to 1.8110816736811483e-06
Epoch 92: reducing lr to 9.028219431526181e-07
Epoch 98: reducing lr to 2.2601151529114258e-08
[I 2024-06-22 17:22:08,640] Trial 594 finished with value: 1.8651028871536255 and parameters: {'hidden_size': 176, 'n_layers': 2, 'rnn_dropout': 0.01656916638281944, 'bidirectional': True, 'fc_dropout': 0.2956166407199415, 'learning_rate_model': 0.0004098556428421621}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 7.055675883762129e-05
Epoch 41: reducing lr to 6.304519467514721e-05
Epoch 45: reducing lr to 5.878170801986715e-05
Epoch 48: reducing lr to 5.5154826621992895e-05
Epoch 53: reducing lr to 4.8462905100059975e-05
Epoch 66: reducing lr to 2.927268445036622e-05
Epoch 72: reducing lr to 2.072040547813223e-05
Epoch 75: reducing lr to 1.6768441186278746e-05
Epoch 79: reducing lr to 1.1976665257867523e-05
Epoch 85: reducing lr to 6.078088330382718e-06
Epoch 96: reducing lr to 3.04448948032878e-07
Epoch 99: reducing lr to 1.1043678411863655e-09
[I 2024-06-22 17:23:06,977] Trial 595 finished with value: 1.865098237991333 and parameters: {'hidden_size': 188, 'n_layers': 2, 'rnn_dropout': 0.06483449901595038, 'bidirectional': True, 'fc_dropout': 0.3317090111227994, 'learning_rate_model': 0.0007164103039298204}. Best is trial 346 with value: 1.8583240509033203.
Epoch 55: reducing lr to 2.1914967236883364e-05
Epoch 58: reducing lr to 1.980311851350323e-05
Epoch 68: reducing lr to 1.2659092954111988e-05
Epoch 72: reducing lr to 9.95497271774182e-06
Epoch 86: reducing lr to 2.53110910051094e-06
Epoch 89: reducing lr to 1.5209353886205249e-06
Epoch 97: reducing lr to 6.758865148149564e-08
[I 2024-06-22 17:24:05,474] Trial 596 finished with value: 1.8648242950439453 and parameters: {'hidden_size': 187, 'n_layers': 2, 'rnn_dropout': 0.04027962178873752, 'bidirectional': True, 'fc_dropout': 0.3147248134548308, 'learning_rate_model': 0.00034419427929908275}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.793876392395057e-05
Epoch 45: reducing lr to 4.8269500471919564e-05
Epoch 53: reducing lr to 3.979605730080697e-05
Epoch 57: reducing lr to 3.506095862271587e-05
Epoch 65: reducing lr to 2.525341443132682e-05
Epoch 68: reducing lr to 2.16367171540862e-05
Epoch 71: reducing lr to 1.8142696494903916e-05
Epoch 74: reducing lr to 1.4826439454022558e-05
Epoch 80: reducing lr to 8.932805439283437e-06
Epoch 86: reducing lr to 4.326130781439567e-06
Epoch 89: reducing lr to 2.599558193664504e-06
Epoch 92: reducing lr to 1.295876499579488e-06
Epoch 97: reducing lr to 1.1552143113508634e-07
[I 2024-06-22 17:24:56,089] Trial 597 finished with value: 1.8656837940216064 and parameters: {'hidden_size': 171, 'n_layers': 2, 'rnn_dropout': 0.014446293880406427, 'bidirectional': True, 'fc_dropout': 0.2600431005949723, 'learning_rate_model': 0.0005882913012997314}. Best is trial 346 with value: 1.8583240509033203.
Epoch 39: reducing lr to 3.7681002747291775e-05
Epoch 45: reducing lr to 3.412994310651809e-05
Epoch 57: reducing lr to 2.4790571921277862e-05
Epoch 68: reducing lr to 1.5298685883654005e-05
Epoch 72: reducing lr to 1.203071982654234e-05
Epoch 75: reducing lr to 9.736123072151277e-06
Epoch 86: reducing lr to 3.0588797480467853e-06
Epoch 89: reducing lr to 1.8380710880458905e-06
Epoch 92: reducing lr to 9.16276132367506e-07
Epoch 98: reducing lr to 2.2937962316062557e-08
[I 2024-06-22 17:25:50,705] Trial 598 finished with value: 1.8681683540344238 and parameters: {'hidden_size': 180, 'n_layers': 2, 'rnn_dropout': 0.03715523335171424, 'bidirectional': True, 'fc_dropout': 0.2904339972415178, 'learning_rate_model': 0.00041596346444686664}. Best is trial 346 with value: 1.8583240509033203.
Epoch 35: reducing lr to 7.782848038477333e-05
Epoch 45: reducing lr to 6.729840568374038e-05
Epoch 50: reducing lr to 6.0179572806994485e-05
Epoch 56: reducing lr to 5.0561245014022884e-05
Epoch 60: reducing lr to 4.3776356838415345e-05
Epoch 66: reducing lr to 3.351391206473684e-05
Epoch 72: reducing lr to 2.372252016439603e-05
Epoch 75: reducing lr to 1.9197968137583183e-05
Epoch 79: reducing lr to 1.3711926795150463e-05
Epoch 85: reducing lr to 6.958723521634561e-06
Epoch 92: reducing lr to 1.8067396913597667e-06
Epoch 96: reducing lr to 3.4855960306188606e-07
Epoch 99: reducing lr to 1.2643762405659987e-09
[I 2024-06-22 17:26:52,370] Trial 599 finished with value: 1.8649866580963135 and parameters: {'hidden_size': 191, 'n_layers': 2, 'rnn_dropout': 0.09147547312217541, 'bidirectional': True, 'fc_dropout': 0.2628476388871241, 'learning_rate_model': 0.0008202087502048355}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.369519256206709e-05
Epoch 41: reducing lr to 5.6914119654902934e-05
Epoch 45: reducing lr to 5.3065252332087836e-05
Epoch 53: reducing lr to 4.374994151261345e-05
Epoch 57: reducing lr to 3.8544393418814266e-05
Epoch 65: reducing lr to 2.776243375099205e-05
Epoch 68: reducing lr to 2.3786404338025625e-05
Epoch 72: reducing lr to 1.8705369104767067e-05
Epoch 75: reducing lr to 1.513772894222319e-05
Epoch 80: reducing lr to 9.820312413317417e-06
Epoch 86: reducing lr to 4.755947737065404e-06
Epoch 89: reducing lr to 2.857833831924634e-06
Epoch 92: reducing lr to 1.4246265813629664e-06
Epoch 97: reducing lr to 1.269989088972215e-07
[I 2024-06-22 17:27:43,714] Trial 600 finished with value: 1.8663573265075684 and parameters: {'hidden_size': 176, 'n_layers': 2, 'rnn_dropout': 0.0676887806321501, 'bidirectional': True, 'fc_dropout': 0.31012989123624596, 'learning_rate_model': 0.0006467401991533619}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.572559982378016e-05
Epoch 41: reducing lr to 4.979298010789993e-05
Epoch 45: reducing lr to 4.642568610063211e-05
Epoch 49: reducing lr to 4.2550485711383e-05
Epoch 53: reducing lr to 3.827591431912243e-05
Epoch 57: reducing lr to 3.372168850913171e-05
Epoch 65: reducing lr to 2.4288776140121677e-05
Epoch 68: reducing lr to 2.0810230663732024e-05
Epoch 72: reducing lr to 1.636493856695113e-05
Epoch 76: reducing lr to 1.2253673015775454e-05
Epoch 80: reducing lr to 8.591587177568728e-06
Epoch 86: reducing lr to 4.160879804551427e-06
Epoch 89: reducing lr to 2.500259408518284e-06
Epoch 92: reducing lr to 1.2463761797092198e-06
Epoch 95: reducing lr to 4.19008299951324e-07
Epoch 98: reducing lr to 3.120165289903411e-08
[I 2024-06-22 17:28:40,959] Trial 601 finished with value: 1.8694027662277222 and parameters: {'hidden_size': 186, 'n_layers': 2, 'rnn_dropout': 0.0005202923334177226, 'bidirectional': True, 'fc_dropout': 0.231922527973877, 'learning_rate_model': 0.0005658195552647613}. Best is trial 346 with value: 1.8583240509033203.
Epoch 39: reducing lr to 3.824107403186574e-05
Epoch 45: reducing lr to 3.463723324437086e-05
Epoch 57: reducing lr to 2.5159046389815503e-05
Epoch 68: reducing lr to 1.5526077779581394e-05
Epoch 72: reducing lr to 1.2209538334977214e-05
Epoch 75: reducing lr to 9.880835859981263e-06
Epoch 86: reducing lr to 3.1043453828478344e-06
Epoch 89: reducing lr to 1.8653912430408124e-06
Epoch 92: reducing lr to 9.298951953717763e-07
Epoch 98: reducing lr to 2.3278900536469886e-08
[I 2024-06-22 17:29:35,634] Trial 602 finished with value: 1.8680553436279297 and parameters: {'hidden_size': 180, 'n_layers': 2, 'rnn_dropout': 0.02344045809281263, 'bidirectional': True, 'fc_dropout': 0.2897487430662202, 'learning_rate_model': 0.0004221461340915947}. Best is trial 346 with value: 1.8583240509033203.
Epoch 39: reducing lr to 2.7452797011850185e-05
Epoch 46: reducing lr to 2.437010490983046e-05
Epoch 57: reducing lr to 1.8061370163813605e-05
Epoch 68: reducing lr to 1.1145980401017783e-05
Epoch 72: reducing lr to 8.765077498587698e-06
Epoch 75: reducing lr to 7.093330614758403e-06
Epoch 80: reducing lr to 4.601662703417785e-06
Epoch 86: reducing lr to 2.2285713936533517e-06
Epoch 89: reducing lr to 1.33914144514376e-06
Epoch 92: reducing lr to 6.675603310608868e-07
Epoch 97: reducing lr to 5.950993388505578e-08
[I 2024-06-22 17:30:36,902] Trial 603 finished with value: 1.8697422742843628 and parameters: {'hidden_size': 193, 'n_layers': 2, 'rnn_dropout': 0.03878857391418687, 'bidirectional': True, 'fc_dropout': 0.2632099146992914, 'learning_rate_model': 0.0003030535209052134}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.816270490051533e-05
Epoch 41: reducing lr to 6.090601482881945e-05
Epoch 45: reducing lr to 5.678719208924394e-05
Epoch 48: reducing lr to 5.32833740212766e-05
Epoch 53: reducing lr to 4.6818515382196025e-05
Epoch 65: reducing lr to 2.970966101162165e-05
Epoch 68: reducing lr to 2.5454757169581927e-05
Epoch 72: reducing lr to 2.0017343586818354e-05
Epoch 75: reducing lr to 1.6199472978236077e-05
Epoch 78: reducing lr to 1.2671880774018577e-05
Epoch 81: reducing lr to 9.49018877090047e-06
Epoch 85: reducing lr to 5.871853356764973e-06
Epoch 88: reducing lr to 3.681506274980377e-06
Epoch 91: reducing lr to 1.9788543803280693e-06
Epoch 94: reducing lr to 7.907408604677576e-07
Epoch 97: reducing lr to 1.3590647585236693e-07
[I 2024-06-22 17:31:39,377] Trial 604 finished with value: 1.8681442737579346 and parameters: {'hidden_size': 196, 'n_layers': 2, 'rnn_dropout': 0.06585156251146314, 'bidirectional': True, 'fc_dropout': 0.3287488312139039, 'learning_rate_model': 0.0006921018615217212}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.564514524424489e-05
Epoch 41: reducing lr to 4.972109082737073e-05
Epoch 45: reducing lr to 4.635865839583082e-05
Epoch 49: reducing lr to 4.248905287893692e-05
Epoch 53: reducing lr to 3.822065295625489e-05
Epoch 57: reducing lr to 3.367300237064597e-05
Epoch 65: reducing lr to 2.4253708894942452e-05
Epoch 68: reducing lr to 2.0780185615076163e-05
Epoch 72: reducing lr to 1.6341311468172683e-05
Epoch 80: reducing lr to 8.579182958751927e-06
Epoch 86: reducing lr to 4.154872478722186e-06
Epoch 89: reducing lr to 2.4966496255805075e-06
Epoch 92: reducing lr to 1.2445767074415632e-06
Epoch 95: reducing lr to 4.1840335111810934e-07
Epoch 98: reducing lr to 3.1156605095645096e-08
[I 2024-06-22 17:32:36,556] Trial 605 finished with value: 1.8692983388900757 and parameters: {'hidden_size': 186, 'n_layers': 2, 'rnn_dropout': 0.022716908239812525, 'bidirectional': True, 'fc_dropout': 0.3080066341458594, 'learning_rate_model': 0.0005650026457194963}. Best is trial 346 with value: 1.8583240509033203.
Epoch 23: reducing lr to 9.130096256311756e-05
Epoch 45: reducing lr to 7.526433320591412e-05
Epoch 50: reducing lr to 6.730286362533429e-05
Epoch 56: reducing lr to 5.654604077731814e-05
Epoch 60: reducing lr to 4.8958044015350705e-05
Epoch 66: reducing lr to 3.748086182795685e-05
Epoch 72: reducing lr to 2.6530489749306138e-05
Epoch 75: reducing lr to 2.147037892062106e-05
Epoch 78: reducing lr to 1.6794995875522423e-05
Epoch 81: reducing lr to 1.2578060361173723e-05
Epoch 85: reducing lr to 7.782408520662398e-06
Epoch 88: reducing lr to 4.879376929648718e-06
Epoch 96: reducing lr to 3.8981764635338984e-07
Epoch 99: reducing lr to 1.4140369849861833e-09
[I 2024-06-22 17:33:27,835] Trial 606 finished with value: 1.8668320178985596 and parameters: {'hidden_size': 176, 'n_layers': 2, 'rnn_dropout': 0.045242719491754055, 'bidirectional': True, 'fc_dropout': 0.29172266160655796, 'learning_rate_model': 0.0009172946081951237}. Best is trial 346 with value: 1.8583240509033203.
Epoch 55: reducing lr to 2.208532747283406e-05
Epoch 58: reducing lr to 1.9957061884992375e-05
Epoch 68: reducing lr to 1.275750086133234e-05
Epoch 72: reducing lr to 1.0032359623355026e-05
Epoch 80: reducing lr to 5.266985387579964e-06
Epoch 86: reducing lr to 2.5507851665948674e-06
Epoch 97: reducing lr to 6.811406493475178e-08
[I 2024-06-22 17:34:24,267] Trial 607 finished with value: 1.8662983179092407 and parameters: {'hidden_size': 181, 'n_layers': 2, 'rnn_dropout': 0.01656577264077522, 'bidirectional': True, 'fc_dropout': 0.24011387991026314, 'learning_rate_model': 0.00034686994009293435}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.706070712186233e-05
Epoch 53: reducing lr to 3.055490547469107e-05
Epoch 57: reducing lr to 2.691935707277754e-05
Epoch 65: reducing lr to 1.9389249669382293e-05
Epoch 68: reducing lr to 1.6612395605640182e-05
Epoch 72: reducing lr to 1.3063806832760788e-05
Epoch 80: reducing lr to 6.858494140714213e-06
Epoch 86: reducing lr to 3.32154806439474e-06
Epoch 89: reducing lr to 1.9959076418800705e-06
Epoch 92: reducing lr to 9.949574565197486e-07
Epoch 95: reducing lr to 3.344860397424328e-07
Epoch 98: reducing lr to 2.490766248026575e-08
[I 2024-06-22 17:35:24,747] Trial 608 finished with value: 1.8669328689575195 and parameters: {'hidden_size': 190, 'n_layers': 2, 'rnn_dropout': 0.03675070853691824, 'bidirectional': True, 'fc_dropout': 0.35015642784660517, 'learning_rate_model': 0.00045168256158963267}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 7.259988183581618e-05
Epoch 41: reducing lr to 6.487080414599757e-05
Epoch 45: reducing lr to 6.0483859047027574e-05
Epoch 48: reducing lr to 5.6751953482541923e-05
Epoch 53: reducing lr to 4.986625295220753e-05
Epoch 57: reducing lr to 4.393296095168149e-05
Epoch 65: reducing lr to 3.16436661657419e-05
Epoch 68: reducing lr to 2.711178151407356e-05
Epoch 71: reducing lr to 2.273361619246796e-05
Epoch 74: reducing lr to 1.8578196694371705e-05
Epoch 80: reducing lr to 1.1193207715054998e-05
Epoch 86: reducing lr to 5.4208367985042215e-06
Epoch 89: reducing lr to 3.257363549092826e-06
Epoch 92: reducing lr to 1.6237916443431667e-06
Epoch 97: reducing lr to 1.447535584452671e-07
[I 2024-06-22 17:36:15,362] Trial 609 finished with value: 1.8663825988769531 and parameters: {'hidden_size': 171, 'n_layers': 2, 'rnn_dropout': 0.08667958566400283, 'bidirectional': True, 'fc_dropout': 0.260507038020628, 'learning_rate_model': 0.0007371555081060976}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.439198628910551e-05
Epoch 45: reducing lr to 4.531463617858283e-05
Epoch 49: reducing lr to 4.1532176283918284e-05
Epoch 53: reducing lr to 3.735990305052328e-05
Epoch 57: reducing lr to 3.291466803110951e-05
Epoch 65: reducing lr to 2.3707502170822447e-05
Epoch 68: reducing lr to 2.0312204525644384e-05
Epoch 72: reducing lr to 1.5973296240336065e-05
Epoch 80: reducing lr to 8.385975089397843e-06
Epoch 86: reducing lr to 4.061302489259146e-06
Epoch 89: reducing lr to 2.4404237172392084e-06
Epoch 92: reducing lr to 1.2165481626432382e-06
Epoch 95: reducing lr to 4.0898067993964777e-07
Epoch 98: reducing lr to 3.0454941392268464e-08
[I 2024-06-22 17:37:12,508] Trial 610 finished with value: 1.8689337968826294 and parameters: {'hidden_size': 186, 'n_layers': 2, 'rnn_dropout': 0.06069368558056692, 'bidirectional': True, 'fc_dropout': 0.30917433420453283, 'learning_rate_model': 0.0005522784786416134}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.594063171891021e-05
Epoch 53: reducing lr to 2.9631453098318584e-05
Epoch 57: reducing lr to 2.6105780860608705e-05
Epoch 68: reducing lr to 1.6110323811899074e-05
Epoch 72: reducing lr to 1.2668983046636606e-05
Epoch 75: reducing lr to 1.0252651538682055e-05
Epoch 80: reducing lr to 6.651211787383901e-06
Epoch 86: reducing lr to 3.2211618447141947e-06
Epoch 97: reducing lr to 8.601525127617938e-08
[I 2024-06-22 17:38:08,787] Trial 611 finished with value: 1.8628876209259033 and parameters: {'hidden_size': 181, 'n_layers': 2, 'rnn_dropout': 0.024492335245784473, 'bidirectional': True, 'fc_dropout': 0.2891796045724029, 'learning_rate_model': 0.00043803148565318604}. Best is trial 346 with value: 1.8583240509033203.
Epoch 57: reducing lr to 2.216206389876684e-05
Epoch 72: reducing lr to 1.075512022839402e-05
Epoch 75: reducing lr to 8.70381620627623e-06
Epoch 80: reducing lr to 5.646434459221771e-06
Epoch 86: reducing lr to 2.7345512096342993e-06
Epoch 89: reducing lr to 1.6431831033629896e-06
Epoch 92: reducing lr to 8.191247164013327e-07
Epoch 97: reducing lr to 7.302120190273361e-08
[I 2024-06-22 17:39:05,862] Trial 612 finished with value: 1.8673808574676514 and parameters: {'hidden_size': 182, 'n_layers': 2, 'rnn_dropout': 0.06085917550769781, 'bidirectional': True, 'fc_dropout': 0.2883495785840139, 'learning_rate_model': 0.00037185946769995704}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.598880649783155e-05
Epoch 53: reducing lr to 2.967117106191182e-05
Epoch 57: reducing lr to 2.6140773017434576e-05
Epoch 68: reducing lr to 1.6131918070287725e-05
Epoch 72: reducing lr to 1.2685964536060696e-05
Epoch 80: reducing lr to 6.660127063551612e-06
Epoch 86: reducing lr to 3.2254794861221826e-06
Epoch 89: reducing lr to 1.93818033948931e-06
Epoch 92: reducing lr to 9.661804686705725e-07
Epoch 95: reducing lr to 3.2481175604485926e-07
Epoch 98: reducing lr to 2.418726233064045e-08
[I 2024-06-22 17:40:06,447] Trial 613 finished with value: 1.8669902086257935 and parameters: {'hidden_size': 190, 'n_layers': 2, 'rnn_dropout': 0.041609276164522556, 'bidirectional': True, 'fc_dropout': 0.32635109400201046, 'learning_rate_model': 0.0004386186225223142}. Best is trial 346 with value: 1.8583240509033203.
Epoch 46: reducing lr to 2.3372102724117718e-05
Epoch 57: reducing lr to 1.732172267492726e-05
Epoch 63: reducing lr to 1.3687330300024876e-05
Epoch 67: reducing lr to 1.1279776801574235e-05
Epoch 72: reducing lr to 8.406130890278126e-06
Epoch 80: reducing lr to 4.413215856228738e-06
Epoch 87: reducing lr to 1.8305222409398767e-06
Epoch 90: reducing lr to 1.0458230344448568e-06
Epoch 98: reducing lr to 1.6027263236538526e-08
[I 2024-06-22 17:41:03,534] Trial 614 finished with value: 1.86956787109375 and parameters: {'hidden_size': 185, 'n_layers': 2, 'rnn_dropout': 0.029502797277025723, 'bidirectional': True, 'fc_dropout': 0.3015526032738559, 'learning_rate_model': 0.0002906429023473367}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.7412591082801716e-05
Epoch 53: reducing lr to 3.084501815735546e-05
Epoch 57: reducing lr to 2.7174950954501475e-05
Epoch 65: reducing lr to 1.9573346695671365e-05
Epoch 68: reducing lr to 1.677012696103994e-05
Epoch 72: reducing lr to 1.3187845051409542e-05
Epoch 75: reducing lr to 1.0672551960998211e-05
Epoch 80: reducing lr to 6.923614163286328e-06
Epoch 86: reducing lr to 3.3530854952782547e-06
Epoch 92: reducing lr to 1.0044043774760903e-06
Epoch 97: reducing lr to 8.953803171987923e-08
[I 2024-06-22 17:41:59,925] Trial 615 finished with value: 1.862593173980713 and parameters: {'hidden_size': 181, 'n_layers': 2, 'rnn_dropout': 0.07858422816631672, 'bidirectional': True, 'fc_dropout': 0.315463181180845, 'learning_rate_model': 0.0004559711966752177}. Best is trial 346 with value: 1.8583240509033203.
Epoch 39: reducing lr to 4.267973493379137e-05
Epoch 45: reducing lr to 3.865759451415517e-05
Epoch 57: reducing lr to 2.807926969335431e-05
Epoch 68: reducing lr to 1.7328197519813016e-05
Epoch 72: reducing lr to 1.3626705655980451e-05
Epoch 75: reducing lr to 1.1027709500964717e-05
Epoch 80: reducing lr to 7.154015831312507e-06
Epoch 86: reducing lr to 3.4646683294637715e-06
Epoch 89: reducing lr to 2.0819081528530285e-06
Epoch 92: reducing lr to 1.037828603391277e-06
Epoch 98: reducing lr to 2.598089435507751e-08
[I 2024-06-22 17:42:54,592] Trial 616 finished with value: 1.8680071830749512 and parameters: {'hidden_size': 180, 'n_layers': 2, 'rnn_dropout': 0.09354811466821081, 'bidirectional': True, 'fc_dropout': 0.2867549632938991, 'learning_rate_model': 0.00047114485046472886}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.35023155008183e-05
Epoch 45: reducing lr to 4.457344044662952e-05
Epoch 53: reducing lr to 3.674882011965503e-05
Epoch 57: reducing lr to 3.237629426226419e-05
Epoch 65: reducing lr to 2.3319726809346802e-05
Epoch 68: reducing lr to 1.997996486599804e-05
Epoch 72: reducing lr to 1.5712026593330522e-05
Epoch 75: reducing lr to 1.2715301065201922e-05
Epoch 80: reducing lr to 8.248808613647423e-06
Epoch 86: reducing lr to 3.99487317800202e-06
Epoch 89: reducing lr to 2.4005065558998237e-06
Epoch 92: reducing lr to 1.1966495077734531e-06
Epoch 97: reducing lr to 1.0667580108902549e-07
[I 2024-06-22 17:43:47,572] Trial 617 finished with value: 1.8629558086395264 and parameters: {'hidden_size': 174, 'n_layers': 2, 'rnn_dropout': 0.07965092515658183, 'bidirectional': True, 'fc_dropout': 0.30351275725208526, 'learning_rate_model': 0.0005432450517901739}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.580195461003973e-05
Epoch 45: reducing lr to 4.64892982169732e-05
Epoch 53: reducing lr to 3.832835968114578e-05
Epoch 57: reducing lr to 3.3767893706142935e-05
Epoch 65: reducing lr to 2.4322056433497614e-05
Epoch 68: reducing lr to 2.0838744680976635e-05
Epoch 71: reducing lr to 1.7473585174188586e-05
Epoch 74: reducing lr to 1.4279633278470126e-05
Epoch 80: reducing lr to 8.60335930392804e-06
Epoch 86: reducing lr to 4.1665810099064736e-06
Epoch 89: reducing lr to 2.5036852446391127e-06
Epoch 92: reducing lr to 1.2480839547192985e-06
Epoch 97: reducing lr to 1.1126094552428361e-07
[I 2024-06-22 17:44:40,546] Trial 618 finished with value: 1.8630399703979492 and parameters: {'hidden_size': 174, 'n_layers': 2, 'rnn_dropout': 0.10207726953460416, 'bidirectional': True, 'fc_dropout': 0.34054507019798136, 'learning_rate_model': 0.0005665948368470205}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 8.149367381045267e-05
Epoch 41: reducing lr to 7.281775147859309e-05
Epoch 45: reducing lr to 6.789338708736273e-05
Epoch 53: reducing lr to 5.597507942818592e-05
Epoch 60: reducing lr to 4.416338113672627e-05
Epoch 65: reducing lr to 3.5520148841421825e-05
Epoch 68: reducing lr to 3.0433089190486443e-05
Epoch 72: reducing lr to 2.393224962531672e-05
Epoch 75: reducing lr to 1.936769628958243e-05
Epoch 79: reducing lr to 1.383315316549398e-05
Epoch 85: reducing lr to 7.020245203259213e-06
Epoch 96: reducing lr to 3.516411988258353e-07
Epoch 99: reducing lr to 1.27555451949566e-09
[I 2024-06-22 17:45:33,660] Trial 619 finished with value: 1.8647698163986206 and parameters: {'hidden_size': 174, 'n_layers': 2, 'rnn_dropout': 0.10897841953678716, 'bidirectional': True, 'fc_dropout': 0.3424741731850717, 'learning_rate_model': 0.00082746016943984}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.556264352814872e-05
Epoch 41: reducing lr to 5.858275936628564e-05
Epoch 45: reducing lr to 5.462104881761316e-05
Epoch 48: reducing lr to 5.1250883632518026e-05
Epoch 53: reducing lr to 4.503262655142752e-05
Epoch 57: reducing lr to 3.9674459312827464e-05
Epoch 65: reducing lr to 2.8576388174295616e-05
Epoch 68: reducing lr to 2.4483787326818225e-05
Epoch 71: reducing lr to 2.0530005515755806e-05
Epoch 74: reducing lr to 1.6777378371269204e-05
Epoch 80: reducing lr to 1.0108229776713446e-05
Epoch 86: reducing lr to 4.895385249363712e-06
Epoch 97: reducing lr to 1.3072233331233497e-07
[I 2024-06-22 17:46:28,021] Trial 620 finished with value: 1.8655891418457031 and parameters: {'hidden_size': 175, 'n_layers': 2, 'rnn_dropout': 0.09693476166706687, 'bidirectional': True, 'fc_dropout': 0.3364098894009891, 'learning_rate_model': 0.0006657016868439107}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.594089925612941e-05
Epoch 45: reducing lr to 4.660505471928311e-05
Epoch 53: reducing lr to 3.842379577130267e-05
Epoch 57: reducing lr to 3.385197441752629e-05
Epoch 65: reducing lr to 2.4382617386011585e-05
Epoch 68: reducing lr to 2.0890632325860866e-05
Epoch 72: reducing lr to 1.6428165557688536e-05
Epoch 75: reducing lr to 1.3294852180536684e-05
Epoch 80: reducing lr to 8.624781326185512e-06
Epoch 86: reducing lr to 4.1769556307933406e-06
Epoch 89: reducing lr to 2.5099193212528656e-06
Epoch 92: reducing lr to 1.2511916340934435e-06
Epoch 98: reducing lr to 3.132220248807348e-08
[I 2024-06-22 17:47:17,694] Trial 621 finished with value: 1.8688675165176392 and parameters: {'hidden_size': 170, 'n_layers': 2, 'rnn_dropout': 0.07884359868418478, 'bidirectional': True, 'fc_dropout': 0.3574459467835427, 'learning_rate_model': 0.0005680056354405842}. Best is trial 346 with value: 1.8583240509033203.
Epoch 23: reducing lr to 0.00010315736849103834
Epoch 35: reducing lr to 9.834400533048548e-05
Epoch 43: reducing lr to 8.824992250737064e-05
Epoch 46: reducing lr to 8.334351333973203e-05
Epoch 50: reducing lr to 7.604285988442971e-05
Epoch 56: reducing lr to 6.388914860719636e-05
Epoch 60: reducing lr to 5.531576935566931e-05
Epoch 63: reducing lr to 4.8808197059150326e-05
Epoch 66: reducing lr to 4.2348152378736706e-05
Epoch 72: reducing lr to 2.9975757434374173e-05
Epoch 75: reducing lr to 2.425853712577881e-05
Epoch 78: reducing lr to 1.8976005615921268e-05
Epoch 81: reducing lr to 1.421145594914324e-05
Epoch 85: reducing lr to 8.793037455205067e-06
Epoch 96: reducing lr to 4.404396345918712e-07
Epoch 99: reducing lr to 1.5976648024950552e-09
[I 2024-06-22 17:48:10,933] Trial 622 finished with value: 1.8680574893951416 and parameters: {'hidden_size': 177, 'n_layers': 2, 'rnn_dropout': 0.08169234931542677, 'bidirectional': True, 'fc_dropout': 0.313259466695248, 'learning_rate_model': 0.0010364151182635237}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 7.038385906238584e-05
Epoch 41: reducing lr to 6.289070203449043e-05
Epoch 45: reducing lr to 5.863766307970797e-05
Epoch 48: reducing lr to 5.501966937719882e-05
Epoch 53: reducing lr to 4.834414645046921e-05
Epoch 65: reducing lr to 3.0677781882118774e-05
Epoch 68: reducing lr to 2.628422747756251e-05
Epoch 71: reducing lr to 2.2039700308157513e-05
Epoch 74: reducing lr to 1.801111991789567e-05
Epoch 80: reducing lr to 1.0851548712628614e-05
Epoch 86: reducing lr to 5.255372372216303e-06
Epoch 96: reducing lr to 3.037028939970542e-07
Epoch 99: reducing lr to 1.101661580948758e-09
[I 2024-06-22 17:49:01,898] Trial 623 finished with value: 1.8665096759796143 and parameters: {'hidden_size': 172, 'n_layers': 2, 'rnn_dropout': 0.07324925974816404, 'bidirectional': True, 'fc_dropout': 0.30728691813594433, 'learning_rate_model': 0.0007146547360357382}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.600275293785531e-05
Epoch 45: reducing lr to 4.665658578617258e-05
Epoch 53: reducing lr to 3.846628084512062e-05
Epoch 57: reducing lr to 3.3889404442414306e-05
Epoch 65: reducing lr to 2.4409577171704937e-05
Epoch 68: reducing lr to 2.0913731034321376e-05
Epoch 72: reducing lr to 1.6446330130250968e-05
Epoch 75: reducing lr to 1.3309552258052466e-05
Epoch 80: reducing lr to 8.634317720598167e-06
Epoch 86: reducing lr to 4.1815740778974354e-06
Epoch 89: reducing lr to 2.5126945313928254e-06
Epoch 92: reducing lr to 1.2525750728679736e-06
Epoch 98: reducing lr to 3.1356835351848637e-08
[I 2024-06-22 17:49:56,559] Trial 624 finished with value: 1.8685548305511475 and parameters: {'hidden_size': 180, 'n_layers': 2, 'rnn_dropout': 0.06535294541218269, 'bidirectional': True, 'fc_dropout': 0.3260845203464386, 'learning_rate_model': 0.0005686336775396609}. Best is trial 346 with value: 1.8583240509033203.
Epoch 41: reducing lr to 7.048021097210051e-05
Epoch 45: reducing lr to 6.571392481041579e-05
Epoch 53: reducing lr to 5.417820966963549e-05
Epoch 65: reducing lr to 3.4379907828378766e-05
Epoch 68: reducing lr to 2.945614912743909e-05
Epoch 72: reducing lr to 2.3163994608171402e-05
Epoch 75: reducing lr to 1.874596911900845e-05
Epoch 78: reducing lr to 1.4663852706112861e-05
Epoch 81: reducing lr to 1.098201070317983e-05
Epoch 85: reducing lr to 6.794886589529532e-06
Epoch 96: reducing lr to 3.403530784250341e-07
Epoch 99: reducing lr to 1.2346076309075262e-09
[I 2024-06-22 17:50:50,004] Trial 625 finished with value: 1.8650610446929932 and parameters: {'hidden_size': 177, 'n_layers': 2, 'rnn_dropout': 0.07895646524559607, 'bidirectional': True, 'fc_dropout': 0.30216282918475657, 'learning_rate_model': 0.0008008976675182658}. Best is trial 346 with value: 1.8583240509033203.
Epoch 39: reducing lr to 3.731501168638656e-05
Epoch 45: reducing lr to 3.3798443062053724e-05
Epoch 57: reducing lr to 2.4549784069140155e-05
Epoch 68: reducing lr to 1.515009158231428e-05
Epoch 72: reducing lr to 1.1913866887614487e-05
Epoch 75: reducing lr to 9.641557276326312e-06
Epoch 80: reducing lr to 6.254776060913702e-06
Epoch 86: reducing lr to 3.0291692158808185e-06
Epoch 89: reducing lr to 1.820218123992762e-06
Epoch 92: reducing lr to 9.073764521754402e-07
Epoch 98: reducing lr to 2.2715168638854373e-08
[I 2024-06-22 17:51:44,719] Trial 626 finished with value: 1.8683085441589355 and parameters: {'hidden_size': 180, 'n_layers': 2, 'rnn_dropout': 0.10473971694958822, 'bidirectional': True, 'fc_dropout': 0.3137481211317519, 'learning_rate_model': 0.0004119232611998429}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.48539498357206e-05
Epoch 45: reducing lr to 4.5699503720124034e-05
Epoch 53: reducing lr to 3.767720922909675e-05
Epoch 57: reducing lr to 3.319421981468458e-05
Epoch 65: reducing lr to 2.3908855394549303e-05
Epoch 68: reducing lr to 2.048472071198777e-05
Epoch 71: reducing lr to 1.7176731017638627e-05
Epoch 74: reducing lr to 1.4037040333149175e-05
Epoch 80: reducing lr to 8.457199088711512e-06
Epoch 86: reducing lr to 4.095796057702168e-06
Epoch 89: reducing lr to 2.4611507925415713e-06
Epoch 92: reducing lr to 1.2268805836887844e-06
Epoch 97: reducing lr to 1.0937076249594671e-07
[I 2024-06-22 17:52:37,842] Trial 627 finished with value: 1.8630826473236084 and parameters: {'hidden_size': 174, 'n_layers': 2, 'rnn_dropout': 0.09579328061967515, 'bidirectional': True, 'fc_dropout': 0.33791861106071774, 'learning_rate_model': 0.0005569691057379726}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.427416726605061e-05
Epoch 41: reducing lr to 5.743145596011263e-05
Epoch 45: reducing lr to 5.35476033153405e-05
Epoch 48: reducing lr to 5.024367063105172e-05
Epoch 53: reducing lr to 4.414761845521597e-05
Epoch 57: reducing lr to 3.88947531221488e-05
Epoch 65: reducing lr to 2.8014787911742557e-05
Epoch 68: reducing lr to 2.4002617302560127e-05
Epoch 71: reducing lr to 2.012653757510699e-05
Epoch 74: reducing lr to 1.6447659302477583e-05
Epoch 80: reducing lr to 9.909576802729302e-06
Epoch 86: reducing lr to 4.799178212121192e-06
Epoch 89: reducing lr to 2.8838108865549384e-06
Epoch 92: reducing lr to 1.437576110519781e-06
Epoch 97: reducing lr to 1.2815329987599316e-07
[I 2024-06-22 17:53:28,879] Trial 628 finished with value: 1.8669724464416504 and parameters: {'hidden_size': 172, 'n_layers': 2, 'rnn_dropout': 0.10435438113479084, 'bidirectional': True, 'fc_dropout': 0.3462981329314978, 'learning_rate_model': 0.0006526189193565261}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 4.6664759794307764e-05
Epoch 53: reducing lr to 3.847301995144955e-05
Epoch 57: reducing lr to 3.389534170213748e-05
Epoch 65: reducing lr to 2.4413853611547588e-05
Epoch 68: reducing lr to 2.0917395018831416e-05
Epoch 72: reducing lr to 1.6449211447733025e-05
Epoch 75: reducing lr to 1.3311884027225026e-05
Epoch 80: reducing lr to 8.635830411295545e-06
Epoch 86: reducing lr to 4.182306669448129e-06
Epoch 89: reducing lr to 2.513134743319936e-06
Epoch 92: reducing lr to 1.2527945179615914e-06
Epoch 98: reducing lr to 3.136232891772645e-08
[I 2024-06-22 17:54:21,856] Trial 629 finished with value: 1.868192434310913 and parameters: {'hidden_size': 173, 'n_layers': 2, 'rnn_dropout': 0.08979993571330087, 'bidirectional': True, 'fc_dropout': 0.3332465768337092, 'learning_rate_model': 0.0005687332994092819}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 7.183679864661138e-05
Epoch 50: reducing lr to 6.423789405489047e-05
Epoch 60: reducing lr to 4.672849675610132e-05
Epoch 66: reducing lr to 3.5773984961376455e-05
Epoch 72: reducing lr to 2.5322292365265155e-05
Epoch 75: reducing lr to 2.0492618770266453e-05
Epoch 78: reducing lr to 1.6030152471818727e-05
Epoch 81: reducing lr to 1.2005256022909407e-05
Epoch 85: reducing lr to 7.427997964918752e-06
Epoch 96: reducing lr to 3.7206536718223167e-07
Epoch 99: reducing lr to 1.3496418003371494e-09
[I 2024-06-22 17:55:15,176] Trial 630 finished with value: 1.865598201751709 and parameters: {'hidden_size': 177, 'n_layers': 2, 'rnn_dropout': 0.06482343370147241, 'bidirectional': True, 'fc_dropout': 0.3250342939368317, 'learning_rate_model': 0.0008755210504323907}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 7.287030377780686e-05
Epoch 41: reducing lr to 6.511243661690616e-05
Epoch 45: reducing lr to 6.070915090989284e-05
Epoch 48: reducing lr to 5.6963344645784056e-05
Epoch 53: reducing lr to 5.005199607770781e-05
Epoch 57: reducing lr to 4.409660359568499e-05
Epoch 65: reducing lr to 3.176153332254523e-05
Epoch 68: reducing lr to 2.7212768188190233e-05
Epoch 71: reducing lr to 2.281829496168608e-05
Epoch 74: reducing lr to 1.8647397248170654e-05
Epoch 80: reducing lr to 1.1234900468415886e-05
Epoch 85: reducing lr to 6.2773878834577065e-06
Epoch 88: reducing lr to 3.93576635506573e-06
Epoch 91: reducing lr to 2.1155222645141712e-06
Epoch 96: reducing lr to 3.1443178078866247e-07
Epoch 99: reducing lr to 1.1405798876662662e-09
[I 2024-06-22 17:56:05,722] Trial 631 finished with value: 1.8665543794631958 and parameters: {'hidden_size': 171, 'n_layers': 2, 'rnn_dropout': 0.036905054083010876, 'bidirectional': True, 'fc_dropout': 0.3684189026785516, 'learning_rate_model': 0.0007399012842562848}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.6450180135527076e-05
Epoch 45: reducing lr to 4.702934291570878e-05
Epoch 53: reducing lr to 3.877360252736874e-05
Epoch 57: reducing lr to 3.4160159726127966e-05
Epoch 65: reducing lr to 2.460459452598408e-05
Epoch 68: reducing lr to 2.108081875016869e-05
Epoch 72: reducing lr to 1.6577726088773376e-05
Epoch 75: reducing lr to 1.3415887310468452e-05
Epoch 80: reducing lr to 8.703300554099618e-06
Epoch 86: reducing lr to 4.2149822564847835e-06
Epoch 89: reducing lr to 2.532769399391443e-06
Epoch 92: reducing lr to 1.2625823693905015e-06
Epoch 98: reducing lr to 3.160735698218807e-08
[I 2024-06-22 17:57:00,339] Trial 632 finished with value: 1.86815345287323 and parameters: {'hidden_size': 180, 'n_layers': 2, 'rnn_dropout': 0.09796916265388507, 'bidirectional': True, 'fc_dropout': 0.2958360518449889, 'learning_rate_model': 0.0005731767072926034}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 3.62589021525893e-05
Epoch 53: reducing lr to 2.98938529220585e-05
Epoch 57: reducing lr to 2.633695927341511e-05
Epoch 68: reducing lr to 1.6252987963893532e-05
Epoch 72: reducing lr to 1.2781172580756686e-05
Epoch 75: reducing lr to 1.0343443372200662e-05
Epoch 80: reducing lr to 6.710111254611356e-06
Epoch 86: reducing lr to 3.2496866793716794e-06
Epoch 89: reducing lr to 1.952726364733701e-06
Epoch 92: reducing lr to 9.734316440135338e-07
Epoch 98: reducing lr to 2.436878750726348e-08
[I 2024-06-22 17:57:54,791] Trial 633 finished with value: 1.8654570579528809 and parameters: {'hidden_size': 175, 'n_layers': 2, 'rnn_dropout': 0.08298567114360925, 'bidirectional': True, 'fc_dropout': 0.3456806482637733, 'learning_rate_model': 0.0004419104511648184}. Best is trial 346 with value: 1.8583240509033203.
Epoch 45: reducing lr to 7.801201235115837e-05
Epoch 60: reducing lr to 5.074535801659079e-05
Epoch 66: reducing lr to 3.8849177708849637e-05
Epoch 72: reducing lr to 2.7499039795419566e-05
Epoch 75: reducing lr to 2.225419922285216e-05
Epoch 79: reducing lr to 1.589480451481038e-05
Epoch 85: reducing lr to 8.066521335871856e-06
Epoch 96: reducing lr to 4.040487405743772e-07
Epoch 99: reducing lr to 1.4656593108446082e-09
[I 2024-06-22 17:58:44,157] Trial 634 finished with value: 1.867889165878296 and parameters: {'hidden_size': 169, 'n_layers': 2, 'rnn_dropout': 0.06557398537349587, 'bidirectional': True, 'fc_dropout': 0.3653255716652568, 'learning_rate_model': 0.0009507823328267517}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.824946105064803e-05
Epoch 45: reducing lr to 5.6859469710410824e-05
Epoch 48: reducing lr to 5.335119205172175e-05
Epoch 53: reducing lr to 4.687810506772004e-05
Epoch 65: reducing lr to 2.9747474883809944e-05
Epoch 68: reducing lr to 2.5487155483848055e-05
Epoch 72: reducing lr to 2.004282126802263e-05
Epoch 75: reducing lr to 1.6220091348821903e-05
Epoch 80: reducing lr to 1.0522474343802371e-05
Epoch 85: reducing lr to 5.879326936226363e-06
Epoch 88: reducing lr to 3.686192023757116e-06
Epoch 96: reducing lr to 2.944930714362951e-07
Epoch 99: reducing lr to 1.0682535763308424e-09
[I 2024-06-22 17:59:47,529] Trial 635 finished with value: 1.8671820163726807 and parameters: {'hidden_size': 200, 'n_layers': 2, 'rnn_dropout': 0.0012849549542507804, 'bidirectional': True, 'fc_dropout': 0.31244904088048087, 'learning_rate_model': 0.0006929827551583946}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.260758040796878e-05
Epoch 45: reducing lr to 4.382802557993974e-05
Epoch 53: reducing lr to 3.613425869975972e-05
Epoch 57: reducing lr to 3.183485588933195e-05
Epoch 65: reducing lr to 2.2929744100436436e-05
Epoch 68: reducing lr to 1.9645833986760947e-05
Epoch 72: reducing lr to 1.5449269711852703e-05
Epoch 75: reducing lr to 1.2502659313668595e-05
Epoch 80: reducing lr to 8.110861340305272e-06
Epoch 86: reducing lr to 3.928065728821869e-06
Epoch 97: reducing lr to 1.0489183002350281e-07
[I 2024-06-22 18:00:43,878] Trial 636 finished with value: 1.8629887104034424 and parameters: {'hidden_size': 181, 'n_layers': 2, 'rnn_dropout': 0.04090281027269944, 'bidirectional': True, 'fc_dropout': 0.2952329070328722, 'learning_rate_model': 0.0005341602036428803}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.124115394980846e-05
Epoch 45: reducing lr to 5.1020762427056225e-05
Epoch 53: reducing lr to 4.206435047446108e-05
Epoch 57: reducing lr to 3.705936093942133e-05
Epoch 65: reducing lr to 2.6692806960417268e-05
Epoch 68: reducing lr to 2.2869965399004757e-05
Epoch 71: reducing lr to 1.9176792769818935e-05
Epoch 74: reducing lr to 1.56715159184811e-05
Epoch 80: reducing lr to 9.441956922465568e-06
Epoch 86: reducing lr to 4.572711311910248e-06
Epoch 89: reducing lr to 2.7477276482573416e-06
Epoch 97: reducing lr to 1.2210591440874486e-07
[I 2024-06-22 18:01:37,215] Trial 637 finished with value: 1.864098072052002 and parameters: {'hidden_size': 177, 'n_layers': 2, 'rnn_dropout': 0.032336813704581094, 'bidirectional': True, 'fc_dropout': 0.32388857180710523, 'learning_rate_model': 0.000621822691929004}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.059275556506336e-05
Epoch 45: reducing lr to 5.048057371713609e-05
Epoch 53: reducing lr to 4.161898889741852e-05
Epoch 57: reducing lr to 3.666699031569825e-05
Epoch 65: reducing lr to 2.641019352482406e-05
Epoch 68: reducing lr to 2.262782677705709e-05
Epoch 72: reducing lr to 1.7794276339065928e-05
Epoch 75: reducing lr to 1.4400407200473148e-05
Epoch 80: reducing lr to 9.34198902142998e-06
Epoch 86: reducing lr to 4.524297158398733e-06
Epoch 89: reducing lr to 2.7186357377699924e-06
Epoch 92: reducing lr to 1.3552365059875133e-06
Epoch 98: reducing lr to 3.3926851093855207e-08
[I 2024-06-22 18:02:30,170] Trial 638 finished with value: 1.8689374923706055 and parameters: {'hidden_size': 173, 'n_layers': 2, 'rnn_dropout': 0.01583153143933682, 'bidirectional': True, 'fc_dropout': 0.3351739758192904, 'learning_rate_model': 0.0006152390663269282}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 7.803010789061457e-05
Epoch 41: reducing lr to 6.9722921283956e-05
Epoch 45: reducing lr to 6.500784750247285e-05
Epoch 48: reducing lr to 6.0996808001155916e-05
Epoch 53: reducing lr to 5.3596080318161417e-05
Epoch 60: reducing lr to 4.228639149252685e-05
Epoch 65: reducing lr to 3.4010505561860355e-05
Epoch 68: reducing lr to 2.9139651238471556e-05
Epoch 72: reducing lr to 2.2915104118045782e-05
Epoch 75: reducing lr to 1.8544549047866495e-05
Epoch 78: reducing lr to 1.450629380710211e-05
Epoch 81: reducing lr to 1.0864012142365316e-05
Epoch 85: reducing lr to 6.721877478526737e-06
Epoch 88: reducing lr to 4.214450295890785e-06
Epoch 91: reducing lr to 2.26531827077841e-06
Epoch 97: reducing lr to 1.5558063591037102e-07
[I 2024-06-22 18:03:21,536] Trial 639 finished with value: 1.8678067922592163 and parameters: {'hidden_size': 176, 'n_layers': 2, 'rnn_dropout': 0.03747273421367828, 'bidirectional': True, 'fc_dropout': 0.3247320002371147, 'learning_rate_model': 0.0007922922513808105}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.5536333340011984e-05
Epoch 45: reducing lr to 4.626800585326683e-05
Epoch 53: reducing lr to 3.814591396490281e-05
Epoch 57: reducing lr to 3.360715613207284e-05
Epoch 65: reducing lr to 2.4206281716201424e-05
Epoch 68: reducing lr to 2.0739550775196337e-05
Epoch 72: reducing lr to 1.6309356673002144e-05
Epoch 75: reducing lr to 1.3198703492839728e-05
Epoch 80: reducing lr to 8.562406702163745e-06
Epoch 86: reducing lr to 4.146747788162555e-06
Epoch 89: reducing lr to 2.491767524926997e-06
Epoch 92: reducing lr to 1.2421429863881668e-06
Epoch 98: reducing lr to 3.1095679573487405e-08
[I 2024-06-22 18:04:16,247] Trial 640 finished with value: 1.8685190677642822 and parameters: {'hidden_size': 180, 'n_layers': 2, 'rnn_dropout': 0.02674840489495725, 'bidirectional': True, 'fc_dropout': 0.3476287117976447, 'learning_rate_model': 0.0005638978051533065}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 6.893101136269169e-05
Epoch 41: reducing lr to 6.159252638740105e-05
Epoch 45: reducing lr to 5.7427277700792006e-05
Epoch 53: reducing lr to 4.7346237513677526e-05
Epoch 57: reducing lr to 4.171278732113658e-05
Epoch 65: reducing lr to 3.0044538047056394e-05
Epoch 68: reducing lr to 2.5741674398806673e-05
Epoch 71: reducing lr to 2.1584761799226466e-05
Epoch 74: reducing lr to 1.7639338454216098e-05
Epoch 80: reducing lr to 1.0627553498451921e-05
Epoch 86: reducing lr to 5.1468921643430235e-06
Epoch 92: reducing lr to 1.5417325408323652e-06
Epoch 98: reducing lr to 3.8595573620024084e-08
[I 2024-06-22 18:05:05,567] Trial 641 finished with value: 1.8685438632965088 and parameters: {'hidden_size': 169, 'n_layers': 2, 'rnn_dropout': 0.017525302466828055, 'bidirectional': True, 'fc_dropout': 0.3139556131976357, 'learning_rate_model': 0.0006999029946115468}. Best is trial 346 with value: 1.8583240509033203.
Epoch 23: reducing lr to 9.581093038816094e-05
Epoch 35: reducing lr to 9.13403549028166e-05
Epoch 45: reducing lr to 7.898214418624588e-05
Epoch 48: reducing lr to 7.410887868983498e-05
Epoch 51: reducing lr to 6.882592321076262e-05
Epoch 56: reducing lr to 5.933922956065652e-05
Epoch 60: reducing lr to 5.1376410668754256e-05
Epoch 66: reducing lr to 3.9332293358945624e-05
Epoch 72: reducing lr to 2.7841008847823827e-05
Epoch 75: reducing lr to 2.253094515568731e-05
Epoch 78: reducing lr to 1.762461353665035e-05
Epoch 81: reducing lr to 1.3199375251376891e-05
Epoch 84: reducing lr to 9.325031989070097e-06
Epoch 87: reducing lr to 6.062668613105457e-06
Epoch 96: reducing lr to 4.0907335837803065e-07
Epoch 99: reducing lr to 1.4838857699994885e-09
[I 2024-06-22 18:05:59,313] Trial 642 finished with value: 1.870690107345581 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.03881356125992321, 'bidirectional': True, 'fc_dropout': 0.29857663788101696, 'learning_rate_model': 0.0009626059505173458}. Best is trial 346 with value: 1.8583240509033203.
Epoch 30: reducing lr to 5.434896805117397e-05
Epoch 45: reducing lr to 4.5278797152764766e-05
Epoch 53: reducing lr to 3.7330355366973285e-05
Epoch 57: reducing lr to 3.288863605255161e-05
