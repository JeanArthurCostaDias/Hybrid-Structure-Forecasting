====================================================================================================
STL + ARIMA + ES + LSTM
Epoch 34: reducing lr to 1.945464739390692e-06
Epoch 42: reducing lr to 4.789127033506297e-07
Epoch 45: reducing lr to 1.6329873249592577e-07
Epoch 48: reducing lr to 1.2089751319672992e-08
[I 2024-06-24 15:33:16,698] Trial 0 finished with value: 1.0754108428955078 and parameters: {'hidden_size': 93, 'n_layers': 6, 'rnn_dropout': 9.149985387590931e-05, 'bidirectional': True, 'fc_dropout': 0.07387087581503825, 'learning_rate_model': 5.5595654267125665e-05}. Best is trial 0 with value: 1.0754108428955078.
Epoch 6: reducing lr to 0.0019420349762262438
Epoch 9: reducing lr to 0.00292992516187009
Epoch 12: reducing lr to 0.003253527628577598
Epoch 15: reducing lr to 0.0031892525359831295
Epoch 18: reducing lr to 0.0030268286671956213
Epoch 21: reducing lr to 0.0027764617396441233
Epoch 28: reducing lr to 0.0019469793678262488
Epoch 31: reducing lr to 0.0015401609003945368
Epoch 34: reducing lr to 0.0011388126007422093
Epoch 37: reducing lr to 0.0007681519038442077
Epoch 40: reducing lr to 0.0004514690790738931
Epoch 43: reducing lr to 0.00020866231546405462
Epoch 46: reducing lr to 5.4988408524748314e-05
Epoch 49: reducing lr to 1.0304035796201983e-07
[I 2024-06-24 15:33:19,375] Trial 1 finished with value: 1.107260823249817 and parameters: {'hidden_size': 79, 'n_layers': 3, 'rnn_dropout': 0.4310533872026856, 'bidirectional': False, 'fc_dropout': 0.16356179978521396, 'learning_rate_model': 0.032543911150884876}. Best is trial 0 with value: 1.0754108428955078.
Epoch 16: reducing lr to 0.001542470497102604
Epoch 19: reducing lr to 0.0014477531000695687
Epoch 22: reducing lr to 0.0013122035311693462
Epoch 25: reducing lr to 0.0011443387602976661
Epoch 28: reducing lr to 0.0009547064978940048
Epoch 31: reducing lr to 0.000755221983194722
Epoch 34: reducing lr to 0.0005584197797771345
Epoch 37: reducing lr to 0.0003766653237771558
Epoch 40: reducing lr to 0.00022137906056564488
Epoch 43: reducing lr to 0.00010231811992006661
Epoch 46: reducing lr to 2.6963711991483118e-05
Epoch 49: reducing lr to 5.052611286859854e-08
[I 2024-06-24 15:33:23,710] Trial 2 finished with value: 1.0177521705627441 and parameters: {'hidden_size': 21, 'n_layers': 5, 'rnn_dropout': 0.3338438418937016, 'bidirectional': True, 'fc_dropout': 0.15848119126790305, 'learning_rate_model': 0.015957993164212966}. Best is trial 2 with value: 1.0177521705627441.
[I 2024-06-24 15:33:26,776] Trial 3 finished with value: 1.1064438819885254 and parameters: {'hidden_size': 195, 'n_layers': 3, 'rnn_dropout': 0.5538580925354513, 'bidirectional': False, 'fc_dropout': 0.06803536909582233, 'learning_rate_model': 1.432910723846784e-05}. Best is trial 2 with value: 1.0177521705627441.
Epoch 28: reducing lr to 0.00035026767494734874
Epoch 31: reducing lr to 0.00027707976085453444
Epoch 34: reducing lr to 0.00020487594704614995
Epoch 37: reducing lr to 0.00013819292890930173
[I 2024-06-24 15:33:30,418] Trial 4 finished with value: 1.1070995330810547 and parameters: {'hidden_size': 47, 'n_layers': 7, 'rnn_dropout': 0.07867746706644008, 'bidirectional': False, 'fc_dropout': 0.42653222797841367, 'learning_rate_model': 0.005854751355295724}. Best is trial 2 with value: 1.0177521705627441.
Epoch 12: reducing lr to 0.0009829860871492667
Epoch 15: reducing lr to 0.000963566697187542
Epoch 18: reducing lr to 0.0009144936843024847
Epoch 21: reducing lr to 0.00083885049495203
Epoch 24: reducing lr to 0.0007413900456616357
Epoch 27: reducing lr to 0.000628236237061796
Epoch 30: reducing lr to 0.0005064987736920169
Epoch 33: reducing lr to 0.0003838269498813498
Epoch 36: reducing lr to 0.00026792873097625437
Epoch 39: reducing lr to 0.00016608637928437303
Epoch 42: reducing lr to 8.469897000136246e-05
Epoch 45: reducing lr to 2.8880491889576344e-05
Epoch 48: reducing lr to 2.1381547768199728e-06
[I 2024-06-24 15:33:33,582] Trial 5 finished with value: 1.0988516807556152 and parameters: {'hidden_size': 74, 'n_layers': 5, 'rnn_dropout': 0.6677005375178984, 'bidirectional': False, 'fc_dropout': 0.7910888711251958, 'learning_rate_model': 0.009832469717408681}. Best is trial 2 with value: 1.0177521705627441.
Epoch 17: reducing lr to 1.3451854401357507e-05
Epoch 20: reducing lr to 1.2483466976910361e-05
Epoch 25: reducing lr to 1.0154947628181115e-05
Epoch 28: reducing lr to 8.472136768203113e-06
Epoch 31: reducing lr to 6.701896285500768e-06
Epoch 34: reducing lr to 4.9554588334508234e-06
Epoch 37: reducing lr to 3.3425562158830798e-06
Epoch 40: reducing lr to 1.964534317998006e-06
Epoch 43: reducing lr to 9.079786381892356e-07
Epoch 46: reducing lr to 2.3927799409997105e-07
Epoch 49: reducing lr to 4.4837249933379374e-10
[I 2024-06-24 15:33:36,985] Trial 6 finished with value: 1.1068992614746094 and parameters: {'hidden_size': 67, 'n_layers': 6, 'rnn_dropout': 0.08258080526211363, 'bidirectional': False, 'fc_dropout': 0.2348913186989436, 'learning_rate_model': 0.00014161242322273667}. Best is trial 2 with value: 1.0177521705627441.
[I 2024-06-24 15:33:39,255] Trial 7 finished with value: 1.1112606525421143 and parameters: {'hidden_size': 40, 'n_layers': 1, 'rnn_dropout': 0.5430684263519128, 'bidirectional': False, 'fc_dropout': 0.39325852742427064, 'learning_rate_model': 1.634745612637677e-05}. Best is trial 2 with value: 1.0177521705627441.
Epoch 16: reducing lr to 0.0005792142290121482
Epoch 22: reducing lr to 0.0004927465115481645
Epoch 25: reducing lr to 0.0004297114881740523
Epoch 28: reducing lr to 0.00035850253807076895
Epoch 31: reducing lr to 0.00028359396147338995
Epoch 34: reducing lr to 0.0002096926215550373
Epoch 37: reducing lr to 0.00014144187231912027
Epoch 40: reducing lr to 8.313021359295028e-05
Epoch 43: reducing lr to 3.8421552343980914e-05
Epoch 46: reducing lr to 1.0125163289534128e-05
Epoch 49: reducing lr to 1.8973097744984236e-08
[I 2024-06-24 15:33:42,194] Trial 8 finished with value: 1.0191650390625 and parameters: {'hidden_size': 122, 'n_layers': 2, 'rnn_dropout': 0.47144442952262744, 'bidirectional': True, 'fc_dropout': 0.3312447902556547, 'learning_rate_model': 0.005992397731141758}. Best is trial 2 with value: 1.0177521705627441.
Epoch 23: reducing lr to 0.00017514179104609657
Epoch 27: reducing lr to 0.0001418014029422349
Epoch 30: reducing lr to 0.00011432361341325304
Epoch 33: reducing lr to 8.663492611436326e-05
Epoch 38: reducing lr to 4.470349683037547e-05
Epoch 41: reducing lr to 2.4650002320883956e-05
Epoch 44: reducing lr to 1.0020147307652356e-05
Epoch 47: reducing lr to 1.7331623375086997e-06
[I 2024-06-24 15:33:44,624] Trial 9 finished with value: 0.9829268455505371 and parameters: {'hidden_size': 92, 'n_layers': 1, 'rnn_dropout': 0.4287171247324093, 'bidirectional': True, 'fc_dropout': 0.7556758047926507, 'learning_rate_model': 0.002219321201267233}. Best is trial 9 with value: 0.9829268455505371.
Epoch 7: reducing lr to 0.0036527207546016966
Epoch 10: reducing lr to 0.004929936043323959
Epoch 18: reducing lr to 0.004770373837948047
Epoch 21: reducing lr to 0.004375787961970578
Epoch 24: reducing lr to 0.0038673943169295275
Epoch 27: reducing lr to 0.003277137678229426
Epoch 30: reducing lr to 0.002642105178469411
Epoch 33: reducing lr to 0.0020021986717272426
Epoch 36: reducing lr to 0.0013976260641522195
Epoch 39: reducing lr to 0.000866374620380237
Epoch 42: reducing lr to 0.00044182453912060065
Epoch 45: reducing lr to 0.00015065248158841886
Epoch 48: reducing lr to 1.1153491581087498e-05
[I 2024-06-24 15:33:47,209] Trial 10 finished with value: 1.172027826309204 and parameters: {'hidden_size': 183, 'n_layers': 1, 'rnn_dropout': 0.11142107780060684, 'bidirectional': True, 'fc_dropout': 0.13228335769354624, 'learning_rate_model': 0.051290191619112704}. Best is trial 9 with value: 0.9829268455505371.
Epoch 24: reducing lr to 1.8748973750140736e-05
Epoch 27: reducing lr to 1.5887433054279965e-05
Epoch 32: reducing lr to 1.073114657876732e-05
Epoch 42: reducing lr to 2.141947783777313e-06
Epoch 45: reducing lr to 7.30357235705244e-07
Epoch 48: reducing lr to 5.407168334524513e-08
[I 2024-06-24 15:33:52,145] Trial 11 finished with value: 1.0751559734344482 and parameters: {'hidden_size': 80, 'n_layers': 6, 'rnn_dropout': 0.5807983882803612, 'bidirectional': True, 'fc_dropout': 0.6007539472218698, 'learning_rate_model': 0.0002486528079375971}. Best is trial 9 with value: 0.9829268455505371.
[I 2024-06-24 15:33:57,518] Trial 12 finished with value: 1.1015129089355469 and parameters: {'hidden_size': 65, 'n_layers': 7, 'rnn_dropout': 0.34247295189703597, 'bidirectional': True, 'fc_dropout': 0.49735657616729745, 'learning_rate_model': 2.877291692654975e-05}. Best is trial 9 with value: 0.9829268455505371.
Epoch 16: reducing lr to 0.00019052700386256702
Epoch 23: reducing lr to 0.00015555628805180647
Epoch 26: reducing lr to 0.00013377036509754328
Epoch 29: reducing lr to 0.000109771924895703
Epoch 33: reducing lr to 7.694684085105582e-05
Epoch 36: reducing lr to 5.37124071882607e-05
Epoch 39: reducing lr to 3.329579175790887e-05
Epoch 42: reducing lr to 1.6979834706650605e-05
Epoch 45: reducing lr to 5.789751380965813e-06
Epoch 48: reducing lr to 4.286417495638368e-07
[I 2024-06-24 15:34:03,006] Trial 13 finished with value: 0.9794161319732666 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.4627116915097054, 'bidirectional': True, 'fc_dropout': 0.7227036164498031, 'learning_rate_model': 0.001971142158600765}. Best is trial 13 with value: 0.9794161319732666.
Epoch 8: reducing lr to 0.0035121568574605563
Epoch 11: reducing lr to 0.0042823259422302565
Epoch 16: reducing lr to 0.004162739125823755
Epoch 23: reducing lr to 0.0033986796276304007
Epoch 28: reducing lr to 0.0025765122249837035
Epoch 31: reducing lr to 0.0020381537955067664
Epoch 34: reducing lr to 0.0015070342481613998
Epoch 37: reducing lr to 0.0010165247786414811
Epoch 40: reducing lr to 0.000597446290730199
Epoch 43: reducing lr to 0.00027613081862638486
Epoch 46: reducing lr to 7.276826305282966e-05
Epoch 49: reducing lr to 1.363572446344552e-07
[I 2024-06-24 15:34:06,170] Trial 14 finished with value: 1.1026959419250488 and parameters: {'hidden_size': 16, 'n_layers': 5, 'rnn_dropout': 0.26131592141767696, 'bidirectional': False, 'fc_dropout': 0.28581580800199985, 'learning_rate_model': 0.04306660168805714}. Best is trial 13 with value: 0.9794161319732666.
[I 2024-06-24 15:34:08,519] Trial 15 finished with value: 1.1016825437545776 and parameters: {'hidden_size': 131, 'n_layers': 1, 'rnn_dropout': 0.7435497869950091, 'bidirectional': False, 'fc_dropout': 0.13787240667626285, 'learning_rate_model': 3.5362503098265445e-05}. Best is trial 13 with value: 0.9794161319732666.
Epoch 12: reducing lr to 0.0007014291139487203
Epoch 15: reducing lr to 0.000687572025153313
Epoch 22: reducing lr to 0.000576928458150632
Epoch 25: reducing lr to 0.0005031243864983442
Epoch 28: reducing lr to 0.0004197499356868403
Epoch 31: reducing lr to 0.0003320438056874602
Epoch 34: reducing lr to 0.0002455169909964678
Epoch 37: reducing lr to 0.0001656061268878862
Epoch 40: reducing lr to 9.733237035656934e-05
Epoch 43: reducing lr to 4.498557865771921e-05
Epoch 46: reducing lr to 1.1854969458435782e-05
Epoch 49: reducing lr to 2.221450537305547e-08
[I 2024-06-24 15:34:15,157] Trial 16 finished with value: 1.1381208896636963 and parameters: {'hidden_size': 188, 'n_layers': 5, 'rnn_dropout': 0.052800138177649995, 'bidirectional': True, 'fc_dropout': 0.7384196284371867, 'learning_rate_model': 0.007016152732955534}. Best is trial 13 with value: 0.9794161319732666.
Epoch 13: reducing lr to 0.0001425126812570336
Epoch 23: reducing lr to 0.00011284800401774833
Epoch 26: reducing lr to 9.704344894727592e-05
Epoch 29: reducing lr to 7.963382757976787e-05
Epoch 32: reducing lr to 6.171305081057547e-05
Epoch 35: reducing lr to 4.440715351419959e-05
Epoch 38: reducing lr to 2.8803521762512243e-05
Epoch 41: reducing lr to 1.58825803043917e-05
Epoch 44: reducing lr to 6.456218226835275e-06
Epoch 47: reducing lr to 1.1167175421605489e-06
[I 2024-06-24 15:34:17,462] Trial 17 finished with value: 1.092000961303711 and parameters: {'hidden_size': 38, 'n_layers': 1, 'rnn_dropout': 0.020968789502175424, 'bidirectional': False, 'fc_dropout': 0.6880223589463105, 'learning_rate_model': 0.0014299612122349629}. Best is trial 13 with value: 0.9794161319732666.
Epoch 5: reducing lr to 8.359503352487266e-05
Epoch 21: reducing lr to 0.00014967277978094018
Epoch 29: reducing lr to 9.769992045771066e-05
Epoch 32: reducing lr to 7.571355463677006e-05
Epoch 35: reducing lr to 5.448156264678812e-05
Epoch 38: reducing lr to 3.533801991723361e-05
Epoch 41: reducing lr to 1.9485774821609986e-05
Epoch 44: reducing lr to 7.920905303560782e-06
Epoch 47: reducing lr to 1.3700611707195483e-06
[I 2024-06-24 15:34:20,899] Trial 18 finished with value: 1.093764066696167 and parameters: {'hidden_size': 118, 'n_layers': 6, 'rnn_dropout': 0.09933865209592892, 'bidirectional': False, 'fc_dropout': 0.7756765986557397, 'learning_rate_model': 0.0017543687266950109}. Best is trial 13 with value: 0.9794161319732666.
Epoch 18: reducing lr to 0.0009057423276735144
Epoch 23: reducing lr to 0.000768521807564351
Epoch 26: reducing lr to 0.000660889020115142
Epoch 29: reducing lr to 0.0005423253485745807
Epoch 32: reducing lr to 0.0004202805869016063
Epoch 35: reducing lr to 0.0003024233010107363
Epoch 38: reducing lr to 0.0001961588492576575
Epoch 41: reducing lr to 0.0001081641578915063
Epoch 44: reducing lr to 4.39683851292309e-05
Epoch 47: reducing lr to 7.60511266025641e-06
[I 2024-06-24 15:34:25,754] Trial 19 finished with value: 1.0086103677749634 and parameters: {'hidden_size': 19, 'n_layers': 6, 'rnn_dropout': 0.18637941907281635, 'bidirectional': True, 'fc_dropout': 0.6908334836475429, 'learning_rate_model': 0.009738376723091037}. Best is trial 13 with value: 0.9794161319732666.
[I 2024-06-24 15:34:28,248] Trial 20 finished with value: 1.0673646926879883 and parameters: {'hidden_size': 118, 'n_layers': 1, 'rnn_dropout': 0.04793415160976933, 'bidirectional': True, 'fc_dropout': 0.08599530328487433, 'learning_rate_model': 7.995347689531267e-05}. Best is trial 13 with value: 0.9794161319732666.
[I 2024-06-24 15:34:31,502] Trial 21 finished with value: 1.1010029315948486 and parameters: {'hidden_size': 147, 'n_layers': 4, 'rnn_dropout': 0.010044784127292684, 'bidirectional': False, 'fc_dropout': 0.4544803695359537, 'learning_rate_model': 6.503886274016054e-05}. Best is trial 13 with value: 0.9794161319732666.
Epoch 17: reducing lr to 8.651110121155873e-06
Epoch 25: reducing lr to 6.530814829299657e-06
Epoch 28: reducing lr to 5.448571323803673e-06
Epoch 31: reducing lr to 4.3101003814449295e-06
Epoch 34: reducing lr to 3.1869375619105275e-06
Epoch 37: reducing lr to 2.1496532037129698e-06
Epoch 40: reducing lr to 1.2634245223525096e-06
Epoch 43: reducing lr to 5.8393608436912e-07
Epoch 46: reducing lr to 1.5388363676603882e-07
Epoch 49: reducing lr to 2.883557724683242e-10
[I 2024-06-24 15:34:34,934] Trial 22 finished with value: 1.1069896221160889 and parameters: {'hidden_size': 62, 'n_layers': 6, 'rnn_dropout': 0.15634358488745503, 'bidirectional': False, 'fc_dropout': 0.6774630411920283, 'learning_rate_model': 9.107329229641331e-05}. Best is trial 13 with value: 0.9794161319732666.
[I 2024-06-24 15:34:39,407] Trial 23 finished with value: 1.019514799118042 and parameters: {'hidden_size': 107, 'n_layers': 5, 'rnn_dropout': 0.663184719640143, 'bidirectional': True, 'fc_dropout': 0.056017714975377865, 'learning_rate_model': 0.0008818210135856344}. Best is trial 13 with value: 0.9794161319732666.
Epoch 22: reducing lr to 0.0001314667127817124
Epoch 25: reducing lr to 0.00011464871992149727
Epoch 28: reducing lr to 9.564989117017374e-05
Epoch 31: reducing lr to 7.566398747808456e-05
Epoch 34: reducing lr to 5.5946818504722695e-05
Epoch 37: reducing lr to 3.773724941260761e-05
Epoch 40: reducing lr to 2.2179468870452936e-05
Epoch 43: reducing lr to 1.0251021708430549e-05
Epoch 46: reducing lr to 2.7014335015195823e-06
Epoch 49: reducing lr to 5.0620973124186616e-09
[I 2024-06-24 15:34:43,380] Trial 24 finished with value: 0.9763447046279907 and parameters: {'hidden_size': 128, 'n_layers': 4, 'rnn_dropout': 0.2538899274577286, 'bidirectional': True, 'fc_dropout': 0.3041129380988403, 'learning_rate_model': 0.0015987953500037988}. Best is trial 24 with value: 0.9763447046279907.
Epoch 9: reducing lr to 6.238537953422173e-06
Epoch 12: reducing lr to 6.927567931610175e-06
Epoch 15: reducing lr to 6.7907103047230825e-06
Epoch 18: reducing lr to 6.444869570236275e-06
Epoch 21: reducing lr to 5.911776233881299e-06
[I 2024-06-24 15:34:46,852] Trial 25 finished with value: 1.107254981994629 and parameters: {'hidden_size': 153, 'n_layers': 5, 'rnn_dropout': 0.2119356461302475, 'bidirectional': False, 'fc_dropout': 0.5037740056172516, 'learning_rate_model': 6.929406508731719e-05}. Best is trial 24 with value: 0.9763447046279907.
Epoch 23: reducing lr to 9.90491746636851e-05
Epoch 27: reducing lr to 8.01939493920349e-05
Epoch 33: reducing lr to 4.8995261938474235e-05
Epoch 38: reducing lr to 2.528148444287616e-05
Epoch 41: reducing lr to 1.3940489992470559e-05
Epoch 44: reducing lr to 5.666764710487039e-06
Epoch 47: reducing lr to 9.80167543469042e-07
[I 2024-06-24 15:34:49,336] Trial 26 finished with value: 0.9835362434387207 and parameters: {'hidden_size': 155, 'n_layers': 1, 'rnn_dropout': 0.20825207886283278, 'bidirectional': True, 'fc_dropout': 0.5115687047039521, 'learning_rate_model': 0.001255108400948586}. Best is trial 24 with value: 0.9763447046279907.
Epoch 5: reducing lr to 0.0025465246007663816
Epoch 8: reducing lr to 0.0043583447146142784
Epoch 14: reducing lr to 0.005290934748605682
Epoch 17: reducing lr to 0.0050765548211483755
Epoch 20: reducing lr to 0.004711098007415653
Epoch 23: reducing lr to 0.0042175272896155195
Epoch 26: reducing lr to 0.003626855412960392
Epoch 29: reducing lr to 0.002976196556754211
Epoch 32: reducing lr to 0.0023064340232202586
Epoch 35: reducing lr to 0.0016596517008030235
Epoch 38: reducing lr to 0.0010764890360960573
Epoch 41: reducing lr to 0.0005935879544023373
Epoch 44: reducing lr to 0.00024129160986408187
Epoch 47: reducing lr to 4.1735666925165477e-05
[I 2024-06-24 15:34:51,875] Trial 27 finished with value: 1.142704725265503 and parameters: {'hidden_size': 187, 'n_layers': 2, 'rnn_dropout': 0.05276887254721903, 'bidirectional': False, 'fc_dropout': 0.7262526820028192, 'learning_rate_model': 0.05344268592242116}. Best is trial 24 with value: 0.9763447046279907.
Epoch 8: reducing lr to 0.0037485102196038816
Epoch 11: reducing lr to 0.004570508439572225
Epoch 14: reducing lr to 0.00455060906722266
Epoch 18: reducing lr to 0.004275072576821406
Epoch 21: reducing lr to 0.003921456001916255
Epoch 24: reducing lr to 0.0034658481598524226
Epoch 27: reducing lr to 0.0029368770445657834
Epoch 30: reducing lr to 0.0023677790834126704
Epoch 33: reducing lr to 0.0017943131766233289
Epoch 36: reducing lr to 0.0012525124995398866
Epoch 39: reducing lr to 0.0007764201521017033
Epoch 42: reducing lr to 0.0003959505135500474
Epoch 45: reducing lr to 0.0001350104445788633
Epoch 48: reducing lr to 9.995440108866855e-06
[I 2024-06-24 15:34:54,443] Trial 28 finished with value: 1.1098073720932007 and parameters: {'hidden_size': 18, 'n_layers': 2, 'rnn_dropout': 0.4934226856013261, 'bidirectional': False, 'fc_dropout': 0.44532255055612513, 'learning_rate_model': 0.04596480257092418}. Best is trial 24 with value: 0.9763447046279907.
Epoch 8: reducing lr to 0.003858042812473094
Epoch 18: reducing lr to 0.004399991479694985
Epoch 21: reducing lr to 0.004036042122414497
Epoch 24: reducing lr to 0.003567121282559753
Epoch 27: reducing lr to 0.0030226934726355335
Epoch 30: reducing lr to 0.002436966298373787
Epoch 33: reducing lr to 0.0018467435457942883
Epoch 36: reducing lr to 0.0012891112904297238
Epoch 39: reducing lr to 0.0007991073818098812
Epoch 42: reducing lr to 0.00040752030630937313
Epoch 45: reducing lr to 0.00013895549026176067
Epoch 48: reducing lr to 1.0287509866678535e-05
[I 2024-06-24 15:34:58,310] Trial 29 finished with value: 1.1301608085632324 and parameters: {'hidden_size': 134, 'n_layers': 3, 'rnn_dropout': 0.3887925336775278, 'bidirectional': True, 'fc_dropout': 0.740945141365163, 'learning_rate_model': 0.047307907887800385}. Best is trial 24 with value: 0.9763447046279907.
[I 2024-06-24 15:35:02,017] Trial 30 finished with value: 1.1095918416976929 and parameters: {'hidden_size': 89, 'n_layers': 7, 'rnn_dropout': 0.13916453334437148, 'bidirectional': False, 'fc_dropout': 0.40452973254151736, 'learning_rate_model': 1.2192681272217677e-05}. Best is trial 24 with value: 0.9763447046279907.
Epoch 3: reducing lr to 0.0004351627280700692
Epoch 6: reducing lr to 0.001032167238589228
Epoch 14: reducing lr to 0.0017124066167024544
Epoch 19: reducing lr to 0.0015692025216897647
Epoch 22: reducing lr to 0.0014222819415701513
Epoch 28: reducing lr to 0.0010347951207266537
Epoch 31: reducing lr to 0.0008185762064040862
Epoch 34: reducing lr to 0.0006052646176655503
Epoch 37: reducing lr to 0.0004082631049975319
Epoch 40: reducing lr to 0.00023995015453410397
Epoch 43: reducing lr to 0.00011090140424179287
Epoch 46: reducing lr to 2.9225649628461524e-05
Epoch 49: reducing lr to 5.476465822842891e-08
[I 2024-06-24 15:35:06,389] Trial 31 finished with value: 1.1970106363296509 and parameters: {'hidden_size': 191, 'n_layers': 6, 'rnn_dropout': 0.012015184593697903, 'bidirectional': False, 'fc_dropout': 0.10479747584873352, 'learning_rate_model': 0.017296680706943544}. Best is trial 24 with value: 0.9763447046279907.
Epoch 27: reducing lr to 4.4155156553781704e-05
Epoch 42: reducing lr to 5.953009488668425e-06
Epoch 47: reducing lr to 5.396847475267223e-07
[I 2024-06-24 15:35:11,851] Trial 32 finished with value: 1.0446336269378662 and parameters: {'hidden_size': 79, 'n_layers': 7, 'rnn_dropout': 0.46561134395766407, 'bidirectional': True, 'fc_dropout': 0.7243138549669195, 'learning_rate_model': 0.0006910684453876749}. Best is trial 24 with value: 0.9763447046279907.
Epoch 4: reducing lr to 8.491800923926532e-05
Epoch 22: reducing lr to 0.0001945129678049095
Epoch 29: reducing lr to 0.0001317343296778956
Epoch 33: reducing lr to 9.234183066368317e-05
Epoch 36: reducing lr to 6.44588127889218e-05
Epoch 39: reducing lr to 3.9957382659458343e-05
Epoch 42: reducing lr to 2.0377042174010793e-05
Epoch 45: reducing lr to 6.948124649338791e-06
Epoch 48: reducing lr to 5.144014155204247e-07
[I 2024-06-24 15:35:15,304] Trial 33 finished with value: 1.0939189195632935 and parameters: {'hidden_size': 117, 'n_layers': 6, 'rnn_dropout': 0.22857508138731808, 'bidirectional': False, 'fc_dropout': 0.012426620440668579, 'learning_rate_model': 0.002365514599564762}. Best is trial 24 with value: 0.9763447046279907.
Epoch 5: reducing lr to 0.0006757351876047445
Epoch 18: reducing lr to 0.0013189703360458962
Epoch 24: reducing lr to 0.0010693036971745362
Epoch 27: reducing lr to 0.0009061024421897748
Epoch 30: reducing lr to 0.000730521018581931
Epoch 33: reducing lr to 0.0005535919708998475
Epoch 36: reducing lr to 0.0003864324646502543
Epoch 39: reducing lr to 0.00023954567566471761
Epoch 42: reducing lr to 0.00012216096277433281
Epoch 45: reducing lr to 4.1654210134671116e-05
Epoch 48: reducing lr to 3.0838515048372053e-06
[I 2024-06-24 15:35:20,305] Trial 34 finished with value: 1.08717942237854 and parameters: {'hidden_size': 96, 'n_layers': 6, 'rnn_dropout': 0.2521958424762984, 'bidirectional': True, 'fc_dropout': 0.14720816130197206, 'learning_rate_model': 0.01418132908946585}. Best is trial 24 with value: 0.9763447046279907.
Epoch 5: reducing lr to 0.0038467697781144504
Epoch 8: reducing lr to 0.006583697925296865
Epoch 11: reducing lr to 0.008027414937751798
Epoch 14: reducing lr to 0.007992464664502607
Epoch 17: reducing lr to 0.00766862321183062
Epoch 20: reducing lr to 0.007116565624855041
Epoch 23: reducing lr to 0.0063709796917663115
Epoch 26: reducing lr to 0.005478713140241431
Epoch 29: reducing lr to 0.004495830499656224
Epoch 32: reducing lr to 0.0034840899212473624
Epoch 35: reducing lr to 0.0025070631569488666
Epoch 38: reducing lr to 0.0016261399906679185
Epoch 41: reducing lr to 0.0008966715667936206
Epoch 44: reducing lr to 0.0003644941314363867
Epoch 47: reducing lr to 6.30457299131762e-05
[I 2024-06-24 15:35:22,637] Trial 35 finished with value: 1.1838713884353638 and parameters: {'hidden_size': 129, 'n_layers': 1, 'rnn_dropout': 0.3361549440009058, 'bidirectional': False, 'fc_dropout': 0.0003216199130859465, 'learning_rate_model': 0.08073030553318121}. Best is trial 24 with value: 0.9763447046279907.
Epoch 28: reducing lr to 8.302267657750138e-06
Epoch 32: reducing lr to 5.989049269862941e-06
Epoch 35: reducing lr to 4.309568670446168e-06
Epoch 38: reducing lr to 2.795287361675754e-06
Epoch 42: reducing lr to 1.1954203324272948e-06
Epoch 45: reducing lr to 4.0761212579970987e-07
Epoch 48: reducing lr to 3.017738815532097e-08
[I 2024-06-24 15:35:28,102] Trial 36 finished with value: 1.075866460800171 and parameters: {'hidden_size': 85, 'n_layers': 7, 'rnn_dropout': 0.48377288077924246, 'bidirectional': True, 'fc_dropout': 0.5024609586458804, 'learning_rate_model': 0.00013877304786559896}. Best is trial 24 with value: 0.9763447046279907.
Epoch 14: reducing lr to 1.9336065836450975e-05
Epoch 24: reducing lr to 1.4726790899432328e-05
Epoch 27: reducing lr to 1.247913126537682e-05
Epoch 30: reducing lr to 1.0060968008175147e-05
Epoch 33: reducing lr to 7.624244843245849e-06
Epoch 42: reducing lr to 1.6824396657419286e-06
Epoch 45: reducing lr to 5.736750413892836e-07
Epoch 48: reducing lr to 4.247178457966478e-08
[I 2024-06-24 15:35:33,140] Trial 37 finished with value: 1.0747745037078857 and parameters: {'hidden_size': 124, 'n_layers': 6, 'rnn_dropout': 0.6866510691432339, 'bidirectional': True, 'fc_dropout': 0.6915835440436799, 'learning_rate_model': 0.00019530977843660444}. Best is trial 24 with value: 0.9763447046279907.
Epoch 16: reducing lr to 0.0002971091747113483
Epoch 22: reducing lr to 0.0002527553745315543
Epoch 27: reducing lr to 0.00019639835112820504
Epoch 30: reducing lr to 0.00015834095222970267
Epoch 33: reducing lr to 0.00011999145485116344
Epoch 36: reducing lr to 8.375951255169761e-05
Epoch 39: reducing lr to 5.1921696190052743e-05
Epoch 42: reducing lr to 2.6478475880862975e-05
Epoch 45: reducing lr to 9.028579779816722e-06
Epoch 48: reducing lr to 6.684270149526737e-07
[I 2024-06-24 15:35:37,670] Trial 38 finished with value: 1.0225369930267334 and parameters: {'hidden_size': 140, 'n_layers': 4, 'rnn_dropout': 0.3056822016252138, 'bidirectional': True, 'fc_dropout': 0.2539071567666215, 'learning_rate_model': 0.003073813202894127}. Best is trial 24 with value: 0.9763447046279907.
Epoch 5: reducing lr to 0.0007389495944000798
Epoch 8: reducing lr to 0.0012647029045589069
Epoch 11: reducing lr to 0.0015420353581025632
Epoch 14: reducing lr to 0.0015353215458051399
Epoch 21: reducing lr to 0.0013230527610106212
Epoch 24: reducing lr to 0.001169336076930503
Epoch 27: reducing lr to 0.000990867494283429
Epoch 30: reducing lr to 0.0007988605895977184
Epoch 44: reducing lr to 7.00179127221253e-05
Epoch 47: reducing lr to 1.2110840844453685e-05
[I 2024-06-24 15:35:41,385] Trial 39 finished with value: 1.0994369983673096 and parameters: {'hidden_size': 95, 'n_layers': 7, 'rnn_dropout': 0.5422407131474489, 'bidirectional': False, 'fc_dropout': 0.2746769918195384, 'learning_rate_model': 0.015507979414037055}. Best is trial 24 with value: 0.9763447046279907.
Epoch 11: reducing lr to 0.00012835054035425725
Epoch 22: reducing lr to 0.00010614059372810952
[I 2024-06-24 15:35:49,809] Trial 40 finished with value: 0.9992977380752563 and parameters: {'hidden_size': 178, 'n_layers': 7, 'rnn_dropout': 0.5301758499002098, 'bidirectional': True, 'fc_dropout': 0.6839183541579219, 'learning_rate_model': 0.001290798895846048}. Best is trial 24 with value: 0.9763447046279907.
Epoch 6: reducing lr to 4.351378044650393e-05
Epoch 16: reducing lr to 7.048198158742115e-05
Epoch 19: reducing lr to 6.615394429514821e-05
Epoch 22: reducing lr to 5.996011288161107e-05
Epoch 25: reducing lr to 5.228966361727911e-05
Epoch 28: reducing lr to 4.362456587166771e-05
Epoch 31: reducing lr to 3.450927717187015e-05
Epoch 34: reducing lr to 2.5516554585799428e-05
Epoch 37: reducing lr to 1.7211427035362978e-05
Epoch 40: reducing lr to 1.0115742829401076e-05
Epoch 43: reducing lr to 4.675346373115088e-06
Epoch 46: reducing lr to 1.2320857064573409e-06
Epoch 49: reducing lr to 2.308751164824697e-09
[I 2024-06-24 15:35:53,435] Trial 41 finished with value: 1.0929067134857178 and parameters: {'hidden_size': 164, 'n_layers': 5, 'rnn_dropout': 0.5865140202300089, 'bidirectional': False, 'fc_dropout': 0.4550863925637725, 'learning_rate_model': 0.000729188002289183}. Best is trial 24 with value: 0.9763447046279907.
Epoch 5: reducing lr to 0.0008423535105367442
Epoch 10: reducing lr to 0.001699189260287838
Epoch 16: reducing lr to 0.001708729755405658
Epoch 22: reducing lr to 0.0014536428560994953
Epoch 25: reducing lr to 0.0012676843373391118
Epoch 28: reducing lr to 0.0010576120604542754
Epoch 31: reducing lr to 0.0008366255802268694
Epoch 34: reducing lr to 0.0006186105312903061
Epoch 37: reducing lr to 0.00041726519098842733
Epoch 40: reducing lr to 0.0002452409875733955
Epoch 43: reducing lr to 0.00011334674883765536
Epoch 46: reducing lr to 2.987006693650311e-05
Epoch 49: reducing lr to 5.5972203453005795e-08
[I 2024-06-24 15:35:55,719] Trial 42 finished with value: 1.109168529510498 and parameters: {'hidden_size': 79, 'n_layers': 1, 'rnn_dropout': 0.3023393434624799, 'bidirectional': False, 'fc_dropout': 0.14529028106461103, 'learning_rate_model': 0.01767806762428918}. Best is trial 24 with value: 0.9763447046279907.
[I 2024-06-24 15:35:59,381] Trial 43 finished with value: 1.1062123775482178 and parameters: {'hidden_size': 177, 'n_layers': 5, 'rnn_dropout': 0.4555955301963006, 'bidirectional': False, 'fc_dropout': 0.276137640924172, 'learning_rate_model': 7.946206212265512e-05}. Best is trial 24 with value: 0.9763447046279907.
[I 2024-06-24 15:36:02,855] Trial 44 finished with value: 1.0668089389801025 and parameters: {'hidden_size': 125, 'n_layers': 3, 'rnn_dropout': 0.7330444427746806, 'bidirectional': True, 'fc_dropout': 0.08871304059522336, 'learning_rate_model': 5.91358615023068e-05}. Best is trial 24 with value: 0.9763447046279907.
Epoch 5: reducing lr to 0.00013957250330996637
Epoch 8: reducing lr to 0.00023887657787535424
Epoch 11: reducing lr to 0.0002912590205798642
Epoch 15: reducing lr to 0.0002870512435197667
Epoch 18: reducing lr to 0.0002724321523732661
Epoch 22: reducing lr to 0.00024085917587637998
Epoch 29: reducing lr to 0.0001631223996986613
Epoch 33: reducing lr to 0.0001143439302971223
Epoch 36: reducing lr to 7.981728262910042e-05
Epoch 39: reducing lr to 4.947794672068842e-05
Epoch 42: reducing lr to 2.5232238447737065e-05
Epoch 45: reducing lr to 8.603640136757357e-06
Epoch 48: reducing lr to 6.36966791520881e-07
[I 2024-06-24 15:36:06,326] Trial 45 finished with value: 1.0935391187667847 and parameters: {'hidden_size': 108, 'n_layers': 6, 'rnn_dropout': 0.1665555507270332, 'bidirectional': False, 'fc_dropout': 0.3326789746140219, 'learning_rate_model': 0.0029291409380281584}. Best is trial 24 with value: 0.9763447046279907.
Epoch 38: reducing lr to 3.021077103497643e-05
Epoch 42: reducing lr to 1.2919805830576935e-05
Epoch 45: reducing lr to 4.405370543453788e-06
Epoch 48: reducing lr to 3.261497105784333e-07
[I 2024-06-24 15:36:08,863] Trial 46 finished with value: 0.9815720915794373 and parameters: {'hidden_size': 59, 'n_layers': 1, 'rnn_dropout': 0.41268561357482386, 'bidirectional': True, 'fc_dropout': 0.4974449853923324, 'learning_rate_model': 0.0014998246092237527}. Best is trial 24 with value: 0.9763447046279907.
[I 2024-06-24 15:36:11,409] Trial 47 finished with value: 1.104675054550171 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.6012222537081952, 'bidirectional': False, 'fc_dropout': 0.6282368225772952, 'learning_rate_model': 1.2283488171661822e-05}. Best is trial 24 with value: 0.9763447046279907.
Epoch 8: reducing lr to 0.0016483950080788006
Epoch 16: reducing lr to 0.00195373916183903
Epoch 19: reducing lr to 0.0018337672802124423
Epoch 22: reducing lr to 0.0016620761511903833
Epoch 25: reducing lr to 0.0014494536230051191
Epoch 28: reducing lr to 0.001209259740462727
Epoch 35: reducing lr to 0.0006277065624432405
Epoch 40: reducing lr to 0.0002804053244782834
Epoch 43: reducing lr to 0.0001295991840551094
Epoch 46: reducing lr to 3.415304225608505e-05
Epoch 49: reducing lr to 6.399788235328385e-08
[I 2024-06-24 15:36:15,101] Trial 48 finished with value: 1.1031651496887207 and parameters: {'hidden_size': 76, 'n_layers': 7, 'rnn_dropout': 0.6757676860816557, 'bidirectional': False, 'fc_dropout': 0.7598447930829013, 'learning_rate_model': 0.020212870358199457}. Best is trial 24 with value: 0.9763447046279907.
[I 2024-06-24 15:36:17,671] Trial 49 finished with value: 1.0999655723571777 and parameters: {'hidden_size': 174, 'n_layers': 1, 'rnn_dropout': 0.5210434658727928, 'bidirectional': True, 'fc_dropout': 0.6396922093888224, 'learning_rate_model': 1.3749435862460687e-05}. Best is trial 24 with value: 0.9763447046279907.
Epoch 8: reducing lr to 0.0012525296409112753
Epoch 14: reducing lr to 0.0015205434711335285
Epoch 19: reducing lr to 0.0013933843901143
Epoch 22: reducing lr to 0.001262925230066073
Epoch 25: reducing lr to 0.001101364428454615
Epoch 28: reducing lr to 0.0009188535885312724
Epoch 31: reducing lr to 0.0007268604863661651
Epoch 34: reducing lr to 0.0005374489643538907
Epoch 37: reducing lr to 0.0003625200888350491
Epoch 40: reducing lr to 0.00021306542343132624
Epoch 43: reducing lr to 9.847568008358124e-05
Epoch 46: reducing lr to 2.5951120661848493e-05
Epoch 49: reducing lr to 4.86286625536736e-08
[I 2024-06-24 15:36:21,579] Trial 50 finished with value: 1.1811827421188354 and parameters: {'hidden_size': 158, 'n_layers': 6, 'rnn_dropout': 0.20775871463868248, 'bidirectional': False, 'fc_dropout': 0.27623796927286526, 'learning_rate_model': 0.015358709003280025}. Best is trial 24 with value: 0.9763447046279907.
Epoch 9: reducing lr to 0.00013201530926241513
Epoch 40: reducing lr to 2.0342099816063634e-05
Epoch 47: reducing lr to 1.1451347571128776e-06
[I 2024-06-24 15:36:26,588] Trial 51 finished with value: 1.0250470638275146 and parameters: {'hidden_size': 98, 'n_layers': 6, 'rnn_dropout': 0.7923774268990726, 'bidirectional': True, 'fc_dropout': 0.7210467490794191, 'learning_rate_model': 0.0014663495679358655}. Best is trial 24 with value: 0.9763447046279907.
[I 2024-06-24 15:36:32,163] Trial 52 finished with value: 1.053260087966919 and parameters: {'hidden_size': 196, 'n_layers': 5, 'rnn_dropout': 0.7951304196883832, 'bidirectional': True, 'fc_dropout': 0.10834232245773592, 'learning_rate_model': 0.000264740682379336}. Best is trial 24 with value: 0.9763447046279907.
Epoch 9: reducing lr to 0.0005310038280021741
Epoch 12: reducing lr to 0.0005896517930795333
Epoch 15: reducing lr to 0.0005780029220923054
Epoch 18: reducing lr to 0.0005485660964670257
Epoch 22: reducing lr to 0.0004849911317653131
Epoch 25: reducing lr to 0.00042294822205295207
Epoch 28: reducing lr to 0.0003528600357481864
Epoch 31: reducing lr to 0.0002791304516893446
Epoch 34: reducing lr to 0.00020639225132469
Epoch 37: reducing lr to 0.0001392157065090654
Epoch 40: reducing lr to 8.182182000165466e-05
Epoch 43: reducing lr to 3.781683222259828e-05
Epoch 46: reducing lr to 9.965802472494505e-06
Epoch 49: reducing lr to 1.8674478525528006e-08
[I 2024-06-24 15:36:34,690] Trial 53 finished with value: 1.0933125019073486 and parameters: {'hidden_size': 20, 'n_layers': 2, 'rnn_dropout': 0.5965097541659968, 'bidirectional': False, 'fc_dropout': 0.6898770022704354, 'learning_rate_model': 0.005898082867158478}. Best is trial 24 with value: 0.9763447046279907.
Epoch 34: reducing lr to 3.4270001652533764e-06
Epoch 37: reducing lr to 2.3115802369047543e-06
Epoch 40: reducing lr to 1.358594563833119e-06
Epoch 43: reducing lr to 6.279222666762007e-07
Epoch 46: reducing lr to 1.6547523708335842e-07
Epoch 49: reducing lr to 3.100767620017051e-10
[I 2024-06-24 15:36:37,234] Trial 54 finished with value: 1.0911762714385986 and parameters: {'hidden_size': 143, 'n_layers': 2, 'rnn_dropout': 0.3535234245987387, 'bidirectional': False, 'fc_dropout': 0.1631249801864975, 'learning_rate_model': 9.793357468944974e-05}. Best is trial 24 with value: 0.9763447046279907.
Epoch 3: reducing lr to 0.00039364605302367306
Epoch 6: reducing lr to 0.0009336933825489996
Epoch 17: reducing lr to 0.001486270242790886
Epoch 20: reducing lr to 0.001379274926791669
Epoch 23: reducing lr to 0.00123477151918081
Epoch 26: reducing lr to 0.001061839665895441
Epoch 29: reducing lr to 0.0008713453384907695
Epoch 32: reducing lr to 0.0006752580000499795
Epoch 35: reducing lr to 0.00048589861100777466
Epoch 38: reducing lr to 0.00031516524048454694
Epoch 41: reducing lr to 0.00017378559755368422
Epoch 44: reducing lr to 7.06432910134452e-05
Epoch 47: reducing lr to 1.221901112059172e-05
[I 2024-06-24 15:36:40,677] Trial 55 finished with value: 1.1111035346984863 and parameters: {'hidden_size': 64, 'n_layers': 6, 'rnn_dropout': 0.36558026194855425, 'bidirectional': False, 'fc_dropout': 0.16956813171494078, 'learning_rate_model': 0.015646491878786785}. Best is trial 24 with value: 0.9763447046279907.
Epoch 38: reducing lr to 2.231964318069365e-05
Epoch 41: reducing lr to 1.2307297979239078e-05
Epoch 44: reducing lr to 5.002877367141982e-06
Epoch 47: reducing lr to 8.65336443236055e-07
[I 2024-06-24 15:36:43,242] Trial 56 finished with value: 0.9839743971824646 and parameters: {'hidden_size': 71, 'n_layers': 1, 'rnn_dropout': 0.47474595957169685, 'bidirectional': True, 'fc_dropout': 0.5998866485809148, 'learning_rate_model': 0.001108066724703625}. Best is trial 24 with value: 0.9763447046279907.
Epoch 7: reducing lr to 0.004393169059587551
Epoch 10: reducing lr to 0.00592929050598557
Epoch 13: reducing lr to 0.006147874865754008
Epoch 16: reducing lr to 0.005962581892911421
Epoch 19: reducing lr to 0.005596441835416816
Epoch 22: reducing lr to 0.005072460724183507
Epoch 25: reducing lr to 0.004423561801878464
Epoch 28: reducing lr to 0.003690518352267069
Epoch 31: reducing lr to 0.0029193899854707966
Epoch 34: reducing lr to 0.0021586303749712766
Epoch 37: reducing lr to 0.0014560394143421287
Epoch 40: reducing lr to 0.0008557640359915748
Epoch 43: reducing lr to 0.0003955214510086978
Epoch 46: reducing lr to 0.00010423106386027864
Epoch 49: reducing lr to 1.9531400197031478e-07
[I 2024-06-24 15:36:46,944] Trial 57 finished with value: 1.1072465181350708 and parameters: {'hidden_size': 116, 'n_layers': 7, 'rnn_dropout': 0.6431687131989174, 'bidirectional': False, 'fc_dropout': 0.3720011852508049, 'learning_rate_model': 0.061687300513606574}. Best is trial 24 with value: 0.9763447046279907.
Epoch 12: reducing lr to 0.0016283122981936714
Epoch 16: reducing lr to 0.0015743146643317119
Epoch 19: reducing lr to 0.001477641834999458
Epoch 22: reducing lr to 0.0013392938572168614
Epoch 25: reducing lr to 0.0011679635329711916
Epoch 28: reducing lr to 0.0009744163292526985
Epoch 31: reducing lr to 0.0007708134743597888
Epoch 34: reducing lr to 0.0005699483068281658
Epoch 37: reducing lr to 0.0003844415461310336
Epoch 40: reducing lr to 0.0002259494117256299
Epoch 43: reducing lr to 0.00010443046847222575
Epoch 46: reducing lr to 2.752037544494178e-05
Epoch 49: reducing lr to 5.156922000815189e-08
[I 2024-06-24 15:36:49,880] Trial 58 finished with value: 1.1170903444290161 and parameters: {'hidden_size': 56, 'n_layers': 2, 'rnn_dropout': 0.0651791719018096, 'bidirectional': True, 'fc_dropout': 0.5070294084895343, 'learning_rate_model': 0.01628744452416877}. Best is trial 24 with value: 0.9763447046279907.
Epoch 23: reducing lr to 0.0002530196549232075
Epoch 26: reducing lr to 0.00021758382152098945
Epoch 29: reducing lr to 0.00017854922424040496
Epoch 32: reducing lr to 0.0001383685511876166
Epoch 35: reducing lr to 9.956651653774524e-05
Epoch 38: reducing lr to 6.458117890838135e-05
Epoch 41: reducing lr to 3.561077595378016e-05
Epoch 44: reducing lr to 1.4475666823543119e-05
Epoch 47: reducing lr to 2.5038235246031652e-06
[I 2024-06-24 15:36:55,778] Trial 59 finished with value: 1.0404871702194214 and parameters: {'hidden_size': 144, 'n_layers': 6, 'rnn_dropout': 0.2739632958748159, 'bidirectional': True, 'fc_dropout': 0.6592078963298551, 'learning_rate_model': 0.003206155887492329}. Best is trial 24 with value: 0.9763447046279907.
[I 2024-06-24 15:36:58,053] Trial 60 finished with value: 1.112064242362976 and parameters: {'hidden_size': 42, 'n_layers': 1, 'rnn_dropout': 0.014666114330405834, 'bidirectional': False, 'fc_dropout': 0.09067353821407238, 'learning_rate_model': 1.2916159380130333e-05}. Best is trial 24 with value: 0.9763447046279907.
Epoch 33: reducing lr to 3.072476384983367e-05
Epoch 37: reducing lr to 1.857774818901566e-05
Epoch 40: reducing lr to 1.0918776382884085e-05
Epoch 43: reducing lr to 5.046496576820777e-06
Epoch 46: reducing lr to 1.3298942589025842e-06
Epoch 49: reducing lr to 2.49203030539772e-09
[I 2024-06-24 15:37:01,964] Trial 61 finished with value: 0.9773032665252686 and parameters: {'hidden_size': 155, 'n_layers': 3, 'rnn_dropout': 0.5975507964860202, 'bidirectional': True, 'fc_dropout': 0.3824580053742648, 'learning_rate_model': 0.000787074253700552}. Best is trial 24 with value: 0.9763447046279907.
Epoch 9: reducing lr to 0.00026818905472148413
Epoch 13: reducing lr to 0.0002968817636621019
Epoch 16: reducing lr to 0.00028793393928816605
Epoch 22: reducing lr to 0.00024494985971345863
Epoch 27: reducing lr to 0.00019033323681432766
Epoch 30: reducing lr to 0.0001534511149661788
Epoch 33: reducing lr to 0.00011628591513466301
Epoch 36: reducing lr to 8.117287668850206e-05
Epoch 39: reducing lr to 5.031826611564486e-05
Epoch 42: reducing lr to 2.5660775619368172e-05
Epoch 45: reducing lr to 8.749761917334777e-06
Epoch 48: reducing lr to 6.47784854604155e-07
[I 2024-06-24 15:37:04,900] Trial 62 finished with value: 1.0925393104553223 and parameters: {'hidden_size': 164, 'n_layers': 3, 'rnn_dropout': 0.7237489282748985, 'bidirectional': False, 'fc_dropout': 0.10051310491019105, 'learning_rate_model': 0.0029788886358192816}. Best is trial 24 with value: 0.9763447046279907.
Epoch 12: reducing lr to 0.00458122933694196
Epoch 15: reducing lr to 0.004490724822014212
Epoch 18: reducing lr to 0.004262018913334329
Epoch 21: reducing lr to 0.003909482084255551
Epoch 24: reducing lr to 0.0034552654118959756
Epoch 27: reducing lr to 0.002927909476424241
Epoch 30: reducing lr to 0.002360549219869724
Epoch 33: reducing lr to 0.0017888343549244735
Epoch 36: reducing lr to 0.0012486880319107288
Epoch 39: reducing lr to 0.0007740494023172264
Epoch 42: reducing lr to 0.0003947415037219001
Epoch 45: reducing lr to 0.0001345981987329492
Epoch 48: reducing lr to 9.964919665238908e-06
[I 2024-06-24 15:37:08,811] Trial 63 finished with value: 1.1906331777572632 and parameters: {'hidden_size': 17, 'n_layers': 4, 'rnn_dropout': 0.0024143652956770904, 'bidirectional': True, 'fc_dropout': 0.6367735535538155, 'learning_rate_model': 0.04582445195599803}. Best is trial 24 with value: 0.9763447046279907.
Epoch 13: reducing lr to 0.0006966571339247921
Epoch 16: reducing lr to 0.0006756603384116015
Epoch 22: reducing lr to 0.0005747947099151568
Epoch 25: reducing lr to 0.000501263599850106
Epoch 28: reducing lr to 0.00041819750631373636
Epoch 33: reducing lr to 0.0002728743300169143
Epoch 36: reducing lr to 0.0001904787378271062
Epoch 39: reducing lr to 0.0001180758919772781
Epoch 42: reducing lr to 6.021509093978167e-05
Epoch 45: reducing lr to 2.0532025896991338e-05
Epoch 48: reducing lr to 1.5200796931470402e-06
[I 2024-06-24 15:37:11,356] Trial 64 finished with value: 1.0930390357971191 and parameters: {'hidden_size': 42, 'n_layers': 2, 'rnn_dropout': 0.1501053383219472, 'bidirectional': False, 'fc_dropout': 0.791964142687259, 'learning_rate_model': 0.006990203755569746}. Best is trial 24 with value: 0.9763447046279907.
Epoch 5: reducing lr to 0.0008482071936425261
Epoch 21: reducing lr to 0.0015186730975458446
Epoch 24: reducing lr to 0.0013422285900886283
Epoch 27: reducing lr to 0.0011373724851694135
Epoch 30: reducing lr to 0.0009169763458147737
Epoch 33: reducing lr to 0.0006948886200886332
Epoch 36: reducing lr to 0.0004850639753350851
Epoch 39: reducing lr to 0.00030068637690008947
Epoch 42: reducing lr to 0.00015334084906067498
Epoch 45: reducing lr to 5.228586779232776e-05
Epoch 48: reducing lr to 3.870961699952574e-06
[I 2024-06-24 15:37:18,187] Trial 65 finished with value: 1.140175700187683 and parameters: {'hidden_size': 151, 'n_layers': 7, 'rnn_dropout': 0.3206989858968967, 'bidirectional': True, 'fc_dropout': 0.0955656402669832, 'learning_rate_model': 0.017800916053720233}. Best is trial 24 with value: 0.9763447046279907.
Epoch 12: reducing lr to 0.00013630071479353638
Epoch 15: reducing lr to 0.00013360802486919203
Epoch 18: reducing lr to 0.00012680356769451988
Epoch 21: reducing lr to 0.0001163148935286116
Epoch 24: reducing lr to 0.0001028010411190578
Epoch 27: reducing lr to 8.7111149679702e-05
Epoch 30: reducing lr to 7.023104985797053e-05
Epoch 33: reducing lr to 5.322139174682608e-05
Epoch 36: reducing lr to 3.7150960754384695e-05
Epoch 39: reducing lr to 2.3029514364319688e-05
Epoch 42: reducing lr to 1.1744347457594241e-05
Epoch 45: reducing lr to 4.004565008192671e-06
Epoch 48: reducing lr to 2.9647624542169986e-07
[I 2024-06-24 15:37:20,742] Trial 66 finished with value: 1.0920495986938477 and parameters: {'hidden_size': 43, 'n_layers': 2, 'rnn_dropout': 0.6552713429068246, 'bidirectional': False, 'fc_dropout': 0.21331096172130817, 'learning_rate_model': 0.0013633688901489999}. Best is trial 24 with value: 0.9763447046279907.
[I 2024-06-24 15:37:24,439] Trial 67 finished with value: 1.107583999633789 and parameters: {'hidden_size': 74, 'n_layers': 7, 'rnn_dropout': 0.29324531508098756, 'bidirectional': False, 'fc_dropout': 0.7511091818957707, 'learning_rate_model': 1.3298296907035845e-05}. Best is trial 24 with value: 0.9763447046279907.
Epoch 9: reducing lr to 0.00015433093437824383
Epoch 40: reducing lr to 2.3780690962047347e-05
Epoch 43: reducing lr to 1.0991082821562971e-05
Epoch 46: reducing lr to 2.8964604891752167e-06
Epoch 49: reducing lr to 5.427549798855437e-09
[I 2024-06-24 15:37:31,181] Trial 68 finished with value: 0.9740266799926758 and parameters: {'hidden_size': 148, 'n_layers': 7, 'rnn_dropout': 0.021829778813975055, 'bidirectional': True, 'fc_dropout': 0.6875914566924687, 'learning_rate_model': 0.0017142186024413223}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:37:35,733] Trial 69 finished with value: 1.0537914037704468 and parameters: {'hidden_size': 143, 'n_layers': 4, 'rnn_dropout': 0.5026472300170858, 'bidirectional': True, 'fc_dropout': 0.4614047479896726, 'learning_rate_model': 0.00017610709636270444}. Best is trial 68 with value: 0.9740266799926758.
Epoch 9: reducing lr to 0.0050333868147849495
Epoch 12: reducing lr to 0.005589311044644059
Epoch 15: reducing lr to 0.005478891363010421
Epoch 18: reducing lr to 0.005199859608138024
Epoch 21: reducing lr to 0.004769748420181406
Epoch 24: reducing lr to 0.00421558313467411
Epoch 27: reducing lr to 0.0035721845755095722
Epoch 30: reducing lr to 0.0028799788999104895
Epoch 33: reducing lr to 0.002182460401271276
Epoch 36: reducing lr to 0.001523456979504171
Epoch 39: reducing lr to 0.000944375964456682
Epoch 42: reducing lr to 0.0004816028371993672
Epoch 45: reducing lr to 0.00016421600916173562
Epoch 48: reducing lr to 1.2157661502510547e-05
[I 2024-06-24 15:37:39,461] Trial 70 finished with value: 1.107361078262329 and parameters: {'hidden_size': 111, 'n_layers': 7, 'rnn_dropout': 0.3411798308011168, 'bidirectional': False, 'fc_dropout': 0.7454888933092325, 'learning_rate_model': 0.05590794448273345}. Best is trial 68 with value: 0.9740266799926758.
Epoch 25: reducing lr to 3.586244036316454e-06
Epoch 28: reducing lr to 2.9919553573579375e-06
Epoch 31: reducing lr to 2.3667906980819596e-06
Epoch 34: reducing lr to 1.7500321359961372e-06
Epoch 37: reducing lr to 1.1804317200019137e-06
Epoch 40: reducing lr to 6.937799917852718e-07
Epoch 43: reducing lr to 3.206548271379038e-07
Epoch 46: reducing lr to 8.45015957523439e-08
Epoch 49: reducing lr to 1.5834382023955275e-10
[I 2024-06-24 15:37:44,087] Trial 71 finished with value: 1.1068346500396729 and parameters: {'hidden_size': 172, 'n_layers': 7, 'rnn_dropout': 0.18232023196105357, 'bidirectional': False, 'fc_dropout': 0.25150129289124035, 'learning_rate_model': 5.001076586958479e-05}. Best is trial 68 with value: 0.9740266799926758.
Epoch 9: reducing lr to 0.00017834590306910675
Epoch 12: reducing lr to 0.00019804373525656927
Epoch 15: reducing lr to 0.00019413128056905963
Epoch 18: reducing lr to 0.00018424446436778952
Epoch 22: reducing lr to 0.00016289182264584117
Epoch 25: reducing lr to 0.00014205374544531152
Epoch 28: reducing lr to 0.00011851353684073618
Epoch 31: reducing lr to 9.375030810591445e-05
Epoch 34: reducing lr to 6.932005102007885e-05
Epoch 37: reducing lr to 4.675776254227178e-05
Epoch 40: reducing lr to 2.7481132167833003e-05
Epoch 43: reducing lr to 1.2701371889026333e-05
Epoch 46: reducing lr to 3.3471699223947816e-06
Epoch 49: reducing lr to 6.272114364035477e-09
[I 2024-06-24 15:37:46,783] Trial 72 finished with value: 1.0926989316940308 and parameters: {'hidden_size': 128, 'n_layers': 3, 'rnn_dropout': 0.6530812094617713, 'bidirectional': False, 'fc_dropout': 0.1922844998793111, 'learning_rate_model': 0.0019809629608084447}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:37:49,348] Trial 73 finished with value: 1.0925095081329346 and parameters: {'hidden_size': 80, 'n_layers': 1, 'rnn_dropout': 0.1830509367683007, 'bidirectional': True, 'fc_dropout': 0.41521278725894306, 'learning_rate_model': 4.9989708054979095e-05}. Best is trial 68 with value: 0.9740266799926758.
Epoch 3: reducing lr to 6.069049177927621e-05
Epoch 6: reducing lr to 0.00014395244185147938
Epoch 9: reducing lr to 0.00021717934365573193
Epoch 12: reducing lr to 0.0002411662264060254
Epoch 15: reducing lr to 0.00023640186497975315
Epoch 18: reducing lr to 0.00022436227104187233
Epoch 23: reducing lr to 0.00019037124888845436
Epoch 26: reducing lr to 0.00016370943140146555
Epoch 29: reducing lr to 0.0001343399144901485
Epoch 32: reducing lr to 0.00010410809351734877
Epoch 35: reducing lr to 7.491355604969229e-05
Epoch 38: reducing lr to 4.859069026557905e-05
Epoch 41: reducing lr to 2.6793443751496746e-05
Epoch 44: reducing lr to 1.0891449411420084e-05
Epoch 47: reducing lr to 1.8838695022316364e-06
[I 2024-06-24 15:37:53,050] Trial 74 finished with value: 1.1074843406677246 and parameters: {'hidden_size': 121, 'n_layers': 7, 'rnn_dropout': 0.6534680870332976, 'bidirectional': False, 'fc_dropout': 0.7212500677354571, 'learning_rate_model': 0.002412302319431398}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:37:55,571] Trial 75 finished with value: 1.1027545928955078 and parameters: {'hidden_size': 21, 'n_layers': 1, 'rnn_dropout': 0.05229737203378111, 'bidirectional': True, 'fc_dropout': 0.7802800274162293, 'learning_rate_model': 4.6982180906249337e-05}. Best is trial 68 with value: 0.9740266799926758.
Epoch 17: reducing lr to 1.0720776231899979e-06
Epoch 20: reducing lr to 9.948996775066422e-07
Epoch 23: reducing lr to 8.906663656135088e-07
Epoch 26: reducing lr to 7.659270248756585e-07
Epoch 29: reducing lr to 6.285195064611818e-07
Epoch 32: reducing lr to 4.870776329170363e-07
Epoch 35: reducing lr to 3.504887691368712e-07
Epoch 38: reducing lr to 2.2733523971812177e-07
Epoch 41: reducing lr to 1.2535516422649705e-07
Epoch 44: reducing lr to 5.0956474363504674e-08
Epoch 47: reducing lr to 8.81382673402498e-09
[I 2024-06-24 15:37:59,782] Trial 76 finished with value: 1.1075388193130493 and parameters: {'hidden_size': 195, 'n_layers': 6, 'rnn_dropout': 0.6593902716995983, 'bidirectional': False, 'fc_dropout': 0.3815058668169547, 'learning_rate_model': 1.1286139856486e-05}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:38:03,738] Trial 77 finished with value: 1.1055004596710205 and parameters: {'hidden_size': 81, 'n_layers': 4, 'rnn_dropout': 0.5840729688720835, 'bidirectional': True, 'fc_dropout': 0.11013019275648253, 'learning_rate_model': 1.1054902973283413e-05}. Best is trial 68 with value: 0.9740266799926758.
Epoch 8: reducing lr to 0.000853098401870044
Epoch 11: reducing lr to 0.001040171486032295
Epoch 14: reducing lr to 0.001035642720801551
Epoch 17: reducing lr to 0.0009936801901890475
Epoch 20: reducing lr to 0.0009221460082546807
Epoch 23: reducing lr to 0.0008255349281000425
Epoch 26: reducing lr to 0.0007099173560630253
Epoch 29: reducing lr to 0.0005825579876012287
Epoch 32: reducing lr to 0.0004514592828396862
Epoch 35: reducing lr to 0.000324858703550544
Epoch 38: reducing lr to 0.0002107109777812621
Epoch 41: reducing lr to 0.00011618836242391142
Epoch 44: reducing lr to 4.7230198673699034e-05
Epoch 47: reducing lr to 8.169301210950685e-06
[I 2024-06-24 15:38:06,634] Trial 78 finished with value: 1.0955311059951782 and parameters: {'hidden_size': 156, 'n_layers': 3, 'rnn_dropout': 0.7875067601548086, 'bidirectional': False, 'fc_dropout': 0.4191169085329054, 'learning_rate_model': 0.010460822384971712}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:38:08,914] Trial 79 finished with value: 1.1032068729400635 and parameters: {'hidden_size': 101, 'n_layers': 1, 'rnn_dropout': 0.25000110279363963, 'bidirectional': False, 'fc_dropout': 0.6161199650496738, 'learning_rate_model': 3.321567816135707e-05}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:38:11,831] Trial 80 finished with value: 1.0933618545532227 and parameters: {'hidden_size': 20, 'n_layers': 4, 'rnn_dropout': 0.6479909685299331, 'bidirectional': False, 'fc_dropout': 0.5494465813255934, 'learning_rate_model': 0.0006265951288394312}. Best is trial 68 with value: 0.9740266799926758.
Epoch 14: reducing lr to 0.00039107577733250235
Epoch 17: reducing lr to 0.0003752300334784611
Epoch 20: reducing lr to 0.0003482175462143443
Epoch 23: reducing lr to 0.0003117356084654209
Epoch 29: reducing lr to 0.00021998350711728147
Epoch 32: reducing lr to 0.00017047847334248348
Epoch 38: reducing lr to 7.956794150449967e-05
Epoch 42: reducing lr to 3.4027676863548864e-05
Epoch 45: reducing lr to 1.1602691811518369e-05
Epoch 48: reducing lr to 8.590002904251652e-07
[I 2024-06-24 15:38:15,572] Trial 81 finished with value: 1.0926530361175537 and parameters: {'hidden_size': 185, 'n_layers': 5, 'rnn_dropout': 0.004191870121172681, 'bidirectional': False, 'fc_dropout': 0.6643196560298716, 'learning_rate_model': 0.003950179114447677}. Best is trial 68 with value: 0.9740266799926758.
Epoch 10: reducing lr to 0.0025475366025506352
Epoch 13: reducing lr to 0.0026414519970979346
Epoch 16: reducing lr to 0.0025618403420381275
Epoch 19: reducing lr to 0.002404527220478999
Epoch 22: reducing lr to 0.0021793972393177657
Epoch 25: reducing lr to 0.0019005959638095115
Epoch 31: reducing lr to 0.0012543242463156049
Epoch 34: reducing lr to 0.0009274617066014127
Epoch 40: reducing lr to 0.00036768146250115637
Epoch 43: reducing lr to 0.00016993692120860422
Epoch 46: reducing lr to 4.478317431719711e-05
Epoch 49: reducing lr to 8.391721884881687e-08
[I 2024-06-24 15:38:19,122] Trial 82 finished with value: 1.0974271297454834 and parameters: {'hidden_size': 140, 'n_layers': 5, 'rnn_dropout': 0.21929582013348173, 'bidirectional': False, 'fc_dropout': 0.2819437041731271, 'learning_rate_model': 0.026504124871653878}. Best is trial 68 with value: 0.9740266799926758.
Epoch 20: reducing lr to 2.2875643147165028e-05
Epoch 24: reducing lr to 1.956697020916107e-05
Epoch 27: reducing lr to 1.6580583738392798e-05
Epoch 32: reducing lr to 1.1199334332382045e-05
Epoch 42: reducing lr to 2.2353985361163887e-06
Epoch 45: reducing lr to 7.622218935040388e-07
Epoch 48: reducing lr to 5.643076955972899e-08
[I 2024-06-24 15:38:24,062] Trial 83 finished with value: 1.0749874114990234 and parameters: {'hidden_size': 52, 'n_layers': 6, 'rnn_dropout': 0.23168219694950196, 'bidirectional': True, 'fc_dropout': 0.6461587272897676, 'learning_rate_model': 0.00025950124791777955}. Best is trial 68 with value: 0.9740266799926758.
Epoch 15: reducing lr to 7.041540982280583e-06
Epoch 18: reducing lr to 6.682925815979482e-06
Epoch 21: reducing lr to 6.130141437486202e-06
Epoch 24: reducing lr to 5.417921152338449e-06
Epoch 27: reducing lr to 4.591017127030665e-06
Epoch 30: reducing lr to 3.701385573865501e-06
Epoch 33: reducing lr to 2.8049259128423764e-06
Epoch 36: reducing lr to 1.9579663193076534e-06
Epoch 39: reducing lr to 1.2137240211217944e-06
Epoch 42: reducing lr to 6.189621021174268e-07
Epoch 45: reducing lr to 2.1105250713048814e-07
Epoch 48: reducing lr to 1.5625181454881678e-08
[I 2024-06-24 15:38:27,500] Trial 84 finished with value: 1.1084543466567993 and parameters: {'hidden_size': 55, 'n_layers': 6, 'rnn_dropout': 0.2469135581045209, 'bidirectional': False, 'fc_dropout': 0.17711736072656087, 'learning_rate_model': 7.185360253135694e-05}. Best is trial 68 with value: 0.9740266799926758.
Epoch 9: reducing lr to 0.0014239440639673387
Epoch 14: reducing lr to 0.0015658504316223751
Epoch 17: reducing lr to 0.0015024047612654196
Epoch 22: reducing lr to 0.0013005560539032337
Epoch 25: reducing lr to 0.0011341812966278186
Epoch 28: reducing lr to 0.000946232262025945
Epoch 31: reducing lr to 0.0007485184264131837
Epoch 34: reducing lr to 0.000553463093153905
Epoch 37: reducing lr to 0.00037332193939248055
Epoch 40: reducing lr to 0.00021941403950459515
Epoch 43: reducing lr to 0.00010140991631645471
Epoch 46: reducing lr to 2.6724374712646173e-05
Epoch 49: reducing lr to 5.007762927840276e-08
[I 2024-06-24 15:38:29,786] Trial 85 finished with value: 1.098707914352417 and parameters: {'hidden_size': 52, 'n_layers': 1, 'rnn_dropout': 0.3016660793658673, 'bidirectional': False, 'fc_dropout': 0.5396512190107284, 'learning_rate_model': 0.015816345654373305}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:38:32,706] Trial 86 finished with value: 1.0595283508300781 and parameters: {'hidden_size': 30, 'n_layers': 2, 'rnn_dropout': 0.1661005294430515, 'bidirectional': True, 'fc_dropout': 0.44310768884113144, 'learning_rate_model': 0.00016522355094605626}. Best is trial 68 with value: 0.9740266799926758.
Epoch 7: reducing lr to 0.005711581162388893
Epoch 10: reducing lr to 0.0077087003711844276
Epoch 13: reducing lr to 0.007992882995324881
Epoch 16: reducing lr to 0.007751982670557931
Epoch 19: reducing lr to 0.007275962142593432
Epoch 22: reducing lr to 0.006594731667786999
Epoch 25: reducing lr to 0.00575109491931188
Epoch 28: reducing lr to 0.0047980614482061494
Epoch 31: reducing lr to 0.0037955135849580173
Epoch 34: reducing lr to 0.002806446193856226
Epoch 37: reducing lr to 0.0018930041566470068
Epoch 40: reducing lr to 0.0011125831219156988
Epoch 43: reducing lr to 0.0005142194252624743
Epoch 46: reducing lr to 0.00013551132970421458
Epoch 49: reducing lr to 2.5392871507434747e-07
[I 2024-06-24 15:38:37,708] Trial 87 finished with value: 1.1075776815414429 and parameters: {'hidden_size': 170, 'n_layers': 4, 'rnn_dropout': 0.7387649720950202, 'bidirectional': True, 'fc_dropout': 0.10096808299948697, 'learning_rate_model': 0.08019996926893054}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:38:40,644] Trial 88 finished with value: 1.0837403535842896 and parameters: {'hidden_size': 45, 'n_layers': 2, 'rnn_dropout': 0.3449454041282968, 'bidirectional': True, 'fc_dropout': 0.5834551354888987, 'learning_rate_model': 5.6882708517701854e-05}. Best is trial 68 with value: 0.9740266799926758.
Epoch 5: reducing lr to 0.00011656517258961006
Epoch 8: reducing lr to 0.00019949982172217588
Epoch 18: reducing lr to 0.0002275240474108565
Epoch 22: reducing lr to 0.0002011556054380569
Epoch 25: reducing lr to 0.0001754226007521735
Epoch 29: reducing lr to 0.00013623307043421112
Epoch 32: reducing lr to 0.00010557521411821713
Epoch 35: reducing lr to 7.59692589989207e-05
Epoch 38: reducing lr to 4.927544397002725e-05
Epoch 41: reducing lr to 2.7171024513644466e-05
Epoch 44: reducing lr to 1.1044934786715553e-05
Epoch 47: reducing lr to 1.9104175223008667e-06
[I 2024-06-24 15:38:44,512] Trial 89 finished with value: 1.0930464267730713 and parameters: {'hidden_size': 135, 'n_layers': 6, 'rnn_dropout': 0.1685859136666566, 'bidirectional': False, 'fc_dropout': 0.5105749684962667, 'learning_rate_model': 0.002446297163720527}. Best is trial 68 with value: 0.9740266799926758.
Epoch 3: reducing lr to 0.0005517919142114715
Epoch 6: reducing lr to 0.0013088012819788468
Epoch 9: reducing lr to 0.001974572989107133
Epoch 12: reducing lr to 0.0021926593410333526
Epoch 15: reducing lr to 0.0021493422408694726
Epoch 18: reducing lr to 0.002039879450396903
Epoch 21: reducing lr to 0.0018711489384566633
Epoch 26: reducing lr to 0.0014884298656866733
Epoch 36: reducing lr to 0.0005976447097130559
Epoch 39: reducing lr to 0.0003704740644014725
Epoch 42: reducing lr to 0.0001889304336829236
Epoch 45: reducing lr to 6.442113590739116e-05
Epoch 48: reducing lr to 4.7693910476043214e-06
[I 2024-06-24 15:38:47,953] Trial 90 finished with value: 1.1036078929901123 and parameters: {'hidden_size': 70, 'n_layers': 6, 'rnn_dropout': 0.7562467519999598, 'bidirectional': False, 'fc_dropout': 0.04491283172953225, 'learning_rate_model': 0.021932412730099944}. Best is trial 68 with value: 0.9740266799926758.
Epoch 15: reducing lr to 0.00013001962762079975
Epoch 18: reducing lr to 0.00012339792215903031
Epoch 23: reducing lr to 0.00010470306100293806
Epoch 26: reducing lr to 9.003921906730479e-05
Epoch 29: reducing lr to 7.388615846205422e-05
Epoch 32: reducing lr to 5.72588357227909e-05
Epoch 35: reducing lr to 4.1202012776697475e-05
Epoch 38: reducing lr to 2.6724592273031743e-05
Epoch 41: reducing lr to 1.4736235602654012e-05
Epoch 44: reducing lr to 5.990232762524521e-06
Epoch 47: reducing lr to 1.0361170847248421e-06
[I 2024-06-24 15:38:50,635] Trial 91 finished with value: 1.0928435325622559 and parameters: {'hidden_size': 51, 'n_layers': 3, 'rnn_dropout': 0.24006483674028517, 'bidirectional': False, 'fc_dropout': 0.2961233420429906, 'learning_rate_model': 0.0013267520089493556}. Best is trial 68 with value: 0.9740266799926758.
Epoch 25: reducing lr to 1.922307096257968e-05
Epoch 28: reducing lr to 1.60375505874489e-05
Epoch 31: reducing lr to 1.2686528044961049e-05
Epoch 34: reducing lr to 9.380564065462307e-06
Epoch 37: reducing lr to 6.327378307301125e-06
Epoch 40: reducing lr to 3.7188160870961394e-06
Epoch 43: reducing lr to 1.7187816652033646e-06
Epoch 46: reducing lr to 4.52947472383107e-07
Epoch 49: reducing lr to 8.48758328246893e-10
[I 2024-06-24 15:38:52,922] Trial 92 finished with value: 1.0917065143585205 and parameters: {'hidden_size': 107, 'n_layers': 1, 'rnn_dropout': 0.16516324711154642, 'bidirectional': False, 'fc_dropout': 0.2092396414663897, 'learning_rate_model': 0.0002680689020235863}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:38:57,837] Trial 93 finished with value: 1.1067509651184082 and parameters: {'hidden_size': 35, 'n_layers': 6, 'rnn_dropout': 0.08526710143104088, 'bidirectional': True, 'fc_dropout': 0.4579240899477195, 'learning_rate_model': 1.5114066835104806e-05}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:39:01,125] Trial 94 finished with value: 1.0510737895965576 and parameters: {'hidden_size': 161, 'n_layers': 2, 'rnn_dropout': 0.4223231827265013, 'bidirectional': True, 'fc_dropout': 0.4408697393495774, 'learning_rate_model': 7.353072250353887e-05}. Best is trial 68 with value: 0.9740266799926758.
Epoch 23: reducing lr to 0.00013576407632975062
Epoch 33: reducing lr to 6.715650588908868e-05
Epoch 36: reducing lr to 4.687830650042942e-05
Epoch 39: reducing lr to 2.905940010714807e-05
Epoch 42: reducing lr to 1.4819404628712402e-05
Epoch 45: reducing lr to 5.053092088144577e-06
Epoch 48: reducing lr to 3.741034961346132e-07
[I 2024-06-24 15:39:07,193] Trial 95 finished with value: 0.9774012565612793 and parameters: {'hidden_size': 156, 'n_layers': 6, 'rnn_dropout': 0.14123922742166642, 'bidirectional': True, 'fc_dropout': 0.6881895979384922, 'learning_rate_model': 0.0017203437921322648}. Best is trial 68 with value: 0.9740266799926758.
Epoch 11: reducing lr to 0.005409405638924722
Epoch 14: reducing lr to 0.005385853822223806
Epoch 17: reducing lr to 0.005167627930852102
Epoch 20: reducing lr to 0.004795614842310644
Epoch 23: reducing lr to 0.0042931894934245835
Epoch 26: reducing lr to 0.003691920996321782
Epoch 29: reducing lr to 0.0030295893566081506
Epoch 32: reducing lr to 0.002347811320663415
Epoch 35: reducing lr to 0.0016894257595381989
Epoch 38: reducing lr to 0.0010958011892261317
Epoch 41: reducing lr to 0.0006042368891217828
Epoch 44: reducing lr to 0.00024562036785645005
Epoch 47: reducing lr to 4.248440245671128e-05
[I 2024-06-24 15:39:12,172] Trial 96 finished with value: 1.1072291135787964 and parameters: {'hidden_size': 90, 'n_layers': 6, 'rnn_dropout': 0.573543201353976, 'bidirectional': True, 'fc_dropout': 0.0030349379274740686, 'learning_rate_model': 0.054401444720336284}. Best is trial 68 with value: 0.9740266799926758.
Epoch 3: reducing lr to 0.00030337446504804296
Epoch 6: reducing lr to 0.0007195772147946977
Epoch 9: reducing lr to 0.0010856176193243645
Epoch 23: reducing lr to 0.0009516115967902518
Epoch 28: reducing lr to 0.0007214092474717299
Epoch 31: reducing lr to 0.0005706718491729594
Epoch 34: reducing lr to 0.000421961298043954
Epoch 37: reducing lr to 0.0002846213452764642
Epoch 40: reducing lr to 0.00016728167435851193
Epoch 43: reducing lr to 7.731510999148192e-05
Epoch 46: reducing lr to 2.0374713296420974e-05
Epoch 49: reducing lr to 3.81792782836593e-08
[I 2024-06-24 15:39:16,310] Trial 97 finished with value: 1.112280249595642 and parameters: {'hidden_size': 174, 'n_layers': 6, 'rnn_dropout': 0.41335102633723075, 'bidirectional': False, 'fc_dropout': 0.2999007548294875, 'learning_rate_model': 0.01205841152767771}. Best is trial 68 with value: 0.9740266799926758.
Epoch 7: reducing lr to 0.0027431437109580905
Epoch 12: reducing lr to 0.00385080177499223
Epoch 18: reducing lr to 0.003582486007450934
Epoch 23: reducing lr to 0.003039737172372293
Epoch 26: reducing lr to 0.0026140168066583875
Epoch 29: reducing lr to 0.0021450614743211143
Epoch 32: reducing lr to 0.0016623373731971599
Epoch 35: reducing lr to 0.0011961760106552282
Epoch 38: reducing lr to 0.0007758678282247012
Epoch 41: reducing lr to 0.0004278220972065608
Epoch 44: reducing lr to 0.00017390831772241336
Epoch 47: reducing lr to 3.0080530475415368e-05
[I 2024-06-24 15:39:21,573] Trial 98 finished with value: 1.0789344310760498 and parameters: {'hidden_size': 154, 'n_layers': 5, 'rnn_dropout': 0.3214927391742699, 'bidirectional': True, 'fc_dropout': 0.6199173173796783, 'learning_rate_model': 0.03851823778112661}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:39:23,857] Trial 99 finished with value: 1.094435691833496 and parameters: {'hidden_size': 60, 'n_layers': 1, 'rnn_dropout': 0.1762271902349821, 'bidirectional': False, 'fc_dropout': 0.4345331441904756, 'learning_rate_model': 0.00014023182899556565}. Best is trial 68 with value: 0.9740266799926758.
Epoch 6: reducing lr to 0.00122554353888497
Epoch 9: reducing lr to 0.0018489630184333368
Epoch 14: reducing lr to 0.0020332256116867083
Epoch 17: reducing lr to 0.0019508426718380164
Epoch 20: reducing lr to 0.0018104031863874336
Epoch 23: reducing lr to 0.00162073147953558
Epoch 26: reducing lr to 0.0013937452767602107
Epoch 29: reducing lr to 0.0011437069916995567
Epoch 32: reducing lr to 0.0008863274545037374
Epoch 35: reducing lr to 0.0006377788623156579
Epoch 38: reducing lr to 0.0004136783352822939
Epoch 41: reducing lr to 0.00022810680702452582
Epoch 44: reducing lr to 9.272468937366052e-05
Epoch 47: reducing lr to 1.6038380918501072e-05
[I 2024-06-24 15:39:26,534] Trial 100 finished with value: 1.0968420505523682 and parameters: {'hidden_size': 41, 'n_layers': 3, 'rnn_dropout': 0.4910968717930021, 'bidirectional': False, 'fc_dropout': 0.35529368561329155, 'learning_rate_model': 0.020537209952065802}. Best is trial 68 with value: 0.9740266799926758.
Epoch 13: reducing lr to 0.0003205912481466736
Epoch 16: reducing lr to 0.00031092883524245016
Epoch 22: reducing lr to 0.0002645119736902008
Epoch 25: reducing lr to 0.0002306740508363024
Epoch 28: reducing lr to 0.00019244827045067017
Epoch 31: reducing lr to 0.0001522364881696717
Epoch 34: reducing lr to 0.00011256540207971337
Epoch 37: reducing lr to 7.592761781716004e-05
Epoch 40: reducing lr to 4.462525123043897e-05
Epoch 43: reducing lr to 2.0625129563711478e-05
Epoch 46: reducing lr to 5.4353036761954635e-06
Epoch 49: reducing lr to 1.0184976278722682e-08
[I 2024-06-24 15:39:29,218] Trial 101 finished with value: 1.092717170715332 and parameters: {'hidden_size': 94, 'n_layers': 3, 'rnn_dropout': 0.5399772838472842, 'bidirectional': False, 'fc_dropout': 0.25181254985921314, 'learning_rate_model': 0.0032167877678542485}. Best is trial 68 with value: 0.9740266799926758.
Epoch 15: reducing lr to 5.286219493144185e-05
Epoch 18: reducing lr to 5.017000228865466e-05
Epoch 21: reducing lr to 4.6020144232797184e-05
Epoch 24: reducing lr to 4.0673370331693807e-05
Epoch 27: reducing lr to 3.446564365859608e-05
Epoch 30: reducing lr to 2.7787009436496144e-05
Epoch 33: reducing lr to 2.1057115302750636e-05
Epoch 36: reducing lr to 1.4698827643110166e-05
Epoch 39: reducing lr to 9.11165836554343e-06
Epoch 42: reducing lr to 4.646666884371294e-06
Epoch 45: reducing lr to 1.5844115373007474e-06
Epoch 48: reducing lr to 1.1730122568137658e-07
[I 2024-06-24 15:39:32,512] Trial 102 finished with value: 1.0929136276245117 and parameters: {'hidden_size': 178, 'n_layers': 4, 'rnn_dropout': 0.6275659350335385, 'bidirectional': False, 'fc_dropout': 0.10547278002200172, 'learning_rate_model': 0.0005394187370487681}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:39:38,958] Trial 103 finished with value: 1.084364414215088 and parameters: {'hidden_size': 184, 'n_layers': 5, 'rnn_dropout': 0.6134196697896818, 'bidirectional': True, 'fc_dropout': 0.67431987215428, 'learning_rate_model': 1.8673586483652047e-05}. Best is trial 68 with value: 0.9740266799926758.
Epoch 6: reducing lr to 0.00037904558954547326
Epoch 9: reducing lr to 0.0005718615904968049
Epoch 13: reducing lr to 0.0006330432751389453
Epoch 16: reducing lr to 0.0006139637601927482
Epoch 19: reducing lr to 0.0005762625209487515
Epoch 22: reducing lr to 0.0005223084756877155
Epoch 25: reducing lr to 0.0004554917125004354
Epoch 28: reducing lr to 0.00038001063386852106
Epoch 31: reducing lr to 0.0003006079723751193
Epoch 34: reducing lr to 0.00022227297598365007
Epoch 37: reducing lr to 0.0001499275733019472
Epoch 40: reducing lr to 8.811754954673881e-05
Epoch 43: reducing lr to 4.0726625086170456e-05
Epoch 46: reducing lr to 1.073261500569517e-05
Epoch 49: reducing lr to 2.011137477386793e-08
[I 2024-06-24 15:39:42,660] Trial 104 finished with value: 1.1074635982513428 and parameters: {'hidden_size': 122, 'n_layers': 7, 'rnn_dropout': 0.41428803853785756, 'bidirectional': False, 'fc_dropout': 0.20131131388468226, 'learning_rate_model': 0.006351907220679005}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:39:48,190] Trial 105 finished with value: 1.1020503044128418 and parameters: {'hidden_size': 115, 'n_layers': 7, 'rnn_dropout': 0.49946936192214075, 'bidirectional': True, 'fc_dropout': 0.7914720449333105, 'learning_rate_model': 2.0457769512926938e-05}. Best is trial 68 with value: 0.9740266799926758.
Epoch 17: reducing lr to 1.0015245025399623e-05
Epoch 20: reducing lr to 9.294256153086592e-06
Epoch 25: reducing lr to 7.560614743649844e-06
Epoch 28: reducing lr to 6.307719596912131e-06
Epoch 31: reducing lr to 4.989730889990318e-06
Epoch 34: reducing lr to 3.689464139998609e-06
Epoch 37: reducing lr to 2.48861744369336e-06
Epoch 40: reducing lr to 1.4626453698139067e-06
Epoch 43: reducing lr to 6.760130066807866e-07
Epoch 46: reducing lr to 1.7814850418359488e-07
Epoch 49: reducing lr to 3.3382463930187004e-10
[I 2024-06-24 15:39:51,875] Trial 106 finished with value: 1.1068201065063477 and parameters: {'hidden_size': 75, 'n_layers': 7, 'rnn_dropout': 0.007151281896996054, 'bidirectional': False, 'fc_dropout': 0.351864651366858, 'learning_rate_model': 0.00010543402232135162}. Best is trial 68 with value: 0.9740266799926758.
Epoch 13: reducing lr to 0.0026628042396333374
Epoch 23: reducing lr to 0.0021085291560170513
Epoch 26: reducing lr to 0.0018132260582438102
Epoch 29: reducing lr to 0.0014879328058896547
Epoch 32: reducing lr to 0.0011530887770101338
Epoch 35: reducing lr to 0.0008297335760204365
Epoch 38: reducing lr to 0.0005381846667192873
Epoch 41: reducing lr to 0.0002967609745168825
Epoch 44: reducing lr to 0.00012063238944616007
Epoch 47: reducing lr to 2.0865513016170636e-05
[I 2024-06-24 15:39:56,746] Trial 107 finished with value: 1.2366456985473633 and parameters: {'hidden_size': 164, 'n_layers': 4, 'rnn_dropout': 0.10747088692455692, 'bidirectional': True, 'fc_dropout': 0.3932361200135483, 'learning_rate_model': 0.026718371620438223}. Best is trial 68 with value: 0.9740266799926758.
Epoch 24: reducing lr to 1.1330442889558257e-05
[I 2024-06-24 15:40:01,193] Trial 108 finished with value: 1.070129156112671 and parameters: {'hidden_size': 93, 'n_layers': 5, 'rnn_dropout': 0.31839251152633796, 'bidirectional': True, 'fc_dropout': 0.7719911392887311, 'learning_rate_model': 0.0001502667013784738}. Best is trial 68 with value: 0.9740266799926758.
Epoch 26: reducing lr to 7.243720069372752e-05
Epoch 37: reducing lr to 2.5193996161390434e-05
Epoch 40: reducing lr to 1.4807371026813227e-05
Epoch 43: reducing lr to 6.843747374079851e-06
Epoch 46: reducing lr to 1.8035205619622687e-06
Epoch 49: reducing lr to 3.37953778410411e-09
[I 2024-06-24 15:40:04,104] Trial 109 finished with value: 0.9791077971458435 and parameters: {'hidden_size': 35, 'n_layers': 2, 'rnn_dropout': 0.011273835353954365, 'bidirectional': True, 'fc_dropout': 0.6356626809478018, 'learning_rate_model': 0.0010673815537119522}. Best is trial 68 with value: 0.9740266799926758.
Epoch 10: reducing lr to 1.8624594121977604e-05
[I 2024-06-24 15:40:07,712] Trial 110 finished with value: 1.0933743715286255 and parameters: {'hidden_size': 162, 'n_layers': 5, 'rnn_dropout': 0.6222787814873403, 'bidirectional': False, 'fc_dropout': 0.14383543815437952, 'learning_rate_model': 0.0001937670170464033}. Best is trial 68 with value: 0.9740266799926758.
Epoch 4: reducing lr to 0.003055188083347597
Epoch 7: reducing lr to 0.006061022979661935
Epoch 10: reducing lr to 0.008180328487800979
Epoch 13: reducing lr to 0.008481897767193922
Epoch 16: reducing lr to 0.008226258853431239
Epoch 19: reducing lr to 0.007721114782682027
Epoch 22: reducing lr to 0.0069982057589186575
Epoch 25: reducing lr to 0.006102954238609995
Epoch 28: reducing lr to 0.0050916129647090875
Epoch 31: reducing lr to 0.004027727945028059
Epoch 34: reducing lr to 0.002978148149965688
Epoch 37: reducing lr to 0.002008820564362639
Epoch 40: reducing lr to 0.0011806523757590462
Epoch 43: reducing lr to 0.0005456800252840746
Epoch 46: reducing lr to 0.00014380208562041473
Epoch 49: reducing lr to 2.6946439759950593e-07
[I 2024-06-24 15:40:10,373] Trial 111 finished with value: 1.1241731643676758 and parameters: {'hidden_size': 47, 'n_layers': 3, 'rnn_dropout': 0.19313499837247905, 'bidirectional': False, 'fc_dropout': 0.256255460657625, 'learning_rate_model': 0.08510670563663286}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:40:14,205] Trial 112 finished with value: 0.9796848893165588 and parameters: {'hidden_size': 133, 'n_layers': 3, 'rnn_dropout': 0.6859875992668774, 'bidirectional': True, 'fc_dropout': 0.6342845423815101, 'learning_rate_model': 0.0005392051542230778}. Best is trial 68 with value: 0.9740266799926758.
Epoch 10: reducing lr to 0.00014430869452913435
Epoch 13: reducing lr to 0.0001496286604796687
Epoch 16: reducing lr to 0.0001451189494123255
Epoch 19: reducing lr to 0.00013620773252077164
Epoch 22: reducing lr to 0.00012345493687959485
Epoch 25: reducing lr to 0.00010766185737629298
Epoch 28: reducing lr to 8.982084534630976e-05
Epoch 31: reducing lr to 7.105291218222938e-05
Epoch 34: reducing lr to 5.2537336645687166e-05
Epoch 37: reducing lr to 3.543748562404967e-05
Epoch 40: reducing lr to 2.0827819236426502e-05
Epoch 43: reducing lr to 9.626309285354654e-06
Epoch 46: reducing lr to 2.536804148805909e-06
Epoch 49: reducing lr to 4.753605615907819e-09
[I 2024-06-24 15:40:16,874] Trial 113 finished with value: 1.0924785137176514 and parameters: {'hidden_size': 82, 'n_layers': 3, 'rnn_dropout': 0.5578951004905116, 'bidirectional': False, 'fc_dropout': 0.23623103551987823, 'learning_rate_model': 0.001501362396927331}. Best is trial 68 with value: 0.9740266799926758.
Epoch 3: reducing lr to 1.5316438744007403e-05
Epoch 6: reducing lr to 3.632922873136697e-05
Epoch 9: reducing lr to 5.480947700447883e-05
Epoch 12: reducing lr to 6.086303843615618e-05
Epoch 15: reducing lr to 5.966065816536839e-05
Epoch 18: reducing lr to 5.662222994298838e-05
Epoch 24: reducing lr to 4.590426195771348e-05
Epoch 27: reducing lr to 3.8898176427060663e-05
Epoch 30: reducing lr to 3.136062120724817e-05
Epoch 33: reducing lr to 2.3765213677855266e-05
Epoch 36: reducing lr to 1.658920392133902e-05
Epoch 39: reducing lr to 1.028348398645425e-05
Epoch 42: reducing lr to 5.244262084772478e-06
Epoch 45: reducing lr to 1.7881783993789784e-06
Epoch 48: reducing lr to 1.3238701754308578e-07
[I 2024-06-24 15:40:20,316] Trial 114 finished with value: 1.1075310707092285 and parameters: {'hidden_size': 106, 'n_layers': 6, 'rnn_dropout': 0.7107091475163088, 'bidirectional': False, 'fc_dropout': 0.7185640368137097, 'learning_rate_model': 0.0006087919149176257}. Best is trial 68 with value: 0.9740266799926758.
Epoch 38: reducing lr to 9.354426858900427e-06
Epoch 41: reducing lr to 5.158134377213953e-06
Epoch 44: reducing lr to 2.0967651694117933e-06
Epoch 47: reducing lr to 3.6267275426672336e-07
[I 2024-06-24 15:40:23,613] Trial 115 finished with value: 0.9786708354949951 and parameters: {'hidden_size': 186, 'n_layers': 2, 'rnn_dropout': 0.4870649393550366, 'bidirectional': True, 'fc_dropout': 0.01101401004617344, 'learning_rate_model': 0.00046440389064944195}. Best is trial 68 with value: 0.9740266799926758.
Epoch 8: reducing lr to 0.0006005049016818015
Epoch 11: reducing lr to 0.0007321876053018193
Epoch 14: reducing lr to 0.000728999760014961
Epoch 17: reducing lr to 0.0006994618951396501
Epoch 20: reducing lr to 0.0006491082351219762
Epoch 23: reducing lr to 0.0005811026837547948
Epoch 27: reducing lr to 0.00047048266046474386
Epoch 33: reducing lr to 0.00028744589039120065
Epoch 36: reducing lr to 0.0002006503520939869
Epoch 39: reducing lr to 0.00012438117539689464
Epoch 42: reducing lr to 6.343059249691889e-05
Epoch 45: reducing lr to 2.162844143357139e-05
Epoch 48: reducing lr to 1.601252345118532e-06
[I 2024-06-24 15:40:26,574] Trial 116 finished with value: 1.0944273471832275 and parameters: {'hidden_size': 189, 'n_layers': 3, 'rnn_dropout': 0.62379543638608, 'bidirectional': False, 'fc_dropout': 0.1156781766704774, 'learning_rate_model': 0.007363482458797472}. Best is trial 68 with value: 0.9740266799926758.
Epoch 37: reducing lr to 7.255475039697276e-06
[I 2024-06-24 15:40:31,752] Trial 117 finished with value: 1.0376465320587158 and parameters: {'hidden_size': 145, 'n_layers': 5, 'rnn_dropout': 0.20271682758903975, 'bidirectional': True, 'fc_dropout': 0.3397192927450718, 'learning_rate_model': 0.0003073891164855588}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:40:34,281] Trial 118 finished with value: 1.0934619903564453 and parameters: {'hidden_size': 81, 'n_layers': 1, 'rnn_dropout': 0.5053173014927518, 'bidirectional': True, 'fc_dropout': 0.5186502049304996, 'learning_rate_model': 4.7836881141741806e-05}. Best is trial 68 with value: 0.9740266799926758.
Epoch 20: reducing lr to 8.59786469013357e-05
Epoch 23: reducing lr to 7.697086519104697e-05
Epoch 26: reducing lr to 6.619096448901517e-05
Epoch 29: reducing lr to 5.431628729849192e-05
Epoch 32: reducing lr to 4.2092963502677066e-05
Epoch 35: reducing lr to 3.0289033965740354e-05
Epoch 38: reducing lr to 1.964617814827305e-05
Epoch 41: reducing lr to 1.0833119806913382e-05
Epoch 44: reducing lr to 4.403628642856284e-06
Epoch 47: reducing lr to 7.616857395244432e-07
[I 2024-06-24 15:40:37,180] Trial 119 finished with value: 1.091817855834961 and parameters: {'hidden_size': 43, 'n_layers': 4, 'rnn_dropout': 0.7002661810836313, 'bidirectional': False, 'fc_dropout': 0.3431458672239224, 'learning_rate_model': 0.0009753415902513679}. Best is trial 68 with value: 0.9740266799926758.
Epoch 18: reducing lr to 0.00024921830395780874
Epoch 21: reducing lr to 0.00022860398186956105
Epoch 24: reducing lr to 0.00020204400852907363
Epoch 27: reducing lr to 0.00017120727258471588
Epoch 30: reducing lr to 0.00013803131448908077
Epoch 33: reducing lr to 0.00010460072399044443
Epoch 36: reducing lr to 7.301607989387038e-05
Epoch 39: reducing lr to 4.526194818646086e-05
Epoch 42: reducing lr to 2.3082208235055247e-05
Epoch 45: reducing lr to 7.870526969989142e-06
Epoch 48: reducing lr to 5.826910740064535e-07
[I 2024-06-24 15:40:40,588] Trial 120 finished with value: 0.9829285144805908 and parameters: {'hidden_size': 45, 'n_layers': 3, 'rnn_dropout': 0.20950432328281618, 'bidirectional': True, 'fc_dropout': 0.3413113266145149, 'learning_rate_model': 0.002679549863220907}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:40:44,488] Trial 121 finished with value: 1.0969408750534058 and parameters: {'hidden_size': 42, 'n_layers': 4, 'rnn_dropout': 0.23755766862952366, 'bidirectional': True, 'fc_dropout': 0.5067792100979368, 'learning_rate_model': 3.155042401877456e-05}. Best is trial 68 with value: 0.9740266799926758.
Epoch 11: reducing lr to 0.00018204869342689317
Epoch 14: reducing lr to 0.00018125607816665474
Epoch 22: reducing lr to 0.00015054674763678244
Epoch 30: reducing lr to 9.431140849160041e-05
Epoch 33: reducing lr to 7.146959112354425e-05
Epoch 36: reducing lr to 4.988903686685452e-05
Epoch 42: reducing lr to 1.5771171764916365e-05
Epoch 45: reducing lr to 5.377623815714109e-06
Epoch 48: reducing lr to 3.9813006279369475e-07
[I 2024-06-24 15:40:49,978] Trial 122 finished with value: 1.0249686241149902 and parameters: {'hidden_size': 103, 'n_layers': 7, 'rnn_dropout': 0.7586393392581321, 'bidirectional': True, 'fc_dropout': 0.511301478975924, 'learning_rate_model': 0.0018308318127610833}. Best is trial 68 with value: 0.9740266799926758.
Epoch 6: reducing lr to 0.0029826619086342643
Epoch 9: reducing lr to 0.004499906686768614
Epoch 12: reducing lr to 0.004996909450778632
Epoch 15: reducing lr to 0.004898192963844913
Epoch 18: reducing lr to 0.004648735311219674
Epoch 21: reducing lr to 0.004264210878276274
Epoch 24: reducing lr to 0.003768780631090738
Epoch 27: reducing lr to 0.003193574793514853
Epoch 30: reducing lr to 0.002574734822961034
Epoch 33: reducing lr to 0.0019511451264664869
Epoch 36: reducing lr to 0.0013619883591974697
Epoch 39: reducing lr to 0.0008442831583695291
Epoch 42: reducing lr to 0.00043055856965222296
Epoch 45: reducing lr to 0.00014681103298692585
Epoch 48: reducing lr to 1.0869091588573687e-05
[I 2024-06-24 15:40:53,422] Trial 123 finished with value: 1.0817463397979736 and parameters: {'hidden_size': 102, 'n_layers': 3, 'rnn_dropout': 0.47941580452642407, 'bidirectional': True, 'fc_dropout': 0.46348843223778646, 'learning_rate_model': 0.0499823563097424}. Best is trial 68 with value: 0.9740266799926758.
Epoch 10: reducing lr to 0.0003002538657073338
Epoch 13: reducing lr to 0.0003113227784106973
Epoch 16: reducing lr to 0.0003019397111907275
Epoch 19: reducing lr to 0.00028339871247560605
Epoch 22: reducing lr to 0.00025686478669703194
Epoch 25: reducing lr to 0.00022400513684875276
Epoch 34: reducing lr to 0.0001093110742447361
Epoch 37: reducing lr to 7.373250852477802e-05
Epoch 40: reducing lr to 4.333511061406e-05
Epoch 43: reducing lr to 2.0028845696740822e-05
Epoch 46: reducing lr to 5.278166050262526e-06
Epoch 49: reducing lr to 9.890522999211893e-09
[I 2024-06-24 15:40:57,048] Trial 124 finished with value: 1.0949748754501343 and parameters: {'hidden_size': 27, 'n_layers': 7, 'rnn_dropout': 0.042395634134391624, 'bidirectional': False, 'fc_dropout': 0.08600710094813416, 'learning_rate_model': 0.0031237886599691338}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:40:59,937] Trial 125 finished with value: 1.0953974723815918 and parameters: {'hidden_size': 24, 'n_layers': 2, 'rnn_dropout': 0.04882934489573687, 'bidirectional': True, 'fc_dropout': 0.3028644746004818, 'learning_rate_model': 5.659486653989453e-05}. Best is trial 68 with value: 0.9740266799926758.
Epoch 7: reducing lr to 0.0002688501172440289
Epoch 10: reducing lr to 0.00036285661354853196
Epoch 13: reducing lr to 0.0003762333877983654
Epoch 16: reducing lr to 0.0003648939567868256
Epoch 19: reducing lr to 0.0003424871711498527
Epoch 22: reducing lr to 0.00031042093803248735
Epoch 25: reducing lr to 0.00027071007123565763
Epoch 29: reducing lr to 0.00021023325411757533
Epoch 33: reducing lr to 0.00014736723220946162
Epoch 36: reducing lr to 0.00010286905472783889
Epoch 39: reducing lr to 6.376751301698575e-05
Epoch 42: reducing lr to 3.2519479895696354e-05
Epoch 45: reducing lr to 1.1088429710134516e-05
Epoch 48: reducing lr to 8.209271172667561e-07
[I 2024-06-24 15:41:02,803] Trial 126 finished with value: 1.0927670001983643 and parameters: {'hidden_size': 154, 'n_layers': 3, 'rnn_dropout': 0.6362409339245143, 'bidirectional': False, 'fc_dropout': 0.022792679593253776, 'learning_rate_model': 0.0037750966900208034}. Best is trial 68 with value: 0.9740266799926758.
Epoch 20: reducing lr to 2.527819989688165e-05
Epoch 23: reducing lr to 2.2629862025718713e-05
Epoch 26: reducing lr to 1.9460511324873234e-05
Epoch 29: reducing lr to 1.5969290253699052e-05
Epoch 32: reducing lr to 1.2375565143445915e-05
Epoch 35: reducing lr to 8.905144275508809e-06
Epoch 38: reducing lr to 5.776085532163442e-06
Epoch 41: reducing lr to 3.1849974133725317e-06
Epoch 44: reducing lr to 1.2946912880996443e-06
Epoch 47: reducing lr to 2.2393983943940973e-07
[I 2024-06-24 15:41:05,325] Trial 127 finished with value: 1.0928308963775635 and parameters: {'hidden_size': 80, 'n_layers': 2, 'rnn_dropout': 0.34710670096936047, 'bidirectional': False, 'fc_dropout': 0.7835866833438452, 'learning_rate_model': 0.00028675584665119325}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:41:09,882] Trial 128 finished with value: 1.0943857431411743 and parameters: {'hidden_size': 159, 'n_layers': 4, 'rnn_dropout': 0.7113048712859863, 'bidirectional': True, 'fc_dropout': 0.7288153917255746, 'learning_rate_model': 1.4932655812821317e-05}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:41:15,313] Trial 129 finished with value: 1.094545841217041 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.30109057334967004, 'bidirectional': True, 'fc_dropout': 0.41336508781408976, 'learning_rate_model': 1.3316445065440956e-05}. Best is trial 68 with value: 0.9740266799926758.
Epoch 12: reducing lr to 0.0006819118440259654
Epoch 16: reducing lr to 0.0006592984754966636
Epoch 22: reducing lr to 0.0005608754198322541
Epoch 25: reducing lr to 0.0004891249469816
Epoch 28: reducing lr to 0.0004080703908376969
Epoch 31: reducing lr to 0.00032280468450913014
Epoch 34: reducing lr to 0.0002386854790323795
Epoch 37: reducing lr to 0.0001609981352675539
Epoch 40: reducing lr to 9.462409648157083e-05
Epoch 43: reducing lr to 4.3733854621984085e-05
Epoch 46: reducing lr to 1.1525104851670739e-05
Epoch 49: reducing lr to 2.1596386608202513e-08
[I 2024-06-24 15:41:18,245] Trial 130 finished with value: 1.0315470695495605 and parameters: {'hidden_size': 121, 'n_layers': 2, 'rnn_dropout': 0.5047673602716244, 'bidirectional': True, 'fc_dropout': 0.36146899953669653, 'learning_rate_model': 0.006820928234876919}. Best is trial 68 with value: 0.9740266799926758.
Epoch 8: reducing lr to 0.0006591865136568047
Epoch 11: reducing lr to 0.0008037373109360211
Epoch 14: reducing lr to 0.0008002379479585727
Epoch 17: reducing lr to 0.0007678136295000711
Epoch 23: reducing lr to 0.0006378883021739446
Epoch 27: reducing lr to 0.000516458439921393
Epoch 31: reducing lr to 0.0003825351626601084
Epoch 34: reducing lr to 0.0002828508783418064
Epoch 37: reducing lr to 0.00019078858151082975
Epoch 40: reducing lr to 0.00011213295802750249
Epoch 43: reducing lr to 5.1826190865272414e-05
Epoch 46: reducing lr to 1.3657663815544625e-05
Epoch 49: reducing lr to 2.5592494968390554e-08
[I 2024-06-24 15:41:21,463] Trial 131 finished with value: 1.0943915843963623 and parameters: {'hidden_size': 159, 'n_layers': 4, 'rnn_dropout': 0.4228267921420521, 'bidirectional': False, 'fc_dropout': 0.11377700245822933, 'learning_rate_model': 0.008083045312026036}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:41:25,408] Trial 132 finished with value: 1.096447467803955 and parameters: {'hidden_size': 104, 'n_layers': 4, 'rnn_dropout': 0.7087983769322811, 'bidirectional': True, 'fc_dropout': 0.2151136177701167, 'learning_rate_model': 1.941101032854524e-05}. Best is trial 68 with value: 0.9740266799926758.
Epoch 10: reducing lr to 0.00047311986192689095
Epoch 15: reducing lr to 0.0004823734330784113
Epoch 22: reducing lr to 0.00040475026734355777
Epoch 27: reducing lr to 0.000314502847950523
Epoch 30: reducing lr to 0.0002535595647181966
Epoch 33: reducing lr to 0.0001921485290667389
Epoch 36: reducing lr to 0.0001341284439972744
Epoch 39: reducing lr to 8.31449002926395e-05
Epoch 42: reducing lr to 4.240135431933731e-05
Epoch 45: reducing lr to 1.4457932245303274e-05
Epoch 48: reducing lr to 1.070386786050214e-06
[I 2024-06-24 15:41:27,657] Trial 133 finished with value: 1.0929746627807617 and parameters: {'hidden_size': 93, 'n_layers': 1, 'rnn_dropout': 0.23292315244194245, 'bidirectional': False, 'fc_dropout': 0.08750633135486768, 'learning_rate_model': 0.004922256224784005}. Best is trial 68 with value: 0.9740266799926758.
Epoch 28: reducing lr to 3.496186241426402e-06
Epoch 31: reducing lr to 2.7656632825822624e-06
Epoch 34: reducing lr to 2.0449630910692042e-06
Epoch 37: reducing lr to 1.3793685551706802e-06
Epoch 40: reducing lr to 8.107019564618489e-07
Epoch 43: reducing lr to 3.746944258808949e-07
Epoch 46: reducing lr to 9.874255500550155e-08
Epoch 49: reducing lr to 1.8502932685025993e-10
[I 2024-06-24 15:41:31,092] Trial 134 finished with value: 1.106812834739685 and parameters: {'hidden_size': 108, 'n_layers': 6, 'rnn_dropout': 0.11488559829936743, 'bidirectional': False, 'fc_dropout': 0.6375698015839202, 'learning_rate_model': 5.8439024207513246e-05}. Best is trial 68 with value: 0.9740266799926758.
Epoch 9: reducing lr to 0.00039063083923929133
Epoch 14: reducing lr to 0.0004295600394046692
Epoch 17: reducing lr to 0.00041215497688515894
Epoch 20: reducing lr to 0.00038248429471522607
Epoch 23: reducing lr to 0.0003424123098843625
Epoch 27: reducing lr to 0.00027722992688543736
Epoch 33: reducing lr to 0.00016937628072829594
Epoch 36: reducing lr to 0.00011823237520720842
Epoch 39: reducing lr to 7.329108394163618e-05
Epoch 42: reducing lr to 3.7376209577735364e-05
Epoch 45: reducing lr to 1.2744468056170335e-05
Epoch 48: reducing lr to 9.435312028796376e-07
[I 2024-06-24 15:41:33,966] Trial 135 finished with value: 1.0925893783569336 and parameters: {'hidden_size': 158, 'n_layers': 3, 'rnn_dropout': 0.17351319008943628, 'bidirectional': False, 'fc_dropout': 0.3048651930347619, 'learning_rate_model': 0.004338901037624098}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:41:36,472] Trial 136 finished with value: 1.0957458019256592 and parameters: {'hidden_size': 29, 'n_layers': 2, 'rnn_dropout': 0.013260706320747318, 'bidirectional': False, 'fc_dropout': 0.3221254817399354, 'learning_rate_model': 0.00019235578705946124}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:41:40,125] Trial 137 finished with value: 1.106951355934143 and parameters: {'hidden_size': 119, 'n_layers': 7, 'rnn_dropout': 0.6675892112144237, 'bidirectional': False, 'fc_dropout': 0.03178296534646785, 'learning_rate_model': 1.9113215939952844e-05}. Best is trial 68 with value: 0.9740266799926758.
Epoch 13: reducing lr to 6.096343605583295e-05
Epoch 16: reducing lr to 5.912603751598827e-05
Epoch 19: reducing lr to 5.54953266654977e-05
Epoch 22: reducing lr to 5.02994354564761e-05
Epoch 25: reducing lr to 4.386483670154672e-05
Epoch 28: reducing lr to 3.659584563676996e-05
Epoch 31: reducing lr to 2.8949197663843484e-05
Epoch 34: reducing lr to 2.1405368148559465e-05
Epoch 37: reducing lr to 1.443834945722048e-05
Epoch 40: reducing lr to 8.48591053433152e-06
Epoch 43: reducing lr to 3.922062048073544e-06
Epoch 46: reducing lr to 1.033574029307802e-06
Epoch 49: reducing lr to 1.936768872167448e-09
[I 2024-06-24 15:41:42,782] Trial 138 finished with value: 1.0920612812042236 and parameters: {'hidden_size': 103, 'n_layers': 3, 'rnn_dropout': 0.7498016497027592, 'bidirectional': False, 'fc_dropout': 0.7162082925849766, 'learning_rate_model': 0.0006117023983794077}. Best is trial 68 with value: 0.9740266799926758.
Epoch 6: reducing lr to 0.00028940363792888315
Epoch 9: reducing lr to 0.00043661984005678045
Epoch 12: reducing lr to 0.00048484334388362545
Epoch 15: reducing lr to 0.00047526501710127195
Epoch 18: reducing lr to 0.00045106047954709754
Epoch 21: reducing lr to 0.0004137505955658733
Epoch 27: reducing lr to 0.00030986822896879334
Epoch 33: reducing lr to 0.00018931696418289962
Epoch 36: reducing lr to 0.00013215188246026277
Epoch 39: reducing lr to 8.191964927936127e-05
Epoch 42: reducing lr to 4.1776513804028716e-05
Epoch 45: reducing lr to 1.4244875328148829e-05
Epoch 48: reducing lr to 1.054613208962673e-06
[I 2024-06-24 15:41:45,332] Trial 139 finished with value: 1.092674732208252 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.21904776332648687, 'bidirectional': False, 'fc_dropout': 0.176102939221585, 'learning_rate_model': 0.004849720213485595}. Best is trial 68 with value: 0.9740266799926758.
Epoch 14: reducing lr to 0.00012556020282467685
Epoch 24: reducing lr to 9.562952815373323e-05
Epoch 27: reducing lr to 8.103418068647163e-05
Epoch 30: reducing lr to 6.533165507420097e-05
Epoch 33: reducing lr to 4.950860930036285e-05
Epoch 36: reducing lr to 3.455926913507732e-05
Epoch 39: reducing lr to 2.142295027653409e-05
Epoch 42: reducing lr to 1.0925048945200082e-05
Epoch 45: reducing lr to 3.7252021771929517e-06
Epoch 48: reducing lr to 2.757937385637144e-07
[I 2024-06-24 15:41:47,581] Trial 140 finished with value: 1.0923454761505127 and parameters: {'hidden_size': 42, 'n_layers': 1, 'rnn_dropout': 0.6961533969643021, 'bidirectional': False, 'fc_dropout': 0.45723367418026584, 'learning_rate_model': 0.0012682587865373047}. Best is trial 68 with value: 0.9740266799926758.
Epoch 28: reducing lr to 1.3631966175502524e-05
Epoch 31: reducing lr to 1.078358694804798e-05
Epoch 34: reducing lr to 7.973507634488332e-06
Epoch 37: reducing lr to 5.378290568401438e-06
Epoch 41: reducing lr to 2.5308342231598132e-06
Epoch 44: reducing lr to 1.0287760381192465e-06
Epoch 47: reducing lr to 1.779450768790578e-07
[I 2024-06-24 15:41:52,916] Trial 141 finished with value: 1.0786669254302979 and parameters: {'hidden_size': 30, 'n_layers': 7, 'rnn_dropout': 0.7609084947349537, 'bidirectional': True, 'fc_dropout': 0.4222774368943676, 'learning_rate_model': 0.0002278593719884827}. Best is trial 68 with value: 0.9740266799926758.
Epoch 23: reducing lr to 1.586388273946441e-05
Epoch 26: reducing lr to 1.364211895578287e-05
Epoch 32: reducing lr to 8.675462274012425e-06
Epoch 42: reducing lr to 1.7316311034117998e-06
[I 2024-06-24 15:41:58,400] Trial 142 finished with value: 1.072595477104187 and parameters: {'hidden_size': 118, 'n_layers': 7, 'rnn_dropout': 0.24936233417518514, 'bidirectional': True, 'fc_dropout': 0.6195818128120637, 'learning_rate_model': 0.00020102027670166017}. Best is trial 68 with value: 0.9740266799926758.
Epoch 19: reducing lr to 1.7580508348272806e-05
Epoch 23: reducing lr to 1.5292723082258405e-05
[I 2024-06-24 15:42:05,838] Trial 143 finished with value: 1.0669949054718018 and parameters: {'hidden_size': 180, 'n_layers': 6, 'rnn_dropout': 0.6101056763560886, 'bidirectional': True, 'fc_dropout': 0.0331800636611991, 'learning_rate_model': 0.0001937827879848057}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:42:08,959] Trial 144 finished with value: 1.1090397834777832 and parameters: {'hidden_size': 22, 'n_layers': 5, 'rnn_dropout': 0.5362800255489347, 'bidirectional': False, 'fc_dropout': 0.428806795791651, 'learning_rate_model': 1.4435761775167338e-05}. Best is trial 68 with value: 0.9740266799926758.
Epoch 10: reducing lr to 0.00028356394625716065
Epoch 13: reducing lr to 0.00029401758208145735
Epoch 16: reducing lr to 0.0002851560822881981
Epoch 22: reducing lr to 0.00024258669375904434
Epoch 25: reducing lr to 0.00021155358129051443
Epoch 28: reducing lr to 0.0001764963188507787
Epoch 31: reducing lr to 0.00013961767333006242
Epoch 34: reducing lr to 0.00010323490593343502
Epoch 37: reducing lr to 6.963401132395515e-05
Epoch 40: reducing lr to 4.0926283990598643e-05
Epoch 43: reducing lr to 1.8915521741456975e-05
Epoch 46: reducing lr to 4.984773770312992e-06
Epoch 49: reducing lr to 9.340748121896073e-09
[I 2024-06-24 15:42:11,457] Trial 145 finished with value: 1.092465877532959 and parameters: {'hidden_size': 40, 'n_layers': 2, 'rnn_dropout': 0.2685310409317198, 'bidirectional': False, 'fc_dropout': 0.40964882201913505, 'learning_rate_model': 0.0029501496595471795}. Best is trial 68 with value: 0.9740266799926758.
Epoch 9: reducing lr to 0.0017635953476174894
Epoch 13: reducing lr to 0.0019522769030626478
Epoch 16: reducing lr to 0.00189343653967213
Epoch 23: reducing lr to 0.001545901388264686
Epoch 26: reducing lr to 0.0013293952671594657
Epoch 29: reducing lr to 0.001090901391477266
Epoch 32: reducing lr to 0.0008454052134330455
Epoch 35: reducing lr to 0.0006083322506589241
Epoch 38: reducing lr to 0.00039457857201069015
Epoch 41: reducing lr to 0.00021757498642087543
Epoch 44: reducing lr to 8.844353789575835e-05
Epoch 47: reducing lr to 1.52978797786623e-05
[I 2024-06-24 15:42:14,869] Trial 146 finished with value: 1.1360998153686523 and parameters: {'hidden_size': 95, 'n_layers': 6, 'rnn_dropout': 0.36324724484732107, 'bidirectional': False, 'fc_dropout': 0.5249612381952792, 'learning_rate_model': 0.01958899532517217}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:42:17,186] Trial 147 finished with value: 1.1014584302902222 and parameters: {'hidden_size': 192, 'n_layers': 1, 'rnn_dropout': 0.18805749127281166, 'bidirectional': False, 'fc_dropout': 0.6910633164011616, 'learning_rate_model': 2.1207815222486866e-05}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:42:19,718] Trial 148 finished with value: 1.066739797592163 and parameters: {'hidden_size': 103, 'n_layers': 1, 'rnn_dropout': 0.6183084817336942, 'bidirectional': True, 'fc_dropout': 0.38915259453631584, 'learning_rate_model': 9.070770449331516e-05}. Best is trial 68 with value: 0.9740266799926758.
Epoch 12: reducing lr to 5.47349400987404e-05
Epoch 37: reducing lr to 1.2922818934851809e-05
Epoch 47: reducing lr to 4.2756187669864154e-07
[I 2024-06-24 15:42:29,002] Trial 149 finished with value: 1.0343329906463623 and parameters: {'hidden_size': 192, 'n_layers': 7, 'rnn_dropout': 0.49114720190776073, 'bidirectional': True, 'fc_dropout': 0.7182805741341455, 'learning_rate_model': 0.0005474946675652392}. Best is trial 68 with value: 0.9740266799926758.
Epoch 6: reducing lr to 0.00015685370938765133
Epoch 9: reducing lr to 0.00023664333314973176
Epoch 12: reducing lr to 0.00026277996193934284
Epoch 15: reducing lr to 0.00025758861017786836
Epoch 18: reducing lr to 0.000244470006947601
Epoch 21: reducing lr to 0.00022424844463014314
Epoch 24: reducing lr to 0.00019819451213818495
Epoch 27: reducing lr to 0.00016794530118201568
Epoch 30: reducing lr to 0.00013540143671728397
Epoch 33: reducing lr to 0.00010260779130010132
Epoch 36: reducing lr to 7.162492190767431e-05
Epoch 39: reducing lr to 4.439958306384803e-05
Epoch 42: reducing lr to 2.264242841707666e-05
Epoch 45: reducing lr to 7.720571693483127e-06
Epoch 48: reducing lr to 5.715891997033415e-07
[I 2024-06-24 15:42:32,665] Trial 150 finished with value: 1.1073637008666992 and parameters: {'hidden_size': 59, 'n_layers': 7, 'rnn_dropout': 0.547374440786582, 'bidirectional': False, 'fc_dropout': 0.561104235706677, 'learning_rate_model': 0.002628497037637164}. Best is trial 68 with value: 0.9740266799926758.
Epoch 11: reducing lr to 0.0023298199296875686
Epoch 14: reducing lr to 0.002319676210470932
Epoch 17: reducing lr to 0.0022256867660053296
Epoch 23: reducing lr to 0.0018490679219416296
Epoch 26: reducing lr to 0.0015901028116967531
Epoch 29: reducing lr to 0.0013048379309927422
Epoch 32: reducing lr to 0.0010111975272601359
Epoch 35: reducing lr to 0.0007276322145221969
Epoch 38: reducing lr to 0.00047195932789057123
Epoch 41: reducing lr to 0.0002602435906078917
Epoch 44: reducing lr to 0.00010578818938098699
Epoch 47: reducing lr to 1.8297945125850603e-05
[I 2024-06-24 15:42:35,869] Trial 151 finished with value: 1.1454139947891235 and parameters: {'hidden_size': 151, 'n_layers': 2, 'rnn_dropout': 0.4803911642196355, 'bidirectional': True, 'fc_dropout': 0.10440853707141279, 'learning_rate_model': 0.023430590821514066}. Best is trial 68 with value: 0.9740266799926758.
[I 2024-06-24 15:42:39,049] Trial 152 finished with value: 1.1064059734344482 and parameters: {'hidden_size': 130, 'n_layers': 4, 'rnn_dropout': 0.19863258319767887, 'bidirectional': False, 'fc_dropout': 0.3373327478797674, 'learning_rate_model': 1.8101769159892324e-05}. Best is trial 68 with value: 0.9740266799926758.
Epoch 23: reducing lr to 0.00010144705652301547
Epoch 27: reducing lr to 8.213536502854218e-05
Epoch 37: reducing lr to 3.0342208732332632e-05
Epoch 40: reducing lr to 1.7833151183899577e-05
Epoch 43: reducing lr to 8.242218106467472e-06
Epoch 46: reducing lr to 2.1720570644515393e-06
Epoch 49: reducing lr to 4.070122111934395e-09
[I 2024-06-24 15:42:42,337] Trial 153 finished with value: 0.969097375869751 and parameters: {'hidden_size': 181, 'n_layers': 2, 'rnn_dropout': 0.6609820283939132, 'bidirectional': True, 'fc_dropout': 0.4787717544837091, 'learning_rate_model': 0.0012854933251677597}. Best is trial 153 with value: 0.969097375869751.
Epoch 24: reducing lr to 6.100828180268109e-06
Epoch 27: reducing lr to 5.169696250118581e-06
Epoch 30: reducing lr to 4.167930241164548e-06
Epoch 34: reducing lr to 2.8313084114356693e-06
Epoch 37: reducing lr to 1.9097742202685293e-06
Epoch 40: reducing lr to 1.1224394604098543e-06
Epoch 43: reducing lr to 5.187748787974256e-07
Epoch 46: reducing lr to 1.3671182026446825e-07
Epoch 49: reducing lr to 2.5617826148572427e-10
[I 2024-06-24 15:42:48,107] Trial 154 finished with value: 1.075117826461792 and parameters: {'hidden_size': 131, 'n_layers': 6, 'rnn_dropout': 0.22571155872388848, 'bidirectional': True, 'fc_dropout': 0.09208243277352271, 'learning_rate_model': 8.091045824612664e-05}. Best is trial 153 with value: 0.969097375869751.
Epoch 6: reducing lr to 0.004266341842232598
Epoch 9: reducing lr to 0.006436579395179877
Epoch 12: reducing lr to 0.007147482525589351
Epoch 15: reducing lr to 0.007006280374080205
Epoch 18: reducing lr to 0.00664946097789611
Epoch 21: reducing lr to 0.0060994446743793725
Epoch 24: reducing lr to 0.0053907908415877365
Epoch 27: reducing lr to 0.004568027548958931
Epoch 30: reducing lr to 0.0036828508373856553
Epoch 33: reducing lr to 0.002790880209793469
Epoch 36: reducing lr to 0.0019481617774568887
Epoch 39: reducing lr to 0.001207646282274593
Epoch 42: reducing lr to 0.000615862641327726
Epoch 45: reducing lr to 0.0002099956589516061
Epoch 48: reducing lr to 1.5546938155193423e-05
[I 2024-06-24 15:42:50,648] Trial 155 finished with value: 1.1618330478668213 and parameters: {'hidden_size': 126, 'n_layers': 2, 'rnn_dropout': 0.10513252842996845, 'bidirectional': False, 'fc_dropout': 0.48174692311416867, 'learning_rate_model': 0.07149379468062945}. Best is trial 153 with value: 0.969097375869751.
Epoch 9: reducing lr to 0.005587593129092662
Epoch 14: reducing lr to 0.006144437365427751
Epoch 18: reducing lr to 0.005772395583294712
Epoch 21: reducing lr to 0.005294926553592301
Epoch 24: reducing lr to 0.004679744320312156
Epoch 27: reducing lr to 0.0039655036905445655
Epoch 30: reducing lr to 0.0031970819857919516
Epoch 33: reducing lr to 0.002422762484067343
Epoch 36: reducing lr to 0.0016911988019958004
Epoch 39: reducing lr to 0.0010483574667415779
Epoch 42: reducing lr to 0.0005346302207853828
Epoch 45: reducing lr to 0.00018229718442935375
Epoch 48: reducing lr to 1.3496293525012277e-05
[I 2024-06-24 15:42:55,055] Trial 156 finished with value: 1.0257993936538696 and parameters: {'hidden_size': 79, 'n_layers': 5, 'rnn_dropout': 0.4791882311787358, 'bidirectional': True, 'fc_dropout': 0.6002537476597487, 'learning_rate_model': 0.06206374712466088}. Best is trial 153 with value: 0.969097375869751.
Epoch 9: reducing lr to 0.00020316837802482603
Epoch 15: reducing lr to 0.0002211507902248546
Epoch 22: reducing lr to 0.00018556337337134962
Epoch 25: reducing lr to 0.0001618250184490768
Epoch 28: reducing lr to 0.00013500844504730697
Epoch 31: reducing lr to 0.00010679862957000944
Epoch 34: reducing lr to 7.896812928127861e-05
Epoch 37: reducing lr to 5.326558453155109e-05
Epoch 40: reducing lr to 3.1306001162589313e-05
Epoch 43: reducing lr to 1.4469169635949976e-05
Epoch 46: reducing lr to 3.8130345155331332e-06
Epoch 49: reducing lr to 7.14507751619145e-09
[I 2024-06-24 15:42:57,590] Trial 157 finished with value: 1.0925853252410889 and parameters: {'hidden_size': 114, 'n_layers': 2, 'rnn_dropout': 0.6023407319839538, 'bidirectional': False, 'fc_dropout': 0.7340850781155146, 'learning_rate_model': 0.002256676630910648}. Best is trial 153 with value: 0.969097375869751.
Epoch 16: reducing lr to 0.0007420821206774973
Epoch 19: reducing lr to 0.0006965136077060289
Epoch 22: reducing lr to 0.0006313007483772151
Epoch 26: reducing lr to 0.0005210211371769301
Epoch 29: reducing lr to 0.000427549802211836
Epoch 32: reducing lr to 0.00033133410097010237
Epoch 35: reducing lr to 0.00023841965504883518
Epoch 38: reducing lr to 0.00015464458267098555
Epoch 41: reducing lr to 8.527273238190446e-05
Epoch 44: reducing lr to 3.4663093685342006e-05
Epoch 47: reducing lr to 5.995597333293724e-06
[I 2024-06-24 15:43:03,750] Trial 158 finished with value: 1.0598922967910767 and parameters: {'hidden_size': 174, 'n_layers': 5, 'rnn_dropout': 0.6580625220190969, 'bidirectional': True, 'fc_dropout': 0.0983638172417428, 'learning_rate_model': 0.007677386005956413}. Best is trial 153 with value: 0.969097375869751.
[I 2024-06-24 15:43:05,997] Trial 159 finished with value: 1.1041940450668335 and parameters: {'hidden_size': 97, 'n_layers': 1, 'rnn_dropout': 0.4718589184879114, 'bidirectional': False, 'fc_dropout': 0.6306409517343752, 'learning_rate_model': 1.1178590317786991e-05}. Best is trial 153 with value: 0.969097375869751.
Epoch 33: reducing lr to 4.16821912117427e-05
Epoch 38: reducing lr to 2.1507950503213046e-05
Epoch 41: reducing lr to 1.185972166413196e-05
Epoch 44: reducing lr to 4.820939022860769e-06
Epoch 47: reducing lr to 8.338669771318797e-07
[I 2024-06-24 15:43:08,474] Trial 160 finished with value: 0.9850569367408752 and parameters: {'hidden_size': 72, 'n_layers': 1, 'rnn_dropout': 0.7979879941826636, 'bidirectional': True, 'fc_dropout': 0.7431595695144926, 'learning_rate_model': 0.001067769949377941}. Best is trial 153 with value: 0.969097375869751.
Epoch 12: reducing lr to 0.00021780799538225897
Epoch 16: reducing lr to 0.00021058510797920642
Epoch 22: reducing lr to 0.00017914801146670583
Epoch 25: reducing lr to 0.00015623034726092747
Epoch 29: reducing lr to 0.00012132837964492146
Epoch 32: reducing lr to 9.402467124026761e-05
Epoch 35: reducing lr to 6.765777991927111e-05
Epoch 38: reducing lr to 4.38844236139758e-05
Epoch 41: reducing lr to 2.4198356295029715e-05
Epoch 44: reducing lr to 9.836554638935173e-06
Epoch 47: reducing lr to 1.7014067266285516e-06
[I 2024-06-24 15:43:11,599] Trial 161 finished with value: 1.092912197113037 and parameters: {'hidden_size': 100, 'n_layers': 5, 'rnn_dropout': 0.2113101153684557, 'bidirectional': False, 'fc_dropout': 0.1955583994972887, 'learning_rate_model': 0.0021786580164286193}. Best is trial 153 with value: 0.969097375869751.
[I 2024-06-24 15:43:13,858] Trial 162 finished with value: 1.1017955541610718 and parameters: {'hidden_size': 144, 'n_layers': 1, 'rnn_dropout': 0.7768666557003711, 'bidirectional': False, 'fc_dropout': 0.6013123232650325, 'learning_rate_model': 2.0904698538829833e-05}. Best is trial 153 with value: 0.969097375869751.
Epoch 9: reducing lr to 0.0005533257698086839
Epoch 15: reducing lr to 0.0006023005766675643
Epoch 18: reducing lr to 0.0005716263077811916
Epoch 22: reducing lr to 0.0005053788262583447
Epoch 28: reducing lr to 0.00036769330204204393
Epoch 31: reducing lr to 0.00029086432886773816
Epoch 34: reducing lr to 0.0002150684144339424
Epoch 37: reducing lr to 0.00014506795226582248
Epoch 40: reducing lr to 8.526138447984555e-05
Epoch 43: reducing lr to 3.9406547933980904e-05
Epoch 46: reducing lr to 1.0384737423835885e-05
Epoch 49: reducing lr to 1.945950228784271e-08
[I 2024-06-24 15:43:16,734] Trial 163 finished with value: 1.0939090251922607 and parameters: {'hidden_size': 105, 'n_layers': 4, 'rnn_dropout': 0.537978120419944, 'bidirectional': False, 'fc_dropout': 0.5453162700820984, 'learning_rate_model': 0.006146022063804241}. Best is trial 153 with value: 0.969097375869751.
[I 2024-06-24 15:43:19,210] Trial 164 finished with value: 1.0968859195709229 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.6086419650446049, 'bidirectional': False, 'fc_dropout': 0.18701495408328475, 'learning_rate_model': 5.0899740979609974e-05}. Best is trial 153 with value: 0.969097375869751.
[I 2024-06-24 15:43:21,823] Trial 165 finished with value: 1.1069068908691406 and parameters: {'hidden_size': 88, 'n_layers': 3, 'rnn_dropout': 0.652381543204501, 'bidirectional': False, 'fc_dropout': 0.3421320797575714, 'learning_rate_model': 2.4989769577785342e-05}. Best is trial 153 with value: 0.969097375869751.
Epoch 42: reducing lr to 7.936981293661649e-06
Epoch 45: reducing lr to 2.7063366163204487e-06
Epoch 48: reducing lr to 2.0036246563919208e-07
[I 2024-06-24 15:43:25,165] Trial 166 finished with value: 0.9799264669418335 and parameters: {'hidden_size': 37, 'n_layers': 3, 'rnn_dropout': 0.15507821400760574, 'bidirectional': True, 'fc_dropout': 0.551076207214932, 'learning_rate_model': 0.0009213822578516836}. Best is trial 153 with value: 0.969097375869751.
Epoch 18: reducing lr to 0.0007382187673561208
Epoch 22: reducing lr to 0.0006526643877124125
Epoch 25: reducing lr to 0.0005691717318118316
Epoch 28: reducing lr to 0.0004748523510966211
Epoch 31: reducing lr to 0.00037563265266440227
Epoch 34: reducing lr to 0.0002777470834345045
Epoch 37: reducing lr to 0.0001873459696427145
Epoch 40: reducing lr to 0.00011010961758933045
Epoch 43: reducing lr to 5.089103291012059e-05
Epoch 46: reducing lr to 1.3411223304431348e-05
Epoch 49: reducing lr to 2.513070094366937e-08
[I 2024-06-24 15:43:28,003] Trial 167 finished with value: 0.9948239922523499 and parameters: {'hidden_size': 32, 'n_layers': 2, 'rnn_dropout': 0.36928166471329765, 'bidirectional': True, 'fc_dropout': 0.5835181586049222, 'learning_rate_model': 0.00793719388055494}. Best is trial 153 with value: 0.969097375869751.
Epoch 35: reducing lr to 1.9725981918004375e-06
Epoch 42: reducing lr to 5.471740135755692e-07
Epoch 45: reducing lr to 1.8657434276948082e-07
Epoch 48: reducing lr to 1.3812951100344786e-08
[I 2024-06-24 15:43:32,857] Trial 168 finished with value: 1.0758516788482666 and parameters: {'hidden_size': 77, 'n_layers': 6, 'rnn_dropout': 0.3427539807608588, 'bidirectional': True, 'fc_dropout': 0.46482852593447216, 'learning_rate_model': 6.351992141755928e-05}. Best is trial 153 with value: 0.969097375869751.
Epoch 5: reducing lr to 0.0002572509958261362
Epoch 8: reducing lr to 0.00044028183260066565
Epoch 11: reducing lr to 0.0005368297573707338
Epoch 14: reducing lr to 0.0005344924735933415
Epoch 17: reducing lr to 0.0005128357223461988
Epoch 20: reducing lr to 0.0004759171199357229
Epoch 23: reducing lr to 0.00042605639656926
Epoch 42: reducing lr to 4.650642722361618e-05
Epoch 45: reducing lr to 1.5857672108919586e-05
Epoch 48: reducing lr to 1.1740159239174465e-06
[I 2024-06-24 15:43:36,499] Trial 169 finished with value: 1.0942859649658203 and parameters: {'hidden_size': 113, 'n_layers': 7, 'rnn_dropout': 0.24399518066817283, 'bidirectional': False, 'fc_dropout': 0.2638756501455059, 'learning_rate_model': 0.005398802811105274}. Best is trial 153 with value: 0.969097375869751.
Epoch 20: reducing lr to 2.1017964017664224e-05
Epoch 23: reducing lr to 1.8815961093809398e-05
Epoch 26: reducing lr to 1.6180753711105434e-05
Epoch 29: reducing lr to 1.327792205572707e-05
Epoch 32: reducing lr to 1.0289861775928644e-05
Epoch 35: reducing lr to 7.404324782550752e-06
Epoch 38: reducing lr to 4.802618792999514e-06
Epoch 41: reducing lr to 2.648217092344286e-06
Epoch 44: reducing lr to 1.0764917999805262e-06
Epoch 47: reducing lr to 1.8619836486219747e-07
[I 2024-06-24 15:43:38,780] Trial 170 finished with value: 1.0921897888183594 and parameters: {'hidden_size': 182, 'n_layers': 1, 'rnn_dropout': 0.7028702905638164, 'bidirectional': False, 'fc_dropout': 0.5336950309593632, 'learning_rate_model': 0.0002384277397661184}. Best is trial 153 with value: 0.969097375869751.
[I 2024-06-24 15:43:43,137] Trial 171 finished with value: 0.9962334632873535 and parameters: {'hidden_size': 44, 'n_layers': 5, 'rnn_dropout': 0.6782304349717934, 'bidirectional': True, 'fc_dropout': 0.7650412078041857, 'learning_rate_model': 0.0022892462334419333}. Best is trial 153 with value: 0.969097375869751.
Epoch 37: reducing lr to 2.8066894917747e-05
Epoch 42: reducing lr to 1.0243125987639063e-05
Epoch 45: reducing lr to 3.4926813986658904e-06
Epoch 48: reducing lr to 2.5857916288341434e-07
[I 2024-06-24 15:43:47,001] Trial 172 finished with value: 0.9779380559921265 and parameters: {'hidden_size': 52, 'n_layers': 4, 'rnn_dropout': 0.2693967989609338, 'bidirectional': True, 'fc_dropout': 0.7941606989318544, 'learning_rate_model': 0.0011890962320255246}. Best is trial 153 with value: 0.969097375869751.
Epoch 5: reducing lr to 0.004382502064575645
Epoch 8: reducing lr to 0.007500596972116863
Epoch 11: reducing lr to 0.009145377697946534
Epoch 14: reducing lr to 0.009105559966835227
Epoch 17: reducing lr to 0.008736617733014877
Epoch 20: reducing lr to 0.008107676139356253
Epoch 23: reducing lr to 0.00725825387611867
Epoch 26: reducing lr to 0.0062417230646162735
Epoch 29: reducing lr to 0.005121956234246816
Epoch 32: reducing lr to 0.003969312476120704
Epoch 35: reducing lr to 0.002856217058754028
Epoch 38: reducing lr to 0.0018526094041126512
Epoch 41: reducing lr to 0.0010215493048418138
Epoch 44: reducing lr to 0.00041525653358144424
Epoch 47: reducing lr to 7.182598841218024e-05
[I 2024-06-24 15:43:49,233] Trial 173 finished with value: 1.1582368612289429 and parameters: {'hidden_size': 47, 'n_layers': 1, 'rnn_dropout': 0.2962445550191101, 'bidirectional': False, 'fc_dropout': 0.3311358765835271, 'learning_rate_model': 0.0919734611324751}. Best is trial 153 with value: 0.969097375869751.
Epoch 10: reducing lr to 8.542453036356333e-05
Epoch 13: reducing lr to 8.857372102291613e-05
Epoch 16: reducing lr to 8.590416634874935e-05
Epoch 19: reducing lr to 8.062910984288403e-05
Epoch 22: reducing lr to 7.308000421189872e-05
Epoch 25: reducing lr to 6.373118150952436e-05
Epoch 28: reducing lr to 5.317007097599162e-05
Epoch 31: reducing lr to 4.206026306270171e-05
Epoch 34: reducing lr to 3.109983999338431e-05
Epoch 37: reducing lr to 2.0977464847683216e-05
Epoch 40: reducing lr to 1.2329171728524799e-05
Epoch 43: reducing lr to 5.6983603969185135e-06
Epoch 46: reducing lr to 1.5016787709373498e-06
Epoch 49: reducing lr to 2.813929739979397e-09
[I 2024-06-24 15:43:51,791] Trial 174 finished with value: 1.0924094915390015 and parameters: {'hidden_size': 188, 'n_layers': 2, 'rnn_dropout': 0.3159058100294404, 'bidirectional': False, 'fc_dropout': 0.6242908186314149, 'learning_rate_model': 0.0008887418605061119}. Best is trial 153 with value: 0.969097375869751.
Epoch 7: reducing lr to 0.0003941030059492007
Epoch 10: reducing lr to 0.0005319055970439603
Epoch 13: reducing lr to 0.0005515143924419481
Epoch 16: reducing lr to 0.0005348921052984201
Epoch 19: reducing lr to 0.000502046363352267
Epoch 22: reducing lr to 0.00045504099474552454
Epoch 25: reducing lr to 0.00039682948219753637
Epoch 28: reducing lr to 0.00033106952097939344
Epoch 31: reducing lr to 0.00026189303284405166
Epoch 34: reducing lr to 0.00019364670650514406
Epoch 37: reducing lr to 0.00013061858129962794
Epoch 40: reducing lr to 7.67689962286957e-05
Epoch 43: reducing lr to 3.5481491981224045e-05
Epoch 46: reducing lr to 9.350374416156743e-06
Epoch 49: reducing lr to 1.752125498386686e-08
[I 2024-06-24 15:43:54,284] Trial 175 finished with value: 1.0931371450424194 and parameters: {'hidden_size': 93, 'n_layers': 2, 'rnn_dropout': 0.7286449112694068, 'bidirectional': False, 'fc_dropout': 0.1507747197331348, 'learning_rate_model': 0.005533852722614425}. Best is trial 153 with value: 0.969097375869751.
Epoch 14: reducing lr to 1.7085577683815538e-05
Epoch 17: reducing lr to 1.6393298327055785e-05
Epoch 20: reducing lr to 1.5213158885200896e-05
Epoch 24: reducing lr to 1.30127675440177e-05
Epoch 27: reducing lr to 1.1026708766123079e-05
Epoch 30: reducing lr to 8.889990959485177e-06
Epoch 33: reducing lr to 6.736873397697135e-06
Epoch 36: reducing lr to 4.702645139302098e-06
Epoch 39: reducing lr to 2.915123366576029e-06
Epoch 42: reducing lr to 1.4866236932839493e-06
Epoch 45: reducing lr to 5.069060877133265e-07
Epoch 48: reducing lr to 3.7528573854883855e-08
[I 2024-06-24 15:43:57,913] Trial 176 finished with value: 1.1066570281982422 and parameters: {'hidden_size': 81, 'n_layers': 7, 'rnn_dropout': 0.09821770355953596, 'bidirectional': False, 'fc_dropout': 0.46672505168658235, 'learning_rate_model': 0.0001725780425093903}. Best is trial 153 with value: 0.969097375869751.
Epoch 14: reducing lr to 0.000626328075025841
Epoch 17: reducing lr to 0.0006009502970587438
Epoch 22: reducing lr to 0.0005202123735792803
Epoch 25: reducing lr to 0.0004536637560658978
Epoch 28: reducing lr to 0.0003784855943029065
Epoch 31: reducing lr to 0.0002994015875775564
Epoch 34: reducing lr to 0.00022138096125424322
Epoch 37: reducing lr to 0.00014932589150442894
Epoch 40: reducing lr to 8.77639206282115e-05
Epoch 43: reducing lr to 4.056318304246206e-05
Epoch 46: reducing lr to 1.0689543414883156e-05
Epoch 49: reducing lr to 2.003066481596691e-08
[I 2024-06-24 15:44:00,387] Trial 177 finished with value: 1.093382477760315 and parameters: {'hidden_size': 16, 'n_layers': 2, 'rnn_dropout': 0.11632290733698857, 'bidirectional': False, 'fc_dropout': 0.6984755026157125, 'learning_rate_model': 0.006326416066049896}. Best is trial 153 with value: 0.969097375869751.
Epoch 42: reducing lr to 3.0420927323013434e-06
[I 2024-06-24 15:44:05,314] Trial 178 finished with value: 1.048413872718811 and parameters: {'hidden_size': 103, 'n_layers': 6, 'rnn_dropout': 0.3966611176681699, 'bidirectional': True, 'fc_dropout': 0.7397926508306973, 'learning_rate_model': 0.00035314815123986564}. Best is trial 153 with value: 0.969097375869751.
[I 2024-06-24 15:44:08,206] Trial 179 finished with value: 1.0639573335647583 and parameters: {'hidden_size': 107, 'n_layers': 2, 'rnn_dropout': 0.7079732150704507, 'bidirectional': True, 'fc_dropout': 0.5740759697558036, 'learning_rate_model': 5.800691551121672e-05}. Best is trial 153 with value: 0.969097375869751.
Epoch 9: reducing lr to 0.0003219645564088714
[I 2024-06-24 15:44:13,030] Trial 180 finished with value: 0.9931470155715942 and parameters: {'hidden_size': 37, 'n_layers': 6, 'rnn_dropout': 0.5953087370956281, 'bidirectional': True, 'fc_dropout': 0.49859771615006726, 'learning_rate_model': 0.003576195751981789}. Best is trial 153 with value: 0.969097375869751.
Epoch 9: reducing lr to 0.00020889859111599486
Epoch 15: reducing lr to 0.0002273881838861553
Epoch 18: reducing lr to 0.0002158076432652259
Epoch 22: reducing lr to 0.0001907970503917175
Epoch 25: reducing lr to 0.00016638917281310987
Epoch 28: reducing lr to 0.00013881625789076971
Epoch 31: reducing lr to 0.00010981080553573072
Epoch 34: reducing lr to 8.119536667221452e-05
Epoch 37: reducing lr to 5.4767900752012115e-05
Epoch 40: reducing lr to 3.2188963656251114e-05
Epoch 43: reducing lr to 1.4877261810885461e-05
Epoch 46: reducing lr to 3.920578319890899e-06
Epoch 49: reducing lr to 7.346599116770172e-09
[I 2024-06-24 15:44:15,232] Trial 181 finished with value: 1.0924965143203735 and parameters: {'hidden_size': 76, 'n_layers': 1, 'rnn_dropout': 0.7044308780514614, 'bidirectional': False, 'fc_dropout': 0.20311089675419175, 'learning_rate_model': 0.00232032451794255}. Best is trial 153 with value: 0.969097375869751.
Epoch 11: reducing lr to 0.0009212093071593503
Epoch 14: reducing lr to 0.0009171984870815815
Epoch 17: reducing lr to 0.0008800351209719695
Epoch 23: reducing lr to 0.0007311202713811678
Epoch 26: reducing lr to 0.0006287256327452459
Epoch 29: reducing lr to 0.0005159320817236968
Epoch 32: reducing lr to 0.00039982685426400165
Epoch 35: reducing lr to 0.00028770531132708816
Epoch 38: reducing lr to 0.00018661241579806106
Epoch 41: reducing lr to 0.00010290014895215811
Epoch 44: reducing lr to 4.182858228806117e-05
Epoch 47: reducing lr to 7.234995776727454e-06
[I 2024-06-24 15:44:18,604] Trial 182 finished with value: 1.1211827993392944 and parameters: {'hidden_size': 95, 'n_layers': 3, 'rnn_dropout': 0.10137070707753822, 'bidirectional': True, 'fc_dropout': 0.24228664377406287, 'learning_rate_model': 0.009264440595593887}. Best is trial 153 with value: 0.969097375869751.
Epoch 29: reducing lr to 2.788427576169771e-06
Epoch 34: reducing lr to 1.752138991015177e-06
Epoch 37: reducing lr to 1.1818528359018817e-06
Epoch 42: reducing lr to 4.3132193755201684e-07
Epoch 45: reducing lr to 1.470713247052116e-07
Epoch 48: reducing lr to 1.088836217381635e-08
[I 2024-06-24 15:44:23,705] Trial 183 finished with value: 1.0741008520126343 and parameters: {'hidden_size': 129, 'n_layers': 5, 'rnn_dropout': 0.006706463620025272, 'bidirectional': True, 'fc_dropout': 0.18472676038734337, 'learning_rate_model': 5.007097358286679e-05}. Best is trial 153 with value: 0.969097375869751.
Epoch 11: reducing lr to 0.00015253077572545184
Epoch 16: reducing lr to 0.00014827125178470276
Epoch 41: reducing lr to 1.7037864706703657e-05
Epoch 44: reducing lr to 6.9258376509109905e-06
Epoch 47: reducing lr to 1.197946557441481e-06
[I 2024-06-24 15:44:28,543] Trial 184 finished with value: 1.0448637008666992 and parameters: {'hidden_size': 50, 'n_layers': 6, 'rnn_dropout': 0.7860718314440683, 'bidirectional': True, 'fc_dropout': 0.624179663753091, 'learning_rate_model': 0.001533975286317709}. Best is trial 153 with value: 0.969097375869751.
[I 2024-06-24 15:44:33,482] Trial 185 finished with value: 1.0216546058654785 and parameters: {'hidden_size': 167, 'n_layers': 4, 'rnn_dropout': 0.471961038639523, 'bidirectional': True, 'fc_dropout': 0.4296056934724326, 'learning_rate_model': 0.00029102706982436836}. Best is trial 153 with value: 0.969097375869751.
Epoch 10: reducing lr to 3.616954079156924e-05
Epoch 13: reducing lr to 3.750293741112472e-05
Epoch 16: reducing lr to 3.637262312936444e-05
Epoch 19: reducing lr to 3.413911513517698e-05
Epoch 22: reducing lr to 3.094275358776539e-05
Epoch 25: reducing lr to 2.6984375091008578e-05
Epoch 28: reducing lr to 2.251270264960155e-05
Epoch 31: reducing lr to 1.7808706633515332e-05
Epoch 34: reducing lr to 1.316796154997402e-05
Epoch 37: reducing lr to 8.882053752977029e-06
Epoch 40: reducing lr to 5.220285998216621e-06
Epoch 43: reducing lr to 2.412738799314714e-06
Epoch 46: reducing lr to 6.35824760523583e-07
Epoch 49: reducing lr to 1.1914440276332342e-09
[I 2024-06-24 15:44:37,101] Trial 186 finished with value: 1.107337474822998 and parameters: {'hidden_size': 127, 'n_layers': 7, 'rnn_dropout': 0.31425835599757, 'bidirectional': False, 'fc_dropout': 0.5160627192683411, 'learning_rate_model': 0.00037630157098835073}. Best is trial 153 with value: 0.969097375869751.
Epoch 7: reducing lr to 0.003594854831272372
Epoch 10: reducing lr to 0.004851836642831306
Epoch 13: reducing lr to 0.005030700472357575
Epoch 18: reducing lr to 0.004694802200994644
Epoch 21: reducing lr to 0.004306467302734917
Epoch 24: reducing lr to 0.0038061275631691206
Epoch 27: reducing lr to 0.003225221692757729
Epoch 30: reducing lr to 0.0026002492946073023
Epoch 33: reducing lr to 0.0019704801028543593
Epoch 36: reducing lr to 0.0013754850552701661
Epoch 39: reducing lr to 0.0008526496272243167
Epoch 42: reducing lr to 0.0004348252126942495
Epoch 45: reducing lr to 0.00014826586472536306
Epoch 48: reducing lr to 1.0976799429662787e-05
[I 2024-06-24 15:44:40,918] Trial 187 finished with value: 1.1981052160263062 and parameters: {'hidden_size': 143, 'n_layers': 3, 'rnn_dropout': 0.10889052339849768, 'bidirectional': True, 'fc_dropout': 0.17437514404893434, 'learning_rate_model': 0.05047765912753394}. Best is trial 153 with value: 0.969097375869751.
Epoch 6: reducing lr to 0.0016836587812636855
Epoch 9: reducing lr to 0.00254011605744289
Epoch 12: reducing lr to 0.0028206651419755533
Epoch 15: reducing lr to 0.0027649414678976017
Epoch 18: reducing lr to 0.0026241271281361687
Epoch 21: reducing lr to 0.0024070700301588657
Epoch 24: reducing lr to 0.0021274086029743453
Epoch 27: reducing lr to 0.0018027152957425456
Epoch 30: reducing lr to 0.0014533913084667021
Epoch 33: reducing lr to 0.0011013861866762318
Epoch 36: reducing lr to 0.0007688178315831108
Epoch 39: reducing lr to 0.000476582595347785
Epoch 42: reducing lr to 0.00024304253678393908
Epoch 45: reducing lr to 8.287217674899337e-05
Epoch 48: reducing lr to 6.1354059085565875e-06
[I 2024-06-24 15:44:44,414] Trial 188 finished with value: 1.104604721069336 and parameters: {'hidden_size': 143, 'n_layers': 5, 'rnn_dropout': 0.597325977698404, 'bidirectional': False, 'fc_dropout': 0.7444478553998761, 'learning_rate_model': 0.02821413746745477}. Best is trial 153 with value: 0.969097375869751.
[I 2024-06-24 15:44:47,268] Trial 189 finished with value: 1.1019319295883179 and parameters: {'hidden_size': 46, 'n_layers': 2, 'rnn_dropout': 0.05728822355344363, 'bidirectional': True, 'fc_dropout': 0.528478684742879, 'learning_rate_model': 2.283088758720842e-05}. Best is trial 153 with value: 0.969097375869751.
Epoch 13: reducing lr to 0.00016677467315124418
Epoch 16: reducing lr to 0.00016174819234969409
Epoch 22: reducing lr to 0.00013760169128694116
Epoch 25: reducing lr to 0.00011999887599893912
Epoch 28: reducing lr to 0.00010011345471367566
Epoch 31: reducing lr to 7.919489600219669e-05
Epoch 34: reducing lr to 5.8557612687523276e-05
Epoch 37: reducing lr to 3.9498282369879615e-05
Epoch 40: reducing lr to 2.321448801635305e-05
Epoch 43: reducing lr to 1.072939221383963e-05
Epoch 46: reducing lr to 2.827497629194595e-06
Epoch 49: reducing lr to 5.298323331524683e-09
[I 2024-06-24 15:44:50,121] Trial 190 finished with value: 1.0926557779312134 and parameters: {'hidden_size': 54, 'n_layers': 4, 'rnn_dropout': 0.08396293466394243, 'bidirectional': False, 'fc_dropout': 0.27136334790808, 'learning_rate_model': 0.0016734041608502312}. Best is trial 153 with value: 0.969097375869751.
Epoch 19: reducing lr to 2.454911751894959e-05
Epoch 23: reducing lr to 2.135449377764995e-05
Epoch 26: reducing lr to 1.8363760571080796e-05
Epoch 29: reducing lr to 1.5069296886058761e-05
Epoch 32: reducing lr to 1.1678106059607061e-05
Epoch 35: reducing lr to 8.403270325038068e-06
Epoch 38: reducing lr to 5.450558311649305e-06
Epoch 41: reducing lr to 3.0054981054854575e-06
Epoch 44: reducing lr to 1.221725392062939e-06
Epoch 47: reducing lr to 2.1131909255310122e-07
[I 2024-06-24 15:44:52,736] Trial 191 finished with value: 1.0922719240188599 and parameters: {'hidden_size': 115, 'n_layers': 3, 'rnn_dropout': 0.5570160664808824, 'bidirectional': False, 'fc_dropout': 0.4491930070394994, 'learning_rate_model': 0.0002705949305417018}. Best is trial 153 with value: 0.969097375869751.
[I 2024-06-24 15:44:59,158] Trial 192 finished with value: 1.1059620380401611 and parameters: {'hidden_size': 129, 'n_layers': 7, 'rnn_dropout': 0.4775108199567785, 'bidirectional': True, 'fc_dropout': 0.5916895700060374, 'learning_rate_model': 1.413889647537952e-05}. Best is trial 153 with value: 0.969097375869751.
[I 2024-06-24 15:45:02,344] Trial 193 finished with value: 1.106532096862793 and parameters: {'hidden_size': 157, 'n_layers': 4, 'rnn_dropout': 0.00425322636888259, 'bidirectional': False, 'fc_dropout': 0.6143703026250493, 'learning_rate_model': 1.6306861470410482e-05}. Best is trial 153 with value: 0.969097375869751.
Epoch 12: reducing lr to 0.004751544167111782
Epoch 15: reducing lr to 0.0046576749961155794
Epoch 18: reducing lr to 0.0044204665643942185
Epoch 21: reducing lr to 0.004054823591580397
Epoch 24: reducing lr to 0.0035837206579744318
Epoch 27: reducing lr to 0.0030367593873441507
Epoch 30: reducing lr to 0.002448306568371728
Epoch 33: reducing lr to 0.001855337251189491
Epoch 36: reducing lr to 0.0012951100890592425
Epoch 39: reducing lr to 0.0008028259779484986
Epoch 42: reducing lr to 0.0004094166765243719
Epoch 45: reducing lr to 0.0001396021109303834
Epoch 48: reducing lr to 1.0335382149349176e-05
[I 2024-06-24 15:45:07,254] Trial 194 finished with value: 1.1072297096252441 and parameters: {'hidden_size': 117, 'n_layers': 6, 'rnn_dropout': 0.5196664439027832, 'bidirectional': True, 'fc_dropout': 0.2214451700804693, 'learning_rate_model': 0.04752805227340119}. Best is trial 153 with value: 0.969097375869751.
Epoch 34: reducing lr to 1.2664261019259893e-06
Epoch 37: reducing lr to 8.542297658442047e-07
[I 2024-06-24 15:45:10,777] Trial 195 finished with value: 1.1068613529205322 and parameters: {'hidden_size': 157, 'n_layers': 5, 'rnn_dropout': 0.6663745150696564, 'bidirectional': False, 'fc_dropout': 0.7988164056718448, 'learning_rate_model': 3.619072928538003e-05}. Best is trial 153 with value: 0.969097375869751.
Epoch 18: reducing lr to 0.0005119294104797645
Epoch 22: reducing lr to 0.0004526003808320566
Epoch 25: reducing lr to 0.0003947010859283887
Epoch 28: reducing lr to 0.0003292938284845242
Epoch 31: reducing lr to 0.00026048836867712986
Epoch 34: reducing lr to 0.00019260808174023032
Epoch 37: reducing lr to 0.00012991800809730448
Epoch 40: reducing lr to 7.635724545792506e-05
Epoch 43: reducing lr to 3.529118687904658e-05
Epoch 46: reducing lr to 9.300223651369233e-06
Epoch 49: reducing lr to 1.7427279673607604e-08
[I 2024-06-24 15:45:13,250] Trial 196 finished with value: 0.9836214780807495 and parameters: {'hidden_size': 29, 'n_layers': 1, 'rnn_dropout': 0.011795032719957544, 'bidirectional': True, 'fc_dropout': 0.6733894609486798, 'learning_rate_model': 0.005504171884830901}. Best is trial 153 with value: 0.969097375869751.
[I 2024-06-24 15:45:15,867] Trial 197 finished with value: 1.1080830097198486 and parameters: {'hidden_size': 106, 'n_layers': 3, 'rnn_dropout': 0.2829182296536041, 'bidirectional': False, 'fc_dropout': 0.5805140363958952, 'learning_rate_model': 1.269705997902816e-05}. Best is trial 153 with value: 0.969097375869751.
Epoch 17: reducing lr to 0.0014585292860055403
Epoch 20: reducing lr to 0.0013535310176171235
Epoch 23: reducing lr to 0.0012117247391490377
Epoch 26: reducing lr to 0.0010420206266410059
Epoch 29: reducing lr to 0.0008550818403164434
Epoch 32: reducing lr to 0.0006626544354632524
Epoch 35: reducing lr to 0.0004768294040883686
Epoch 38: reducing lr to 0.00030928274007189816
Epoch 41: reducing lr to 0.00017054192179886355
Epoch 44: reducing lr to 6.932474716673624e-05
Epoch 47: reducing lr to 1.1990945557750591e-05
[I 2024-06-24 15:45:18,745] Trial 198 finished with value: 1.168614387512207 and parameters: {'hidden_size': 101, 'n_layers': 2, 'rnn_dropout': 0.023172948942318784, 'bidirectional': True, 'fc_dropout': 0.2637880633954088, 'learning_rate_model': 0.015354453027065824}. Best is trial 153 with value: 0.969097375869751.
Epoch 8: reducing lr to 0.002906827604875008
Epoch 11: reducing lr to 0.0035442560703133645
Epoch 21: reducing lr to 0.003040940504325882
Epoch 24: reducing lr to 0.0026876338905723702
Epoch 27: reducing lr to 0.002277436838939626
Epoch 30: reducing lr to 0.0018361229391651267
Epoch 33: reducing lr to 0.0013914218630970746
Epoch 36: reducing lr to 0.0009712759725377721
Epoch 39: reducing lr to 0.0006020844012395356
Epoch 42: reducing lr to 0.0003070446165339087
Epoch 45: reducing lr to 0.0001046954828069692
Epoch 48: reducing lr to 7.751084972205253e-06
[I 2024-06-24 15:45:22,105] Trial 199 finished with value: 1.015221357345581 and parameters: {'hidden_size': 76, 'n_layers': 3, 'rnn_dropout': 0.742487262800581, 'bidirectional': True, 'fc_dropout': 0.76154813405434, 'learning_rate_model': 0.03564396230455292}. Best is trial 153 with value: 0.969097375869751.
Epoch 8: reducing lr to 0.0010164787407363769
Epoch 14: reducing lr to 0.0012339828633901476
Epoch 18: reducing lr to 0.0011592659843150295
Epoch 21: reducing lr to 0.0010633762281971756
Epoch 27: reducing lr to 0.0007963892066628175
Epoch 31: reducing lr to 0.0005898768442198062
Epoch 34: reducing lr to 0.00043616169123075564
Epoch 37: reducing lr to 0.0002941997948428535
Epoch 40: reducing lr to 0.00017291125593352636
Epoch 43: reducing lr to 7.991701913871736e-05
Epoch 46: reducing lr to 2.1060389781963133e-05
Epoch 49: reducing lr to 3.9464137264154374e-08
[I 2024-06-24 15:45:24,724] Trial 200 finished with value: 1.0952370166778564 and parameters: {'hidden_size': 111, 'n_layers': 3, 'rnn_dropout': 0.5682836626556875, 'bidirectional': False, 'fc_dropout': 0.7923249575119574, 'learning_rate_model': 0.012464216955083157}. Best is trial 153 with value: 0.969097375869751.
Epoch 9: reducing lr to 0.004866269249132282
Epoch 12: reducing lr to 0.005403735786904527
Epoch 15: reducing lr to 0.005296982238845314
Epoch 18: reducing lr to 0.005027214844001257
Epoch 21: reducing lr to 0.0046113841270945
Epoch 24: reducing lr to 0.004075618133533366
Epoch 27: reducing lr to 0.003453581572742603
Epoch 30: reducing lr to 0.002784358380249584
Epoch 33: reducing lr to 0.002109998760071268
Epoch 36: reducing lr to 0.0014728754464013588
Epoch 39: reducing lr to 0.0009130209706824481
Epoch 42: reducing lr to 0.00046561275006206103
Epoch 45: reducing lr to 0.00015876374000338352
Epoch 48: reducing lr to 1.1754005103928322e-05
[I 2024-06-24 15:45:27,196] Trial 201 finished with value: 1.124412178993225 and parameters: {'hidden_size': 77, 'n_layers': 1, 'rnn_dropout': 0.1479244722227267, 'bidirectional': True, 'fc_dropout': 0.6405277265033917, 'learning_rate_model': 0.05405169938844537}. Best is trial 153 with value: 0.969097375869751.
[I 2024-06-24 15:45:32,533] Trial 202 finished with value: 1.1070233583450317 and parameters: {'hidden_size': 48, 'n_layers': 7, 'rnn_dropout': 0.7659159410613351, 'bidirectional': True, 'fc_dropout': 0.04606680448653897, 'learning_rate_model': 1.0548472419055645e-05}. Best is trial 153 with value: 0.969097375869751.
[I 2024-06-24 15:45:34,732] Trial 203 finished with value: 1.0950431823730469 and parameters: {'hidden_size': 73, 'n_layers': 1, 'rnn_dropout': 0.23508998837643535, 'bidirectional': False, 'fc_dropout': 0.38156073798942236, 'learning_rate_model': 0.00012970411945941392}. Best is trial 153 with value: 0.969097375869751.
Epoch 10: reducing lr to 0.003506180687153268
Epoch 13: reducing lr to 0.003635436668110886
Epoch 16: reducing lr to 0.003525866957841157
Epoch 19: reducing lr to 0.003309356534362187
Epoch 22: reducing lr to 0.0029995096056639357
Epoch 25: reducing lr to 0.002615794746861899
Epoch 28: reducing lr to 0.002182322515525433
Epoch 31: reducing lr to 0.0017263294444745624
Epoch 34: reducing lr to 0.0012764677533992172
Epoch 37: reducing lr to 0.0008610030608462736
Epoch 40: reducing lr to 0.0005060408716228447
Epoch 43: reducing lr to 0.00023388458897090624
Epoch 46: reducing lr to 6.163518936024918e-05
Epoch 49: reducing lr to 1.1549546795677768e-07
[I 2024-06-24 15:45:37,855] Trial 204 finished with value: 1.1226756572723389 and parameters: {'hidden_size': 109, 'n_layers': 5, 'rnn_dropout': 0.08899653805394801, 'bidirectional': False, 'fc_dropout': 0.6753998876555427, 'learning_rate_model': 0.03647769011909392}. Best is trial 153 with value: 0.969097375869751.
Epoch 7: reducing lr to 0.0030350560767036473
Epoch 10: reducing lr to 0.004096297897177274
Epoch 13: reducing lr to 0.004247308655103776
Epoch 16: reducing lr to 0.004119297518821855
Epoch 22: reducing lr to 0.0035043501709035307
Epoch 28: reducing lr to 0.002549624200505065
Epoch 31: reducing lr to 0.0020168839840875082
Epoch 34: reducing lr to 0.0014913071061118538
Epoch 37: reducing lr to 0.00100591650639413
Epoch 40: reducing lr to 0.0005912114472336083
Epoch 43: reducing lr to 0.00027324916639180977
Epoch 46: reducing lr to 7.200886636949122e-05
Epoch 49: reducing lr to 1.3493424462010517e-07
[I 2024-06-24 15:45:40,719] Trial 205 finished with value: 1.1155028343200684 and parameters: {'hidden_size': 86, 'n_layers': 4, 'rnn_dropout': 0.08126518075693295, 'bidirectional': False, 'fc_dropout': 0.22769917517498836, 'learning_rate_model': 0.042617166273324125}. Best is trial 153 with value: 0.969097375869751.
Epoch 12: reducing lr to 2.400758154178815e-05
Epoch 19: reducing lr to 2.1786119826436155e-05
Epoch 23: reducing lr to 1.8951050273544552e-05
[I 2024-06-24 15:45:49,295] Trial 206 finished with value: 1.0560168027877808 and parameters: {'hidden_size': 180, 'n_layers': 7, 'rnn_dropout': 0.1249196515526358, 'bidirectional': True, 'fc_dropout': 0.16850254273504195, 'learning_rate_model': 0.0002401395315598264}. Best is trial 153 with value: 0.969097375869751.
Epoch 13: reducing lr to 0.00019667118309843077
Epoch 16: reducing lr to 0.00019074364082007837
Epoch 22: reducing lr to 0.00016226856818484412
Epoch 25: reducing lr to 0.00014151022134992075
Epoch 28: reducing lr to 0.00011806008196912408
Epoch 31: reducing lr to 9.339160196095407e-05
Epoch 34: reducing lr to 6.905481959020687e-05
Epoch 37: reducing lr to 4.6578858631582095e-05
Epoch 40: reducing lr to 2.7375984236287708e-05
Epoch 43: reducing lr to 1.265277407385037e-05
Epoch 46: reducing lr to 3.334363026677358e-06
Epoch 49: reducing lr to 6.248116085974531e-09
[I 2024-06-24 15:45:52,139] Trial 207 finished with value: 1.0926096439361572 and parameters: {'hidden_size': 45, 'n_layers': 4, 'rnn_dropout': 0.06701996307636841, 'bidirectional': False, 'fc_dropout': 0.5528218732453634, 'learning_rate_model': 0.0019733834274571717}. Best is trial 153 with value: 0.969097375869751.
Epoch 6: reducing lr to 0.0051557693133157225
Epoch 9: reducing lr to 0.007778448083996602
Epoch 12: reducing lr to 0.008637557053705228
Epoch 18: reducing lr to 0.008035710247256373
Epoch 21: reducing lr to 0.007371029055650081
Epoch 24: reducing lr to 0.00651463830685844
Epoch 27: reducing lr to 0.00552034907896142
Epoch 30: reducing lr to 0.004450634767460546
Epoch 33: reducing lr to 0.003372710175343898
Epoch 36: reducing lr to 0.002354305651309607
Epoch 39: reducing lr to 0.001459410866203091
Epoch 42: reducing lr to 0.0007442548733304133
Epoch 45: reducing lr to 0.0002537745952831641
Epoch 48: reducing lr to 1.8788092848795455e-05
[I 2024-06-24 15:45:54,659] Trial 208 finished with value: 1.1320397853851318 and parameters: {'hidden_size': 177, 'n_layers': 2, 'rnn_dropout': 0.5036863653751306, 'bidirectional': False, 'fc_dropout': 0.6905811853710797, 'learning_rate_model': 0.08639849462086026}. Best is trial 153 with value: 0.969097375869751.
[I 2024-06-24 15:45:57,489] Trial 209 finished with value: 1.1114344596862793 and parameters: {'hidden_size': 17, 'n_layers': 2, 'rnn_dropout': 0.17907973626202028, 'bidirectional': True, 'fc_dropout': 0.39215624979538544, 'learning_rate_model': 1.5202975046828169e-05}. Best is trial 153 with value: 0.969097375869751.
Epoch 10: reducing lr to 0.0006207248031327303
Epoch 13: reducing lr to 0.0006436079345205732
Epoch 16: reducing lr to 0.000624210007572402
Epoch 22: reducing lr to 0.0005310251169577248
Epoch 25: reducing lr to 0.00046309326990212385
Epoch 28: reducing lr to 0.00038635251137655754
Epoch 31: reducing lr to 0.00030562472393108186
Epoch 34: reducing lr to 0.00022598241951338766
Epoch 37: reducing lr to 0.0001524296672441054
Epoch 40: reducing lr to 8.958811551444557e-05
Epoch 43: reducing lr to 4.140629887577699e-05
Epoch 46: reducing lr to 1.0911728229486082e-05
Epoch 49: reducing lr to 2.0447007158686663e-08
[I 2024-06-24 15:46:00,590] Trial 210 finished with value: 1.0976122617721558 and parameters: {'hidden_size': 67, 'n_layers': 5, 'rnn_dropout': 0.2357536332487234, 'bidirectional': False, 'fc_dropout': 0.30442726176771207, 'learning_rate_model': 0.006457912195134263}. Best is trial 153 with value: 0.969097375869751.
Epoch 9: reducing lr to 0.001491867200815778
Epoch 18: reducing lr to 0.0015412087891678039
Epoch 21: reducing lr to 0.0014137262813399601
Epoch 24: reducing lr to 0.0012494748451398658
Epoch 27: reducing lr to 0.001058775174562151
Epoch 30: reducing lr to 0.0008536093524935129
Epoch 33: reducing lr to 0.0006468688399174756
Epoch 36: reducing lr to 0.0004515439768904889
Epoch 39: reducing lr to 0.00027990766028020543
Epoch 42: reducing lr to 0.0001427443395621987
Epoch 45: reducing lr to 4.8672690363799064e-05
Epoch 48: reducing lr to 3.6034616654016804e-06
[I 2024-06-24 15:46:04,998] Trial 211 finished with value: 1.1388760805130005 and parameters: {'hidden_size': 107, 'n_layers': 5, 'rnn_dropout': 0.28271547760130356, 'bidirectional': True, 'fc_dropout': 0.7496643078391715, 'learning_rate_model': 0.0165707965050135}. Best is trial 153 with value: 0.969097375869751.
Epoch 18: reducing lr to 0.00014936346763035898
Epoch 22: reducing lr to 0.00013205328888711934
Epoch 32: reducing lr to 6.930731171478848e-05
Epoch 37: reducing lr to 3.790562487679049e-05
Epoch 40: reducing lr to 2.2278428874812585e-05
Epoch 43: reducing lr to 1.029675955539534e-05
Epoch 46: reducing lr to 2.713486714905748e-06
Epoch 49: reducing lr to 5.084683298364756e-09
[I 2024-06-24 15:46:08,282] Trial 212 finished with value: 0.9678940773010254 and parameters: {'hidden_size': 189, 'n_layers': 2, 'rnn_dropout': 0.7465246102585805, 'bidirectional': True, 'fc_dropout': 0.2197073987390904, 'learning_rate_model': 0.001605928829877941}. Best is trial 212 with value: 0.9678940773010254.
Epoch 16: reducing lr to 0.00036728676696715237
Epoch 22: reducing lr to 0.00031245653869644863
Epoch 25: reducing lr to 0.00027248526592529756
Epoch 28: reducing lr to 0.00022733080708687068
Epoch 31: reducing lr to 0.0001798303702217646
Epoch 34: reducing lr to 0.00013296863434997022
Epoch 37: reducing lr to 8.96900065567638e-05
Epoch 40: reducing lr to 5.2713876590906696e-05
Epoch 43: reducing lr to 2.4363572294050115e-05
Epoch 46: reducing lr to 6.420488833587491e-06
Epoch 49: reducing lr to 1.2031071374048186e-08
[I 2024-06-24 15:46:12,123] Trial 213 finished with value: 0.999207079410553 and parameters: {'hidden_size': 50, 'n_layers': 4, 'rnn_dropout': 0.14825411675014752, 'bidirectional': True, 'fc_dropout': 0.7711342139391983, 'learning_rate_model': 0.003799852073396136}. Best is trial 212 with value: 0.9678940773010254.
Epoch 26: reducing lr to 1.1723866569655842e-06
Epoch 29: reducing lr to 9.620601690315182e-07
Epoch 32: reducing lr to 7.455583876688751e-07
Epoch 35: reducing lr to 5.364849953154038e-07
Epoch 38: reducing lr to 3.479767563324542e-07
Epoch 41: reducing lr to 1.918782301025782e-07
Epoch 44: reducing lr to 7.799788842739967e-08
Epoch 47: reducing lr to 1.349111928966709e-08
[I 2024-06-24 15:46:15,763] Trial 214 finished with value: 1.107891321182251 and parameters: {'hidden_size': 105, 'n_layers': 7, 'rnn_dropout': 0.11455182536701028, 'bidirectional': False, 'fc_dropout': 0.3470718617221808, 'learning_rate_model': 1.727543140097415e-05}. Best is trial 212 with value: 0.9678940773010254.
Epoch 9: reducing lr to 0.0005013724227798435
Epoch 12: reducing lr to 0.0005567476777051674
Epoch 16: reducing lr to 0.000538284968010246
Epoch 19: reducing lr to 0.0005052308829384654
Epoch 22: reducing lr to 0.00045792735558003884
Epoch 25: reducing lr to 0.00039934660282759315
Epoch 28: reducing lr to 0.00033316952100112936
Epoch 33: reducing lr to 0.00021739347665319453
Epoch 36: reducing lr to 0.00015175056972995676
Epoch 39: reducing lr to 9.406868232814869e-05
Epoch 42: reducing lr to 4.7972148811417996e-05
Epoch 45: reducing lr to 1.635745103690661e-05
Epoch 48: reducing lr to 1.211016841571981e-06
[I 2024-06-24 15:46:18,727] Trial 215 finished with value: 1.0931222438812256 and parameters: {'hidden_size': 107, 'n_layers': 4, 'rnn_dropout': 0.6453974486198909, 'bidirectional': False, 'fc_dropout': 0.1921377657771018, 'learning_rate_model': 0.005568954385864473}. Best is trial 212 with value: 0.9678940773010254.
Epoch 6: reducing lr to 0.003283974407439604
Epoch 9: reducing lr to 0.004954493284148655
Epoch 12: reducing lr to 0.005501703932700836
Epoch 18: reducing lr to 0.005118356775473931
Epoch 23: reducing lr to 0.004342922573741492
Epoch 26: reducing lr to 0.003734688874076763
Epoch 29: reducing lr to 0.0030646846653594206
Epoch 32: reducing lr to 0.002375008789854639
Epoch 35: reducing lr to 0.0017089963718107885
Epoch 38: reducing lr to 0.0011084951475614474
Epoch 41: reducing lr to 0.00061123647807148
Epoch 44: reducing lr to 0.0002484656784351662
Epoch 47: reducing lr to 4.297654942642462e-05
[I 2024-06-24 15:46:21,936] Trial 216 finished with value: 1.0786954164505005 and parameters: {'hidden_size': 199, 'n_layers': 2, 'rnn_dropout': 0.6571573921810379, 'bidirectional': True, 'fc_dropout': 0.11105321443653891, 'learning_rate_model': 0.055031640853951194}. Best is trial 212 with value: 0.9678940773010254.
Epoch 16: reducing lr to 0.00031963231307090723
Epoch 22: reducing lr to 0.00027191615701909323
Epoch 25: reducing lr to 0.00023713104761335793
Epoch 28: reducing lr to 0.00019783525636237002
Epoch 31: reducing lr to 0.0001564978713200435
Epoch 34: reducing lr to 0.00011571631756327798
Epoch 37: reducing lr to 7.805297340768907e-05
Epoch 40: reducing lr to 4.587439521662024e-05
Epoch 43: reducing lr to 2.1202465395966625e-05
Epoch 46: reducing lr to 5.587447960271955e-06
Epoch 49: reducing lr to 1.0470072754750412e-08
[I 2024-06-24 15:46:26,052] Trial 217 finished with value: 0.987665057182312 and parameters: {'hidden_size': 180, 'n_layers': 3, 'rnn_dropout': 0.7582112528812855, 'bidirectional': True, 'fc_dropout': 0.3744396438931783, 'learning_rate_model': 0.003306831655210467}. Best is trial 212 with value: 0.9678940773010254.
[I 2024-06-24 15:46:28,259] Trial 218 finished with value: 1.0933399200439453 and parameters: {'hidden_size': 45, 'n_layers': 1, 'rnn_dropout': 0.2124096139779087, 'bidirectional': False, 'fc_dropout': 0.4411750686502212, 'learning_rate_model': 0.00019346062318522451}. Best is trial 212 with value: 0.9678940773010254.
Epoch 6: reducing lr to 0.0018017395956177514
Epoch 9: reducing lr to 0.002718263183187439
Epoch 12: reducing lr to 0.003018488145479039
Epoch 18: reducing lr to 0.0028081662408751215
Epoch 21: reducing lr to 0.0025758861777841558
Epoch 24: reducing lr to 0.002276611127321061
Epoch 27: reducing lr to 0.001929145955291057
Epoch 30: reducing lr to 0.0015553226684243656
Epoch 33: reducing lr to 0.0011786302098050973
Epoch 36: reducing lr to 0.0008227376855663056
Epoch 39: reducing lr to 0.0005100069813290129
Epoch 42: reducing lr to 0.0002600879506085774
Epoch 45: reducing lr to 8.868428917148258e-05
Epoch 48: reducing lr to 6.565703148197696e-06
[I 2024-06-24 15:46:31,379] Trial 219 finished with value: 1.1872494220733643 and parameters: {'hidden_size': 160, 'n_layers': 2, 'rnn_dropout': 0.1959045110095069, 'bidirectional': True, 'fc_dropout': 0.07035094701648266, 'learning_rate_model': 0.030192892524910118}. Best is trial 212 with value: 0.9678940773010254.
Epoch 8: reducing lr to 0.002255589007909497
Epoch 12: reducing lr to 0.002765103696852997
Epoch 17: reducing lr to 0.0026272867343962807
Epoch 20: reducing lr to 0.002438150622891133
Epoch 23: reducing lr to 0.0021827112855750826
Epoch 32: reducing lr to 0.0011936566680463942
Epoch 35: reducing lr to 0.0008589252063374037
Epoch 38: reducing lr to 0.0005571190431109003
Epoch 41: reducing lr to 0.0003072015989666596
Epoch 44: reducing lr to 0.00012487646997843722
Epoch 47: reducing lr to 2.159960207794298e-05
[I 2024-06-24 15:46:34,397] Trial 220 finished with value: 1.1012738943099976 and parameters: {'hidden_size': 32, 'n_layers': 5, 'rnn_dropout': 0.09632070148617872, 'bidirectional': False, 'fc_dropout': 0.7256872441762966, 'learning_rate_model': 0.02765837555610633}. Best is trial 212 with value: 0.9678940773010254.
Epoch 8: reducing lr to 0.001388509160839823
Epoch 11: reducing lr to 0.0016929906726284405
Epoch 16: reducing lr to 0.001645712775645173
Epoch 22: reducing lr to 0.0014000333358391963
Epoch 25: reducing lr to 0.0012209328612932011
Epoch 28: reducing lr to 0.0010186079302825796
Epoch 31: reducing lr to 0.0008057713055298477
Epoch 34: reducing lr to 0.0005957965273750473
Epoch 37: reducing lr to 0.0004018766884987342
Epoch 40: reducing lr to 0.00023619663968780185
Epoch 43: reducing lr to 0.00010916658532448276
Epoch 46: reducing lr to 2.876847588758093e-05
Epoch 49: reducing lr to 5.39079804819792e-08
[I 2024-06-24 15:46:36,586] Trial 221 finished with value: 1.1047585010528564 and parameters: {'hidden_size': 78, 'n_layers': 1, 'rnn_dropout': 0.6406874841040265, 'bidirectional': False, 'fc_dropout': 0.5357172799004265, 'learning_rate_model': 0.017026110563109636}. Best is trial 212 with value: 0.9678940773010254.
Epoch 10: reducing lr to 0.00011589414798076417
Epoch 13: reducing lr to 0.00012016660656779255
Epoch 16: reducing lr to 0.00011654486275329462
Epoch 19: reducing lr to 0.0001093882746316418
Epoch 22: reducing lr to 9.914651899779396e-05
Epoch 25: reducing lr to 8.646311486195988e-05
Epoch 28: reducing lr to 7.21350184497785e-05
Epoch 31: reducing lr to 5.706251273203122e-05
Epoch 34: reducing lr to 4.219267513712584e-05
Epoch 37: reducing lr to 2.845980428539363e-05
Epoch 40: reducing lr to 1.6726797872983983e-05
Epoch 43: reducing lr to 7.730878007494311e-06
Epoch 46: reducing lr to 2.0373045184784263e-06
Epoch 49: reducing lr to 3.817615248273912e-09
[I 2024-06-24 15:46:39,766] Trial 222 finished with value: 1.092610239982605 and parameters: {'hidden_size': 182, 'n_layers': 4, 'rnn_dropout': 0.4462602429719284, 'bidirectional': False, 'fc_dropout': 0.5463254941038999, 'learning_rate_model': 0.0012057424285486997}. Best is trial 212 with value: 0.9678940773010254.
Epoch 11: reducing lr to 0.0009232030231643467
Epoch 18: reducing lr to 0.0008635275438935436
Epoch 21: reducing lr to 0.0007921000658985544
Epoch 24: reducing lr to 0.0007000712374362922
Epoch 27: reducing lr to 0.0005932236647305851
Epoch 30: reducing lr to 0.00047827081754529593
Epoch 33: reducing lr to 0.0003624356832644468
Epoch 36: reducing lr to 0.00025299665046337415
Epoch 39: reducing lr to 0.00015683012976409747
Epoch 42: reducing lr to 7.997856605360346e-05
Epoch 45: reducing lr to 2.727093763022024e-05
Epoch 48: reducing lr to 2.018992120541435e-06
[I 2024-06-24 15:46:44,775] Trial 223 finished with value: 1.069620132446289 and parameters: {'hidden_size': 169, 'n_layers': 4, 'rnn_dropout': 0.6598868499081856, 'bidirectional': True, 'fc_dropout': 0.7758964562757713, 'learning_rate_model': 0.00928449104813407}. Best is trial 212 with value: 0.9678940773010254.
Epoch 15: reducing lr to 7.7480618577978e-06
Epoch 18: reducing lr to 7.3534646384338255e-06
Epoch 21: reducing lr to 6.745216022205072e-06
Epoch 24: reducing lr to 5.96153431963605e-06
Epoch 27: reducing lr to 5.0516619558070085e-06
Epoch 30: reducing lr to 4.07276822758504e-06
Epoch 33: reducing lr to 3.086361285680395e-06
Epoch 36: reducing lr to 2.1544210557966594e-06
Epoch 39: reducing lr to 1.3355043757625084e-06
Epoch 42: reducing lr to 6.810663556324509e-07
Epoch 45: reducing lr to 2.3222869604895865e-07
Epoch 48: reducing lr to 1.7192951479852772e-08
[I 2024-06-24 15:46:47,816] Trial 224 finished with value: 1.107984185218811 and parameters: {'hidden_size': 64, 'n_layers': 5, 'rnn_dropout': 0.7642098028305495, 'bidirectional': False, 'fc_dropout': 0.4953078651359258, 'learning_rate_model': 7.906311395751618e-05}. Best is trial 212 with value: 0.9678940773010254.
[I 2024-06-24 15:46:52,217] Trial 225 finished with value: 0.9916723966598511 and parameters: {'hidden_size': 157, 'n_layers': 4, 'rnn_dropout': 0.5329504721787962, 'bidirectional': True, 'fc_dropout': 0.5000968546443917, 'learning_rate_model': 0.0004354367145964692}. Best is trial 212 with value: 0.9678940773010254.
Epoch 6: reducing lr to 0.00418868278154667
Epoch 10: reducing lr to 0.006746789079670655
Epoch 13: reducing lr to 0.006995510671231021
Epoch 16: reducing lr to 0.006784670503347178
Epoch 19: reducing lr to 0.006368049030838642
Epoch 22: reducing lr to 0.00577182423199402
Epoch 25: reducing lr to 0.005033458628487465
Epoch 28: reducing lr to 0.004199347104390334
Epoch 31: reducing lr to 0.00332189972027695
Epoch 34: reducing lr to 0.0024562506806167653
Epoch 37: reducing lr to 0.0016567902703260521
Epoch 40: reducing lr to 0.0009737521625857899
Epoch 43: reducing lr to 0.0004500538139845134
Epoch 46: reducing lr to 0.00011860188039447272
Epoch 49: reducing lr to 2.222428424222822e-07
[I 2024-06-24 15:46:54,910] Trial 226 finished with value: 1.1078026294708252 and parameters: {'hidden_size': 32, 'n_layers': 3, 'rnn_dropout': 0.5377155277254789, 'bidirectional': False, 'fc_dropout': 0.7598771111040699, 'learning_rate_model': 0.07019241257270516}. Best is trial 212 with value: 0.9678940773010254.
Epoch 8: reducing lr to 0.0001953537866322838
Epoch 13: reducing lr to 0.0002387359308928624
Epoch 16: reducing lr to 0.00023154058431775677
Epoch 22: reducing lr to 0.00019697515960369368
Epoch 25: reducing lr to 0.00017177694206436028
Epoch 28: reducing lr to 0.00014331120160130544
Epoch 31: reducing lr to 0.00011336653738725556
Epoch 34: reducing lr to 8.38245155074691e-05
Epoch 37: reducing lr to 5.654131428991496e-05
Epoch 40: reducing lr to 3.323125929174634e-05
Epoch 43: reducing lr to 1.5358995401913755e-05
Epoch 46: reducing lr to 4.047528715532024e-06
Epoch 49: reducing lr to 7.584485874371703e-09
[I 2024-06-24 15:46:57,693] Trial 227 finished with value: 1.0924142599105835 and parameters: {'hidden_size': 151, 'n_layers': 3, 'rnn_dropout': 0.5949993570809757, 'bidirectional': False, 'fc_dropout': 0.4847170889742864, 'learning_rate_model': 0.002395457850714959}. Best is trial 212 with value: 0.9678940773010254.
Epoch 12: reducing lr to 0.0004246646096270094
Epoch 22: reducing lr to 0.00034928846492271774
Epoch 25: reducing lr to 0.0003046054361549755
Epoch 28: reducing lr to 0.0002541282348203841
Epoch 31: reducing lr to 0.00020102851495217597
Epoch 34: reducing lr to 0.00014864278522937866
Epoch 37: reducing lr to 0.00010026253519871236
Epoch 40: reducing lr to 5.8927712351223866e-05
Epoch 43: reducing lr to 2.723551506435204e-05
Epoch 46: reducing lr to 7.177326799090965e-06
Epoch 49: reducing lr to 1.3449276719049163e-08
[I 2024-06-24 15:47:00,887] Trial 228 finished with value: 0.9916993975639343 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.6129506921622574, 'bidirectional': True, 'fc_dropout': 0.3808712944016839, 'learning_rate_model': 0.004247773156507247}. Best is trial 212 with value: 0.9678940773010254.
[I 2024-06-24 15:47:03,390] Trial 229 finished with value: 1.10111665725708 and parameters: {'hidden_size': 172, 'n_layers': 2, 'rnn_dropout': 0.6849273903898975, 'bidirectional': False, 'fc_dropout': 0.022169392097891996, 'learning_rate_model': 2.195637079696368e-05}. Best is trial 212 with value: 0.9678940773010254.
Epoch 9: reducing lr to 1.5879705202143495e-05
Epoch 12: reducing lr to 1.7633576543595023e-05
Epoch 38: reducing lr to 3.552850947966728e-06
Epoch 41: reducing lr to 1.959081287207639e-06
Epoch 44: reducing lr to 7.963602935994281e-07
Epoch 47: reducing lr to 1.3774464841448463e-07
[I 2024-06-24 15:47:07,039] Trial 230 finished with value: 1.0920395851135254 and parameters: {'hidden_size': 197, 'n_layers': 5, 'rnn_dropout': 0.3341058774513466, 'bidirectional': False, 'fc_dropout': 0.029580054687486257, 'learning_rate_model': 0.00017638256496317842}. Best is trial 212 with value: 0.9678940773010254.
Epoch 14: reducing lr to 0.00020339477979618043
Epoch 17: reducing lr to 0.00019515355963193798
Epoch 20: reducing lr to 0.0001811046227831572
Epoch 23: reducing lr to 0.00016213071510318508
Epoch 26: reducing lr to 0.00013942403244834295
Epoch 29: reducing lr to 0.0001144113227725401
Epoch 32: reducing lr to 8.86642271275277e-05
Epoch 35: reducing lr to 6.380053965174025e-05
Epoch 38: reducing lr to 4.1382527068106504e-05
Epoch 41: reducing lr to 2.28187828827685e-05
Epoch 44: reducing lr to 9.275762447817767e-06
Epoch 47: reducing lr to 1.6044077629435213e-06
[I 2024-06-24 15:47:10,099] Trial 231 finished with value: 1.0929582118988037 and parameters: {'hidden_size': 75, 'n_layers': 5, 'rnn_dropout': 0.5206650053405577, 'bidirectional': False, 'fc_dropout': 0.5283661820888798, 'learning_rate_model': 0.00205445046128604}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.0004974885924452333
Epoch 23: reducing lr to 0.0004221187644953159
Epoch 26: reducing lr to 0.00036300031293017574
Epoch 29: reducing lr to 0.0002978779571920278
Epoch 32: reducing lr to 0.0002308435757295245
Epoch 35: reducing lr to 0.00016610920981127497
Epoch 38: reducing lr to 0.00010774233115893976
Epoch 41: reducing lr to 5.941031242371842e-05
Epoch 44: reducing lr to 2.415010247585076e-05
Epoch 47: reducing lr to 4.177188894833369e-06
[I 2024-06-24 15:47:15,311] Trial 232 finished with value: 1.0243594646453857 and parameters: {'hidden_size': 89, 'n_layers': 7, 'rnn_dropout': 0.12326333746588594, 'bidirectional': True, 'fc_dropout': 0.43879127173288324, 'learning_rate_model': 0.0053489068365791615}. Best is trial 212 with value: 0.9678940773010254.
Epoch 7: reducing lr to 0.002854637791618262
Epoch 10: reducing lr to 0.0038527943100507165
Epoch 13: reducing lr to 0.003994828264489576
Epoch 18: reducing lr to 0.0037280948511593678
Epoch 23: reducing lr to 0.003163286186639513
Epoch 26: reducing lr to 0.0027202625711527386
Epoch 29: reducing lr to 0.0022322467195139196
Epoch 32: reducing lr to 0.0017299024724776956
Epoch 35: reducing lr to 0.0012447941505226343
Epoch 38: reducing lr to 0.0008074026945447392
Epoch 41: reducing lr to 0.0004452107710932421
Epoch 44: reducing lr to 0.00018097675818587108
Epoch 47: reducing lr to 3.1303142720529725e-05
[I 2024-06-24 15:47:18,424] Trial 233 finished with value: 1.1019556522369385 and parameters: {'hidden_size': 59, 'n_layers': 2, 'rnn_dropout': 0.4067856107436928, 'bidirectional': True, 'fc_dropout': 0.09517866973841188, 'learning_rate_model': 0.040083797577684495}. Best is trial 212 with value: 0.9678940773010254.
Epoch 4: reducing lr to 0.00023921237259807217
Epoch 7: reducing lr to 0.0004745605336833244
Epoch 10: reducing lr to 0.0006404960129506494
Epoch 13: reducing lr to 0.0006641080135405552
Epoch 16: reducing lr to 0.000644092227467391
Epoch 19: reducing lr to 0.00060454091070022
Epoch 22: reducing lr to 0.0005479392292228845
Epoch 25: reducing lr to 0.0004778436297367755
Epoch 28: reducing lr to 0.00039865853898743176
Epoch 31: reducing lr to 0.0003153594252220385
Epoch 34: reducing lr to 0.000233180369085905
Epoch 37: reducing lr to 0.0001572848283692109
Epoch 40: reducing lr to 9.24416593394848e-05
Epoch 43: reducing lr to 4.2725164528840694e-05
Epoch 46: reducing lr to 1.1259286547137503e-05
Epoch 49: reducing lr to 2.1098281389499152e-08
[I 2024-06-24 15:47:22,378] Trial 234 finished with value: 1.107422113418579 and parameters: {'hidden_size': 104, 'n_layers': 7, 'rnn_dropout': 0.4427469649986126, 'bidirectional': False, 'fc_dropout': 0.6833903783331062, 'learning_rate_model': 0.006663608401168412}. Best is trial 212 with value: 0.9678940773010254.
Epoch 37: reducing lr to 1.2029920671836208e-05
Epoch 47: reducing lr to 3.9801961823627804e-07
[I 2024-06-24 15:47:26,784] Trial 235 finished with value: 0.9748578071594238 and parameters: {'hidden_size': 181, 'n_layers': 3, 'rnn_dropout': 0.42240111731010094, 'bidirectional': True, 'fc_dropout': 0.18517034842739824, 'learning_rate_model': 0.000509665689217419}. Best is trial 212 with value: 0.9678940773010254.
Epoch 34: reducing lr to 0.00011110255119895453
Epoch 37: reducing lr to 7.49408956046005e-05
Epoch 40: reducing lr to 4.404532092449733e-05
Epoch 43: reducing lr to 2.0357094373585523e-05
Epoch 46: reducing lr to 5.364668839709158e-06
Epoch 49: reducing lr to 1.0052616768174705e-08
[I 2024-06-24 15:47:30,630] Trial 236 finished with value: 1.0942424535751343 and parameters: {'hidden_size': 61, 'n_layers': 6, 'rnn_dropout': 0.7893419559744661, 'bidirectional': False, 'fc_dropout': 0.26409244175925944, 'learning_rate_model': 0.0031749837967185394}. Best is trial 212 with value: 0.9678940773010254.
Epoch 14: reducing lr to 0.00028173722652173985
Epoch 17: reducing lr to 0.0002703216999553481
Epoch 22: reducing lr to 0.00023400386662927536
Epoch 25: reducing lr to 0.00020406871973951968
Epoch 33: reducing lr to 0.00011108948503937355
Epoch 36: reducing lr to 7.754553128852923e-05
Epoch 39: reducing lr to 4.80697104579513e-05
Epoch 42: reducing lr to 2.4514081055864626e-05
Epoch 45: reducing lr to 8.358764210508437e-06
Epoch 48: reducing lr to 6.188375078009303e-07
[I 2024-06-24 15:47:34,098] Trial 237 finished with value: 1.093583106994629 and parameters: {'hidden_size': 63, 'n_layers': 5, 'rnn_dropout': 0.29691774757018363, 'bidirectional': False, 'fc_dropout': 0.753999099380966, 'learning_rate_model': 0.00284577202801892}. Best is trial 212 with value: 0.9678940773010254.
Epoch 34: reducing lr to 3.1942075727915504e-06
Epoch 37: reducing lr to 2.154556971633685e-06
Epoch 40: reducing lr to 1.2663066340495283e-06
Epoch 43: reducing lr to 5.85268153669104e-07
Epoch 46: reducing lr to 1.54234674617258e-07
Epoch 49: reducing lr to 2.8901356684393404e-10
[I 2024-06-24 15:47:40,404] Trial 238 finished with value: 1.077906608581543 and parameters: {'hidden_size': 55, 'n_layers': 7, 'rnn_dropout': 0.6003628373786594, 'bidirectional': True, 'fc_dropout': 0.6641252209298395, 'learning_rate_model': 9.128104780247618e-05}. Best is trial 212 with value: 0.9678940773010254.
Epoch 3: reducing lr to 5.628193226430451e-05
Epoch 14: reducing lr to 0.00022147474265000886
Epoch 17: reducing lr to 0.00021250095228613283
Epoch 22: reducing lr to 0.0001839513605662143
Epoch 29: reducing lr to 0.00012458145824926066
Epoch 32: reducing lr to 9.654567784389075e-05
Epoch 35: reducing lr to 6.947183263238407e-05
Epoch 38: reducing lr to 4.506106076960404e-05
Epoch 41: reducing lr to 2.4847167029615638e-05
Epoch 44: reducing lr to 1.0100294132778112e-05
Epoch 47: reducing lr to 1.7470251535446016e-06
[I 2024-06-24 15:47:44,649] Trial 239 finished with value: 1.093230128288269 and parameters: {'hidden_size': 132, 'n_layers': 5, 'rnn_dropout': 0.5939067437656814, 'bidirectional': False, 'fc_dropout': 0.722348721697368, 'learning_rate_model': 0.0022370725918161565}. Best is trial 212 with value: 0.9678940773010254.
[I 2024-06-24 15:47:47,478] Trial 240 finished with value: 1.099199652671814 and parameters: {'hidden_size': 32, 'n_layers': 1, 'rnn_dropout': 0.08421002133022953, 'bidirectional': True, 'fc_dropout': 0.6226407120541021, 'learning_rate_model': 1.4376264309585954e-05}. Best is trial 212 with value: 0.9678940773010254.
Epoch 4: reducing lr to 0.0022354514644863177
Epoch 7: reducing lr to 0.004434791681081907
Epoch 10: reducing lr to 0.005985466949712933
Epoch 13: reducing lr to 0.006206122264172125
Epoch 16: reducing lr to 0.006019073752407729
Epoch 19: reducing lr to 0.005649464739172903
Epoch 22: reducing lr to 0.005120519223618423
Epoch 25: reducing lr to 0.004465472376235865
Epoch 28: reducing lr to 0.0037254837830098613
Epoch 31: reducing lr to 0.0029470494410281634
Epoch 34: reducing lr to 0.002179082093041986
Epoch 37: reducing lr to 0.0014698345077250625
Epoch 40: reducing lr to 0.0008638718829866332
Epoch 43: reducing lr to 0.00039926877769358925
Epoch 46: reducing lr to 0.00010521859019039861
Epoch 49: reducing lr to 1.9716448408646503e-07
[I 2024-06-24 15:47:52,458] Trial 241 finished with value: 1.1072635650634766 and parameters: {'hidden_size': 60, 'n_layers': 7, 'rnn_dropout': 0.7788150886690349, 'bidirectional': False, 'fc_dropout': 0.03476123337198143, 'learning_rate_model': 0.06227175040056983}. Best is trial 212 with value: 0.9678940773010254.
Epoch 28: reducing lr to 6.43374052191204e-06
Epoch 34: reducing lr to 3.76317535631587e-06
Epoch 37: reducing lr to 2.5383371351613628e-06
Epoch 42: reducing lr to 9.263763287943995e-07
Epoch 45: reducing lr to 3.158740188931671e-07
Epoch 48: reducing lr to 2.3385596926536636e-08
[I 2024-06-24 15:47:57,780] Trial 242 finished with value: 1.0752136707305908 and parameters: {'hidden_size': 73, 'n_layers': 6, 'rnn_dropout': 0.6485032827575212, 'bidirectional': True, 'fc_dropout': 0.7445155137323587, 'learning_rate_model': 0.00010754047185755203}. Best is trial 212 with value: 0.9678940773010254.
Epoch 7: reducing lr to 6.641686011352699e-05
Epoch 16: reducing lr to 9.014357565700565e-05
Epoch 19: reducing lr to 8.460819273621704e-05
Epoch 22: reducing lr to 7.668653534154799e-05
Epoch 25: reducing lr to 6.687634402726273e-05
Epoch 28: reducing lr to 5.5794037931228215e-05
Epoch 31: reducing lr to 4.413595599256298e-05
Epoch 34: reducing lr to 3.2634631107216643e-05
Epoch 37: reducing lr to 2.201271218804906e-05
Epoch 40: reducing lr to 1.2937621907492862e-05
Epoch 43: reducing lr to 5.97957704955934e-06
Epoch 46: reducing lr to 1.5757872947739843e-06
Epoch 49: reducing lr to 2.952798440296594e-09
[I 2024-06-24 15:48:01,925] Trial 243 finished with value: 1.0925215482711792 and parameters: {'hidden_size': 188, 'n_layers': 5, 'rnn_dropout': 0.7883559319558645, 'bidirectional': False, 'fc_dropout': 0.45244338830430486, 'learning_rate_model': 0.0009326016716911777}. Best is trial 212 with value: 0.9678940773010254.
[I 2024-06-24 15:48:04,903] Trial 244 finished with value: 1.1066150665283203 and parameters: {'hidden_size': 63, 'n_layers': 3, 'rnn_dropout': 0.1995697653611102, 'bidirectional': False, 'fc_dropout': 0.4019160047186067, 'learning_rate_model': 2.15159018655157e-05}. Best is trial 212 with value: 0.9678940773010254.
[I 2024-06-24 15:48:10,079] Trial 245 finished with value: 1.0964365005493164 and parameters: {'hidden_size': 159, 'n_layers': 4, 'rnn_dropout': 0.5915102298771057, 'bidirectional': True, 'fc_dropout': 0.7889974259490361, 'learning_rate_model': 1.2970911036527296e-05}. Best is trial 212 with value: 0.9678940773010254.
Epoch 9: reducing lr to 0.0014696690865700838
Epoch 15: reducing lr to 0.0015997493459552984
Epoch 18: reducing lr to 0.0015182765008517213
Epoch 21: reducing lr to 0.0013926908584228504
Epoch 24: reducing lr to 0.0012308833878410062
Epoch 27: reducing lr to 0.0010430212171907553
Epoch 30: reducing lr to 0.0008409081429505437
Epoch 33: reducing lr to 0.0006372438086796981
Epoch 36: reducing lr to 0.00044482526574750705
Epoch 39: reducing lr to 0.0002757427974708699
Epoch 42: reducing lr to 0.00014062038700409212
Epoch 45: reducing lr to 4.794846910553263e-05
Epoch 48: reducing lr to 3.5498442565031625e-06
[I 2024-06-24 15:48:15,280] Trial 246 finished with value: 1.1072977781295776 and parameters: {'hidden_size': 188, 'n_layers': 7, 'rnn_dropout': 0.3127349692011665, 'bidirectional': False, 'fc_dropout': 0.14237786837613556, 'learning_rate_model': 0.016324232713169764}. Best is trial 212 with value: 0.9678940773010254.
[I 2024-06-24 15:48:19,713] Trial 247 finished with value: 1.0539982318878174 and parameters: {'hidden_size': 62, 'n_layers': 3, 'rnn_dropout': 0.7327326320079657, 'bidirectional': True, 'fc_dropout': 0.6902711844628926, 'learning_rate_model': 0.00025711669394827373}. Best is trial 212 with value: 0.9678940773010254.
Epoch 6: reducing lr to 0.0005177586437311348
Epoch 9: reducing lr to 0.0007811363320507244
Epoch 12: reducing lr to 0.0008674107690828048
Epoch 15: reducing lr to 0.000850274628295025
Epoch 18: reducing lr to 0.0008069714112868538
Epoch 21: reducing lr to 0.0007402220260126035
Epoch 27: reducing lr to 0.0005543708956612227
Epoch 30: reducing lr to 0.00044694680481370395
Epoch 33: reducing lr to 0.0003386982116469492
Epoch 36: reducing lr to 0.0002364268117664641
Epoch 39: reducing lr to 0.00014655865008937827
Epoch 42: reducing lr to 7.474042580039794e-05
Epoch 45: reducing lr to 2.548484664119475e-05
Epoch 48: reducing lr to 1.8867596435247214e-06
[I 2024-06-24 15:48:23,878] Trial 248 finished with value: 1.0937447547912598 and parameters: {'hidden_size': 200, 'n_layers': 4, 'rnn_dropout': 0.2358496094387963, 'bidirectional': False, 'fc_dropout': 0.37191010472945796, 'learning_rate_model': 0.008676409799751065}. Best is trial 212 with value: 0.9678940773010254.
Epoch 7: reducing lr to 0.005107939038052859
Epoch 10: reducing lr to 0.006893987923679699
Epoch 13: reducing lr to 0.007148136027070424
Epoch 18: reducing lr to 0.0066708572568170196
Epoch 21: reducing lr to 0.006119071140336463
Epoch 24: reducing lr to 0.005408137039247113
Epoch 27: reducing lr to 0.004582726303020475
Epoch 30: reducing lr to 0.003694701317297147
Epoch 33: reducing lr to 0.002799860554456306
Epoch 36: reducing lr to 0.001954430467943544
Epoch 39: reducing lr to 0.0012115321817150499
Epoch 42: reducing lr to 0.0006178443311059837
Epoch 45: reducing lr to 0.00021067137172081364
Epoch 48: reducing lr to 1.559696425900009e-05
[I 2024-06-24 15:48:28,235] Trial 249 finished with value: 1.1077113151550293 and parameters: {'hidden_size': 134, 'n_layers': 6, 'rnn_dropout': 0.04088872306089302, 'bidirectional': False, 'fc_dropout': 0.4415960217233577, 'learning_rate_model': 0.07172384357890647}. Best is trial 212 with value: 0.9678940773010254.
Epoch 24: reducing lr to 5.6380661091750176e-05
Epoch 28: reducing lr to 4.47339898139332e-05
Epoch 33: reducing lr to 2.9188977253974455e-05
Epoch 36: reducing lr to 2.037523846767314e-05
Epoch 39: reducing lr to 1.2630409481734249e-05
Epoch 42: reducing lr to 6.4411222546230144e-06
Epoch 45: reducing lr to 2.1962814781740276e-06
Epoch 48: reducing lr to 1.6260075319194388e-07
[I 2024-06-24 15:48:31,237] Trial 250 finished with value: 0.9841709136962891 and parameters: {'hidden_size': 148, 'n_layers': 1, 'rnn_dropout': 0.43608203277624996, 'bidirectional': True, 'fc_dropout': 0.3641028384377997, 'learning_rate_model': 0.0007477321095367409}. Best is trial 212 with value: 0.9678940773010254.
Epoch 33: reducing lr to 2.8937243266461667e-05
Epoch 38: reducing lr to 1.4931575758885624e-05
Epoch 41: reducing lr to 8.233435932485956e-06
Epoch 44: reducing lr to 3.3468654411335055e-06
Epoch 47: reducing lr to 5.788997859195291e-07
[I 2024-06-24 15:48:34,541] Trial 251 finished with value: 0.9835171103477478 and parameters: {'hidden_size': 149, 'n_layers': 1, 'rnn_dropout': 0.4330004870326982, 'bidirectional': True, 'fc_dropout': 0.3744022888693995, 'learning_rate_model': 0.0007412834565439608}. Best is trial 212 with value: 0.9678940773010254.
Epoch 24: reducing lr to 5.571879814797501e-05
Epoch 28: reducing lr to 4.420884928504021e-05
Epoch 33: reducing lr to 2.884632248482119e-05
Epoch 36: reducing lr to 2.013604979816837e-05
Epoch 39: reducing lr to 1.2482138783257291e-05
Epoch 42: reducing lr to 6.365508736545897e-06
Epoch 45: reducing lr to 2.1704989262075247e-06
Epoch 48: reducing lr to 1.6069195306289652e-07
[I 2024-06-24 15:48:37,610] Trial 252 finished with value: 0.9842046499252319 and parameters: {'hidden_size': 148, 'n_layers': 1, 'rnn_dropout': 0.44157597851985025, 'bidirectional': True, 'fc_dropout': 0.48181831640622513, 'learning_rate_model': 0.0007389543448637116}. Best is trial 212 with value: 0.9678940773010254.
Epoch 28: reducing lr to 4.56103131537913e-05
Epoch 33: reducing lr to 2.9760779191173402e-05
Epoch 36: reducing lr to 2.077438232000252e-05
Epoch 39: reducing lr to 1.2877834821321593e-05
Epoch 42: reducing lr to 6.567301604823688e-06
Epoch 45: reducing lr to 2.2393058703247666e-06
Epoch 48: reducing lr to 1.6578604553214295e-07
[I 2024-06-24 15:48:40,404] Trial 253 finished with value: 0.9832673072814941 and parameters: {'hidden_size': 147, 'n_layers': 1, 'rnn_dropout': 0.4277433863769233, 'bidirectional': True, 'fc_dropout': 0.483594034774835, 'learning_rate_model': 0.0007623799221345855}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 8.944456383680063e-05
Epoch 28: reducing lr to 6.780721851813024e-05
Epoch 33: reducing lr to 4.4244284205664894e-05
Epoch 38: reducing lr to 2.2829986790076573e-05
Epoch 41: reducing lr to 1.2588707086975568e-05
Epoch 44: reducing lr to 5.117269271715788e-06
Epoch 47: reducing lr to 8.851225536230575e-07
[I 2024-06-24 15:48:43,268] Trial 254 finished with value: 0.9832742810249329 and parameters: {'hidden_size': 148, 'n_layers': 1, 'rnn_dropout': 0.43631826665227075, 'bidirectional': True, 'fc_dropout': 0.36275789303974226, 'learning_rate_model': 0.0011334029170049205}. Best is trial 212 with value: 0.9678940773010254.
Epoch 28: reducing lr to 4.629353063755993e-05
Epoch 33: reducing lr to 3.0206579346182805e-05
Epoch 36: reducing lr to 2.108557117695383e-05
Epoch 39: reducing lr to 1.3070737726270836e-05
Epoch 42: reducing lr to 6.66567618213623e-06
Epoch 45: reducing lr to 2.2728494444930272e-06
Epoch 48: reducing lr to 1.6826942959685113e-07
[I 2024-06-24 15:48:46,038] Trial 255 finished with value: 0.9832248091697693 and parameters: {'hidden_size': 147, 'n_layers': 1, 'rnn_dropout': 0.4338190839628328, 'bidirectional': True, 'fc_dropout': 0.6605473010340513, 'learning_rate_model': 0.0007737999553696175}. Best is trial 212 with value: 0.9678940773010254.
Epoch 28: reducing lr to 4.412892728580159e-05
Epoch 33: reducing lr to 2.8794173292952297e-05
Epoch 36: reducing lr to 2.0099647281869842e-05
Epoch 39: reducing lr to 1.245957322223353e-05
Epoch 42: reducing lr to 6.3540009910916875e-06
Epoch 45: reducing lr to 2.1665750372953906e-06
Epoch 48: reducing lr to 1.6040144963749054e-07
[I 2024-06-24 15:48:48,852] Trial 256 finished with value: 0.9833581447601318 and parameters: {'hidden_size': 147, 'n_layers': 1, 'rnn_dropout': 0.43791264985470163, 'bidirectional': True, 'fc_dropout': 0.662939987828469, 'learning_rate_model': 0.0007376184424472795}. Best is trial 212 with value: 0.9678940773010254.
Epoch 28: reducing lr to 4.509535988450292e-05
Epoch 33: reducing lr to 2.942477162004822e-05
Epoch 36: reducing lr to 2.0539833698135783e-05
Epoch 39: reducing lr to 1.2732440442637212e-05
Epoch 42: reducing lr to 6.493154921804784e-06
Epoch 45: reducing lr to 2.214023476955257e-06
Epoch 48: reducing lr to 1.6391427442937096e-07
[I 2024-06-24 15:48:51,692] Trial 257 finished with value: 0.9832991361618042 and parameters: {'hidden_size': 147, 'n_layers': 1, 'rnn_dropout': 0.4224080437440172, 'bidirectional': True, 'fc_dropout': 0.70815060818676, 'learning_rate_model': 0.0007537724382960232}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 9.53672497297125e-05
Epoch 28: reducing lr to 7.229715998944959e-05
Epoch 33: reducing lr to 4.717397592382228e-05
Epoch 38: reducing lr to 2.4341703488071344e-05
Epoch 41: reducing lr to 1.342228438531276e-05
Epoch 44: reducing lr to 5.4561157843011946e-06
Epoch 47: reducing lr to 9.437320726029446e-07
[I 2024-06-24 15:48:54,568] Trial 258 finished with value: 0.9822635650634766 and parameters: {'hidden_size': 149, 'n_layers': 1, 'rnn_dropout': 0.3820092887743882, 'bidirectional': True, 'fc_dropout': 0.6544955902977807, 'learning_rate_model': 0.0012084526369608283}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 8.430759331893351e-05
Epoch 28: reducing lr to 6.391292167677323e-05
Epoch 33: reducing lr to 4.170325125967916e-05
Epoch 36: reducing lr to 2.9110772943493712e-05
Epoch 39: reducing lr to 1.8045481194709432e-05
Epoch 42: reducing lr to 9.202643088232173e-06
Epoch 45: reducing lr to 3.1378995407863867e-06
Epoch 48: reducing lr to 2.323130408570049e-07
[I 2024-06-24 15:48:57,387] Trial 259 finished with value: 0.9833513498306274 and parameters: {'hidden_size': 148, 'n_layers': 1, 'rnn_dropout': 0.423641521113025, 'bidirectional': True, 'fc_dropout': 0.6570726628849466, 'learning_rate_model': 0.0010683094432400746}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 8.671198832265695e-05
Epoch 28: reducing lr to 6.573567456893164e-05
Epoch 33: reducing lr to 4.289259951433131e-05
Epoch 36: reducing lr to 2.9940992313593395e-05
Epoch 39: reducing lr to 1.8560125998531645e-05
Epoch 42: reducing lr to 9.465096186361691e-06
Epoch 45: reducing lr to 3.2273902934106806e-06
Epoch 48: reducing lr to 2.389384501795414e-07
[I 2024-06-24 15:49:00,210] Trial 260 finished with value: 0.9832654595375061 and parameters: {'hidden_size': 140, 'n_layers': 1, 'rnn_dropout': 0.45404582574835023, 'bidirectional': True, 'fc_dropout': 0.6583120967434469, 'learning_rate_model': 0.0010987768992145313}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 8.532427538196803e-05
Epoch 28: reducing lr to 6.468366033158324e-05
Epoch 33: reducing lr to 4.22061590744657e-05
Epoch 38: reducing lr to 2.177831716410841e-05
Epoch 41: reducing lr to 1.2008804829680478e-05
Epoch 44: reducing lr to 4.881540854067158e-06
Epoch 47: reducing lr to 8.443491395398886e-07
[I 2024-06-24 15:49:03,304] Trial 261 finished with value: 0.9824992418289185 and parameters: {'hidden_size': 147, 'n_layers': 1, 'rnn_dropout': 0.3900913394088787, 'bidirectional': True, 'fc_dropout': 0.7030380103674337, 'learning_rate_model': 0.0010811923996376527}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 8.47889904210091e-05
Epoch 28: reducing lr to 6.427786502373832e-05
Epoch 33: reducing lr to 4.194137719250699e-05
Epoch 38: reducing lr to 2.1641690095190985e-05
Epoch 41: reducing lr to 1.1933467153554398e-05
Epoch 44: reducing lr to 4.8509163290560726e-06
Epoch 47: reducing lr to 8.390520843446327e-07
[I 2024-06-24 15:49:06,168] Trial 262 finished with value: 0.9828023910522461 and parameters: {'hidden_size': 138, 'n_layers': 1, 'rnn_dropout': 0.41998241029073735, 'bidirectional': True, 'fc_dropout': 0.6559108643419111, 'learning_rate_model': 0.0010744094995914668}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 8.437168222139985e-05
Epoch 28: reducing lr to 6.396150696835173e-05
Epoch 33: reducing lr to 4.173495321554452e-05
Epoch 38: reducing lr to 2.153517562102069e-05
Epoch 41: reducing lr to 1.1874733895047401e-05
Epoch 44: reducing lr to 4.827041446837498e-06
Epoch 47: reducing lr to 8.349224996785469e-07
[I 2024-06-24 15:49:09,054] Trial 263 finished with value: 0.9827499389648438 and parameters: {'hidden_size': 142, 'n_layers': 1, 'rnn_dropout': 0.3868278460799341, 'bidirectional': True, 'fc_dropout': 0.707961459231556, 'learning_rate_model': 0.0010691215501573326}. Best is trial 212 with value: 0.9678940773010254.
Epoch 27: reducing lr to 9.156559053261739e-05
Epoch 33: reducing lr to 5.594287507608717e-05
Epoch 38: reducing lr to 2.886644279403777e-05
Epoch 41: reducing lr to 1.5917275656726713e-05
Epoch 44: reducing lr to 6.470321776878081e-06
Epoch 47: reducing lr to 1.119157001482746e-06
[I 2024-06-24 15:49:11,907] Trial 264 finished with value: 0.9830434322357178 and parameters: {'hidden_size': 142, 'n_layers': 1, 'rnn_dropout': 0.39190788750104466, 'bidirectional': True, 'fc_dropout': 0.699748142258268, 'learning_rate_model': 0.0014330849495076876}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 0.00011271903928280447
Epoch 27: reducing lr to 9.126158761500687e-05
Epoch 33: reducing lr to 5.575714158009018e-05
Epoch 38: reducing lr to 2.8770604578181996e-05
Epoch 41: reducing lr to 1.5864429405073547e-05
Epoch 44: reducing lr to 6.448839944165502e-06
Epoch 47: reducing lr to 1.115441337206052e-06
[I 2024-06-24 15:49:14,734] Trial 265 finished with value: 0.9837794899940491 and parameters: {'hidden_size': 139, 'n_layers': 1, 'rnn_dropout': 0.38197030310383634, 'bidirectional': True, 'fc_dropout': 0.654355749227038, 'learning_rate_model': 0.0014283270267629107}. Best is trial 212 with value: 0.9678940773010254.
Epoch 27: reducing lr to 0.00010506546818382246
Epoch 30: reducing lr to 8.470624209989526e-05
Epoch 33: reducing lr to 6.419075470631575e-05
Epoch 36: reducing lr to 4.480807680176623e-05
Epoch 39: reducing lr to 2.7776085123775036e-05
Epoch 42: reducing lr to 1.4164953265829094e-05
Epoch 45: reducing lr to 4.829938521134602e-06
Epoch 48: reducing lr to 3.5758241792404993e-07
[I 2024-06-24 15:49:17,585] Trial 266 finished with value: 0.9844813346862793 and parameters: {'hidden_size': 141, 'n_layers': 1, 'rnn_dropout': 0.3844327661476741, 'bidirectional': True, 'fc_dropout': 0.699559259113046, 'learning_rate_model': 0.0016443703392441911}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 9.317774181702716e-05
Epoch 28: reducing lr to 7.063731130648753e-05
Epoch 33: reducing lr to 4.6090922843747404e-05
Epoch 38: reducing lr to 2.378284966197881e-05
Epoch 41: reducing lr to 1.3114126207833125e-05
Epoch 44: reducing lr to 5.3308504682087966e-06
Epoch 47: reducing lr to 9.220652126874741e-07
[I 2024-06-24 15:49:20,441] Trial 267 finished with value: 0.9832401871681213 and parameters: {'hidden_size': 153, 'n_layers': 1, 'rnn_dropout': 0.458584842046029, 'bidirectional': True, 'fc_dropout': 0.6452832503601994, 'learning_rate_model': 0.0011807081374787714}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 7.27260674129031e-05
Epoch 28: reducing lr to 5.513305821501541e-05
Epoch 33: reducing lr to 3.597438075328787e-05
Epoch 36: reducing lr to 2.5111759833082393e-05
Epoch 39: reducing lr to 1.5566532386946753e-05
Epoch 42: reducing lr to 7.938455069875152e-06
Epoch 45: reducing lr to 2.7068391416992126e-06
Epoch 48: reducing lr to 2.0039966988914847e-07
[I 2024-06-24 15:49:23,279] Trial 268 finished with value: 0.9832949638366699 and parameters: {'hidden_size': 137, 'n_layers': 1, 'rnn_dropout': 0.4602120026107279, 'bidirectional': True, 'fc_dropout': 0.6466918327161052, 'learning_rate_model': 0.0009215533444657161}. Best is trial 212 with value: 0.9678940773010254.
Epoch 27: reducing lr to 8.298798644105996e-05
Epoch 33: reducing lr to 5.0702305650881554e-05
Epoch 38: reducing lr to 2.616231653461451e-05
Epoch 41: reducing lr to 1.4426190544891476e-05
Epoch 44: reducing lr to 5.864200435616454e-06
Epoch 47: reducing lr to 1.0143175566741052e-06
[I 2024-06-24 15:49:26,134] Trial 269 finished with value: 0.9834065437316895 and parameters: {'hidden_size': 140, 'n_layers': 1, 'rnn_dropout': 0.40382643883803204, 'bidirectional': True, 'fc_dropout': 0.6670845776818038, 'learning_rate_model': 0.0012988376273974488}. Best is trial 212 with value: 0.9678940773010254.
Epoch 28: reducing lr to 2.6396246009770595e-05
Epoch 34: reducing lr to 1.5439494667978758e-05
Epoch 42: reducing lr to 3.800721740207508e-06
Epoch 45: reducing lr to 1.295962789049659e-06
Epoch 48: reducing lr to 9.594604685343168e-08
[I 2024-06-24 15:49:28,842] Trial 270 finished with value: 0.985194206237793 and parameters: {'hidden_size': 153, 'n_layers': 1, 'rnn_dropout': 0.3958287903663637, 'bidirectional': True, 'fc_dropout': 0.7006368379247627, 'learning_rate_model': 0.00044121529948104485}. Best is trial 212 with value: 0.9678940773010254.
Epoch 27: reducing lr to 0.00011101199148235868
Epoch 30: reducing lr to 8.95004685083018e-05
Epoch 33: reducing lr to 6.782384010544888e-05
Epoch 36: reducing lr to 4.734413624422852e-05
Epoch 39: reducing lr to 2.9348163373516153e-05
Epoch 42: reducing lr to 1.4966665056334421e-05
Epoch 45: reducing lr to 5.103304665529207e-06
Epoch 48: reducing lr to 3.7782096267229526e-07
[I 2024-06-24 15:49:31,605] Trial 271 finished with value: 0.9833256006240845 and parameters: {'hidden_size': 138, 'n_layers': 1, 'rnn_dropout': 0.45236931781879247, 'bidirectional': True, 'fc_dropout': 0.642878900891806, 'learning_rate_model': 0.001737438848838889}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 8.446900930165268e-05
Epoch 28: reducing lr to 6.403528986040625e-05
Epoch 33: reducing lr to 4.178309663326491e-05
Epoch 38: reducing lr to 2.1560017555076537e-05
Epoch 41: reducing lr to 1.1888432012097518e-05
Epoch 44: reducing lr to 4.8326096877201154e-06
Epoch 47: reducing lr to 8.358856257771477e-07
[I 2024-06-24 15:49:34,397] Trial 272 finished with value: 0.9834946393966675 and parameters: {'hidden_size': 144, 'n_layers': 1, 'rnn_dropout': 0.3736005654622183, 'bidirectional': True, 'fc_dropout': 0.6840352955491704, 'learning_rate_model': 0.001070354836921002}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 0.00010461561550599371
Epoch 27: reducing lr to 8.470074994557364e-05
Epoch 33: reducing lr to 5.174873492863255e-05
Epoch 38: reducing lr to 2.6702272531568947e-05
Epoch 41: reducing lr to 1.4723928250480944e-05
Epoch 44: reducing lr to 5.985229863127747e-06
Epoch 47: reducing lr to 1.035251744471201e-06
[I 2024-06-24 15:49:37,187] Trial 273 finished with value: 0.9834248423576355 and parameters: {'hidden_size': 153, 'n_layers': 1, 'rnn_dropout': 0.40694641383020574, 'bidirectional': True, 'fc_dropout': 0.7033413986037939, 'learning_rate_model': 0.0013256439373454008}. Best is trial 212 with value: 0.9678940773010254.
[I 2024-06-24 15:49:39,951] Trial 274 finished with value: 1.0269005298614502 and parameters: {'hidden_size': 37, 'n_layers': 1, 'rnn_dropout': 0.4584525583641338, 'bidirectional': True, 'fc_dropout': 0.6744714206729608, 'learning_rate_model': 0.0005622958169112283}. Best is trial 212 with value: 0.9678940773010254.
Epoch 28: reducing lr to 5.181435133470337e-05
Epoch 33: reducing lr to 3.380892088608271e-05
Epoch 36: reducing lr to 2.3600170002353633e-05
Epoch 39: reducing lr to 1.4629512751038165e-05
Epoch 42: reducing lr to 7.460603735078941e-06
Epoch 45: reducing lr to 2.543902312611651e-06
Epoch 48: reducing lr to 1.8833671193242479e-07
[I 2024-06-24 15:49:42,764] Trial 275 finished with value: 0.9838753342628479 and parameters: {'hidden_size': 135, 'n_layers': 1, 'rnn_dropout': 0.41643931941069545, 'bidirectional': True, 'fc_dropout': 0.6564328559252933, 'learning_rate_model': 0.0008660809015454356}. Best is trial 212 with value: 0.9678940773010254.
Epoch 27: reducing lr to 9.455172792800746e-05
Epoch 33: reducing lr to 5.7767284336144654e-05
Epoch 38: reducing lr to 2.9807835338963875e-05
Epoch 41: reducing lr to 1.643636991249249e-05
Epoch 44: reducing lr to 6.681331935888177e-06
Epoch 47: reducing lr to 1.1556549539777856e-06
[I 2024-06-24 15:49:45,637] Trial 276 finished with value: 0.982149600982666 and parameters: {'hidden_size': 185, 'n_layers': 1, 'rnn_dropout': 0.3865901688472135, 'bidirectional': True, 'fc_dropout': 0.7067397756192848, 'learning_rate_model': 0.001479820721467474}. Best is trial 212 with value: 0.9678940773010254.
Epoch 27: reducing lr to 9.80989940318867e-05
Epoch 33: reducing lr to 5.9934520558361396e-05
Epoch 36: reducing lr to 4.1837031088713194e-05
Epoch 39: reducing lr to 2.593436317267503e-05
Epoch 42: reducing lr to 1.3225731440660772e-05
Epoch 45: reducing lr to 4.509684469593592e-06
Epoch 48: reducing lr to 3.338725471671317e-07
[I 2024-06-24 15:49:48,627] Trial 277 finished with value: 0.9823644161224365 and parameters: {'hidden_size': 193, 'n_layers': 1, 'rnn_dropout': 0.39848847399660847, 'bidirectional': True, 'fc_dropout': 0.7095422575525528, 'learning_rate_model': 0.0015353386691571937}. Best is trial 212 with value: 0.9678940773010254.
Epoch 27: reducing lr to 0.00010591801171335557
Epoch 30: reducing lr to 8.539358266822533e-05
Epoch 33: reducing lr to 6.471162434623367e-05
Epoch 36: reducing lr to 4.5171667585765157e-05
Epoch 39: reducing lr to 2.8001471466760772e-05
Epoch 42: reducing lr to 1.4279893402314244e-05
Epoch 45: reducing lr to 4.869130587809001e-06
Epoch 48: reducing lr to 3.6048398569838053e-07
[I 2024-06-24 15:49:51,648] Trial 278 finished with value: 0.9828068017959595 and parameters: {'hidden_size': 195, 'n_layers': 1, 'rnn_dropout': 0.38212718376211036, 'bidirectional': True, 'fc_dropout': 0.711692015826812, 'learning_rate_model': 0.0016577134225341838}. Best is trial 212 with value: 0.9678940773010254.
Epoch 33: reducing lr to 6.513172184369602e-05
Epoch 38: reducing lr to 3.360787446338969e-05
Epoch 41: reducing lr to 1.8531753492707135e-05
Epoch 44: reducing lr to 7.533098676778031e-06
Epoch 47: reducing lr to 1.3029831309323585e-06
[I 2024-06-24 15:49:54,530] Trial 279 finished with value: 0.9816138744354248 and parameters: {'hidden_size': 185, 'n_layers': 1, 'rnn_dropout': 0.38578642621175185, 'bidirectional': True, 'fc_dropout': 0.7157734473040445, 'learning_rate_model': 0.0016684750324821938}. Best is trial 212 with value: 0.9678940773010254.
Epoch 27: reducing lr to 0.00010310811072104865
Epoch 33: reducing lr to 6.299488840564251e-05
Epoch 36: reducing lr to 4.397330753802462e-05
Epoch 39: reducing lr to 2.725861988573375e-05
Epoch 42: reducing lr to 1.3901061832573388e-05
Epoch 45: reducing lr to 4.739957327765348e-06
Epoch 48: reducing lr to 3.5092069903220823e-07
[I 2024-06-24 15:49:57,474] Trial 280 finished with value: 0.9831540584564209 and parameters: {'hidden_size': 192, 'n_layers': 1, 'rnn_dropout': 0.39645529077169583, 'bidirectional': True, 'fc_dropout': 0.7257036856988575, 'learning_rate_model': 0.0016137359109137314}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.0001691255324848814
Epoch 22: reducing lr to 0.00014952506897258523
Epoch 27: reducing lr to 0.00011618537114383188
Epoch 30: reducing lr to 9.367136840200159e-05
Epoch 33: reducing lr to 7.098456598991602e-05
Epoch 36: reducing lr to 4.955046718438493e-05
Epoch 39: reducing lr to 3.071584617490314e-05
Epoch 42: reducing lr to 1.5664141424144877e-05
Epoch 45: reducing lr to 5.341128815969246e-06
Epoch 48: reducing lr to 3.9542817120775174e-07
[I 2024-06-24 15:50:02,056] Trial 281 finished with value: 0.9751477241516113 and parameters: {'hidden_size': 194, 'n_layers': 3, 'rnn_dropout': 0.3711481510262898, 'bidirectional': True, 'fc_dropout': 0.7140529599992267, 'learning_rate_model': 0.0018184069558299695}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00016799552703254848
Epoch 22: reducing lr to 0.00014852602322995272
Epoch 27: reducing lr to 0.00011540908325314578
Epoch 30: reducing lr to 9.304550691635507e-05
Epoch 36: reducing lr to 4.9219397728098066e-05
Epoch 39: reducing lr to 3.051061948239435e-05
Epoch 42: reducing lr to 1.5559482092373178e-05
Epoch 47: reducing lr to 1.4105831984062958e-06
[I 2024-06-24 15:50:06,770] Trial 282 finished with value: 0.9708616733551025 and parameters: {'hidden_size': 195, 'n_layers': 3, 'rnn_dropout': 0.36285351329196525, 'bidirectional': True, 'fc_dropout': 0.713788008363972, 'learning_rate_model': 0.0018062573427913135}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00016977784232773263
Epoch 22: reducing lr to 0.00015010178067785388
Epoch 27: reducing lr to 0.00011663349308070868
Epoch 30: reducing lr to 9.40326548068644e-05
Epoch 33: reducing lr to 7.125835038193198e-05
Epoch 36: reducing lr to 4.9741581187027565e-05
Epoch 39: reducing lr to 3.083431586127759e-05
Epoch 42: reducing lr to 1.5724557338174276e-05
Epoch 45: reducing lr to 5.3617293181371055e-06
Epoch 48: reducing lr to 3.9695332051212665e-07
[I 2024-06-24 15:50:11,388] Trial 283 finished with value: 0.9733068943023682 and parameters: {'hidden_size': 185, 'n_layers': 3, 'rnn_dropout': 0.358866808733998, 'bidirectional': True, 'fc_dropout': 0.7114308080814433, 'learning_rate_model': 0.0018254204725838817}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00016901323001884547
Epoch 22: reducing lr to 0.00014942578157503447
Epoch 27: reducing lr to 0.0001161082219191995
Epoch 30: reducing lr to 9.360916888952146e-05
Epoch 33: reducing lr to 7.093743093174928e-05
Epoch 36: reducing lr to 4.9517564762282974e-05
Epoch 39: reducing lr to 3.0695450287770446e-05
Epoch 42: reducing lr to 1.565374014596752e-05
Epoch 47: reducing lr to 1.4191283945719137e-06
[I 2024-06-24 15:50:16,101] Trial 284 finished with value: 0.9705414772033691 and parameters: {'hidden_size': 195, 'n_layers': 3, 'rnn_dropout': 0.3585458206692097, 'bidirectional': True, 'fc_dropout': 0.7106456944977932, 'learning_rate_model': 0.001817199500146631}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00016326222969923065
Epoch 22: reducing lr to 0.00014434128187343774
Epoch 32: reducing lr to 7.575658509093212e-05
Epoch 36: reducing lr to 4.7832634353328084e-05
Epoch 39: reducing lr to 2.9650978536085635e-05
Epoch 42: reducing lr to 1.5121091520930367e-05
Epoch 45: reducing lr to 5.1559607044187985e-06
Epoch 48: reducing lr to 3.817193298299602e-07
[I 2024-06-24 15:50:20,699] Trial 285 finished with value: 0.9755646586418152 and parameters: {'hidden_size': 184, 'n_layers': 3, 'rnn_dropout': 0.3589435209658518, 'bidirectional': True, 'fc_dropout': 0.7144592757558663, 'learning_rate_model': 0.00175536579100456}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00017597197850013847
Epoch 22: reducing lr to 0.00015557805989363326
Epoch 27: reducing lr to 0.0001208887229063457
Epoch 32: reducing lr to 8.165413511394815e-05
Epoch 36: reducing lr to 5.155634171807775e-05
Epoch 39: reducing lr to 3.1959268025877734e-05
Epoch 42: reducing lr to 1.629824851052076e-05
Epoch 45: reducing lr to 5.5573454307038556e-06
Epoch 48: reducing lr to 4.114356751446684e-07
[I 2024-06-24 15:50:25,560] Trial 286 finished with value: 0.9779325723648071 and parameters: {'hidden_size': 184, 'n_layers': 3, 'rnn_dropout': 0.3548892477947076, 'bidirectional': True, 'fc_dropout': 0.7152943251293137, 'learning_rate_model': 0.0018920186977943046}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00017738497984375178
Epoch 22: reducing lr to 0.00015682730428777
Epoch 27: reducing lr to 0.00012185942249925974
Epoch 32: reducing lr to 8.230979292726019e-05
Epoch 36: reducing lr to 5.1970323425508345e-05
Epoch 39: reducing lr to 3.221589120558147e-05
Epoch 42: reducing lr to 1.642911847766096e-05
Epoch 45: reducing lr to 5.601969220397169e-06
Epoch 48: reducing lr to 4.147393781929725e-07
[I 2024-06-24 15:50:30,197] Trial 287 finished with value: 0.9784103035926819 and parameters: {'hidden_size': 184, 'n_layers': 3, 'rnn_dropout': 0.35553125321313594, 'bidirectional': True, 'fc_dropout': 0.7287143903272238, 'learning_rate_model': 0.0019072110311698298}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.0001829875614857891
Epoch 22: reducing lr to 0.00016178058599598908
Epoch 27: reducing lr to 0.0001257082678975849
Epoch 30: reducing lr to 0.00010134895088328268
Epoch 33: reducing lr to 7.680267102652266e-05
Epoch 36: reducing lr to 5.3611770070038076e-05
Epoch 39: reducing lr to 3.323340780033066e-05
Epoch 42: reducing lr to 1.6948020797681984e-05
Epoch 45: reducing lr to 5.778903535473383e-06
Epoch 48: reducing lr to 4.2783863399551736e-07
[I 2024-06-24 15:50:34,831] Trial 288 finished with value: 0.9801077842712402 and parameters: {'hidden_size': 184, 'n_layers': 3, 'rnn_dropout': 0.3512285454114787, 'bidirectional': True, 'fc_dropout': 0.7144028236023017, 'learning_rate_model': 0.0019674489696927828}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00018051163763353414
Epoch 22: reducing lr to 0.00015959160436004128
Epoch 27: reducing lr to 0.0001240073648614107
Epoch 30: reducing lr to 9.997764300390844e-05
Epoch 33: reducing lr to 7.576348801557123e-05
Epoch 36: reducing lr to 5.288637289440367e-05
Epoch 39: reducing lr to 3.278374124159602e-05
Epoch 42: reducing lr to 1.671870461574709e-05
Epoch 45: reducing lr to 5.700711744800898e-06
Epoch 48: reducing lr to 4.22049738454033e-07
[I 2024-06-24 15:50:39,408] Trial 289 finished with value: 0.9746414422988892 and parameters: {'hidden_size': 185, 'n_layers': 3, 'rnn_dropout': 0.3573495547452912, 'bidirectional': True, 'fc_dropout': 0.7386428159257871, 'learning_rate_model': 0.0019408282868845956}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00017888560135863137
Epoch 22: reducing lr to 0.00015815401428960946
Epoch 27: reducing lr to 0.00012289031514504215
Epoch 30: reducing lr to 9.907705135045687e-05
Epoch 33: reducing lr to 7.50810157858495e-05
Epoch 36: reducing lr to 5.240997612629657e-05
Epoch 39: reducing lr to 3.2488427581021484e-05
Epoch 42: reducing lr to 1.6568103687568827e-05
Epoch 45: reducing lr to 5.649360129961374e-06
Epoch 48: reducing lr to 4.1824794376902135e-07
[I 2024-06-24 15:50:43,967] Trial 290 finished with value: 0.9796531200408936 and parameters: {'hidden_size': 186, 'n_layers': 3, 'rnn_dropout': 0.3591627035656853, 'bidirectional': True, 'fc_dropout': 0.7307135938483387, 'learning_rate_model': 0.0019233454406858445}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00018491827491864097
Epoch 23: reducing lr to 0.000156903042454898
Epoch 27: reducing lr to 0.00012703462385030496
Epoch 30: reducing lr to 0.00010241829012845858
Epoch 33: reducing lr to 7.761302090727848e-05
Epoch 36: reducing lr to 5.417743127560162e-05
Epoch 39: reducing lr to 3.3584055605780094e-05
Epoch 42: reducing lr to 1.712684044612499e-05
Epoch 45: reducing lr to 5.839877115275719e-06
Epoch 48: reducing lr to 4.3235278671185927e-07
[I 2024-06-24 15:50:48,529] Trial 291 finished with value: 0.9752063751220703 and parameters: {'hidden_size': 185, 'n_layers': 3, 'rnn_dropout': 0.3633977245211553, 'bidirectional': True, 'fc_dropout': 0.7345113338297404, 'learning_rate_model': 0.0019882076492630956}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00018008674693521443
Epoch 22: reducing lr to 0.0001592159555148396
Epoch 27: reducing lr to 0.00012371547467336798
Epoch 30: reducing lr to 9.974231429541525e-05
Epoch 33: reducing lr to 7.558515490778887e-05
Epoch 36: reducing lr to 5.276188824507419e-05
Epoch 39: reducing lr to 3.27065744345564e-05
Epoch 42: reducing lr to 1.667935190601429e-05
Epoch 45: reducing lr to 5.687293333523258e-06
Epoch 48: reducing lr to 4.210563121550626e-07
[I 2024-06-24 15:50:53,108] Trial 292 finished with value: 0.9751739501953125 and parameters: {'hidden_size': 189, 'n_layers': 3, 'rnn_dropout': 0.3607017632583824, 'bidirectional': True, 'fc_dropout': 0.7308402496101702, 'learning_rate_model': 0.0019362599393977305}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00018004153363603215
Epoch 22: reducing lr to 0.00015917598212005185
Epoch 27: reducing lr to 0.00012368441417133172
Epoch 30: reducing lr to 9.971727259093605e-05
Epoch 33: reducing lr to 7.556617819639425e-05
Epoch 36: reducing lr to 5.2748641634319966e-05
Epoch 39: reducing lr to 3.269836299112535e-05
Epoch 42: reducing lr to 1.667516432119348e-05
Epoch 45: reducing lr to 5.685865458905041e-06
Epoch 48: reducing lr to 4.2095060007274565e-07
[I 2024-06-24 15:50:57,641] Trial 293 finished with value: 0.9748444557189941 and parameters: {'hidden_size': 185, 'n_layers': 3, 'rnn_dropout': 0.3537043650521436, 'bidirectional': True, 'fc_dropout': 0.7289274341755332, 'learning_rate_model': 0.0019357738142306957}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00017392058821268325
Epoch 22: reducing lr to 0.00015376441135863837
Epoch 27: reducing lr to 0.0001194794647156574
Epoch 30: reducing lr to 9.63271438191593e-05
Epoch 33: reducing lr to 7.299712402733657e-05
Epoch 36: reducing lr to 5.095532455864913e-05
Epoch 39: reducing lr to 3.158670341314076e-05
Epoch 42: reducing lr to 1.6108251961172486e-05
Epoch 45: reducing lr to 5.492560772727339e-06
Epoch 48: reducing lr to 4.066393708972594e-07
[I 2024-06-24 15:51:02,210] Trial 294 finished with value: 0.9779301285743713 and parameters: {'hidden_size': 191, 'n_layers': 3, 'rnn_dropout': 0.3568590235844197, 'bidirectional': True, 'fc_dropout': 0.7322809325364988, 'learning_rate_model': 0.00186996252263835}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.0001839881264025633
Epoch 22: reducing lr to 0.0001626651924536544
Epoch 27: reducing lr to 0.00012639563310200367
Epoch 30: reducing lr to 0.00010190312081583056
Epoch 36: reducing lr to 5.39049159856549e-05
Epoch 39: reducing lr to 3.341512606380092e-05
Epoch 42: reducing lr to 1.704069155017045e-05
Epoch 45: reducing lr to 5.81050223042326e-06
Epoch 48: reducing lr to 4.301780297650236e-07
[I 2024-06-24 15:51:06,720] Trial 295 finished with value: 0.9750511646270752 and parameters: {'hidden_size': 190, 'n_layers': 3, 'rnn_dropout': 0.35686958261175233, 'bidirectional': True, 'fc_dropout': 0.7344118253859235, 'learning_rate_model': 0.0019782068616425643}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.0001754318705956331
Epoch 22: reducing lr to 0.0001551005467086798
Epoch 27: reducing lr to 0.00012051768113387836
Epoch 32: reducing lr to 8.14035154176392e-05
Epoch 36: reducing lr to 5.139810068489363e-05
Epoch 39: reducing lr to 3.186117596923265e-05
Epoch 42: reducing lr to 1.624822456395181e-05
Epoch 45: reducing lr to 5.540288361613812e-06
Epoch 48: reducing lr to 4.101728623818893e-07
[I 2024-06-24 15:51:11,252] Trial 296 finished with value: 0.9733585119247437 and parameters: {'hidden_size': 190, 'n_layers': 3, 'rnn_dropout': 0.3600806651128343, 'bidirectional': True, 'fc_dropout': 0.7356444093984625, 'learning_rate_model': 0.001886211555868297}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.0001817269365478222
Epoch 22: reducing lr to 0.00016066605865036246
Epoch 27: reducing lr to 0.0001248422474089047
Epoch 30: reducing lr to 0.00010065074487472744
Epoch 36: reducing lr to 5.324243166380937e-05
Epoch 39: reducing lr to 3.3004458562980574e-05
Epoch 42: reducing lr to 1.6831263694121147e-05
Epoch 45: reducing lr to 5.739091922860062e-06
Epoch 48: reducing lr to 4.248911984044588e-07
[I 2024-06-24 15:51:15,757] Trial 297 finished with value: 0.9745249152183533 and parameters: {'hidden_size': 190, 'n_layers': 3, 'rnn_dropout': 0.36042495908843986, 'bidirectional': True, 'fc_dropout': 0.7367660969392315, 'learning_rate_model': 0.0019538949597085326}. Best is trial 212 with value: 0.9678940773010254.
Epoch 16: reducing lr to 0.0002029347693348823
Epoch 22: reducing lr to 0.00017263974994560677
Epoch 27: reducing lr to 0.00013414615729276966
Epoch 30: reducing lr to 0.00010815177501071289
Epoch 33: reducing lr to 8.195788041899102e-05
Epoch 36: reducing lr to 5.7210341537903715e-05
Epoch 39: reducing lr to 3.546412678114359e-05
Epoch 42: reducing lr to 1.8085619201906587e-05
Epoch 45: reducing lr to 6.166799651403411e-06
Epoch 48: reducing lr to 4.5655635585277463e-07
[I 2024-06-24 15:51:20,242] Trial 298 finished with value: 0.9784889221191406 and parameters: {'hidden_size': 189, 'n_layers': 3, 'rnn_dropout': 0.3575731086415943, 'bidirectional': True, 'fc_dropout': 0.7473982790577121, 'learning_rate_model': 0.002099509629461502}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00018522019063928858
Epoch 22: reducing lr to 0.00016375446908308035
Epoch 27: reducing lr to 0.0001272420330424134
Epoch 30: reducing lr to 0.000102585508278667
Epoch 36: reducing lr to 5.426588666603616e-05
Epoch 39: reducing lr to 3.363888823038116e-05
Epoch 42: reducing lr to 1.7154803406400193e-05
Epoch 45: reducing lr to 5.8494118716891865e-06
Epoch 48: reducing lr to 4.3305868829587697e-07
[I 2024-06-24 15:51:24,747] Trial 299 finished with value: 0.9749987721443176 and parameters: {'hidden_size': 190, 'n_layers': 3, 'rnn_dropout': 0.3596961994807495, 'bidirectional': True, 'fc_dropout': 0.7332549138005608, 'learning_rate_model': 0.0019914537921631873}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00018140918672393842
Epoch 22: reducing lr to 0.0001603851338033912
Epoch 27: reducing lr to 0.00012462396055016486
Epoch 30: reducing lr to 0.00010047475689482053
Epoch 33: reducing lr to 7.614020306090411e-05
Epoch 36: reducing lr to 5.314933719137891e-05
Epoch 39: reducing lr to 3.2946750217178954e-05
Epoch 42: reducing lr to 1.6801834204051333e-05
Epoch 45: reducing lr to 5.729057111937791e-06
Epoch 48: reducing lr to 4.241482755002954e-07
[I 2024-06-24 15:51:29,306] Trial 300 finished with value: 0.9768927097320557 and parameters: {'hidden_size': 189, 'n_layers': 3, 'rnn_dropout': 0.33072541172742, 'bidirectional': True, 'fc_dropout': 0.742736322489503, 'learning_rate_model': 0.0019504785714111847}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00017667905114170552
Epoch 22: reducing lr to 0.00015620318777317675
Epoch 27: reducing lr to 0.00012137446563294115
Epoch 33: reducing lr to 7.415489299893417e-05
Epoch 36: reducing lr to 5.176350014772571e-05
Epoch 39: reducing lr to 3.208768349439843e-05
Epoch 42: reducing lr to 1.6363736469032537e-05
Epoch 45: reducing lr to 5.579675388844244e-06
Epoch 48: reducing lr to 4.130888639770035e-07
[I 2024-06-24 15:51:34,015] Trial 301 finished with value: 0.976649284362793 and parameters: {'hidden_size': 193, 'n_layers': 3, 'rnn_dropout': 0.33151019286358135, 'bidirectional': True, 'fc_dropout': 0.7357824695217666, 'learning_rate_model': 0.0018996210141968709}. Best is trial 212 with value: 0.9678940773010254.
Epoch 16: reducing lr to 0.00024782074211019913
Epoch 22: reducing lr to 0.00021082494187399663
Epoch 27: reducing lr to 0.000163817173175811
Epoch 30: reducing lr to 0.0001320732431979715
Epoch 33: reducing lr to 0.00010008567193183339
Epoch 36: reducing lr to 6.986436746531544e-05
Epoch 39: reducing lr to 4.330823271930314e-05
Epoch 42: reducing lr to 2.2085873144501826e-05
Epoch 45: reducing lr to 7.530798547063017e-06
Epoch 48: reducing lr to 5.575394265525894e-07
[I 2024-06-24 15:51:38,750] Trial 302 finished with value: 0.9948474168777466 and parameters: {'hidden_size': 193, 'n_layers': 3, 'rnn_dropout': 0.3379544120458163, 'bidirectional': True, 'fc_dropout': 0.7370146730598716, 'learning_rate_model': 0.0025638880717481086}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00017072112226633766
Epoch 22: reducing lr to 0.00015093574108470718
Epoch 27: reducing lr to 0.00011728150481583305
Epoch 32: reducing lr to 7.921764421333383e-05
Epoch 36: reducing lr to 5.0017943726478636e-05
Epoch 39: reducing lr to 3.100563027530112e-05
Epoch 42: reducing lr to 1.5811922445877273e-05
Epoch 45: reducing lr to 5.3915189045323966e-06
Epoch 48: reducing lr to 3.9915877970907955e-07
[I 2024-06-24 15:51:43,318] Trial 303 finished with value: 0.9780858755111694 and parameters: {'hidden_size': 196, 'n_layers': 3, 'rnn_dropout': 0.3246007684380354, 'bidirectional': True, 'fc_dropout': 0.7346531136422958, 'learning_rate_model': 0.0018355624468703933}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00023000683169887922
Epoch 27: reducing lr to 0.00015800943071052882
Epoch 30: reducing lr to 0.00012739090514892463
Epoch 33: reducing lr to 9.65373760128153e-05
Epoch 36: reducing lr to 6.738749495022857e-05
Epoch 39: reducing lr to 4.177284386242012e-05
Epoch 42: reducing lr to 2.1302871821395757e-05
Epoch 45: reducing lr to 7.263812261856405e-06
Epoch 48: reducing lr to 5.377732119312209e-07
[I 2024-06-24 15:51:47,900] Trial 304 finished with value: 0.9932551383972168 and parameters: {'hidden_size': 190, 'n_layers': 3, 'rnn_dropout': 0.3326053399978919, 'bidirectional': True, 'fc_dropout': 0.76169857332691, 'learning_rate_model': 0.0024729916086859522}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.0001784428247801497
Epoch 22: reducing lr to 0.00015776255241236253
Epoch 27: reducing lr to 0.0001225861377665657
Epoch 30: reducing lr to 9.883181642115109e-05
Epoch 33: reducing lr to 7.489517570131448e-05
Epoch 36: reducing lr to 5.22802512645343e-05
Epoch 39: reducing lr to 3.2408012417948985e-05
Epoch 42: reducing lr to 1.652709441568223e-05
Epoch 45: reducing lr to 5.635376867306582e-06
Epoch 48: reducing lr to 4.172126989416365e-07
[I 2024-06-24 15:51:52,308] Trial 305 finished with value: 0.9794365763664246 and parameters: {'hidden_size': 196, 'n_layers': 3, 'rnn_dropout': 0.36673907556204527, 'bidirectional': True, 'fc_dropout': 0.748951659527305, 'learning_rate_model': 0.0019185847874695029}. Best is trial 212 with value: 0.9678940773010254.
Epoch 16: reducing lr to 0.00021510653854649002
Epoch 22: reducing lr to 0.00018299446244743526
Epoch 27: reducing lr to 0.00014219207309390666
Epoch 30: reducing lr to 0.00011463858084280667
Epoch 36: reducing lr to 6.064174501794267e-05
Epoch 39: reducing lr to 3.759121997412398e-05
Epoch 42: reducing lr to 1.9170371626028427e-05
Epoch 45: reducing lr to 6.536676446676666e-06
Epoch 48: reducing lr to 4.839400250669818e-07
[I 2024-06-24 15:51:57,074] Trial 306 finished with value: 0.9824610352516174 and parameters: {'hidden_size': 189, 'n_layers': 3, 'rnn_dropout': 0.3487324186155131, 'bidirectional': True, 'fc_dropout': 0.7355580445812148, 'learning_rate_model': 0.0022254355452181215}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00015856493536434152
Epoch 22: reducing lr to 0.00014018837102024284
Epoch 27: reducing lr to 0.00010893048255354709
Epoch 30: reducing lr to 8.782230724080977e-05
Epoch 33: reducing lr to 6.655212227676531e-05
Epoch 36: reducing lr to 4.645641915165754e-05
Epoch 39: reducing lr to 2.8797876298304972e-05
Epoch 42: reducing lr to 1.468603549070551e-05
Epoch 45: reducing lr to 5.007616135975783e-06
Epoch 48: reducing lr to 3.7073670360447156e-07
[I 2024-06-24 15:52:01,957] Trial 307 finished with value: 0.9753672480583191 and parameters: {'hidden_size': 192, 'n_layers': 3, 'rnn_dropout': 0.36174165711104517, 'bidirectional': True, 'fc_dropout': 0.7686757005062883, 'learning_rate_model': 0.001704861336906793}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.0001719150192270186
Epoch 22: reducing lr to 0.00015199127375779982
Epoch 27: reducing lr to 0.00011810168471088588
Epoch 30: reducing lr to 9.521634529837052e-05
Epoch 33: reducing lr to 7.215535612914552e-05
Epoch 36: reducing lr to 5.0367732142825804e-05
Epoch 39: reducing lr to 3.122246066663329e-05
Epoch 42: reducing lr to 1.5922499308892525e-05
Epoch 45: reducing lr to 5.4292231906109175e-06
Epoch 48: reducing lr to 4.0195020028783567e-07
[I 2024-06-24 15:52:06,345] Trial 308 finished with value: 0.9752324819564819 and parameters: {'hidden_size': 194, 'n_layers': 3, 'rnn_dropout': 0.36581485708940803, 'bidirectional': True, 'fc_dropout': 0.7606309401445085, 'learning_rate_model': 0.0018483990097828593}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.0002564153713999726
Epoch 27: reducing lr to 0.00017615149324512827
Epoch 30: reducing lr to 0.00014201746102700372
Epoch 33: reducing lr to 0.00010762144298701479
Epoch 36: reducing lr to 7.512467963559553e-05
Epoch 39: reducing lr to 4.656904838130386e-05
Epoch 42: reducing lr to 2.3748789327790324e-05
Epoch 45: reducing lr to 8.097816508954859e-06
Epoch 48: reducing lr to 5.99518357119206e-07
[I 2024-06-24 15:52:11,933] Trial 309 finished with value: 1.0036793947219849 and parameters: {'hidden_size': 200, 'n_layers': 3, 'rnn_dropout': 0.3694050420068787, 'bidirectional': True, 'fc_dropout': 0.7767331369179618, 'learning_rate_model': 0.0027569314229778765}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.0001592365656420046
Epoch 22: reducing lr to 0.00014078216405738006
Epoch 27: reducing lr to 0.00010939187718707879
Epoch 30: reducing lr to 8.819429440469076e-05
Epoch 36: reducing lr to 4.66531936631368e-05
Epoch 39: reducing lr to 2.891985487831004e-05
Epoch 42: reducing lr to 1.4748240833089231e-05
Epoch 45: reducing lr to 5.028826793982257e-06
Epoch 48: reducing lr to 3.7230702553351507e-07
[I 2024-06-24 15:52:17,359] Trial 310 finished with value: 0.9726389050483704 and parameters: {'hidden_size': 193, 'n_layers': 3, 'rnn_dropout': 0.3393666065216531, 'bidirectional': True, 'fc_dropout': 0.7540458087464437, 'learning_rate_model': 0.0017120825834607853}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00013724386210496261
Epoch 27: reducing lr to 9.428339306073375e-05
Epoch 30: reducing lr to 7.60134805151119e-05
Epoch 33: reducing lr to 5.760334257733401e-05
Epoch 36: reducing lr to 4.0209762450256394e-05
Epoch 40: reducing lr to 2.047071930585337e-05
Epoch 43: reducing lr to 9.461262991335367e-06
Epoch 46: reducing lr to 2.493309792765404e-06
Epoch 49: reducing lr to 4.672103456893062e-09
[I 2024-06-24 15:52:22,032] Trial 311 finished with value: 0.9719246625900269 and parameters: {'hidden_size': 194, 'n_layers': 3, 'rnn_dropout': 0.33650346634484474, 'bidirectional': True, 'fc_dropout': 0.7688450127013824, 'learning_rate_model': 0.0014756210362201964}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00014064185860681658
Epoch 27: reducing lr to 9.661773890971807e-05
Epoch 30: reducing lr to 7.789548483153486e-05
Epoch 33: reducing lr to 5.902953354551865e-05
Epoch 36: reducing lr to 4.12053088451972e-05
Epoch 39: reducing lr to 2.554276478098081e-05
Epoch 42: reducing lr to 1.3026028246614368e-05
Epoch 45: reducing lr to 4.441590058576711e-06
Epoch 48: reducing lr to 3.288312067790391e-07
[I 2024-06-24 15:52:26,647] Trial 312 finished with value: 0.9722083806991577 and parameters: {'hidden_size': 194, 'n_layers': 3, 'rnn_dropout': 0.3391609688896605, 'bidirectional': True, 'fc_dropout': 0.7621528155508279, 'learning_rate_model': 0.0015121556764017996}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00012538630182440367
Epoch 22: reducing lr to 0.00011085490849932622
Epoch 27: reducing lr to 8.613752045465487e-05
Epoch 30: reducing lr to 6.944608716491823e-05
Epoch 37: reducing lr to 3.182067340191982e-05
Epoch 40: reducing lr to 1.8702095307426988e-05
Epoch 43: reducing lr to 8.643831198544668e-06
Epoch 46: reducing lr to 2.2778934476379732e-06
Epoch 49: reducing lr to 4.2684442510924904e-09
[I 2024-06-24 15:52:31,369] Trial 313 finished with value: 0.9716821908950806 and parameters: {'hidden_size': 195, 'n_layers': 3, 'rnn_dropout': 0.3289838567776995, 'bidirectional': True, 'fc_dropout': 0.7647156829415455, 'learning_rate_model': 0.001348130705360372}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.0001353616646410855
Epoch 22: reducing lr to 0.00011967419670067596
Epoch 30: reducing lr to 7.49710121813616e-05
Epoch 37: reducing lr to 3.435223193452429e-05
Epoch 40: reducing lr to 2.0189978620111436e-05
Epoch 43: reducing lr to 9.331508808275839e-06
Epoch 46: reducing lr to 2.4591159038976238e-06
Epoch 49: reducing lr to 4.608029033868755e-09
[I 2024-06-24 15:52:36,051] Trial 314 finished with value: 0.9713049530982971 and parameters: {'hidden_size': 195, 'n_layers': 3, 'rnn_dropout': 0.3305393141670096, 'bidirectional': True, 'fc_dropout': 0.7656508587581484, 'learning_rate_model': 0.0014553839915216638}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00013778382539482185
Epoch 22: reducing lr to 0.00012181564600430193
Epoch 30: reducing lr to 7.631254298962316e-05
Epoch 37: reducing lr to 3.496693055112063e-05
Epoch 40: reducing lr to 2.0551257967274312e-05
Epoch 43: reducing lr to 9.498486766684397e-06
Epoch 46: reducing lr to 2.5031193080157992e-06
Epoch 49: reducing lr to 4.690485075667903e-09
[I 2024-06-24 15:52:40,797] Trial 315 finished with value: 0.9715873003005981 and parameters: {'hidden_size': 195, 'n_layers': 3, 'rnn_dropout': 0.36959614591816936, 'bidirectional': True, 'fc_dropout': 0.7623419033896652, 'learning_rate_model': 0.001481426623275839}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.0001339730627986215
Epoch 22: reducing lr to 0.00011844652407656497
Epoch 36: reducing lr to 3.9251482341308015e-05
Epoch 39: reducing lr to 2.433160699062962e-05
Epoch 42: reducing lr to 1.2408374843644897e-05
Epoch 45: reducing lr to 4.230983789164662e-06
Epoch 48: reducing lr to 3.1323906234143485e-07
[I 2024-06-24 15:52:45,547] Trial 316 finished with value: 0.9708471298217773 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.34395474712623875, 'bidirectional': True, 'fc_dropout': 0.7642554575382866, 'learning_rate_model': 0.0014404539971434316}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00013064853177087796
Epoch 22: reducing lr to 0.00011550728288736551
Epoch 36: reducing lr to 3.827745989080427e-05
Epoch 39: reducing lr to 2.3727820074771777e-05
Epoch 42: reducing lr to 1.2100461996764783e-05
Epoch 45: reducing lr to 4.125992258844128e-06
Epoch 48: reducing lr to 3.0546605961907605e-07
[I 2024-06-24 15:52:50,212] Trial 317 finished with value: 0.9706220626831055 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.34501651717223775, 'bidirectional': True, 'fc_dropout': 0.7601843585863973, 'learning_rate_model': 0.0014047092443736993}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00012595829154351466
Epoch 22: reducing lr to 0.000111360608620089
Epoch 36: reducing lr to 3.6903311404421084e-05
Epoch 39: reducing lr to 2.2875998973425574e-05
Epoch 42: reducing lr to 1.1666059306909555e-05
Epoch 45: reducing lr to 3.977870465143754e-06
Epoch 48: reducing lr to 2.9449992642567937e-07
[I 2024-06-24 15:52:54,919] Trial 318 finished with value: 0.9704656600952148 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.34099694497558153, 'bidirectional': True, 'fc_dropout': 0.7732337521582978, 'learning_rate_model': 0.0013542806347566776}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00013278651444059508
Epoch 22: reducing lr to 0.00011739748835459869
Epoch 32: reducing lr to 6.161530991386818e-05
Epoch 36: reducing lr to 3.890384692155063e-05
Epoch 39: reducing lr to 2.4116111220660764e-05
Epoch 42: reducing lr to 1.2298478596675958e-05
Epoch 45: reducing lr to 4.193511578236601e-06
Epoch 48: reducing lr to 3.104648233464603e-07
[I 2024-06-24 15:52:59,649] Trial 319 finished with value: 0.9710922837257385 and parameters: {'hidden_size': 198, 'n_layers': 3, 'rnn_dropout': 0.3405236374181927, 'bidirectional': True, 'fc_dropout': 0.776040182222545, 'learning_rate_model': 0.0014276964450697567}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.0001268535322837502
Epoch 32: reducing lr to 5.8862300424557505e-05
Epoch 37: reducing lr to 3.2193028759505866e-05
Epoch 40: reducing lr to 1.8920941253829397e-05
Epoch 43: reducing lr to 8.744978550651082e-06
Epoch 46: reducing lr to 2.3045486292717864e-06
Epoch 49: reducing lr to 4.318392222524108e-09
[I 2024-06-24 15:53:04,294] Trial 320 finished with value: 0.9708101749420166 and parameters: {'hidden_size': 198, 'n_layers': 3, 'rnn_dropout': 0.34146799717561915, 'bidirectional': True, 'fc_dropout': 0.780648136918196, 'learning_rate_model': 0.001363906100322217}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.0001309724584193978
Epoch 22: reducing lr to 0.0001157936687083019
Epoch 32: reducing lr to 6.077355558047015e-05
Epoch 36: reducing lr to 3.837236405182481e-05
Epoch 39: reducing lr to 2.3786650228691757e-05
Epoch 42: reducing lr to 1.213046357463961e-05
Epoch 45: reducing lr to 4.136222139165856e-06
Epoch 48: reducing lr to 3.0622342440220394e-07
[I 2024-06-24 15:53:09,075] Trial 321 finished with value: 0.971125602722168 and parameters: {'hidden_size': 198, 'n_layers': 3, 'rnn_dropout': 0.3389195964884541, 'bidirectional': True, 'fc_dropout': 0.7703990060486683, 'learning_rate_model': 0.0014081920447657676}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00012836958820289685
Epoch 32: reducing lr to 5.956577739809283e-05
Epoch 36: reducing lr to 3.76097740788444e-05
Epoch 39: reducing lr to 2.331392822150207e-05
Epoch 42: reducing lr to 1.1889389819654613e-05
Epoch 45: reducing lr to 4.054021273848106e-06
Epoch 48: reducing lr to 3.0013771874627755e-07
[I 2024-06-24 15:53:13,872] Trial 322 finished with value: 0.9712830781936646 and parameters: {'hidden_size': 198, 'n_layers': 3, 'rnn_dropout': 0.3370027607653397, 'bidirectional': True, 'fc_dropout': 0.7802537212324462, 'learning_rate_model': 0.0013802064577448899}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00013315843495292578
Epoch 21: reducing lr to 0.00012214411207497498
Epoch 30: reducing lr to 7.375073788710456e-05
Epoch 38: reducing lr to 2.8838450595997642e-05
Epoch 41: reducing lr to 1.59018404492915e-05
Epoch 44: reducing lr to 6.464047414295399e-06
Epoch 47: reducing lr to 1.1180717391022195e-06
[I 2024-06-24 15:53:18,517] Trial 323 finished with value: 0.9718698263168335 and parameters: {'hidden_size': 200, 'n_layers': 3, 'rnn_dropout': 0.3182267700575357, 'bidirectional': True, 'fc_dropout': 0.7812521548088786, 'learning_rate_model': 0.0014316952667538711}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00012595036851390282
Epoch 21: reducing lr to 0.00011553226750588607
Epoch 30: reducing lr to 6.975849947723479e-05
Epoch 37: reducing lr to 3.196382286595452e-05
Epoch 40: reducing lr to 1.8786229130924974e-05
Epoch 43: reducing lr to 8.682716604508571e-06
Epoch 46: reducing lr to 2.28814084944622e-06
Epoch 49: reducing lr to 4.287646406236217e-09
[I 2024-06-24 15:53:23,213] Trial 324 finished with value: 0.9723451733589172 and parameters: {'hidden_size': 200, 'n_layers': 3, 'rnn_dropout': 0.31446502356259387, 'bidirectional': True, 'fc_dropout': 0.7810578954771451, 'learning_rate_model': 0.0013541954477837488}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00013263524592077022
Epoch 21: reducing lr to 0.00012166419910661819
Epoch 30: reducing lr to 7.346096595347113e-05
Epoch 38: reducing lr to 2.8725142257239253e-05
Epoch 41: reducing lr to 1.583936097874878e-05
Epoch 44: reducing lr to 6.438649708835056e-06
Epoch 47: reducing lr to 1.1136787551258932e-06
[I 2024-06-24 15:53:27,930] Trial 325 finished with value: 0.9719758033752441 and parameters: {'hidden_size': 200, 'n_layers': 3, 'rnn_dropout': 0.3191636066178996, 'bidirectional': True, 'fc_dropout': 0.7829554260252259, 'learning_rate_model': 0.0014260700334652746}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00012778507395765006
Epoch 21: reducing lr to 0.00011721521359506809
Epoch 30: reducing lr to 7.077466402084541e-05
Epoch 36: reducing lr to 3.743852285791774e-05
Epoch 39: reducing lr to 2.3207771277720425e-05
Epoch 42: reducing lr to 1.1835253027489361e-05
Epoch 45: reducing lr to 4.035561814576868e-06
Epoch 48: reducing lr to 2.9877108063051764e-07
[I 2024-06-24 15:53:32,647] Trial 326 finished with value: 0.9722070693969727 and parameters: {'hidden_size': 200, 'n_layers': 3, 'rnn_dropout': 0.30788865694954703, 'bidirectional': True, 'fc_dropout': 0.7840645734595999, 'learning_rate_model': 0.0013739218669221913}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00012900818092738577
Epoch 22: reducing lr to 0.00011405703720648741
Epoch 32: reducing lr to 5.986209580658399e-05
Epoch 36: reducing lr to 3.779686923457912e-05
Epoch 39: reducing lr to 2.3429906664293192e-05
Epoch 42: reducing lr to 1.1948535275706434e-05
Epoch 45: reducing lr to 4.074188577698254e-06
Epoch 48: reducing lr to 3.016307963998053e-07
[I 2024-06-24 15:53:37,083] Trial 327 finished with value: 0.9710811376571655 and parameters: {'hidden_size': 198, 'n_layers': 3, 'rnn_dropout': 0.31191124113586066, 'bidirectional': True, 'fc_dropout': 0.7856178717403844, 'learning_rate_model': 0.0013870724905377613}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00012947698177044118
Epoch 21: reducing lr to 0.00011876717369116823
Epoch 30: reducing lr to 7.171173908991193e-05
Epoch 38: reducing lr to 2.804114920264549e-05
Epoch 41: reducing lr to 1.5462199647339292e-05
Epoch 44: reducing lr to 6.285334830796632e-06
Epoch 47: reducing lr to 1.0871602255836101e-06
[I 2024-06-24 15:53:41,788] Trial 328 finished with value: 0.9719130992889404 and parameters: {'hidden_size': 200, 'n_layers': 3, 'rnn_dropout': 0.31683641649670796, 'bidirectional': True, 'fc_dropout': 0.7981095656614122, 'learning_rate_model': 0.001392112951912138}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.0001259996088204198
Epoch 27: reducing lr to 8.655884832815977e-05
Epoch 32: reducing lr to 5.8466064714497005e-05
Epoch 36: reducing lr to 3.691541655698651e-05
Epoch 39: reducing lr to 2.288350283817704e-05
Epoch 42: reducing lr to 1.1669886048260763e-05
Epoch 45: reducing lr to 3.979175300049742e-06
Epoch 48: reducing lr to 2.9459652931590646e-07
[I 2024-06-24 15:53:46,473] Trial 329 finished with value: 0.9713562726974487 and parameters: {'hidden_size': 199, 'n_layers': 3, 'rnn_dropout': 0.3065189783346039, 'bidirectional': True, 'fc_dropout': 0.7989770882316704, 'learning_rate_model': 0.0013547248706010022}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00013118114428893737
Epoch 21: reducing lr to 0.00012033037483367756
Epoch 30: reducing lr to 7.265560151412166e-05
Epoch 36: reducing lr to 3.843350492262379e-05
Epoch 39: reducing lr to 2.3824550851817547e-05
Epoch 42: reducing lr to 1.2149791732403025e-05
Epoch 45: reducing lr to 4.1428126172261916e-06
Epoch 48: reducing lr to 3.06711347606561e-07
[I 2024-06-24 15:53:51,091] Trial 330 finished with value: 0.9717886447906494 and parameters: {'hidden_size': 200, 'n_layers': 3, 'rnn_dropout': 0.3086842396836726, 'bidirectional': True, 'fc_dropout': 0.7826920415922535, 'learning_rate_model': 0.0014104357972682948}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.000122647881653157
Epoch 21: reducing lr to 0.00011250294889465671
Epoch 30: reducing lr to 6.792939384882624e-05
Epoch 36: reducing lr to 3.593342617048286e-05
Epoch 39: reducing lr to 2.2274776677335002e-05
Epoch 42: reducing lr to 1.1359454337615013e-05
Epoch 45: reducing lr to 3.873324892406907e-06
Epoch 48: reducing lr to 2.867599399809665e-07
[I 2024-06-24 15:53:55,467] Trial 331 finished with value: 0.9729161262512207 and parameters: {'hidden_size': 200, 'n_layers': 3, 'rnn_dropout': 0.30854235894461257, 'bidirectional': True, 'fc_dropout': 0.7974073281964942, 'learning_rate_model': 0.0013186877098870233}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00013134266065677555
Epoch 22: reducing lr to 0.00011612096710177577
Epoch 32: reducing lr to 6.094533601828745e-05
Epoch 36: reducing lr to 3.848082605366011e-05
Epoch 39: reducing lr to 2.3853884754489446e-05
Epoch 42: reducing lr to 1.2164751124938019e-05
Epoch 45: reducing lr to 4.147913442121382e-06
Epoch 48: reducing lr to 3.070889849804935e-07
[I 2024-06-24 15:54:00,232] Trial 332 finished with value: 0.970807671546936 and parameters: {'hidden_size': 198, 'n_layers': 3, 'rnn_dropout': 0.31744709017233147, 'bidirectional': True, 'fc_dropout': 0.7796705627599377, 'learning_rate_model': 0.0014121723918701986}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00012656512421611003
Epoch 32: reducing lr to 5.872847393966051e-05
Epoch 37: reducing lr to 3.211983624331132e-05
Epoch 40: reducing lr to 1.887792351512943e-05
Epoch 43: reducing lr to 8.725096389548109e-06
Epoch 46: reducing lr to 2.2993091187513936e-06
Epoch 49: reducing lr to 4.30857413442811e-09
[I 2024-06-24 15:54:04,886] Trial 333 finished with value: 0.9699176549911499 and parameters: {'hidden_size': 198, 'n_layers': 3, 'rnn_dropout': 0.291963610736818, 'bidirectional': True, 'fc_dropout': 0.7805921543921838, 'learning_rate_model': 0.0013608051892497787}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00012680289615167915
Epoch 32: reducing lr to 5.8838804356575433e-05
Epoch 37: reducing lr to 3.218017826628989e-05
Epoch 40: reducing lr to 1.8913388580577115e-05
Epoch 43: reducing lr to 8.741487817039845e-06
Epoch 46: reducing lr to 2.3036287224575656e-06
Epoch 49: reducing lr to 4.3166684496418615e-09
[I 2024-06-24 15:54:09,668] Trial 334 finished with value: 0.9696583151817322 and parameters: {'hidden_size': 198, 'n_layers': 3, 'rnn_dropout': 0.28626575999091775, 'bidirectional': True, 'fc_dropout': 0.783048136698263, 'learning_rate_model': 0.0013633616698425519}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00012875495221881242
Epoch 23: reducing lr to 0.0001092484977115163
Epoch 36: reducing lr to 3.7722678184712485e-05
Epoch 39: reducing lr to 2.338391636380261e-05
Epoch 42: reducing lr to 1.1925081630089082e-05
Epoch 45: reducing lr to 4.066191398724027e-06
Epoch 48: reducing lr to 3.010387286992095e-07
[I 2024-06-24 15:54:14,407] Trial 335 finished with value: 0.9706343412399292 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.2854536623371318, 'bidirectional': True, 'fc_dropout': 0.7829429312898512, 'learning_rate_model': 0.0013843498215337375}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00011579920044623103
Epoch 23: reducing lr to 9.825555030649192e-05
Epoch 27: reducing lr to 7.955140116532695e-05
Epoch 36: reducing lr to 3.3926896769427244e-05
Epoch 39: reducing lr to 2.103094888053741e-05
Epoch 42: reducing lr to 1.0725140231294255e-05
Epoch 45: reducing lr to 3.6570376884096724e-06
Epoch 48: reducing lr to 2.707472099983902e-07
[I 2024-06-24 15:54:19,180] Trial 336 finished with value: 0.970599353313446 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.28867905514245684, 'bidirectional': True, 'fc_dropout': 0.7986424289885011, 'learning_rate_model': 0.0012450519355485185}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00012343343217792644
Epoch 23: reducing lr to 0.00010473319123211569
Epoch 36: reducing lr to 3.616357708221904e-05
Epoch 39: reducing lr to 2.2417444958858894e-05
Epoch 42: reducing lr to 1.1432210794520185e-05
Epoch 45: reducing lr to 3.898133249149982e-06
Epoch 48: reducing lr to 2.885966159862615e-07
[I 2024-06-24 15:54:23,949] Trial 337 finished with value: 0.970361590385437 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.28197449075111064, 'bidirectional': True, 'fc_dropout': 0.799503553767459, 'learning_rate_model': 0.0013271338062120965}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.0001162114433492509
Epoch 23: reducing lr to 9.860533815597686e-05
Epoch 27: reducing lr to 7.983460260738631e-05
Epoch 36: reducing lr to 3.4047675862553865e-05
Epoch 39: reducing lr to 2.110581864981358e-05
Epoch 42: reducing lr to 1.076332152207351e-05
Epoch 45: reducing lr to 3.6700566715055278e-06
Epoch 48: reducing lr to 2.7171106480397295e-07
[I 2024-06-24 15:54:28,718] Trial 338 finished with value: 0.9706354141235352 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.28229102668171446, 'bidirectional': True, 'fc_dropout': 0.7988907631586026, 'learning_rate_model': 0.0012494842962413658}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00011362262471492472
Epoch 27: reducing lr to 7.805614343901597e-05
Epoch 32: reducing lr to 5.2722923442419484e-05
Epoch 38: reducing lr to 2.460753200191426e-05
Epoch 41: reducing lr to 1.3568865166410237e-05
Epoch 44: reducing lr to 5.515700404211074e-06
Epoch 47: reducing lr to 9.54038290261407e-07
[I 2024-06-24 15:54:33,483] Trial 339 finished with value: 0.971822202205658 and parameters: {'hidden_size': 196, 'n_layers': 3, 'rnn_dropout': 0.27499379240906136, 'bidirectional': True, 'fc_dropout': 0.7980477832304934, 'learning_rate_model': 0.0012216497892755902}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 7.777591135034242e-05
Epoch 27: reducing lr to 6.297031267475581e-05
Epoch 38: reducing lr to 1.985165953689777e-05
Epoch 41: reducing lr to 1.0946424516066617e-05
Epoch 44: reducing lr to 4.449686645674575e-06
Epoch 47: reducing lr to 7.696522886553896e-07
[I 2024-06-24 15:54:38,267] Trial 340 finished with value: 0.9703183174133301 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.2852976787898228, 'bidirectional': True, 'fc_dropout': 0.7997771119770575, 'learning_rate_model': 0.0009855427877990903}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 7.380937204933407e-05
Epoch 27: reducing lr to 5.975885278075223e-05
Epoch 37: reducing lr to 2.2075942367191335e-05
Epoch 40: reducing lr to 1.2974784440846203e-05
Epoch 43: reducing lr to 5.9967530215526395e-06
Epoch 46: reducing lr to 1.5803136480959886e-06
Epoch 49: reducing lr to 2.9612801745134734e-09
[I 2024-06-24 15:54:43,052] Trial 341 finished with value: 0.9709838032722473 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.2878335588045838, 'bidirectional': True, 'fc_dropout': 0.7791996359025266, 'learning_rate_model': 0.0009352805133652826}. Best is trial 212 with value: 0.9678940773010254.
Epoch 27: reducing lr to 5.9458836560570917e-05
Epoch 37: reducing lr to 2.1965111243805404e-05
Epoch 40: reducing lr to 1.2909645208674324e-05
Epoch 43: reducing lr to 5.966646634110971e-06
Epoch 46: reducing lr to 1.5723797654101555e-06
Epoch 49: reducing lr to 2.946413221022702e-09
[I 2024-06-24 15:54:47,788] Trial 342 finished with value: 0.9711705446243286 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.2925451971620824, 'bidirectional': True, 'fc_dropout': 0.7751032270224807, 'learning_rate_model': 0.0009305849860689576}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 7.212228823661643e-05
Epoch 27: reducing lr to 5.839292606448648e-05
Epoch 37: reducing lr to 2.1571345674602184e-05
Epoch 40: reducing lr to 1.2678215750503981e-05
Epoch 43: reducing lr to 5.859683369411922e-06
Epoch 46: reducing lr to 1.5441919266845414e-06
Epoch 49: reducing lr to 2.893593270959609e-09
[I 2024-06-24 15:54:52,479] Trial 343 finished with value: 0.9711950421333313 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.2872359152504501, 'bidirectional': True, 'fc_dropout': 0.7980796342097192, 'learning_rate_model': 0.0009139025152786148}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 7.650085249975598e-05
Epoch 27: reducing lr to 6.193797691544248e-05
Epoch 37: reducing lr to 2.2880948095545477e-05
Epoch 40: reducing lr to 1.3447913769837566e-05
Epoch 43: reducing lr to 6.215426383422316e-06
Epoch 46: reducing lr to 1.637940249857924e-06
Epoch 49: reducing lr to 3.0692641266429415e-09
[I 2024-06-24 15:54:57,282] Trial 344 finished with value: 0.9705871939659119 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.2871090619733977, 'bidirectional': True, 'fc_dropout': 0.780082518912997, 'learning_rate_model': 0.0009693857922409879}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 7.727001299004407e-05
Epoch 27: reducing lr to 6.256071827236899e-05
Epoch 37: reducing lr to 2.3110999404522488e-05
Epoch 40: reducing lr to 1.3583122772228666e-05
Epoch 43: reducing lr to 6.277917979897508e-06
Epoch 46: reducing lr to 1.6544085490268357e-06
Epoch 49: reducing lr to 3.1001233474639913e-09
[I 2024-06-24 15:55:01,755] Trial 345 finished with value: 0.9705359935760498 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.28803818532139647, 'bidirectional': True, 'fc_dropout': 0.7794102774297541, 'learning_rate_model': 0.0009791322620759581}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 7.121356738386333e-05
Epoch 37: reducing lr to 2.1299552694710227e-05
Epoch 40: reducing lr to 1.2518473744116333e-05
Epoch 43: reducing lr to 5.785852982183423e-06
Epoch 46: reducing lr to 1.5247355361742012e-06
Epoch 49: reducing lr to 2.857134797317178e-09
[I 2024-06-24 15:55:06,778] Trial 346 finished with value: 0.9712941646575928 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.28656176943418915, 'bidirectional': True, 'fc_dropout': 0.7800265634498711, 'learning_rate_model': 0.0009023875967517293}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 7.084636471116583e-05
Epoch 27: reducing lr to 5.735989023177492e-05
Epoch 37: reducing lr to 2.1189724568356167e-05
Epoch 40: reducing lr to 1.2453924007516918e-05
Epoch 43: reducing lr to 5.75601905085626e-06
Epoch 46: reducing lr to 1.5168734533631918e-06
Epoch 49: reducing lr to 2.842402386453013e-09
[I 2024-06-24 15:55:11,486] Trial 347 finished with value: 0.9714010953903198 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.27056170106413463, 'bidirectional': True, 'fc_dropout': 0.7992698883640474, 'learning_rate_model': 0.0008977345629337466}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 7.701952222819121e-05
Epoch 27: reducing lr to 6.23579114993958e-05
Epoch 38: reducing lr to 1.9658597455468444e-05
Epoch 41: reducing lr to 1.0839967950188436e-05
Epoch 44: reducing lr to 4.406412391251384e-06
Epoch 47: reducing lr to 7.621672382217586e-07
[I 2024-06-24 15:55:16,148] Trial 348 finished with value: 0.9705461263656616 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.2603100838229656, 'bidirectional': True, 'fc_dropout': 0.7778771764575925, 'learning_rate_model': 0.0009759581512302187}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 7.787066342970531e-05
Epoch 27: reducing lr to 6.3047027533643e-05
Epoch 38: reducing lr to 1.987584422322631e-05
Epoch 41: reducing lr to 1.0959760219454436e-05
Epoch 44: reducing lr to 4.455107566559595e-06
Epoch 47: reducing lr to 7.705899331454354e-07
[I 2024-06-24 15:55:20,799] Trial 349 finished with value: 0.9704641103744507 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.2589530568609423, 'bidirectional': True, 'fc_dropout': 0.7772451642693436, 'learning_rate_model': 0.000986743445262613}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 9.598295467932013e-05
Epoch 23: reducing lr to 8.144147796977836e-05
Epoch 27: reducing lr to 6.593809372866452e-05
Epoch 37: reducing lr to 2.4358659666661278e-05
Epoch 40: reducing lr to 1.4316415271701708e-05
Epoch 43: reducing lr to 6.61683490232848e-06
Epoch 46: reducing lr to 1.7437227222408127e-06
Epoch 49: reducing lr to 3.2674852447648827e-09
[I 2024-06-24 15:55:25,453] Trial 350 finished with value: 0.9721477031707764 and parameters: {'hidden_size': 196, 'n_layers': 3, 'rnn_dropout': 0.2547769727175554, 'bidirectional': True, 'fc_dropout': 0.7795922606277459, 'learning_rate_model': 0.001031991266283763}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 8.438907201619412e-05
Epoch 27: reducing lr to 6.83245770950241e-05
Epoch 36: reducing lr to 2.913890692007057e-05
Epoch 39: reducing lr to 1.8062921169465018e-05
Epoch 42: reducing lr to 9.211536941569295e-06
Epoch 45: reducing lr to 3.14093214979173e-06
Epoch 48: reducing lr to 2.3253755875841075e-07
[I 2024-06-24 15:55:30,059] Trial 351 finished with value: 0.9700982570648193 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.2633297013654394, 'bidirectional': True, 'fc_dropout': 0.7734618152381076, 'learning_rate_model': 0.0010693419061331515}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00010683917836861684
Epoch 27: reducing lr to 7.339607100760762e-05
Epoch 32: reducing lr to 4.9575283407792005e-05
Epoch 38: reducing lr to 2.3138424300267285e-05
Epoch 41: reducing lr to 1.2758782939674484e-05
Epoch 44: reducing lr to 5.1864045632802e-06
Epoch 47: reducing lr to 8.970807294714873e-07
[I 2024-06-24 15:55:35,222] Trial 352 finished with value: 0.9718505144119263 and parameters: {'hidden_size': 196, 'n_layers': 3, 'rnn_dropout': 0.27161921293640773, 'bidirectional': True, 'fc_dropout': 0.7838253537950853, 'learning_rate_model': 0.0011487154082901034}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 8.158251359680741e-05
Epoch 27: reducing lr to 6.605228149423494e-05
Epoch 38: reducing lr to 2.0823263346833446e-05
Epoch 41: reducing lr to 1.1482177597324907e-05
Epoch 44: reducing lr to 4.66746856410418e-06
Epoch 47: reducing lr to 8.073215371427875e-07
[I 2024-06-24 15:55:39,547] Trial 353 finished with value: 0.9701440930366516 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.26128770947497965, 'bidirectional': True, 'fc_dropout': 0.7730156807565378, 'learning_rate_model': 0.001033778408891647}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 9.451543399320066e-05
Epoch 23: reducing lr to 8.019628757083617e-05
Epoch 27: reducing lr to 6.492994059487743e-05
Epoch 37: reducing lr to 2.3986230655006027e-05
Epoch 40: reducing lr to 1.4097526036290137e-05
Epoch 43: reducing lr to 6.5156675426837745e-06
Epoch 46: reducing lr to 1.7170622680560706e-06
Epoch 49: reducing lr to 3.217527393345812e-09
[I 2024-06-24 15:55:45,005] Trial 354 finished with value: 0.9728121161460876 and parameters: {'hidden_size': 194, 'n_layers': 3, 'rnn_dropout': 0.259189801362674, 'bidirectional': True, 'fc_dropout': 0.7851017548306836, 'learning_rate_model': 0.0010162127508564571}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00010649115310655514
Epoch 23: reducing lr to 9.035767786769736e-05
Epoch 27: reducing lr to 7.31569856155561e-05
Epoch 36: reducing lr to 3.1199821280121436e-05
Epoch 39: reducing lr to 1.934046166625614e-05
Epoch 42: reducing lr to 9.863043493036511e-06
Epoch 45: reducing lr to 3.363081600668777e-06
Epoch 48: reducing lr to 2.4898429766358114e-07
[I 2024-06-24 15:55:49,261] Trial 355 finished with value: 0.9704920053482056 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.26028874046440875, 'bidirectional': True, 'fc_dropout': 0.7722411223873905, 'learning_rate_model': 0.001144973503989556}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00010067462041546163
Epoch 27: reducing lr to 6.916116074183197e-05
Epoch 31: reducing lr to 5.122692133400974e-05
Epoch 36: reducing lr to 2.9495691170359964e-05
Epoch 39: reducing lr to 1.828408820929811e-05
Epoch 42: reducing lr to 9.324325362587488e-06
Epoch 45: reducing lr to 3.1793905286646006e-06
Epoch 48: reducing lr to 2.3538480827239167e-07
[I 2024-06-24 15:55:53,586] Trial 356 finished with value: 0.9737582802772522 and parameters: {'hidden_size': 193, 'n_layers': 3, 'rnn_dropout': 0.2627861665350394, 'bidirectional': True, 'fc_dropout': 0.7621687022340523, 'learning_rate_model': 0.0010824352027118214}. Best is trial 212 with value: 0.9678940773010254.
Epoch 28: reducing lr to 3.9599441555568827e-05
Epoch 37: reducing lr to 1.562337378851578e-05
Epoch 42: reducing lr to 5.701812991310093e-06
Epoch 45: reducing lr to 1.9441932274827356e-06
Epoch 48: reducing lr to 1.4393750813861953e-07
[I 2024-06-24 15:55:57,889] Trial 357 finished with value: 0.9733858108520508 and parameters: {'hidden_size': 196, 'n_layers': 3, 'rnn_dropout': 0.28011556758261297, 'bidirectional': True, 'fc_dropout': 0.7996492547103827, 'learning_rate_model': 0.0006619077371363792}. Best is trial 212 with value: 0.9678940773010254.
Epoch 27: reducing lr to 5.6091351862495516e-05
Epoch 37: reducing lr to 2.0721104796930377e-05
Epoch 40: reducing lr to 1.2178500181080206e-05
Epoch 43: reducing lr to 5.628722241346853e-06
Epoch 46: reducing lr to 1.4833271517723464e-06
Epoch 49: reducing lr to 2.77954144871969e-09
[I 2024-06-24 15:56:02,189] Trial 358 finished with value: 0.9740180969238281 and parameters: {'hidden_size': 193, 'n_layers': 3, 'rnn_dropout': 0.25941961664965096, 'bidirectional': True, 'fc_dropout': 0.7569306729988606, 'learning_rate_model': 0.0008778807812422524}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00010000588124394781
Epoch 23: reducing lr to 8.485492868383262e-05
Epoch 26: reducing lr to 7.297085146813176e-05
Epoch 31: reducing lr to 5.088664243560678e-05
Epoch 38: reducing lr to 2.1658520292630026e-05
Epoch 41: reducing lr to 1.1942747510469383e-05
Epoch 44: reducing lr to 4.854688764536824e-06
Epoch 47: reducing lr to 8.3970459402291e-07
[I 2024-06-24 15:56:06,382] Trial 359 finished with value: 0.9744454026222229 and parameters: {'hidden_size': 200, 'n_layers': 3, 'rnn_dropout': 0.29330591811793594, 'bidirectional': True, 'fc_dropout': 0.7819241500590193, 'learning_rate_model': 0.0010752450408051594}. Best is trial 212 with value: 0.9678940773010254.
Epoch 27: reducing lr to 5.528590210750726e-05
Epoch 37: reducing lr to 2.042355788055925e-05
Epoch 40: reducing lr to 1.2003621707638845e-05
Epoch 43: reducing lr to 5.547896003439386e-06
Epoch 46: reducing lr to 1.4620271571868854e-06
Epoch 49: reducing lr to 2.7396283265742304e-09
[I 2024-06-24 15:56:10,669] Trial 360 finished with value: 0.9725428819656372 and parameters: {'hidden_size': 196, 'n_layers': 3, 'rnn_dropout': 0.2794683060069541, 'bidirectional': True, 'fc_dropout': 0.7675685951601467, 'learning_rate_model': 0.0008652747584476179}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00010820880343372153
Epoch 23: reducing lr to 9.1815103113118e-05
Epoch 27: reducing lr to 7.433697208965629e-05
Epoch 36: reducing lr to 3.170305917046255e-05
Epoch 39: reducing lr to 1.9652413873923147e-05
Epoch 42: reducing lr to 1.0022129570973139e-05
Epoch 45: reducing lr to 3.4173264655534438e-06
Epoch 48: reducing lr to 2.530002928694333e-07
[I 2024-06-24 15:56:15,333] Trial 361 finished with value: 0.9724944233894348 and parameters: {'hidden_size': 192, 'n_layers': 3, 'rnn_dropout': 0.29468036203258235, 'bidirectional': True, 'fc_dropout': 0.7874329730140924, 'learning_rate_model': 0.001163441367810663}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 6.611920302974335e-05
Epoch 37: reducing lr to 1.9775858741537775e-05
Epoch 40: reducing lr to 1.1622946827647546e-05
Epoch 43: reducing lr to 5.37195371729007e-06
Epoch 46: reducing lr to 1.4156613997378634e-06
Epoch 49: reducing lr to 2.6527455748541484e-09
[I 2024-06-24 15:56:19,648] Trial 362 finished with value: 0.9721935391426086 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.25413067783134763, 'bidirectional': True, 'fc_dropout': 0.7703576443980001, 'learning_rate_model': 0.0008378340099090391}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00011048216086387986
Epoch 27: reducing lr to 7.589871662866694e-05
Epoch 31: reducing lr to 5.621735587409447e-05
Epoch 36: reducing lr to 3.236910835349473e-05
Epoch 39: reducing lr to 2.006529119705327e-05
Epoch 42: reducing lr to 1.0232684368764017e-05
Epoch 45: reducing lr to 3.4891210355442487e-06
Epoch 48: reducing lr to 2.5831557293330874e-07
[I 2024-06-24 15:56:23,947] Trial 363 finished with value: 0.9736393094062805 and parameters: {'hidden_size': 193, 'n_layers': 3, 'rnn_dropout': 0.2693497183754711, 'bidirectional': True, 'fc_dropout': 0.7543042789989052, 'learning_rate_model': 0.0011878840933019024}. Best is trial 212 with value: 0.9678940773010254.
Epoch 28: reducing lr to 3.8682798512801156e-05
Epoch 37: reducing lr to 1.5261725837806544e-05
Epoch 40: reducing lr to 8.969836922346792e-06
Epoch 43: reducing lr to 4.14572565056113e-06
Epoch 46: reducing lr to 1.0925157002959416e-06
Epoch 49: reducing lr to 2.0472170746176426e-09
[I 2024-06-24 15:56:28,301] Trial 364 finished with value: 0.9734972715377808 and parameters: {'hidden_size': 197, 'n_layers': 3, 'rnn_dropout': 0.28633295286982796, 'bidirectional': True, 'fc_dropout': 0.7989755209222966, 'learning_rate_model': 0.000646585977576999}. Best is trial 212 with value: 0.9678940773010254.
Epoch 27: reducing lr to 6.64578062815241e-05
Epoch 37: reducing lr to 2.4550650373151693e-05
Epoch 40: reducing lr to 1.4429254759589698e-05
Epoch 43: reducing lr to 6.6689876408211934e-06
Epoch 46: reducing lr to 1.7574664405712758e-06
Epoch 49: reducing lr to 3.2932389934861584e-09
[I 2024-06-24 15:56:32,915] Trial 365 finished with value: 0.9694219827651978 and parameters: {'hidden_size': 191, 'n_layers': 3, 'rnn_dropout': 0.2996937345258924, 'bidirectional': True, 'fc_dropout': 0.783753038943244, 'learning_rate_model': 0.0010401252414292406}. Best is trial 212 with value: 0.9678940773010254.
Epoch 27: reducing lr to 6.301647066513266e-05
Epoch 32: reducing lr to 4.256439547368891e-05
Epoch 37: reducing lr to 2.3279362133861914e-05
Epoch 40: reducing lr to 1.368207529188636e-05
Epoch 43: reducing lr to 6.323652367544019e-06
Epoch 46: reducing lr to 1.6664608507850447e-06
Epoch 49: reducing lr to 3.1227076251628267e-09
[I 2024-06-24 15:56:37,763] Trial 366 finished with value: 0.9692648649215698 and parameters: {'hidden_size': 191, 'n_layers': 3, 'rnn_dropout': 0.24753806778158927, 'bidirectional': True, 'fc_dropout': 0.769302809335531, 'learning_rate_model': 0.0009862652024192967}. Best is trial 212 with value: 0.9678940773010254.
Epoch 27: reducing lr to 6.579292843320703e-05
Epoch 32: reducing lr to 4.443975036438578e-05
Epoch 37: reducing lr to 2.4305033123528092e-05
Epoch 40: reducing lr to 1.4284897122854973e-05
Epoch 43: reducing lr to 6.6022676811779365e-06
Epoch 46: reducing lr to 1.7398838483841886e-06
Epoch 49: reducing lr to 3.260291748011755e-09
[I 2024-06-24 15:56:42,740] Trial 367 finished with value: 0.9690128564834595 and parameters: {'hidden_size': 191, 'n_layers': 3, 'rnn_dropout': 0.2533359868377427, 'bidirectional': True, 'fc_dropout': 0.7552291170809224, 'learning_rate_model': 0.0010297192970986043}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 9.355746185804675e-05
Epoch 23: reducing lr to 7.938344880377112e-05
Epoch 27: reducing lr to 6.427183565688852e-05
Epoch 32: reducing lr to 4.341233017090268e-05
Epoch 37: reducing lr to 2.3743115434305414e-05
Epoch 40: reducing lr to 1.3954638927309123e-05
Epoch 43: reducing lr to 6.449627239168174e-06
Epoch 46: reducing lr to 1.6996587844383438e-06
Epoch 49: reducing lr to 3.1849157715226755e-09
[I 2024-06-24 15:56:47,972] Trial 368 finished with value: 0.9717002511024475 and parameters: {'hidden_size': 190, 'n_layers': 3, 'rnn_dropout': 0.25711919681571127, 'bidirectional': True, 'fc_dropout': 0.7584136985875634, 'learning_rate_model': 0.0010059128087456415}. Best is trial 212 with value: 0.9678940773010254.
Epoch 27: reducing lr to 7.085077063663566e-05
Epoch 32: reducing lr to 4.785606348884276e-05
Epoch 37: reducing lr to 2.617348654573306e-05
Epoch 40: reducing lr to 1.538305094060092e-05
Epoch 43: reducing lr to 7.109818095962956e-06
Epoch 46: reducing lr to 1.873637705629731e-06
Epoch 49: reducing lr to 3.5109272128247202e-09
[I 2024-06-24 15:56:53,218] Trial 369 finished with value: 0.9687815308570862 and parameters: {'hidden_size': 191, 'n_layers': 3, 'rnn_dropout': 0.24556354384923343, 'bidirectional': True, 'fc_dropout': 0.7730062402679116, 'learning_rate_model': 0.0011088791375644003}. Best is trial 212 with value: 0.9678940773010254.
Epoch 27: reducing lr to 4.878459421329635e-05
Epoch 37: reducing lr to 1.8021863542307278e-05
Epoch 40: reducing lr to 1.0592064012238643e-05
Epoch 43: reducing lr to 4.895494962514277e-06
Epoch 46: reducing lr to 1.2901010723038619e-06
Epoch 49: reducing lr to 2.417463604862183e-09
[I 2024-06-24 15:56:58,121] Trial 370 finished with value: 0.9723978042602539 and parameters: {'hidden_size': 188, 'n_layers': 3, 'rnn_dropout': 0.24566272858741814, 'bidirectional': True, 'fc_dropout': 0.7730588391713408, 'learning_rate_model': 0.0007635233642708906}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00010649102580768807
Epoch 27: reducing lr to 7.315689816414708e-05
Epoch 37: reducing lr to 2.7025409499734824e-05
Epoch 40: reducing lr to 1.5883755123667155e-05
Epoch 43: reducing lr to 7.341236146597686e-06
Epoch 46: reducing lr to 1.934622892533285e-06
Epoch 49: reducing lr to 3.625204669788218e-09
[I 2024-06-24 15:57:02,937] Trial 371 finished with value: 0.972544252872467 and parameters: {'hidden_size': 192, 'n_layers': 3, 'rnn_dropout': 0.22799538779111894, 'bidirectional': True, 'fc_dropout': 0.7896701794703481, 'learning_rate_model': 0.001144972135295297}. Best is trial 212 with value: 0.9678940773010254.
Epoch 27: reducing lr to 6.300050115030623e-05
Epoch 37: reducing lr to 2.327346272193321e-05
Epoch 40: reducing lr to 1.3678608006239861e-05
Epoch 43: reducing lr to 6.3220498395195995e-06
Epoch 46: reducing lr to 1.6660385394276544e-06
Epoch 49: reducing lr to 3.1219162744972743e-09
[I 2024-06-24 15:57:07,646] Trial 372 finished with value: 0.9693387746810913 and parameters: {'hidden_size': 191, 'n_layers': 3, 'rnn_dropout': 0.27101344310455083, 'bidirectional': True, 'fc_dropout': 0.7529188045784232, 'learning_rate_model': 0.000986015264956811}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 7.592520046218593e-05
Epoch 27: reducing lr to 6.147190730380704e-05
Epoch 30: reducing lr to 4.956009193532876e-05
Epoch 37: reducing lr to 2.2708774009083145e-05
Epoch 40: reducing lr to 1.334672118557587e-05
Epoch 43: reducing lr to 6.168656671124079e-06
Epoch 46: reducing lr to 1.6256151108373943e-06
Epoch 49: reducing lr to 3.0461685912304243e-09
[I 2024-06-24 15:57:12,352] Trial 373 finished with value: 0.9727250933647156 and parameters: {'hidden_size': 189, 'n_layers': 3, 'rnn_dropout': 0.269095450347786, 'bidirectional': True, 'fc_dropout': 0.7539036216481694, 'learning_rate_model': 0.0009620913780186527}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 6.226398139699574e-05
Epoch 26: reducing lr to 5.3543804806713106e-05
Epoch 37: reducing lr to 1.862278497577806e-05
Epoch 40: reducing lr to 1.0945246038875718e-05
Epoch 43: reducing lr to 5.058730459415973e-06
Epoch 46: reducing lr to 1.3331182322036156e-06
Epoch 49: reducing lr to 2.498071567040138e-09
[I 2024-06-24 15:57:17,075] Trial 374 finished with value: 0.9732101559638977 and parameters: {'hidden_size': 192, 'n_layers': 3, 'rnn_dropout': 0.24933823192907154, 'bidirectional': True, 'fc_dropout': 0.7988490494466225, 'learning_rate_model': 0.0007889823049331035}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00010308638633529325
Epoch 27: reducing lr to 7.081798874639555e-05
Epoch 32: reducing lr to 4.783392100250819e-05
Epoch 37: reducing lr to 2.6161376354757948e-05
Epoch 40: reducing lr to 1.53759333682307e-05
Epoch 43: reducing lr to 7.106528459529149e-06
Epoch 46: reducing lr to 1.872770793596686e-06
Epoch 49: reducing lr to 3.5093027445348007e-09
[I 2024-06-24 15:57:22,238] Trial 375 finished with value: 0.9708994626998901 and parameters: {'hidden_size': 188, 'n_layers': 3, 'rnn_dropout': 0.27178774658632643, 'bidirectional': True, 'fc_dropout': 0.7550786108677118, 'learning_rate_model': 0.0011083660711030117}. Best is trial 212 with value: 0.9678940773010254.
Epoch 37: reducing lr to 1.5699243495070373e-05
Epoch 40: reducing lr to 9.226980975254764e-06
Epoch 43: reducing lr to 4.264573819737314e-06
Epoch 46: reducing lr to 1.1238355467403887e-06
Epoch 49: reducing lr to 2.1059059560632825e-09
[I 2024-06-24 15:57:26,968] Trial 376 finished with value: 0.9725522994995117 and parameters: {'hidden_size': 191, 'n_layers': 3, 'rnn_dropout': 0.24773377279266556, 'bidirectional': True, 'fc_dropout': 0.7722606794318145, 'learning_rate_model': 0.0006651220714064629}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 9.056288513644884e-05
Epoch 27: reducing lr to 6.221462996650208e-05
Epoch 37: reducing lr to 2.2983148464640687e-05
Epoch 40: reducing lr to 1.3507980413278172e-05
Epoch 43: reducing lr to 6.243188295551886e-06
Epoch 46: reducing lr to 1.6452562971384921e-06
Epoch 49: reducing lr to 3.0829733455592714e-09
[I 2024-06-24 15:57:31,666] Trial 377 finished with value: 0.974024772644043 and parameters: {'hidden_size': 193, 'n_layers': 3, 'rnn_dropout': 0.27751341596285756, 'bidirectional': True, 'fc_dropout': 0.7999242126331252, 'learning_rate_model': 0.0009737156646461429}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00010886893827361646
Epoch 23: reducing lr to 9.237522711846882e-05
Epoch 31: reducing lr to 5.539648934006006e-05
Epoch 38: reducing lr to 2.3578014407816703e-05
Epoch 41: reducing lr to 1.3001177784365188e-05
Epoch 44: reducing lr to 5.284937294385006e-06
Epoch 47: reducing lr to 9.141237143019034e-07
[I 2024-06-24 15:57:36,375] Trial 378 finished with value: 0.9744162559509277 and parameters: {'hidden_size': 200, 'n_layers': 3, 'rnn_dropout': 0.29838039589362936, 'bidirectional': True, 'fc_dropout': 0.7516611710292714, 'learning_rate_model': 0.001170539017509167}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 6.494990856899003e-05
Epoch 27: reducing lr to 5.2585896838457264e-05
Epoch 37: reducing lr to 1.9426129751720415e-05
Epoch 40: reducing lr to 1.141739916947197e-05
Epoch 43: reducing lr to 5.276952636859241e-06
Epoch 46: reducing lr to 1.3906259341368745e-06
Epoch 49: reducing lr to 2.605832718008405e-09
[I 2024-06-24 15:57:41,125] Trial 379 finished with value: 0.9744639992713928 and parameters: {'hidden_size': 193, 'n_layers': 3, 'rnn_dropout': 0.255643851053047, 'bidirectional': True, 'fc_dropout': 0.7713593119154375, 'learning_rate_model': 0.0008230172150608512}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00010770030342300344
Epoch 27: reducing lr to 7.398764421701705e-05
Epoch 31: reducing lr to 5.4801845274718823e-05
Epoch 37: reducing lr to 2.733230130122598e-05
Epoch 40: reducing lr to 1.606412590489047e-05
Epoch 43: reducing lr to 7.424600847740284e-06
Epoch 46: reducing lr to 1.956591843816041e-06
Epoch 49: reducing lr to 3.666371320475495e-09
[I 2024-06-24 15:57:45,856] Trial 380 finished with value: 0.9704159498214722 and parameters: {'hidden_size': 188, 'n_layers': 3, 'rnn_dropout': 0.2230376512789391, 'bidirectional': True, 'fc_dropout': 0.7685809330250787, 'learning_rate_model': 0.00115797406820815}. Best is trial 212 with value: 0.9678940773010254.
Epoch 37: reducing lr to 1.5387787560522033e-05
Epoch 40: reducing lr to 9.043927697330247e-06
Epoch 43: reducing lr to 4.179969308386571e-06
Epoch 46: reducing lr to 1.1015398704806653e-06
Epoch 49: reducing lr to 2.0641270698533094e-09
[I 2024-06-24 15:57:50,566] Trial 381 finished with value: 0.9736080169677734 and parameters: {'hidden_size': 188, 'n_layers': 3, 'rnn_dropout': 0.2294197270578174, 'bidirectional': True, 'fc_dropout': 0.7521148636525111, 'learning_rate_model': 0.000651926772129547}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 7.871711771681718e-05
Epoch 27: reducing lr to 6.373234886513117e-05
Epoch 37: reducing lr to 2.354381978573615e-05
Epoch 40: reducing lr to 1.3837506075756308e-05
Epoch 43: reducing lr to 6.39549017163733e-06
Epoch 46: reducing lr to 1.6853921394090415e-06
Epoch 49: reducing lr to 3.1581821334980208e-09
[I 2024-06-24 15:57:55,074] Trial 382 finished with value: 0.9716675281524658 and parameters: {'hidden_size': 187, 'n_layers': 3, 'rnn_dropout': 0.2204860381230493, 'bidirectional': True, 'fc_dropout': 0.7692164634581421, 'learning_rate_model': 0.0009974693487381375}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00010987240509849129
Epoch 27: reducing lr to 7.547982836934931e-05
Epoch 32: reducing lr to 5.098275468443172e-05
Epoch 36: reducing lr to 3.219046186174708e-05
Epoch 39: reducing lr to 1.995454999778692e-05
Epoch 42: reducing lr to 1.0176209746612635e-05
Epoch 45: reducing lr to 3.4698644275006717e-06
Epoch 48: reducing lr to 2.5688991825156565e-07
[I 2024-06-24 15:57:59,752] Trial 383 finished with value: 0.9692447185516357 and parameters: {'hidden_size': 191, 'n_layers': 3, 'rnn_dropout': 0.24227779915520486, 'bidirectional': True, 'fc_dropout': 0.7864145466165852, 'learning_rate_model': 0.0011813281102468949}. Best is trial 212 with value: 0.9678940773010254.
Epoch 27: reducing lr to 5.074195820506074e-05
Epoch 37: reducing lr to 1.8744947280750855e-05
Epoch 40: reducing lr to 1.1017044993024761e-05
Epoch 43: reducing lr to 5.091914871627255e-06
Epoch 46: reducing lr to 1.341863261277354e-06
Epoch 49: reducing lr to 2.514458492027666e-09
[I 2024-06-24 15:58:04,472] Trial 384 finished with value: 0.9713752865791321 and parameters: {'hidden_size': 191, 'n_layers': 3, 'rnn_dropout': 0.24151740123120732, 'bidirectional': True, 'fc_dropout': 0.755102133538385, 'learning_rate_model': 0.0007941578947860037}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 7.759626473723182e-05
Epoch 27: reducing lr to 6.282486399788235e-05
Epoch 37: reducing lr to 2.320857935362839e-05
Epoch 40: reducing lr to 1.364047383721783e-05
Epoch 43: reducing lr to 6.304424791924411e-06
Epoch 46: reducing lr to 1.6613938420117397e-06
Epoch 49: reducing lr to 3.1132127804736968e-09
[I 2024-06-24 15:58:09,151] Trial 385 finished with value: 0.9717789888381958 and parameters: {'hidden_size': 187, 'n_layers': 3, 'rnn_dropout': 0.24150142754289275, 'bidirectional': True, 'fc_dropout': 0.7860743115306407, 'learning_rate_model': 0.000983266383436483}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00011088214617337965
Epoch 27: reducing lr to 7.617349738443762e-05
Epoch 31: reducing lr to 5.642088299840789e-05
Epoch 36: reducing lr to 3.248629621189438e-05
Epoch 39: reducing lr to 2.0137934795322008e-05
Epoch 42: reducing lr to 1.0269730380466274e-05
Epoch 45: reducing lr to 3.5017529133638848e-06
Epoch 48: reducing lr to 2.5925076856652547e-07
[I 2024-06-24 15:58:13,803] Trial 386 finished with value: 0.9736993908882141 and parameters: {'hidden_size': 193, 'n_layers': 3, 'rnn_dropout': 0.2615801521755067, 'bidirectional': True, 'fc_dropout': 0.7702325621993248, 'learning_rate_model': 0.0011921846625792777}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 6.624169162655916e-05
Epoch 27: reducing lr to 5.363177314682562e-05
Epoch 37: reducing lr to 1.981249434930499e-05
Epoch 40: reducing lr to 1.1644478824140948e-05
Epoch 43: reducing lr to 5.3819054868644e-06
Epoch 46: reducing lr to 1.41828397185723e-06
Epoch 49: reducing lr to 2.657659897297056e-09
[I 2024-06-24 15:58:18,583] Trial 387 finished with value: 0.9730550646781921 and parameters: {'hidden_size': 194, 'n_layers': 3, 'rnn_dropout': 0.26554595568734085, 'bidirectional': True, 'fc_dropout': 0.7520236514437739, 'learning_rate_model': 0.0008393861325532299}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00010589759051885475
Epoch 23: reducing lr to 8.985404037735974e-05
Epoch 27: reducing lr to 7.274922169880258e-05
Epoch 32: reducing lr to 4.913836985961556e-05
Epoch 37: reducing lr to 2.6874806840302517e-05
Epoch 40: reducing lr to 1.5795240802971337e-05
Epoch 43: reducing lr to 7.300326139768418e-06
Epoch 46: reducing lr to 1.9238419512632946e-06
Epoch 49: reducing lr to 3.605002738557919e-09
[I 2024-06-24 15:58:23,276] Trial 388 finished with value: 0.9718433618545532 and parameters: {'hidden_size': 190, 'n_layers': 3, 'rnn_dropout': 0.21954981259189857, 'bidirectional': True, 'fc_dropout': 0.7845262955130886, 'learning_rate_model': 0.0011385916270349851}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 9.671155187933962e-05
Epoch 37: reducing lr to 2.4543563864376918e-05
Epoch 40: reducing lr to 1.4425089776629442e-05
Epoch 43: reducing lr to 6.667062647441489e-06
Epoch 46: reducing lr to 1.7569591504928852e-06
Epoch 49: reducing lr to 3.292288404941647e-09
[I 2024-06-24 15:58:27,941] Trial 389 finished with value: 0.9705159068107605 and parameters: {'hidden_size': 188, 'n_layers': 3, 'rnn_dropout': 0.2974549377182875, 'bidirectional': True, 'fc_dropout': 0.7682884514255335, 'learning_rate_model': 0.0010398250108227913}. Best is trial 212 with value: 0.9678940773010254.
Epoch 37: reducing lr to 1.5552706427481002e-05
Epoch 40: reducing lr to 9.140856141581007e-06
Epoch 43: reducing lr to 4.224768198386265e-06
Epoch 46: reducing lr to 1.1133456422094107e-06
Epoch 49: reducing lr to 2.086249385768214e-09
[I 2024-06-24 15:58:32,583] Trial 390 finished with value: 0.9739482998847961 and parameters: {'hidden_size': 187, 'n_layers': 3, 'rnn_dropout': 0.24263764671572338, 'bidirectional': True, 'fc_dropout': 0.7661219106749924, 'learning_rate_model': 0.0006589138080615776}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 7.243821290934966e-05
Epoch 27: reducing lr to 5.8648710600833084e-05
Epoch 32: reducing lr to 3.961419745801524e-05
Epoch 37: reducing lr to 2.1665836857415274e-05
Epoch 40: reducing lr to 1.2733751442170873e-05
Epoch 43: reducing lr to 5.885351142801592e-06
Epoch 46: reducing lr to 1.5509561093110699e-06
Epoch 49: reducing lr to 2.9062683750032124e-09
[I 2024-06-24 15:58:37,188] Trial 391 finished with value: 0.9739255905151367 and parameters: {'hidden_size': 182, 'n_layers': 3, 'rnn_dropout': 0.2972260882792719, 'bidirectional': True, 'fc_dropout': 0.766883659178557, 'learning_rate_model': 0.0009179057764078546}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 9.851695749008398e-05
Epoch 23: reducing lr to 8.359157779508248e-05
Epoch 27: reducing lr to 6.767889568045988e-05
Epoch 32: reducing lr to 4.571362455815132e-05
Epoch 37: reducing lr to 2.5001741683337918e-05
Epoch 40: reducing lr to 1.4694376511379715e-05
Epoch 43: reducing lr to 6.791522984153837e-06
Epoch 46: reducing lr to 1.7897579614571408e-06
Epoch 49: reducing lr to 3.3537486529003e-09
[I 2024-06-24 15:58:41,886] Trial 392 finished with value: 0.971430778503418 and parameters: {'hidden_size': 190, 'n_layers': 3, 'rnn_dropout': 0.2635074132857557, 'bidirectional': True, 'fc_dropout': 0.7498223072907494, 'learning_rate_model': 0.0010592364034873818}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 6.31441685562941e-05
Epoch 27: reducing lr to 5.112390158524087e-05
Epoch 37: reducing lr to 1.888604369836988e-05
Epoch 40: reducing lr to 1.1099972171105418e-05
Epoch 43: reducing lr to 5.130242583967388e-06
Epoch 46: reducing lr to 1.3519636950777418e-06
Epoch 49: reducing lr to 2.5333852502786025e-09
[I 2024-06-24 15:58:46,845] Trial 393 finished with value: 0.9731330871582031 and parameters: {'hidden_size': 194, 'n_layers': 3, 'rnn_dropout': 0.23306564255535633, 'bidirectional': True, 'fc_dropout': 0.7857610368681722, 'learning_rate_model': 0.0008001356568090124}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00010658370854624408
Epoch 27: reducing lr to 7.322056908490956e-05
Epoch 30: reducing lr to 5.9032138330617096e-05
Epoch 37: reducing lr to 2.7048930626928746e-05
Epoch 40: reducing lr to 1.5897579292532567e-05
Epoch 43: reducing lr to 7.347625472508418e-06
Epoch 46: reducing lr to 1.936306660216009e-06
Epoch 49: reducing lr to 3.628359807939831e-09
[I 2024-06-24 15:58:51,820] Trial 394 finished with value: 0.9731960892677307 and parameters: {'hidden_size': 182, 'n_layers': 3, 'rnn_dropout': 0.26917315534757036, 'bidirectional': True, 'fc_dropout': 0.7640873207706528, 'learning_rate_model': 0.0011459686432382398}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 7.492146280155592e-05
Epoch 27: reducing lr to 6.065924341808811e-05
Epoch 37: reducing lr to 2.240856206942544e-05
Epoch 40: reducing lr to 1.3170276387032824e-05
Epoch 43: reducing lr to 6.087106500974877e-06
Epoch 46: reducing lr to 1.6041243396770463e-06
Epoch 49: reducing lr to 3.0058979812500575e-09
[I 2024-06-24 15:58:56,576] Trial 395 finished with value: 0.9718815088272095 and parameters: {'hidden_size': 187, 'n_layers': 3, 'rnn_dropout': 0.2508154346703235, 'bidirectional': True, 'fc_dropout': 0.7735917042465731, 'learning_rate_model': 0.0009493724475027471}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 4.74534555210836e-05
Epoch 33: reducing lr to 2.347313333034416e-05
Epoch 37: reducing lr to 1.419304513940748e-05
Epoch 40: reducing lr to 8.341736818297426e-06
Epoch 43: reducing lr to 3.855427093851555e-06
Epoch 46: reducing lr to 1.0160138384480577e-06
Epoch 49: reducing lr to 1.9038636035642423e-09
[I 2024-06-24 15:59:01,092] Trial 396 finished with value: 0.9762271642684937 and parameters: {'hidden_size': 200, 'n_layers': 3, 'rnn_dropout': 0.2962754158764068, 'bidirectional': True, 'fc_dropout': 0.7495621922507818, 'learning_rate_model': 0.0006013097118757577}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00011035341953126425
Epoch 27: reducing lr to 7.581027427882344e-05
Epoch 32: reducing lr to 5.120595395638497e-05
Epoch 36: reducing lr to 3.233138966558689e-05
Epoch 39: reducing lr to 2.0041909754222785e-05
Epoch 42: reducing lr to 1.0220760548560218e-05
Epoch 45: reducing lr to 3.4850552742642138e-06
Epoch 48: reducing lr to 2.580145660482664e-07
[I 2024-06-24 15:59:06,227] Trial 397 finished with value: 0.9695586562156677 and parameters: {'hidden_size': 191, 'n_layers': 3, 'rnn_dropout': 0.2770951733392139, 'bidirectional': True, 'fc_dropout': 0.7884781284448906, 'learning_rate_model': 0.0011864998899158645}. Best is trial 212 with value: 0.9678940773010254.
Epoch 27: reducing lr to 7.45818894149302e-05
Epoch 32: reducing lr to 5.037624295243985e-05
Epoch 36: reducing lr to 3.1807510942397314e-05
Epoch 39: reducing lr to 1.9717162497735426e-05
Epoch 42: reducing lr to 1.00551493873454e-05
Epoch 45: reducing lr to 3.4285854990330704e-06
Epoch 48: reducing lr to 2.5383385056328753e-07
[I 2024-06-24 15:59:10,967] Trial 398 finished with value: 0.969402551651001 and parameters: {'hidden_size': 191, 'n_layers': 3, 'rnn_dropout': 0.2778549989091733, 'bidirectional': True, 'fc_dropout': 0.7881029955366855, 'learning_rate_model': 0.0011672745471816702}. Best is trial 212 with value: 0.9678940773010254.
Epoch 27: reducing lr to 4.8751408091115054e-05
Epoch 37: reducing lr to 1.800960402113887e-05
Epoch 40: reducing lr to 1.0584858673419483e-05
Epoch 43: reducing lr to 4.8921647617288845e-06
Epoch 46: reducing lr to 1.2892234704194472e-06
Epoch 49: reducing lr to 2.4158191053299736e-09
[I 2024-06-24 15:59:15,615] Trial 399 finished with value: 0.9730777144432068 and parameters: {'hidden_size': 187, 'n_layers': 3, 'rnn_dropout': 0.2702549692653086, 'bidirectional': True, 'fc_dropout': 0.7865090725790821, 'learning_rate_model': 0.0007630039712111843}. Best is trial 212 with value: 0.9678940773010254.
Epoch 27: reducing lr to 6.446967568915027e-05
Epoch 32: reducing lr to 4.354596097067352e-05
Epoch 37: reducing lr to 2.381620092619327e-05
Epoch 40: reducing lr to 1.3997593764172964e-05
Epoch 43: reducing lr to 6.469480327975013e-06
Epoch 46: reducing lr to 1.7048906335883057e-06
Epoch 49: reducing lr to 3.1947195033507624e-09
[I 2024-06-24 15:59:20,371] Trial 400 finished with value: 0.9690490961074829 and parameters: {'hidden_size': 191, 'n_layers': 3, 'rnn_dropout': 0.21843929131668063, 'bidirectional': True, 'fc_dropout': 0.7789866463631215, 'learning_rate_model': 0.0010090091855722994}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00010805515612612644
Epoch 27: reducing lr to 7.423141990486252e-05
Epoch 32: reducing lr to 5.0139518764770536e-05
Epoch 36: reducing lr to 3.1658043519891616e-05
Epoch 39: reducing lr to 1.962450911586598e-05
Epoch 42: reducing lr to 1.0007898998449583e-05
Epoch 45: reducing lr to 3.41247415230402e-06
Epoch 48: reducing lr to 2.526410539481407e-07
[I 2024-06-24 15:59:25,026] Trial 401 finished with value: 0.9694230556488037 and parameters: {'hidden_size': 191, 'n_layers': 3, 'rnn_dropout': 0.21599415329743307, 'bidirectional': True, 'fc_dropout': 0.7684663986218767, 'learning_rate_model': 0.0011617893799128543}. Best is trial 212 with value: 0.9678940773010254.
Epoch 23: reducing lr to 9.29254254092791e-05
Epoch 27: reducing lr to 7.523593091823336e-05
Epoch 32: reducing lr to 5.0818014459300904e-05
Epoch 37: reducing lr to 2.7793439760072998e-05
Epoch 40: reducing lr to 1.633515270870274e-05
Epoch 43: reducing lr to 7.549865418577111e-06
Epoch 46: reducing lr to 1.9896025931673627e-06
Epoch 49: reducing lr to 3.7282287104105706e-09
[I 2024-06-24 15:59:29,559] Trial 402 finished with value: 0.9703701734542847 and parameters: {'hidden_size': 178, 'n_layers': 3, 'rnn_dropout': 0.22214320486167818, 'bidirectional': True, 'fc_dropout': 0.7531359513584592, 'learning_rate_model': 0.0011775108928360266}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00011014902386961634
Epoch 23: reducing lr to 9.346137895880716e-05
Epoch 32: reducing lr to 5.111111072557543e-05
Epoch 36: reducing lr to 3.2271505741638153e-05
Epoch 39: reducing lr to 2.0004788300059415e-05
Epoch 42: reducing lr to 1.0201829743119464e-05
Epoch 45: reducing lr to 3.4786002846346397e-06
Epoch 48: reducing lr to 2.575366736715318e-07
[I 2024-06-24 15:59:33,954] Trial 403 finished with value: 0.973029613494873 and parameters: {'hidden_size': 176, 'n_layers': 3, 'rnn_dropout': 0.2163079913741055, 'bidirectional': True, 'fc_dropout': 0.7999064691897656, 'learning_rate_model': 0.00118430226494806}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00010571671943816749
Epoch 23: reducing lr to 8.970057137671918e-05
Epoch 27: reducing lr to 7.262496740478748e-05
Epoch 37: reducing lr to 2.6828905178776028e-05
Epoch 40: reducing lr to 1.5768262830575997e-05
Epoch 43: reducing lr to 7.28785732086706e-06
Epoch 46: reducing lr to 1.920556065615757e-06
Epoch 49: reducing lr to 3.598845462111401e-09
[I 2024-06-24 15:59:38,339] Trial 404 finished with value: 0.9691376686096191 and parameters: {'hidden_size': 180, 'n_layers': 3, 'rnn_dropout': 0.21915663978612493, 'bidirectional': True, 'fc_dropout': 0.7509648874809154, 'learning_rate_model': 0.0011366469340817812}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00010951209642191505
Epoch 23: reducing lr to 9.292094641144894e-05
Epoch 27: reducing lr to 7.523230455257671e-05
Epoch 32: reducing lr to 5.081556503520095e-05
Epoch 38: reducing lr to 2.37173047538746e-05
Epoch 41: reducing lr to 1.3077984020947354e-05
Epoch 44: reducing lr to 5.316158784536649e-06
Epoch 47: reducing lr to 9.195240252145254e-07
[I 2024-06-24 15:59:42,701] Trial 405 finished with value: 0.9681299924850464 and parameters: {'hidden_size': 180, 'n_layers': 3, 'rnn_dropout': 0.19697524065491115, 'bidirectional': True, 'fc_dropout': 0.7492524234532467, 'learning_rate_model': 0.0011774541369082407}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00011373188892573817
Epoch 27: reducing lr to 7.813120545182726e-05
Epoch 38: reducing lr to 2.4631195621470674e-05
Epoch 41: reducing lr to 1.3581913547819867e-05
Epoch 44: reducing lr to 5.5210045296285365e-06
Epoch 47: reducing lr to 9.54955732902188e-07
[I 2024-06-24 15:59:47,049] Trial 406 finished with value: 0.9681257605552673 and parameters: {'hidden_size': 180, 'n_layers': 3, 'rnn_dropout': 0.20622493917421256, 'bidirectional': True, 'fc_dropout': 0.7534036181464986, 'learning_rate_model': 0.0012228245781915348}. Best is trial 212 with value: 0.9678940773010254.
Epoch 27: reducing lr to 5.374666219737889e-05
Epoch 38: reducing lr to 1.6943864400005645e-05
Epoch 41: reducing lr to 9.343034133765642e-06
Epoch 44: reducing lr to 3.7979135702328222e-06
Epoch 47: reducing lr to 6.56916566088182e-07
[I 2024-06-24 15:59:51,328] Trial 407 finished with value: 0.9706759452819824 and parameters: {'hidden_size': 178, 'n_layers': 3, 'rnn_dropout': 0.19651507843337976, 'bidirectional': True, 'fc_dropout': 0.7448180748148605, 'learning_rate_model': 0.0008411842509102831}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00011355847681902105
Epoch 27: reducing lr to 7.801207530226475e-05
Epoch 32: reducing lr to 5.2693157675142005e-05
Epoch 36: reducing lr to 3.327040865123451e-05
Epoch 39: reducing lr to 2.0623998367255182e-05
Epoch 42: reducing lr to 1.0517607925122929e-05
Epoch 45: reducing lr to 3.5862737217981542e-06
Epoch 48: reducing lr to 2.655082302117657e-07
[I 2024-06-24 15:59:55,597] Trial 408 finished with value: 0.968475878238678 and parameters: {'hidden_size': 179, 'n_layers': 3, 'rnn_dropout': 0.18320592707854375, 'bidirectional': True, 'fc_dropout': 0.7453102065474666, 'learning_rate_model': 0.0012209600827694273}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.0001140940641248533
Epoch 23: reducing lr to 9.680874318728087e-05
Epoch 27: reducing lr to 7.838001152687714e-05
Epoch 32: reducing lr to 5.2941679733076974e-05
Epoch 38: reducing lr to 2.4709632797384235e-05
Epoch 41: reducing lr to 1.362516467369151e-05
Epoch 44: reducing lr to 5.538585974320266e-06
Epoch 47: reducing lr to 9.579967558376095e-07
[I 2024-06-24 15:59:59,808] Trial 409 finished with value: 0.9679386615753174 and parameters: {'hidden_size': 174, 'n_layers': 3, 'rnn_dropout': 0.1824418814346043, 'bidirectional': True, 'fc_dropout': 0.7542076723728297, 'learning_rate_model': 0.0012267186200409458}. Best is trial 212 with value: 0.9678940773010254.
Epoch 18: reducing lr to 0.00011265012324399276
Epoch 23: reducing lr to 9.558356023858797e-05
Epoch 27: reducing lr to 7.738805717978568e-05
Epoch 32: reducing lr to 5.227166542291652e-05
Epoch 38: reducing lr to 2.4396914960388346e-05
Epoch 41: reducing lr to 1.3452728601475935e-05
Epoch 44: reducing lr to 5.468491261051661e-06
Epoch 47: reducing lr to 9.458726309754042e-07
[I 2024-06-24 16:00:04,046] Trial 410 finished with value: 0.9678924083709717 and parameters: {'hidden_size': 174, 'n_layers': 3, 'rnn_dropout': 0.18661789979970037, 'bidirectional': True, 'fc_dropout': 0.7479940413406241, 'learning_rate_model': 0.0012111936303898486}. Best is trial 410 with value: 0.9678924083709717.
Epoch 18: reducing lr to 0.00011286541276439737
Epoch 23: reducing lr to 9.57662332641448e-05
Epoch 27: reducing lr to 7.753595615437618e-05
Epoch 32: reducing lr to 5.237156359839632e-05
Epoch 36: reducing lr to 3.3067354463077685e-05
Epoch 39: reducing lr to 2.0498127077578213e-05
Epoch 42: reducing lr to 1.0453417419951341e-05
Epoch 45: reducing lr to 3.564386166802309e-06
Epoch 48: reducing lr to 2.6388779450566174e-07
[I 2024-06-24 16:00:08,271] Trial 411 finished with value: 0.9715216159820557 and parameters: {'hidden_size': 172, 'n_layers': 3, 'rnn_dropout': 0.18613382524011668, 'bidirectional': True, 'fc_dropout': 0.7519617134353687, 'learning_rate_model': 0.0012135083841450572}. Best is trial 410 with value: 0.9678924083709717.
Epoch 18: reducing lr to 0.00014553879655794492
Epoch 23: reducing lr to 0.00012348957930314344
Epoch 27: reducing lr to 9.998182323785473e-05
Epoch 30: reducing lr to 8.060768843629152e-05
Epoch 36: reducing lr to 4.2640015714623e-05
Epoch 39: reducing lr to 2.643212542703438e-05
Epoch 42: reducing lr to 1.347957495529122e-05
Epoch 45: reducing lr to 4.596239542995189e-06
Epoch 48: reducing lr to 3.4028061474296323e-07
[I 2024-06-24 16:00:12,508] Trial 412 finished with value: 0.9732521772384644 and parameters: {'hidden_size': 173, 'n_layers': 3, 'rnn_dropout': 0.16633578968640694, 'bidirectional': True, 'fc_dropout': 0.7414837065216068, 'learning_rate_model': 0.0015648066623396887}. Best is trial 410 with value: 0.9678924083709717.
Epoch 18: reducing lr to 0.0001125647772317582
Epoch 23: reducing lr to 9.551114419973573e-05
Epoch 27: reducing lr to 7.732942642213823e-05
Epoch 32: reducing lr to 5.2232063351758176e-05
Epoch 36: reducing lr to 3.297927414264571e-05
Epoch 39: reducing lr to 2.044352695517457e-05
Epoch 42: reducing lr to 1.0425573028680578e-05
Epoch 45: reducing lr to 3.5548918398199116e-06
Epoch 48: reducing lr to 2.631848860971866e-07
[I 2024-06-24 16:00:16,644] Trial 413 finished with value: 0.9688079357147217 and parameters: {'hidden_size': 166, 'n_layers': 3, 'rnn_dropout': 0.2026292248921378, 'bidirectional': True, 'fc_dropout': 0.7459059689726019, 'learning_rate_model': 0.0012102760055935245}. Best is trial 410 with value: 0.9678924083709717.
Epoch 37: reducing lr to 1.64634015272976e-05
Epoch 40: reducing lr to 9.676102719730073e-06
Epoch 43: reducing lr to 4.472151231948388e-06
Epoch 46: reducing lr to 1.1785380526422706e-06
Epoch 49: reducing lr to 2.208410573693604e-09
[I 2024-06-24 16:00:20,840] Trial 414 finished with value: 0.9724881649017334 and parameters: {'hidden_size': 175, 'n_layers': 3, 'rnn_dropout': 0.20687624947832367, 'bidirectional': True, 'fc_dropout': 0.743743392850081, 'learning_rate_model': 0.0006974967761772024}. Best is trial 410 with value: 0.9678924083709717.
Epoch 18: reducing lr to 0.00011485674336383633
Epoch 23: reducing lr to 9.745587605214387e-05
Epoch 27: reducing lr to 7.890395471359177e-05
Epoch 32: reducing lr to 5.329557649641008e-05
Epoch 38: reducing lr to 2.4874808120763354e-05
Epoch 41: reducing lr to 1.3716244172910481e-05
Epoch 44: reducing lr to 5.5756095002007466e-06
Epoch 47: reducing lr to 9.644006318174341e-07
[I 2024-06-24 16:00:25,028] Trial 415 finished with value: 0.9717611074447632 and parameters: {'hidden_size': 171, 'n_layers': 3, 'rnn_dropout': 0.2048040366479503, 'bidirectional': True, 'fc_dropout': 0.7439445754683053, 'learning_rate_model': 0.001234918808462276}. Best is trial 410 with value: 0.9678924083709717.
Epoch 23: reducing lr to 6.598478610648809e-05
Epoch 27: reducing lr to 5.3423772743540414e-05
Epoch 37: reducing lr to 1.9735655442572233e-05
Epoch 40: reducing lr to 1.1599317977326571e-05
Epoch 43: reducing lr to 5.361032812961825e-06
Epoch 46: reducing lr to 1.4127834332621067e-06
Epoch 49: reducing lr to 2.647352680183821e-09
[I 2024-06-24 16:00:29,186] Trial 416 finished with value: 0.9738439917564392 and parameters: {'hidden_size': 168, 'n_layers': 3, 'rnn_dropout': 0.17852095877963575, 'bidirectional': True, 'fc_dropout': 0.7532287183197545, 'learning_rate_model': 0.0008361307366593609}. Best is trial 410 with value: 0.9678924083709717.
Epoch 18: reducing lr to 0.00014405965064381477
Epoch 23: reducing lr to 0.00012223452490538926
Epoch 27: reducing lr to 9.896568383841512e-05
Epoch 30: reducing lr to 7.978845304464427e-05
Epoch 36: reducing lr to 4.2206654944062975e-05
Epoch 39: reducing lr to 2.6163489357121502e-05
Epoch 42: reducing lr to 1.3342578781824917e-05
Epoch 45: reducing lr to 4.549526851251322e-06
Epoch 48: reducing lr to 3.3682226073111777e-07
[I 2024-06-24 16:00:33,349] Trial 417 finished with value: 0.9718506336212158 and parameters: {'hidden_size': 178, 'n_layers': 3, 'rnn_dropout': 0.2013491912635388, 'bidirectional': True, 'fc_dropout': 0.7598237830896508, 'learning_rate_model': 0.00154890315457582}. Best is trial 410 with value: 0.9678924083709717.
Epoch 18: reducing lr to 0.00011164417192036251
Epoch 27: reducing lr to 7.669699163710035e-05
Epoch 32: reducing lr to 5.180488607544391e-05
Epoch 36: reducing lr to 3.270955482468194e-05
Epoch 39: reducing lr to 2.0276330608666935e-05
Epoch 42: reducing lr to 1.0340308009368317e-05
Epoch 45: reducing lr to 3.5258183375249916e-06
Epoch 48: reducing lr to 2.6103244187814475e-07
[I 2024-06-24 16:00:37,574] Trial 418 finished with value: 0.9691928625106812 and parameters: {'hidden_size': 181, 'n_layers': 3, 'rnn_dropout': 0.1920906957572187, 'bidirectional': True, 'fc_dropout': 0.7478056125772522, 'learning_rate_model': 0.001200377824773514}. Best is trial 410 with value: 0.9678924083709717.
Epoch 23: reducing lr to 8.046304162893997e-05
Epoch 27: reducing lr to 6.514591474618493e-05
Epoch 32: reducing lr to 4.4002725787150354e-05
Epoch 37: reducing lr to 2.4066015200645806e-05
Epoch 40: reducing lr to 1.4144418135579421e-05
Epoch 43: reducing lr to 6.537340375814983e-06
Epoch 46: reducing lr to 1.7227736711882337e-06
Epoch 49: reducing lr to 3.2282297402104478e-09
[I 2024-06-24 16:00:41,805] Trial 419 finished with value: 0.9691566824913025 and parameters: {'hidden_size': 181, 'n_layers': 3, 'rnn_dropout': 0.17366346692709256, 'bidirectional': True, 'fc_dropout': 0.7438020431614157, 'learning_rate_model': 0.001019592943174566}. Best is trial 410 with value: 0.9678924083709717.
Epoch 18: reducing lr to 0.00011257070206053278
Epoch 23: reducing lr to 9.551617141330437e-05
Epoch 27: reducing lr to 7.733349664394345e-05
Epoch 32: reducing lr to 5.2234812577932765e-05
Epoch 38: reducing lr to 2.4379714518851944e-05
Epoch 41: reducing lr to 1.3443244087872879e-05
Epoch 44: reducing lr to 5.464635836528492e-06
Epoch 47: reducing lr to 9.452057668691725e-07
[I 2024-06-24 16:00:46,183] Trial 420 finished with value: 0.967819333076477 and parameters: {'hidden_size': 180, 'n_layers': 3, 'rnn_dropout': 0.1863815618218075, 'bidirectional': True, 'fc_dropout': 0.7308017252336049, 'learning_rate_model': 0.0012103397082745893}. Best is trial 420 with value: 0.967819333076477.
Epoch 18: reducing lr to 0.00011418632244229949
Epoch 27: reducing lr to 7.84433908800481e-05
Epoch 32: reducing lr to 5.298448923708063e-05
Epoch 36: reducing lr to 3.3454355116892484e-05
Epoch 39: reducing lr to 2.0738024968105217e-05
Epoch 42: reducing lr to 1.0575758001525149e-05
Epoch 45: reducing lr to 3.606101623009777e-06
Epoch 48: reducing lr to 2.669761803371102e-07
[I 2024-06-24 16:00:51,006] Trial 421 finished with value: 0.9689753651618958 and parameters: {'hidden_size': 181, 'n_layers': 3, 'rnn_dropout': 0.16548489166609637, 'bidirectional': True, 'fc_dropout': 0.726361606015429, 'learning_rate_model': 0.0012277105646853316}. Best is trial 420 with value: 0.967819333076477.
Epoch 18: reducing lr to 0.00014288214046726512
Epoch 27: reducing lr to 9.815676128912298e-05
Epoch 32: reducing lr to 6.629986036711522e-05
Epoch 36: reducing lr to 4.186166753438478e-05
Epoch 39: reducing lr to 2.5949635062498575e-05
Epoch 42: reducing lr to 1.3233519637041464e-05
Epoch 45: reducing lr to 4.512340073816465e-06
Epoch 48: reducing lr to 3.340691536818591e-07
[I 2024-06-24 16:00:55,811] Trial 422 finished with value: 0.9698948860168457 and parameters: {'hidden_size': 181, 'n_layers': 3, 'rnn_dropout': 0.16999357447773247, 'bidirectional': True, 'fc_dropout': 0.7266917073603967, 'learning_rate_model': 0.0015362427793850434}. Best is trial 420 with value: 0.967819333076477.
Epoch 23: reducing lr to 0.00012723018650108586
Epoch 27: reducing lr to 0.00010301035997575102
Epoch 32: reducing lr to 6.957821746626202e-05
Epoch 37: reducing lr to 3.8053788923818875e-05
Epoch 40: reducing lr to 2.236550993980639e-05
Epoch 43: reducing lr to 1.0337007132686692e-05
Epoch 46: reducing lr to 2.724093087298914e-06
Epoch 49: reducing lr to 5.10455811268077e-09
[I 2024-06-24 16:00:59,710] Trial 423 finished with value: 0.9706845283508301 and parameters: {'hidden_size': 180, 'n_layers': 2, 'rnn_dropout': 0.15685648468848656, 'bidirectional': True, 'fc_dropout': 0.72559106337178, 'learning_rate_model': 0.0016122060226546775}. Best is trial 420 with value: 0.967819333076477.
Epoch 18: reducing lr to 0.00011497564345920105
Epoch 27: reducing lr to 7.898563635861613e-05
Epoch 32: reducing lr to 5.33507482603166e-05
Epoch 36: reducing lr to 3.3685610708944644e-05
Epoch 39: reducing lr to 2.0881378030067843e-05
Epoch 42: reducing lr to 1.0648863675494875e-05
Epoch 45: reducing lr to 3.631029055115877e-06
Epoch 48: reducing lr to 2.688216720356544e-07
[I 2024-06-24 16:01:04,205] Trial 424 finished with value: 0.9692773222923279 and parameters: {'hidden_size': 181, 'n_layers': 3, 'rnn_dropout': 0.16067794078515898, 'bidirectional': True, 'fc_dropout': 0.7258066225456227, 'learning_rate_model': 0.0012361972006558343}. Best is trial 420 with value: 0.967819333076477.
Epoch 23: reducing lr to 6.621002864582033e-05
Epoch 26: reducing lr to 5.693720142075176e-05
Epoch 37: reducing lr to 1.9803024140867277e-05
Epoch 40: reducing lr to 1.1638912859569859e-05
Epoch 43: reducing lr to 5.379332980553282e-06
Epoch 46: reducing lr to 1.4176060438487408e-06
Epoch 49: reducing lr to 2.656389557842767e-09
[I 2024-06-24 16:01:08,356] Trial 425 finished with value: 0.9710742235183716 and parameters: {'hidden_size': 176, 'n_layers': 3, 'rnn_dropout': 0.18588955239929653, 'bidirectional': True, 'fc_dropout': 0.7267232340581936, 'learning_rate_model': 0.0008389849129240981}. Best is trial 420 with value: 0.967819333076477.
Epoch 18: reducing lr to 0.00011295736319266295
Epoch 27: reducing lr to 7.759912399472475e-05
Epoch 32: reducing lr to 5.24142302363314e-05
Epoch 38: reducing lr to 2.4463454673655282e-05
Epoch 41: reducing lr to 1.3489419334925369e-05
Epoch 44: reducing lr to 5.483405927152033e-06
Epoch 47: reducing lr to 9.484523872172997e-07
[I 2024-06-24 16:01:12,526] Trial 426 finished with value: 0.9677461385726929 and parameters: {'hidden_size': 180, 'n_layers': 3, 'rnn_dropout': 0.145120382639702, 'bidirectional': True, 'fc_dropout': 0.7419347179431519, 'learning_rate_model': 0.0012144970184209886}. Best is trial 426 with value: 0.9677461385726929.
Epoch 27: reducing lr to 7.549254013895315e-05
Epoch 37: reducing lr to 2.788823559539942e-05
Epoch 42: reducing lr to 1.0177923550638607e-05
Epoch 45: reducing lr to 3.4704487971012756e-06
Epoch 48: reducing lr to 2.569331818032261e-07
[I 2024-06-24 16:01:15,862] Trial 427 finished with value: 0.9678080081939697 and parameters: {'hidden_size': 174, 'n_layers': 2, 'rnn_dropout': 0.13840244859109985, 'bidirectional': True, 'fc_dropout': 0.7391410528050636, 'learning_rate_model': 0.0011815270610273675}. Best is trial 426 with value: 0.9677461385726929.
Epoch 27: reducing lr to 7.62515482209807e-05
Epoch 37: reducing lr to 2.816862616341363e-05
Epoch 42: reducing lr to 1.0280253214191792e-05
Epoch 45: reducing lr to 3.5053409689690545e-06
Epoch 48: reducing lr to 2.59516408717699e-07
[I 2024-06-24 16:01:19,189] Trial 428 finished with value: 0.9718794822692871 and parameters: {'hidden_size': 167, 'n_layers': 2, 'rnn_dropout': 0.15012976717030255, 'bidirectional': True, 'fc_dropout': 0.7267956654589198, 'learning_rate_model': 0.001193406229310795}. Best is trial 426 with value: 0.9677461385726929.
Epoch 37: reducing lr to 2.0644648606622493e-05
Epoch 42: reducing lr to 7.534347396386171e-06
Epoch 45: reducing lr to 2.5690472844130364e-06
Epoch 48: reducing lr to 1.901983090885832e-07
[I 2024-06-24 16:01:22,524] Trial 429 finished with value: 0.9710325002670288 and parameters: {'hidden_size': 174, 'n_layers': 2, 'rnn_dropout': 0.16550799581065345, 'bidirectional': True, 'fc_dropout': 0.7399799108567623, 'learning_rate_model': 0.0008746415997055497}. Best is trial 426 with value: 0.9677461385726929.
Epoch 23: reducing lr to 5.745410313926845e-05
Epoch 37: reducing lr to 1.7184179114996276e-05
Epoch 40: reducing lr to 1.0099728297050024e-05
Epoch 43: reducing lr to 4.66794469367272e-06
Epoch 46: reducing lr to 1.230135155050674e-06
Epoch 49: reducing lr to 2.3050961124137896e-09
[I 2024-06-24 16:01:25,868] Trial 430 finished with value: 0.9758943319320679 and parameters: {'hidden_size': 180, 'n_layers': 2, 'rnn_dropout': 0.18777573112265458, 'bidirectional': True, 'fc_dropout': 0.743245139108573, 'learning_rate_model': 0.000728033603146224}. Best is trial 426 with value: 0.9677461385726929.
Epoch 37: reducing lr to 2.2876132976919792e-05
Epoch 40: reducing lr to 1.344508376035547e-05
Epoch 43: reducing lr to 6.21411839499367e-06
Epoch 46: reducing lr to 1.637595557995859e-06
Epoch 49: reducing lr to 3.068618223737128e-09
[I 2024-06-24 16:01:29,225] Trial 431 finished with value: 0.9709804058074951 and parameters: {'hidden_size': 181, 'n_layers': 2, 'rnn_dropout': 0.15065510859054793, 'bidirectional': True, 'fc_dropout': 0.7227258559759574, 'learning_rate_model': 0.0009691817924956885}. Best is trial 426 with value: 0.9677461385726929.
Epoch 23: reducing lr to 9.212739021026133e-05
Epoch 27: reducing lr to 7.458981150754274e-05
Epoch 37: reducing lr to 2.7554752198163783e-05
Epoch 42: reducing lr to 1.0056217445895596e-05
Epoch 45: reducing lr to 3.428949683583317e-06
Epoch 48: reducing lr to 2.5386081280959034e-07
[I 2024-06-24 16:01:32,544] Trial 432 finished with value: 0.9707958698272705 and parameters: {'hidden_size': 175, 'n_layers': 2, 'rnn_dropout': 0.17650318968154258, 'bidirectional': True, 'fc_dropout': 0.7405021246774206, 'learning_rate_model': 0.001167398535151666}. Best is trial 426 with value: 0.9677461385726929.
Epoch 23: reducing lr to 0.00012602764318065808
Epoch 31: reducing lr to 7.557750286293358e-05
Epoch 37: reducing lr to 3.769411539864555e-05
Epoch 40: reducing lr to 2.2154117538948377e-05
Epoch 43: reducing lr to 1.0239304698834495e-05
Epoch 46: reducing lr to 2.698345738839838e-06
Epoch 49: reducing lr to 5.056311289867825e-09
[I 2024-06-24 16:01:35,871] Trial 433 finished with value: 0.9664180278778076 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.14211961515776492, 'bidirectional': True, 'fc_dropout': 0.7413435291577516, 'learning_rate_model': 0.0015969679126038034}. Best is trial 433 with value: 0.9664180278778076.
Epoch 23: reducing lr to 0.0001266670325594997
Epoch 31: reducing lr to 7.596093820608812e-05
Epoch 37: reducing lr to 3.7885352943222785e-05
Epoch 40: reducing lr to 2.226651436788619e-05
Epoch 43: reducing lr to 1.0291252846922707e-05
Epoch 46: reducing lr to 2.712035541825239e-06
Epoch 49: reducing lr to 5.081964009009433e-09
[I 2024-06-24 16:01:39,200] Trial 434 finished with value: 0.969365119934082 and parameters: {'hidden_size': 177, 'n_layers': 2, 'rnn_dropout': 0.13337751984277593, 'bidirectional': True, 'fc_dropout': 0.7189599512727859, 'learning_rate_model': 0.0016050699789116375}. Best is trial 433 with value: 0.9664180278778076.
Epoch 23: reducing lr to 0.00012641195918941623
Epoch 31: reducing lr to 7.580797328608159e-05
Epoch 36: reducing lr to 4.364909133847685e-05
Epoch 39: reducing lr to 2.70576414595231e-05
Epoch 42: reducing lr to 1.3798568986586341e-05
Epoch 45: reducing lr to 4.7050095142651224e-06
Epoch 48: reducing lr to 3.4833335271343847e-07
[I 2024-06-24 16:01:42,522] Trial 435 finished with value: 0.9674537181854248 and parameters: {'hidden_size': 170, 'n_layers': 2, 'rnn_dropout': 0.1295486701799912, 'bidirectional': True, 'fc_dropout': 0.7217965395649172, 'learning_rate_model': 0.0016018377992318262}. Best is trial 433 with value: 0.9664180278778076.
Epoch 31: reducing lr to 7.693976671151355e-05
Epoch 37: reducing lr to 3.837354153428942e-05
Epoch 40: reducing lr to 2.2553439457208043e-05
Epoch 43: reducing lr to 1.0423865369635095e-05
Epoch 46: reducing lr to 2.746982683853212e-06
Epoch 49: reducing lr to 5.1474499199958415e-09
[I 2024-06-24 16:01:45,832] Trial 436 finished with value: 0.9682350158691406 and parameters: {'hidden_size': 165, 'n_layers': 2, 'rnn_dropout': 0.12919679270984702, 'bidirectional': True, 'fc_dropout': 0.7208955139355299, 'learning_rate_model': 0.0016257528230900337}. Best is trial 433 with value: 0.9664180278778076.
Epoch 27: reducing lr to 9.491434070063792e-05
Epoch 36: reducing lr to 4.047884753401823e-05
Epoch 42: reducing lr to 1.2796375664830763e-05
Epoch 45: reducing lr to 4.3632835629308414e-06
Epoch 48: reducing lr to 3.230338191042995e-07
[I 2024-06-24 16:01:49,142] Trial 437 finished with value: 0.9683085083961487 and parameters: {'hidden_size': 167, 'n_layers': 2, 'rnn_dropout': 0.1329967829115007, 'bidirectional': True, 'fc_dropout': 0.6909952577393988, 'learning_rate_model': 0.001485495941863403}. Best is trial 433 with value: 0.9664180278778076.
Epoch 36: reducing lr to 4.328599251110126e-05
Epoch 39: reducing lr to 2.6832560075598797e-05
Epoch 42: reducing lr to 1.3683784369888626e-05
Epoch 45: reducing lr to 4.665870476428728e-06
Epoch 48: reducing lr to 3.454357109062981e-07
[I 2024-06-24 16:01:52,452] Trial 438 finished with value: 0.9696429967880249 and parameters: {'hidden_size': 166, 'n_layers': 2, 'rnn_dropout': 0.13948286091530634, 'bidirectional': True, 'fc_dropout': 0.7198241059699264, 'learning_rate_model': 0.0015885127698048267}. Best is trial 433 with value: 0.9664180278778076.
Epoch 31: reducing lr to 7.613402782212576e-05
Epoch 37: reducing lr to 3.797168101327111e-05
Epoch 40: reducing lr to 2.2317252319700025e-05
Epoch 43: reducing lr to 1.0314703176077245e-05
Epoch 46: reducing lr to 2.7182153653200567e-06
Epoch 49: reducing lr to 5.093544108193682e-09
[I 2024-06-24 16:01:55,777] Trial 439 finished with value: 0.9660595059394836 and parameters: {'hidden_size': 171, 'n_layers': 2, 'rnn_dropout': 0.13456257589723578, 'bidirectional': True, 'fc_dropout': 0.6953128579723807, 'learning_rate_model': 0.0016087273974865585}. Best is trial 439 with value: 0.9660595059394836.
Epoch 23: reducing lr to 0.0001246083573395167
Epoch 31: reducing lr to 7.472637149988477e-05
Epoch 37: reducing lr to 3.7269615479981215e-05
Epoch 40: reducing lr to 2.1904624455110124e-05
Epoch 43: reducing lr to 1.0123992694139185e-05
Epoch 46: reducing lr to 2.667957771525817e-06
Epoch 49: reducing lr to 4.999368615702521e-09
[I 2024-06-24 16:01:59,098] Trial 440 finished with value: 0.9687986373901367 and parameters: {'hidden_size': 168, 'n_layers': 2, 'rnn_dropout': 0.1334762460403613, 'bidirectional': True, 'fc_dropout': 0.6888489550028553, 'learning_rate_model': 0.0015789833348563117}. Best is trial 439 with value: 0.9660595059394836.
Epoch 18: reducing lr to 0.00022212433820449368
Epoch 27: reducing lr to 0.0001525943380351169
Epoch 30: reducing lr to 0.0001230251305601293
Epoch 33: reducing lr to 9.322897324596923e-05
Epoch 36: reducing lr to 6.507807880539193e-05
Epoch 39: reducing lr to 4.034125955879129e-05
Epoch 42: reducing lr to 2.0572807643285924e-05
Epoch 45: reducing lr to 7.014876382536615e-06
Epoch 48: reducing lr to 5.193433513345138e-07
[I 2024-06-24 16:02:02,406] Trial 441 finished with value: 0.9783596992492676 and parameters: {'hidden_size': 165, 'n_layers': 2, 'rnn_dropout': 0.14119171224401897, 'bidirectional': True, 'fc_dropout': 0.6945950026719455, 'learning_rate_model': 0.0023882404727168373}. Best is trial 439 with value: 0.9660595059394836.
Epoch 22: reducing lr to 0.00013268600097774753
Epoch 36: reducing lr to 4.3970241127128864e-05
Epoch 39: reducing lr to 2.725671904784594e-05
Epoch 42: reducing lr to 1.3900092463429861e-05
Epoch 45: reducing lr to 4.739626794139184e-06
Epoch 48: reducing lr to 3.5089622811759065e-07
[I 2024-06-24 16:02:05,724] Trial 442 finished with value: 0.9710921049118042 and parameters: {'hidden_size': 173, 'n_layers': 2, 'rnn_dropout': 0.1279896673135319, 'bidirectional': True, 'fc_dropout': 0.7001476317422631, 'learning_rate_model': 0.0016136233795246426}. Best is trial 439 with value: 0.9660595059394836.
Epoch 31: reducing lr to 7.540899284890691e-05
Epoch 37: reducing lr to 3.761007139515327e-05
Epoch 40: reducing lr to 2.210472201096955e-05
Epoch 43: reducing lr to 1.0216474818075484e-05
Epoch 46: reducing lr to 2.6923294210061574e-06
Epoch 49: reducing lr to 5.045037576763278e-09
[I 2024-06-24 16:02:09,050] Trial 443 finished with value: 0.9660463333129883 and parameters: {'hidden_size': 171, 'n_layers': 2, 'rnn_dropout': 0.1258914379041145, 'bidirectional': True, 'fc_dropout': 0.6858054121901836, 'learning_rate_model': 0.001593407261945087}. Best is trial 443 with value: 0.9660463333129883.
Epoch 23: reducing lr to 0.00012226235332146948
Epoch 37: reducing lr to 3.6567939689256506e-05
Epoch 40: reducing lr to 2.1492225655521673e-05
Epoch 43: reducing lr to 9.933388082648971e-06
Epoch 46: reducing lr to 2.617728077582192e-06
Epoch 49: reducing lr to 4.905245403505169e-09
[I 2024-06-24 16:02:12,356] Trial 444 finished with value: 0.9673525094985962 and parameters: {'hidden_size': 170, 'n_layers': 2, 'rnn_dropout': 0.12274649022111578, 'bidirectional': True, 'fc_dropout': 0.6843243945509223, 'learning_rate_model': 0.0015492557842562383}. Best is trial 443 with value: 0.9660463333129883.
Epoch 23: reducing lr to 0.00012371344880009687
Epoch 31: reducing lr to 7.418970390067312e-05
Epoch 37: reducing lr to 3.700195368051577e-05
Epoch 40: reducing lr to 2.174731048439262e-05
Epoch 43: reducing lr to 1.005128450900256e-05
Epoch 46: reducing lr to 2.6487971129349415e-06
Epoch 49: reducing lr to 4.963464301089952e-09
[I 2024-06-24 16:02:15,636] Trial 445 finished with value: 0.9664593935012817 and parameters: {'hidden_size': 163, 'n_layers': 2, 'rnn_dropout': 0.11809820559061776, 'bidirectional': True, 'fc_dropout': 0.6795649807409236, 'learning_rate_model': 0.0015676434400038787}. Best is trial 443 with value: 0.9660463333129883.
Epoch 18: reducing lr to 0.00021574962972514236
Epoch 23: reducing lr to 0.00018306342803212067
Epoch 31: reducing lr to 0.0001097812861291325
Epoch 34: reducing lr to 8.117353968505243e-05
Epoch 37: reducing lr to 5.475317801208739e-05
Epoch 40: reducing lr to 3.218031059973713e-05
Epoch 43: reducing lr to 1.4873262496443518e-05
Epoch 46: reducing lr to 3.919524387675788e-06
Epoch 49: reducing lr to 7.344624199621952e-09
[I 2024-06-24 16:02:18,921] Trial 446 finished with value: 0.9725843667984009 and parameters: {'hidden_size': 171, 'n_layers': 2, 'rnn_dropout': 0.12188596912167945, 'bidirectional': True, 'fc_dropout': 0.6891643671804742, 'learning_rate_model': 0.0023197007669141256}. Best is trial 443 with value: 0.9660463333129883.
Epoch 36: reducing lr to 4.510380555938409e-05
Epoch 39: reducing lr to 2.7959404465541976e-05
Epoch 42: reducing lr to 1.425844051924439e-05
Epoch 45: reducing lr to 4.861815625000171e-06
Epoch 48: reducing lr to 3.5994242557771743e-07
[I 2024-06-24 16:02:22,180] Trial 447 finished with value: 0.9677854180335999 and parameters: {'hidden_size': 162, 'n_layers': 2, 'rnn_dropout': 0.10657400693755013, 'bidirectional': True, 'fc_dropout': 0.6771040009182097, 'learning_rate_model': 0.0016552230165335937}. Best is trial 443 with value: 0.9660463333129883.
Epoch 22: reducing lr to 0.00013332312873878637
Epoch 27: reducing lr to 0.00010359598762273658
Epoch 38: reducing lr to 3.2659076766815744e-05
Epoch 41: reducing lr to 1.8008575954463505e-05
Epoch 44: reducing lr to 7.320428676466724e-06
Epoch 47: reducing lr to 1.2661980794215992e-06
[I 2024-06-24 16:02:25,463] Trial 448 finished with value: 0.9710574150085449 and parameters: {'hidden_size': 169, 'n_layers': 2, 'rnn_dropout': 0.11071586528581685, 'bidirectional': True, 'fc_dropout': 0.6775607082663695, 'learning_rate_model': 0.0016213716291016938}. Best is trial 443 with value: 0.9660463333129883.
Epoch 18: reducing lr to 0.00020300448225659553
Epoch 27: reducing lr to 0.0001394594344703832
Epoch 31: reducing lr to 0.00010329608991913187
Epoch 34: reducing lr to 7.637831136810031e-05
Epoch 37: reducing lr to 5.151870048818769e-05
Epoch 40: reducing lr to 3.0279297816077662e-05
Epoch 43: reducing lr to 1.39946425697392e-05
Epoch 46: reducing lr to 3.6879832425478017e-06
Epoch 49: reducing lr to 6.9107494409317846e-09
[I 2024-06-24 16:02:28,728] Trial 449 finished with value: 0.968392550945282 and parameters: {'hidden_size': 163, 'n_layers': 2, 'rnn_dropout': 0.13121017278706557, 'bidirectional': True, 'fc_dropout': 0.6820592139045358, 'learning_rate_model': 0.002182667260090098}. Best is trial 443 with value: 0.9660463333129883.
Epoch 18: reducing lr to 0.00024151739130593474
Epoch 27: reducing lr to 0.00016591692179345266
Epoch 30: reducing lr to 0.00013376610972994836
Epoch 33: reducing lr to 0.00010136853347321197
Epoch 36: reducing lr to 7.075986337800749e-05
Epoch 39: reducing lr to 4.386334180842941e-05
Epoch 42: reducing lr to 2.2368961789639203e-05
Epoch 45: reducing lr to 7.627325568817719e-06
Epoch 48: reducing lr to 5.646857630293381e-07
[I 2024-06-24 16:02:31,998] Trial 450 finished with value: 0.980861246585846 and parameters: {'hidden_size': 165, 'n_layers': 2, 'rnn_dropout': 0.132471019402546, 'bidirectional': True, 'fc_dropout': 0.6805895815208817, 'learning_rate_model': 0.002596751051435002}. Best is trial 443 with value: 0.9660463333129883.
Epoch 23: reducing lr to 0.00013110214670617614
Epoch 31: reducing lr to 7.862063129927229e-05
Epoch 37: reducing lr to 3.921186909659829e-05
Epoch 40: reducing lr to 2.304615316477518e-05
Epoch 43: reducing lr to 1.0651590340950384e-05
Epoch 46: reducing lr to 2.806994640138275e-06
Epoch 49: reducing lr to 5.259903682942359e-09
[I 2024-06-24 16:02:35,263] Trial 451 finished with value: 0.9664382934570312 and parameters: {'hidden_size': 163, 'n_layers': 2, 'rnn_dropout': 0.12061965069721953, 'bidirectional': True, 'fc_dropout': 0.6757769249625578, 'learning_rate_model': 0.0016612698315965322}. Best is trial 443 with value: 0.9660463333129883.
Epoch 18: reducing lr to 0.0002121446511471145
Epoch 23: reducing lr to 0.00018000460592745756
Epoch 27: reducing lr to 0.00014573852136672268
Epoch 31: reducing lr to 0.0001079469414525365
Epoch 34: reducing lr to 7.981720423251833e-05
Epoch 37: reducing lr to 5.383830258882947e-05
Epoch 40: reducing lr to 3.164260710289161e-05
Epoch 43: reducing lr to 1.4624743911482866e-05
Epoch 46: reducing lr to 3.85403272740439e-06
Epoch 49: reducing lr to 7.2219022605440195e-09
[I 2024-06-24 16:02:38,525] Trial 452 finished with value: 0.9733850359916687 and parameters: {'hidden_size': 161, 'n_layers': 2, 'rnn_dropout': 0.10710542734113448, 'bidirectional': True, 'fc_dropout': 0.6753098084933673, 'learning_rate_model': 0.0022809406931062876}. Best is trial 443 with value: 0.9660463333129883.
Epoch 18: reducing lr to 0.00020758016761273217
Epoch 22: reducing lr to 0.0001835230814862871
Epoch 27: reducing lr to 0.00014260282561617403
Epoch 30: reducing lr to 0.000114969739150058
Epoch 33: reducing lr to 8.712456297762742e-05
Epoch 36: reducing lr to 6.081692179945261e-05
Epoch 39: reducing lr to 3.769981033421587e-05
Epoch 42: reducing lr to 1.9225749386031934e-05
Epoch 45: reducing lr to 6.55555904877465e-06
Epoch 48: reducing lr to 4.853379903796765e-07
[I 2024-06-24 16:02:41,786] Trial 453 finished with value: 0.9764788746833801 and parameters: {'hidden_size': 164, 'n_layers': 2, 'rnn_dropout': 0.13190166759991678, 'bidirectional': True, 'fc_dropout': 0.6900624925136467, 'learning_rate_model': 0.002231864196572956}. Best is trial 443 with value: 0.9660463333129883.
Epoch 37: reducing lr to 3.94111802285771e-05
Epoch 40: reducing lr to 2.3163295116456014e-05
Epoch 43: reducing lr to 1.070573161442549e-05
Epoch 46: reducing lr to 2.8212623935525758e-06
Epoch 49: reducing lr to 5.286639397929748e-09
[I 2024-06-24 16:02:45,048] Trial 454 finished with value: 0.9682184457778931 and parameters: {'hidden_size': 162, 'n_layers': 2, 'rnn_dropout': 0.12216611812467743, 'bidirectional': True, 'fc_dropout': 0.6961012838684333, 'learning_rate_model': 0.0016697139475819777}. Best is trial 443 with value: 0.9660463333129883.
Epoch 22: reducing lr to 0.0001404608436086347
Epoch 38: reducing lr to 3.440754441889689e-05
Epoch 41: reducing lr to 1.897270034601458e-05
Epoch 44: reducing lr to 7.712342165986274e-06
Epoch 47: reducing lr to 1.333986474017163e-06
[I 2024-06-24 16:02:48,332] Trial 455 finished with value: 0.971599817276001 and parameters: {'hidden_size': 169, 'n_layers': 2, 'rnn_dropout': 0.122480172513626, 'bidirectional': True, 'fc_dropout': 0.6918788148685037, 'learning_rate_model': 0.0017081749354452135}. Best is trial 443 with value: 0.9660463333129883.
Epoch 17: reducing lr to 0.00026563353583257567
Epoch 23: reducing lr to 0.00022068444562911693
Epoch 27: reducing lr to 0.0001786744546280326
Epoch 30: reducing lr to 0.0001440515316060742
Epoch 33: reducing lr to 0.00010916287042328881
Epoch 36: reducing lr to 7.620066634528473e-05
Epoch 39: reducing lr to 4.723604193634079e-05
Epoch 42: reducing lr to 2.4088935626075548e-05
Epoch 45: reducing lr to 8.21379893954097e-06
Epoch 48: reducing lr to 6.081050664084268e-07
[I 2024-06-24 16:02:51,598] Trial 456 finished with value: 0.9767888784408569 and parameters: {'hidden_size': 163, 'n_layers': 2, 'rnn_dropout': 0.14261762192324812, 'bidirectional': True, 'fc_dropout': 0.6816707401670492, 'learning_rate_model': 0.002796418068179398}. Best is trial 443 with value: 0.9660463333129883.
Epoch 18: reducing lr to 0.00020904342286471697
Epoch 22: reducing lr to 0.00018481675571313446
Epoch 27: reducing lr to 0.00014360804849430578
Epoch 30: reducing lr to 0.00011578017338645627
Epoch 36: reducing lr to 6.124562691736454e-05
Epoch 39: reducing lr to 3.796556041094424e-05
Epoch 42: reducing lr to 1.9361273791306236e-05
Epoch 45: reducing lr to 6.601769899831044e-06
Epoch 48: reducing lr to 4.88759190832343e-07
[I 2024-06-24 16:02:54,854] Trial 457 finished with value: 0.9716323614120483 and parameters: {'hidden_size': 162, 'n_layers': 2, 'rnn_dropout': 0.09652531258087615, 'bidirectional': True, 'fc_dropout': 0.6692420000667201, 'learning_rate_model': 0.0022475968508284673}. Best is trial 443 with value: 0.9660463333129883.
Epoch 31: reducing lr to 7.838193867280084e-05
Epoch 37: reducing lr to 3.909282166758574e-05
Epoch 40: reducing lr to 2.2976184929490695e-05
Epoch 43: reducing lr to 1.0619252059858482e-05
Epoch 46: reducing lr to 2.798472590492102e-06
Epoch 49: reducing lr to 5.243934589273421e-09
[I 2024-06-24 16:02:58,124] Trial 458 finished with value: 0.9695426225662231 and parameters: {'hidden_size': 168, 'n_layers': 2, 'rnn_dropout': 0.11821514248153836, 'bidirectional': True, 'fc_dropout': 0.7023240167595468, 'learning_rate_model': 0.0016562262081502624}. Best is trial 443 with value: 0.9660463333129883.
Epoch 23: reducing lr to 0.00013222551030631161
Epoch 31: reducing lr to 7.929430108760319e-05
Epoch 36: reducing lr to 4.565646647394202e-05
Epoch 39: reducing lr to 2.830199351874453e-05
Epoch 42: reducing lr to 1.4433150450697086e-05
Epoch 45: reducing lr to 4.92138788140738e-06
Epoch 48: reducing lr to 3.6435283192017604e-07
[I 2024-06-24 16:03:01,406] Trial 459 finished with value: 0.9674321413040161 and parameters: {'hidden_size': 170, 'n_layers': 2, 'rnn_dropout': 0.14766980065344398, 'bidirectional': True, 'fc_dropout': 0.6950465336661982, 'learning_rate_model': 0.0016755046103984483}. Best is trial 443 with value: 0.9660463333129883.
Epoch 18: reducing lr to 0.00020062387346264418
Epoch 23: reducing lr to 0.00017022923315299708
Epoch 27: reducing lr to 0.0001378240106984094
Epoch 31: reducing lr to 0.00010208474927626149
Epoch 34: reducing lr to 7.548263222994105e-05
Epoch 37: reducing lr to 5.091454697358647e-05
Epoch 40: reducing lr to 2.992421618509913e-05
Epoch 43: reducing lr to 1.383052910387192e-05
Epoch 46: reducing lr to 3.6447347130495138e-06
Epoch 49: reducing lr to 6.829707925462905e-09
[I 2024-06-24 16:03:04,675] Trial 460 finished with value: 0.9730731844902039 and parameters: {'hidden_size': 168, 'n_layers': 2, 'rnn_dropout': 0.14555461327488142, 'bidirectional': True, 'fc_dropout': 0.6932442365572205, 'learning_rate_model': 0.0021570713874478754}. Best is trial 443 with value: 0.9660463333129883.
Epoch 23: reducing lr to 0.00013562607008822515
Epoch 31: reducing lr to 8.133358239261604e-05
Epoch 34: reducing lr to 6.013898188720983e-05
Epoch 37: reducing lr to 4.056494756187706e-05
Epoch 40: reducing lr to 2.3841403538532028e-05
Epoch 43: reducing lr to 1.1019143274369759e-05
Epoch 46: reducing lr to 2.903855210348999e-06
Epoch 49: reducing lr to 5.441406441338177e-09
[I 2024-06-24 16:03:07,960] Trial 461 finished with value: 0.967866063117981 and parameters: {'hidden_size': 171, 'n_layers': 2, 'rnn_dropout': 0.07355116723652053, 'bidirectional': True, 'fc_dropout': 0.6707511312225689, 'learning_rate_model': 0.0017185950365902819}. Best is trial 443 with value: 0.9660463333129883.
Epoch 31: reducing lr to 7.869569988082493e-05
Epoch 34: reducing lr to 5.818850136083387e-05
Epoch 37: reducing lr to 3.92493093885988e-05
Epoch 40: reducing lr to 2.3068158101643976e-05
Epoch 43: reducing lr to 1.0661760696555012e-05
Epoch 46: reducing lr to 2.809674815845058e-06
Epoch 49: reducing lr to 5.264925946189998e-09
[I 2024-06-24 16:03:11,247] Trial 462 finished with value: 0.9670104384422302 and parameters: {'hidden_size': 171, 'n_layers': 2, 'rnn_dropout': 0.09239134394959772, 'bidirectional': True, 'fc_dropout': 0.6750155266181022, 'learning_rate_model': 0.0016628560458989772}. Best is trial 443 with value: 0.9660463333129883.
Epoch 18: reducing lr to 0.00020777308422557637
Epoch 23: reducing lr to 0.00017629533408514614
Epoch 27: reducing lr to 0.0001427353549151734
Epoch 30: reducing lr to 0.0001150765873760301
Epoch 33: reducing lr to 8.720553302297651e-05
Epoch 36: reducing lr to 6.087344258702151e-05
Epoch 39: reducing lr to 3.773484701328877e-05
Epoch 42: reducing lr to 1.924361701998562e-05
Epoch 45: reducing lr to 6.5616515202353546e-06
Epoch 48: reducing lr to 4.857890438799123e-07
[I 2024-06-24 16:03:14,523] Trial 463 finished with value: 0.976253867149353 and parameters: {'hidden_size': 172, 'n_layers': 2, 'rnn_dropout': 0.07044037145707994, 'bidirectional': True, 'fc_dropout': 0.6641506247457145, 'learning_rate_model': 0.0022339384008964363}. Best is trial 443 with value: 0.9660463333129883.
Epoch 18: reducing lr to 0.0002480394079550359
Epoch 23: reducing lr to 0.00021046128498646054
Epoch 27: reducing lr to 0.0001703973980045168
Epoch 30: reducing lr to 0.00013737837462742358
Epoch 33: reducing lr to 0.00010410592335404997
Epoch 36: reducing lr to 7.267068646426258e-05
Epoch 39: reducing lr to 4.504784220408583e-05
Epoch 42: reducing lr to 2.297302073722171e-05
Epoch 45: reducing lr to 7.833296426978118e-06
Epoch 48: reducing lr to 5.799347267916588e-07
[I 2024-06-24 16:03:17,785] Trial 464 finished with value: 0.9785997271537781 and parameters: {'hidden_size': 161, 'n_layers': 2, 'rnn_dropout': 0.09388219141897386, 'bidirectional': True, 'fc_dropout': 0.6760890634714223, 'learning_rate_model': 0.002666874587878706}. Best is trial 443 with value: 0.9660463333129883.
Epoch 31: reducing lr to 8.663923587874493e-05
Epoch 34: reducing lr to 6.406204281131692e-05
Epoch 37: reducing lr to 4.321113071421106e-05
Epoch 40: reducing lr to 2.539665565060305e-05
Epoch 43: reducing lr to 1.1737957744456626e-05
Epoch 46: reducing lr to 3.093283107987008e-06
Epoch 49: reducing lr to 5.796367039470618e-09
[I 2024-06-24 16:03:21,073] Trial 465 finished with value: 0.9683007001876831 and parameters: {'hidden_size': 171, 'n_layers': 2, 'rnn_dropout': 0.10756964276542261, 'bidirectional': True, 'fc_dropout': 0.6740253704699162, 'learning_rate_model': 0.0018307045672281066}. Best is trial 443 with value: 0.9660463333129883.
Epoch 23: reducing lr to 0.00014108175232580038
Epoch 31: reducing lr to 8.460529984700448e-05
Epoch 34: reducing lr to 6.25581272259672e-05
Epoch 37: reducing lr to 4.219670953608706e-05
Epoch 40: reducing lr to 2.480044571766041e-05
Epoch 43: reducing lr to 1.1462398352071052e-05
Epoch 46: reducing lr to 3.0206654318741403e-06
Epoch 49: reducing lr to 5.660291973064863e-09
[I 2024-06-24 16:03:24,360] Trial 466 finished with value: 0.9684581756591797 and parameters: {'hidden_size': 171, 'n_layers': 2, 'rnn_dropout': 0.07823492147748759, 'bidirectional': True, 'fc_dropout': 0.6760934558657902, 'learning_rate_model': 0.0017877270877410037}. Best is trial 443 with value: 0.9660463333129883.
Epoch 13: reducing lr to 0.00028309050533682897
Epoch 22: reducing lr to 0.00023357103081411086
Epoch 27: reducing lr to 0.0001814915524871646
Epoch 30: reducing lr to 0.0001463227419038024
Epoch 33: reducing lr to 0.00011088400335863961
Epoch 36: reducing lr to 7.740209569607032e-05
Epoch 39: reducing lr to 4.7980796148070455e-05
Epoch 42: reducing lr to 2.4468737479240582e-05
Epoch 45: reducing lr to 8.343303044960416e-06
Epoch 48: reducing lr to 6.176928470694523e-07
[I 2024-06-24 16:03:27,640] Trial 467 finished with value: 0.9804478883743286 and parameters: {'hidden_size': 173, 'n_layers': 2, 'rnn_dropout': 0.06919699338738097, 'bidirectional': True, 'fc_dropout': 0.6696699726346548, 'learning_rate_model': 0.0028405082173252644}. Best is trial 443 with value: 0.9660463333129883.
Epoch 18: reducing lr to 0.00019275168154829644
Epoch 23: reducing lr to 0.00016354968315885262
Epoch 31: reducing lr to 9.807909070751475e-05
Epoch 34: reducing lr to 7.252080242943695e-05
Epoch 37: reducing lr to 4.891673346270947e-05
Epoch 40: reducing lr to 2.8750033030174666e-05
Epoch 43: reducing lr to 1.3287839056553446e-05
Epoch 46: reducing lr to 3.501720570999508e-06
Epoch 49: reducing lr to 6.561720020635542e-09
[I 2024-06-24 16:03:30,927] Trial 468 finished with value: 0.9710691571235657 and parameters: {'hidden_size': 171, 'n_layers': 2, 'rnn_dropout': 0.08508433155088252, 'bidirectional': True, 'fc_dropout': 0.6470570535981697, 'learning_rate_model': 0.0020724310121931337}. Best is trial 443 with value: 0.9660463333129883.
Epoch 27: reducing lr to 0.00011240807281425427
Epoch 31: reducing lr to 8.325943985900366e-05
Epoch 36: reducing lr to 4.7939533767529365e-05
Epoch 39: reducing lr to 2.9717244429211812e-05
Epoch 42: reducing lr to 1.515488509821342e-05
Epoch 45: reducing lr to 5.1674835734056e-06
Epoch 48: reducing lr to 3.825724204719466e-07
[I 2024-06-24 16:03:34,196] Trial 469 finished with value: 0.9700348377227783 and parameters: {'hidden_size': 165, 'n_layers': 2, 'rnn_dropout': 0.10451084219263926, 'bidirectional': True, 'fc_dropout': 0.6785617659521699, 'learning_rate_model': 0.0017592887941446592}. Best is trial 443 with value: 0.9660463333129883.
Epoch 23: reducing lr to 0.00013540194707181014
Epoch 31: reducing lr to 8.119917808664585e-05
Epoch 36: reducing lr to 4.6753266012507755e-05
Epoch 39: reducing lr to 2.898188874124418e-05
Epoch 42: reducing lr to 1.4779876204504814e-05
Epoch 45: reducing lr to 5.039613762083256e-06
Epoch 48: reducing lr to 3.7310563407041107e-07
[I 2024-06-24 16:03:37,474] Trial 470 finished with value: 0.9684152603149414 and parameters: {'hidden_size': 170, 'n_layers': 2, 'rnn_dropout': 0.11888028286780722, 'bidirectional': True, 'fc_dropout': 0.7009760529228551, 'learning_rate_model': 0.0017157550464368702}. Best is trial 443 with value: 0.9660463333129883.
Epoch 18: reducing lr to 0.0001979727864638339
Epoch 23: reducing lr to 0.00016797978746619854
Epoch 27: reducing lr to 0.0001360027745883685
Epoch 31: reducing lr to 0.00010073577945072624
Epoch 34: reducing lr to 7.448518849860945e-05
Epoch 37: reducing lr to 5.024175120306154e-05
Epoch 40: reducing lr to 2.952879115861204e-05
Epoch 43: reducing lr to 1.3647769518678388e-05
Epoch 46: reducing lr to 3.5965724049198416e-06
Epoch 49: reducing lr to 6.739458696496115e-09
[I 2024-06-24 16:03:40,736] Trial 471 finished with value: 0.9721102118492126 and parameters: {'hidden_size': 161, 'n_layers': 2, 'rnn_dropout': 0.10109497721991562, 'bidirectional': True, 'fc_dropout': 0.703845356419794, 'learning_rate_model': 0.0021285673823558116}. Best is trial 443 with value: 0.9660463333129883.
Epoch 27: reducing lr to 0.00010651844227324045
Epoch 31: reducing lr to 7.889705442222469e-05
Epoch 36: reducing lr to 4.542773781613216e-05
Epoch 39: reducing lr to 2.816020687841001e-05
Epoch 42: reducing lr to 1.4360843603813956e-05
Epoch 45: reducing lr to 4.896732831824891e-06
Epoch 48: reducing lr to 3.625275059444561e-07
[I 2024-06-24 16:03:44,020] Trial 472 finished with value: 0.9674423336982727 and parameters: {'hidden_size': 175, 'n_layers': 2, 'rnn_dropout': 0.11944287582392699, 'bidirectional': True, 'fc_dropout': 0.7000344744436372, 'learning_rate_model': 0.0016671107080602242}. Best is trial 443 with value: 0.9660463333129883.
Epoch 22: reducing lr to 0.00013988596643353557
Epoch 31: reducing lr to 8.05095813885682e-05
Epoch 36: reducing lr to 4.6356206600992914e-05
Epoch 39: reducing lr to 2.873575552597087e-05
Epoch 42: reducing lr to 1.465435579815601e-05
Epoch 45: reducing lr to 4.996814055339775e-06
Epoch 48: reducing lr to 3.6993697621755425e-07
[I 2024-06-24 16:03:47,306] Trial 473 finished with value: 0.9720979928970337 and parameters: {'hidden_size': 176, 'n_layers': 2, 'rnn_dropout': 0.12246495514857612, 'bidirectional': True, 'fc_dropout': 0.6897952285290284, 'learning_rate_model': 0.0017011837288125665}. Best is trial 443 with value: 0.9660463333129883.
Epoch 12: reducing lr to 0.00032746198075453046
Epoch 23: reducing lr to 0.0002584911957562954
Epoch 27: reducing lr to 0.00020928422615486042
Epoch 30: reducing lr to 0.00016872984658809596
Epoch 33: reducing lr to 0.00012786420369348606
Epoch 36: reducing lr to 8.925505059890958e-05
Epoch 39: reducing lr to 5.532832605447684e-05
Epoch 42: reducing lr to 2.8215752844427607e-05
Epoch 45: reducing lr to 9.62095313754908e-06
Epoch 48: reducing lr to 7.122831213285732e-07
[I 2024-06-24 16:03:50,570] Trial 474 finished with value: 0.9997608661651611 and parameters: {'hidden_size': 164, 'n_layers': 2, 'rnn_dropout': 0.14107589832692527, 'bidirectional': True, 'fc_dropout': 0.6592399857029639, 'learning_rate_model': 0.0032754888919222956}. Best is trial 443 with value: 0.9660463333129883.
Epoch 18: reducing lr to 0.000235482284299353
Epoch 23: reducing lr to 0.00019980657329327677
Epoch 27: reducing lr to 0.00016177094136607325
Epoch 30: reducing lr to 0.0001304235231704083
Epoch 33: reducing lr to 9.883550699713449e-05
Epoch 36: reducing lr to 6.899169527653757e-05
Epoch 39: reducing lr to 4.276727183165157e-05
Epoch 42: reducing lr to 2.180999964908016e-05
Epoch 45: reducing lr to 7.436731733181544e-06
Epoch 48: reducing lr to 5.505752305060094e-07
[I 2024-06-24 16:03:53,850] Trial 475 finished with value: 0.9760175943374634 and parameters: {'hidden_size': 174, 'n_layers': 2, 'rnn_dropout': 0.1124697326272627, 'bidirectional': True, 'fc_dropout': 0.6941003903275811, 'learning_rate_model': 0.0025318626788829305}. Best is trial 443 with value: 0.9660463333129883.
Epoch 22: reducing lr to 0.00013638441731563807
Epoch 38: reducing lr to 3.3408975599693136e-05
Epoch 41: reducing lr to 1.8422078460564903e-05
Epoch 44: reducing lr to 7.488516126085711e-06
Epoch 47: reducing lr to 1.2952717874363572e-06
[I 2024-06-24 16:03:57,136] Trial 476 finished with value: 0.9712173938751221 and parameters: {'hidden_size': 169, 'n_layers': 2, 'rnn_dropout': 0.1532688990246861, 'bidirectional': True, 'fc_dropout': 0.6714106832041604, 'learning_rate_model': 0.0016586006267553957}. Best is trial 443 with value: 0.9660463333129883.
Epoch 18: reducing lr to 0.00020294926599983458
Epoch 23: reducing lr to 0.00017220232728956968
Epoch 27: reducing lr to 0.00013942150216536287
Epoch 30: reducing lr to 0.00011240488164662973
Epoch 33: reducing lr to 8.518090292639933e-05
Epoch 36: reducing lr to 5.946015836443127e-05
Epoch 39: reducing lr to 3.6858766054839604e-05
Epoch 42: reducing lr to 1.8796842545533346e-05
Epoch 45: reducing lr to 6.40931121921773e-06
Epoch 48: reducing lr to 4.7451059531438376e-07
[I 2024-06-24 16:04:00,360] Trial 477 finished with value: 0.9760143756866455 and parameters: {'hidden_size': 158, 'n_layers': 2, 'rnn_dropout': 0.09209942832865607, 'bidirectional': True, 'fc_dropout': 0.7077180197984141, 'learning_rate_model': 0.0021820735849430416}. Best is trial 443 with value: 0.9660463333129883.
Epoch 18: reducing lr to 0.0001883636043265618
Epoch 22: reducing lr to 0.0001665335831617953
Epoch 27: reducing lr to 0.00012940148632275763
Epoch 30: reducing lr to 0.00010432651010857572
Epoch 36: reducing lr to 5.5186845284582424e-05
Epoch 40: reducing lr to 2.80955258214673e-05
Epoch 43: reducing lr to 1.2985335527547796e-05
Epoch 46: reducing lr to 3.42200235453026e-06
Epoch 49: reducing lr to 6.412339564236882e-09
[I 2024-06-24 16:04:03,641] Trial 478 finished with value: 0.9750798940658569 and parameters: {'hidden_size': 173, 'n_layers': 2, 'rnn_dropout': 0.13274790658213956, 'bidirectional': True, 'fc_dropout': 0.685541538366004, 'learning_rate_model': 0.002025251204239331}. Best is trial 443 with value: 0.9660463333129883.
Epoch 36: reducing lr to 4.332609930329577e-05
Epoch 39: reducing lr to 2.6857421880736387e-05
Epoch 42: reducing lr to 1.3696463129559355e-05
Epoch 45: reducing lr to 4.67019365551621e-06
Epoch 48: reducing lr to 3.4575577560781985e-07
[I 2024-06-24 16:04:06,903] Trial 479 finished with value: 0.9696882367134094 and parameters: {'hidden_size': 164, 'n_layers': 2, 'rnn_dropout': 0.11034287107606874, 'bidirectional': True, 'fc_dropout': 0.6405337939856582, 'learning_rate_model': 0.0015899846120304738}. Best is trial 443 with value: 0.9660463333129883.
Epoch 22: reducing lr to 0.0001995281424678948
Epoch 27: reducing lr to 0.00015503922817465535
Epoch 30: reducing lr to 0.00012499625827361416
Epoch 33: reducing lr to 9.472270230789492e-05
Epoch 36: reducing lr to 6.61207698725716e-05
Epoch 39: reducing lr to 4.098761347324097e-05
Epoch 42: reducing lr to 2.0902428356592632e-05
Epoch 45: reducing lr to 7.127269819400548e-06
Epoch 48: reducing lr to 5.276643510194589e-07
[I 2024-06-24 16:04:10,176] Trial 480 finished with value: 0.9741994738578796 and parameters: {'hidden_size': 167, 'n_layers': 2, 'rnn_dropout': 0.15561270350743778, 'bidirectional': True, 'fc_dropout': 0.6617042135183959, 'learning_rate_model': 0.0024265052318014654}. Best is trial 443 with value: 0.9660463333129883.
Epoch 23: reducing lr to 0.00012743406018459098
Epoch 27: reducing lr to 0.00010317542380301552
Epoch 32: reducing lr to 6.968970961978839e-05
Epoch 36: reducing lr to 4.400201506485216e-05
Epoch 39: reducing lr to 2.7276415398636097e-05
Epoch 42: reducing lr to 1.3910136999483315e-05
Epoch 45: reducing lr to 4.743051760724027e-06
Epoch 48: reducing lr to 3.511497940433773e-07
[I 2024-06-24 16:04:13,461] Trial 481 finished with value: 0.9667865037918091 and parameters: {'hidden_size': 175, 'n_layers': 2, 'rnn_dropout': 0.12689933639155013, 'bidirectional': True, 'fc_dropout': 0.7048269985981291, 'learning_rate_model': 0.0016147894219991797}. Best is trial 443 with value: 0.9660463333129883.
Epoch 23: reducing lr to 0.00012644421025434087
Epoch 27: reducing lr to 0.00010237400394786057
Epoch 32: reducing lr to 6.914839159141845e-05
Epoch 36: reducing lr to 4.3660227386740695e-05
Epoch 39: reducing lr to 2.706454458607072e-05
Epoch 42: reducing lr to 1.3802089369840476e-05
Epoch 45: reducing lr to 4.7062098877763674e-06
Epoch 48: reducing lr to 3.4842222184927346e-07
[I 2024-06-24 16:04:16,743] Trial 482 finished with value: 0.9668567776679993 and parameters: {'hidden_size': 175, 'n_layers': 2, 'rnn_dropout': 0.10435373216421497, 'bidirectional': True, 'fc_dropout': 0.7068253215637297, 'learning_rate_model': 0.0016022464708099983}. Best is trial 443 with value: 0.9660463333129883.
Epoch 23: reducing lr to 0.0001292302790864907
Epoch 27: reducing lr to 0.00010462971040565568
Epoch 32: reducing lr to 7.067200566768676e-05
Epoch 36: reducing lr to 4.462223583680807e-05
Epoch 39: reducing lr to 2.7660884141483387e-05
Epoch 42: reducing lr to 1.410620429083615e-05
Epoch 45: reducing lr to 4.809906408633391e-06
Epoch 48: reducing lr to 3.5609934910380266e-07
[I 2024-06-24 16:04:20,028] Trial 483 finished with value: 0.9671750068664551 and parameters: {'hidden_size': 175, 'n_layers': 2, 'rnn_dropout': 0.10158949142364129, 'bidirectional': True, 'fc_dropout': 0.7032697972263818, 'learning_rate_model': 0.0016375503328434325}. Best is trial 443 with value: 0.9660463333129883.
Epoch 22: reducing lr to 0.00013424222865437491
Epoch 27: reducing lr to 0.00010431015525726545
Epoch 31: reducing lr to 7.7261400190328e-05
Epoch 36: reducing lr to 4.4485952699455884e-05
Epoch 39: reducing lr to 2.7576403568019465e-05
Epoch 42: reducing lr to 1.4063121784080637e-05
Epoch 45: reducing lr to 4.795216218340448e-06
Epoch 48: reducing lr to 3.550117671932695e-07
[I 2024-06-24 16:04:23,314] Trial 484 finished with value: 0.9717308282852173 and parameters: {'hidden_size': 176, 'n_layers': 2, 'rnn_dropout': 0.05877475744948333, 'bidirectional': True, 'fc_dropout': 0.6990537681633189, 'learning_rate_model': 0.0016325490035118363}. Best is trial 443 with value: 0.9660463333129883.
Epoch 23: reducing lr to 0.00013058408574410476
Epoch 32: reducing lr to 7.141236026922576e-05
Epoch 36: reducing lr to 4.508969501418139e-05
Epoch 39: reducing lr to 2.7950657477662376e-05
Epoch 42: reducing lr to 1.4253979823146325e-05
Epoch 45: reducing lr to 4.860294625423905e-06
Epoch 48: reducing lr to 3.5982981902926096e-07
[I 2024-06-24 16:04:26,598] Trial 485 finished with value: 0.9675424695014954 and parameters: {'hidden_size': 175, 'n_layers': 2, 'rnn_dropout': 0.08778736327504803, 'bidirectional': True, 'fc_dropout': 0.7093710131236421, 'learning_rate_model': 0.0016547051866319773}. Best is trial 443 with value: 0.9660463333129883.
Epoch 23: reducing lr to 0.00011909682349325751
Epoch 27: reducing lr to 9.642528237514007e-05
Epoch 31: reducing lr to 7.14211604007e-05
Epoch 36: reducing lr to 4.112323042941093e-05
Epoch 39: reducing lr to 2.5491885180104617e-05
Epoch 42: reducing lr to 1.3000081207447983e-05
Epoch 45: reducing lr to 4.43274268706567e-06
Epoch 48: reducing lr to 3.2817619544020777e-07
[I 2024-06-24 16:04:29,884] Trial 486 finished with value: 0.9664369821548462 and parameters: {'hidden_size': 175, 'n_layers': 2, 'rnn_dropout': 0.07994892680436329, 'bidirectional': True, 'fc_dropout': 0.7075037069155242, 'learning_rate_model': 0.0015091435562206942}. Best is trial 443 with value: 0.9660463333129883.
Epoch 23: reducing lr to 0.00011629598446732878
Epoch 27: reducing lr to 9.415761741111372e-05
Epoch 37: reducing lr to 3.4783434398012755e-05
Epoch 40: reducing lr to 2.044341101819741e-05
Epoch 43: reducing lr to 9.448641505617385e-06
Epoch 46: reducing lr to 2.489983675103445e-06
Epoch 49: reducing lr to 4.665870791424262e-09
[I 2024-06-24 16:04:33,171] Trial 487 finished with value: 0.9662184715270996 and parameters: {'hidden_size': 175, 'n_layers': 2, 'rnn_dropout': 0.08178761406924301, 'bidirectional': True, 'fc_dropout': 0.7116796219839145, 'learning_rate_model': 0.0014736525326651323}. Best is trial 443 with value: 0.9660463333129883.
Epoch 23: reducing lr to 0.00011650871750775471
Epoch 27: reducing lr to 9.43298541080459e-05
Epoch 37: reducing lr to 3.484706157989559e-05
Epoch 40: reducing lr to 2.0480806883604962e-05
Epoch 43: reducing lr to 9.465925320227076e-06
Epoch 46: reducing lr to 2.4945384479981663e-06
Epoch 49: reducing lr to 4.674405779852079e-09
[I 2024-06-24 16:04:36,455] Trial 488 finished with value: 0.9661374092102051 and parameters: {'hidden_size': 175, 'n_layers': 2, 'rnn_dropout': 0.08689323931215441, 'bidirectional': True, 'fc_dropout': 0.7057136258490064, 'learning_rate_model': 0.0014763481939576622}. Best is trial 443 with value: 0.9660463333129883.
Epoch 22: reducing lr to 0.0001524384252695277
Epoch 31: reducing lr to 8.773398875442878e-05
Epoch 36: reducing lr to 5.0515911752175366e-05
Epoch 39: reducing lr to 3.1314315745821564e-05
Epoch 42: reducing lr to 1.596934258785473e-05
Epoch 45: reducing lr to 5.445195721777798e-06
Epoch 48: reducing lr to 4.031327197526278e-07
[I 2024-06-24 16:04:39,742] Trial 489 finished with value: 0.9724466800689697 and parameters: {'hidden_size': 176, 'n_layers': 2, 'rnn_dropout': 0.031187526358601635, 'bidirectional': True, 'fc_dropout': 0.7049142345393102, 'learning_rate_model': 0.0018538369167828217}. Best is trial 443 with value: 0.9660463333129883.
Epoch 23: reducing lr to 0.00011787636254975847
Epoch 27: reducing lr to 9.543715114163671e-05
Epoch 36: reducing lr to 4.070181451639584e-05
Epoch 39: reducing lr to 2.523065360964009e-05
Epoch 42: reducing lr to 1.286686110206956e-05
Epoch 45: reducing lr to 4.3873175517559084e-06
Epoch 48: reducing lr to 3.248131651143512e-07
[I 2024-06-24 16:04:43,029] Trial 490 finished with value: 0.9663068056106567 and parameters: {'hidden_size': 175, 'n_layers': 2, 'rnn_dropout': 0.08162181102286463, 'bidirectional': True, 'fc_dropout': 0.7114057904454196, 'learning_rate_model': 0.0014936784017818366}. Best is trial 443 with value: 0.9660463333129883.
Epoch 36: reducing lr to 4.0263196248199675e-05
Epoch 39: reducing lr to 2.495875846876721e-05
Epoch 42: reducing lr to 1.272820290216458e-05
Epoch 45: reducing lr to 4.3400381454335646e-06
Epoch 48: reducing lr to 3.2131285463280533e-07
[I 2024-06-24 16:04:46,307] Trial 491 finished with value: 0.9676612615585327 and parameters: {'hidden_size': 174, 'n_layers': 2, 'rnn_dropout': 0.0826889974472058, 'bidirectional': True, 'fc_dropout': 0.7086121355518238, 'learning_rate_model': 0.0014775819539547344}. Best is trial 443 with value: 0.9660463333129883.
Epoch 36: reducing lr to 4.056310519792825e-05
Epoch 39: reducing lr to 2.5144669070418245e-05
Epoch 42: reducing lr to 1.2823011618809626e-05
Epoch 45: reducing lr to 4.372365839289626e-06
Epoch 48: reducing lr to 3.237062215223326e-07
[I 2024-06-24 16:04:49,592] Trial 492 finished with value: 0.9679226875305176 and parameters: {'hidden_size': 174, 'n_layers': 2, 'rnn_dropout': 0.0712458479059449, 'bidirectional': True, 'fc_dropout': 0.7101703348626436, 'learning_rate_model': 0.001488588036264165}. Best is trial 443 with value: 0.9660463333129883.
Epoch 18: reducing lr to 0.00018873408623945107
Epoch 23: reducing lr to 0.0001601407560120484
Epoch 31: reducing lr to 9.60347915784117e-05
Epoch 34: reducing lr to 7.100922425126576e-05
Epoch 37: reducing lr to 4.7897143712284524e-05
Epoch 40: reducing lr to 2.81507853509671e-05
Epoch 43: reducing lr to 1.3010875662877838e-05
Epoch 46: reducing lr to 3.4287329009261706e-06
Epoch 49: reducing lr to 6.424951638859394e-09
[I 2024-06-24 16:04:52,879] Trial 493 finished with value: 0.9707530736923218 and parameters: {'hidden_size': 171, 'n_layers': 2, 'rnn_dropout': 0.08456180882321014, 'bidirectional': True, 'fc_dropout': 0.7088603672961066, 'learning_rate_model': 0.0020292345583639796}. Best is trial 443 with value: 0.9660463333129883.
Epoch 18: reducing lr to 0.00013738983277577105
Epoch 27: reducing lr to 9.438367157170021e-05
Epoch 36: reducing lr to 4.025252899666431e-05
Epoch 39: reducing lr to 2.4952145944690473e-05
Epoch 42: reducing lr to 1.2724830717276091e-05
Epoch 45: reducing lr to 4.338888304315997e-06
Epoch 48: reducing lr to 3.212277266409751e-07
[I 2024-06-24 16:04:56,167] Trial 494 finished with value: 0.971123456954956 and parameters: {'hidden_size': 176, 'n_layers': 2, 'rnn_dropout': 0.051803399008257346, 'bidirectional': True, 'fc_dropout': 0.7047283990802394, 'learning_rate_model': 0.0014771904863159057}. Best is trial 443 with value: 0.9660463333129883.
Epoch 22: reducing lr to 0.0001607263746283347
Epoch 27: reducing lr to 0.00012488911469567314
Epoch 30: reducing lr to 0.00010068853037940558
Epoch 36: reducing lr to 5.32624195153967e-05
Epoch 39: reducing lr to 3.301684883515376e-05
Epoch 42: reducing lr to 1.683758235369833e-05
Epoch 45: reducing lr to 5.741246447250035e-06
Epoch 48: reducing lr to 4.2505070768962276e-07
[I 2024-06-24 16:04:59,456] Trial 495 finished with value: 0.9741877317428589 and parameters: {'hidden_size': 176, 'n_layers': 2, 'rnn_dropout': 0.0741757292262295, 'bidirectional': True, 'fc_dropout': 0.7118644154894292, 'learning_rate_model': 0.001954628475463757}. Best is trial 443 with value: 0.9660463333129883.
Epoch 18: reducing lr to 0.00013925047990748663
Epoch 27: reducing lr to 9.566189357869021e-05
Epoch 36: reducing lr to 4.0797662148869056e-05
Epoch 39: reducing lr to 2.5290068612213537e-05
Epoch 42: reducing lr to 1.289716093485708e-05
Epoch 45: reducing lr to 4.397649130464154e-06
Epoch 48: reducing lr to 3.2557805909371253e-07
[I 2024-06-24 16:05:02,739] Trial 496 finished with value: 0.9716408252716064 and parameters: {'hidden_size': 173, 'n_layers': 2, 'rnn_dropout': 0.09312282705202957, 'bidirectional': True, 'fc_dropout': 0.7138305761617854, 'learning_rate_model': 0.0014971958257637451}. Best is trial 443 with value: 0.9660463333129883.
Epoch 23: reducing lr to 0.00021181910022917161
Epoch 27: reducing lr to 0.00017149673646167638
Epoch 30: reducing lr to 0.0001382646870487318
Epoch 33: reducing lr to 0.00010477757472021801
Epoch 36: reducing lr to 7.313952977568588e-05
Epoch 39: reducing lr to 4.533847355131858e-05
Epoch 42: reducing lr to 2.312123382890801e-05
Epoch 45: reducing lr to 7.883833841923178e-06
Epoch 48: reducing lr to 5.836762425381892e-07
[I 2024-06-24 16:05:06,021] Trial 497 finished with value: 0.9777541160583496 and parameters: {'hidden_size': 170, 'n_layers': 2, 'rnn_dropout': 0.05229733536285223, 'bidirectional': True, 'fc_dropout': 0.6875161608630229, 'learning_rate_model': 0.002684080236727868}. Best is trial 443 with value: 0.9660463333129883.
Epoch 22: reducing lr to 0.0001491678559819018
Epoch 27: reducing lr to 0.0001159078061563343
Epoch 30: reducing lr to 9.344758900582274e-05
Epoch 36: reducing lr to 4.9432091913306214e-05
Epoch 39: reducing lr to 3.064246651121932e-05
Epoch 42: reducing lr to 1.5626720041609808e-05
Epoch 45: reducing lr to 5.328368944924975e-06
Epoch 48: reducing lr to 3.9448349965127713e-07
[I 2024-06-24 16:05:09,308] Trial 498 finished with value: 0.9728501439094543 and parameters: {'hidden_size': 176, 'n_layers': 2, 'rnn_dropout': 0.09616471830674152, 'bidirectional': True, 'fc_dropout': 0.6987632212353243, 'learning_rate_model': 0.0018140628107883735}. Best is trial 443 with value: 0.9660463333129883.
Epoch 23: reducing lr to 0.00011477823031287948
Epoch 27: reducing lr to 9.292878637577464e-05
Epoch 37: reducing lr to 3.432948319492175e-05
Epoch 40: reducing lr to 2.017660840978319e-05
Epoch 43: reducing lr to 9.325329295271196e-06
Epoch 46: reducing lr to 2.4574874278365496e-06
Epoch 49: reducing lr to 4.604977504288517e-09
[I 2024-06-24 16:05:12,591] Trial 499 finished with value: 0.9669418931007385 and parameters: {'hidden_size': 170, 'n_layers': 2, 'rnn_dropout': 0.07608692919890674, 'bidirectional': True, 'fc_dropout': 0.7146005830620964, 'learning_rate_model': 0.0014544202069411468}. Best is trial 443 with value: 0.9660463333129883.

Optuna study saved to optuna/no-name-89061ccd-9025-4f3b-847b-8dee65c6bea4.pkl
To reload the study run: study = joblib.load('optuna/no-name-89061ccd-9025-4f3b-847b-8dee65c6bea4.pkl')

Study statistics    : 
  Study name        : no-name-89061ccd-9025-4f3b-847b-8dee65c6bea4
  # finished trials : 500
  # pruned trials   : 0
  # complete trials : 500

Best trial          :
  value             : 0.9660463333129883
  best_params = {'hidden_size': 171, 'n_layers': 2, 'rnn_dropout': 0.1258914379041145, 'bidirectional': True, 'fc_dropout': 0.6858054121901836, 'learning_rate_model': 0.001593407261945087}

O Melhor modelo foi o de nmero 443
Best hyperparameters:  {'hidden_size': 171, 'n_layers': 2, 'rnn_dropout': 0.1258914379041145, 'bidirectional': True, 'fc_dropout': 0.6858054121901836, 'learning_rate_model': 0.001593407261945087}
Epoch 17: reducing lr to 0.0001513587720763586
Epoch 28: reducing lr to 9.532754219884988e-05
Epoch 44: reducing lr to 7.194170666533569e-06
Epoch 47: reducing lr to 1.2443595154857256e-06
Mtricas de Treinamento para <class 'tsai.models.RNNPlus.LSTMPlus'>
train_loss    0.419144
valid_loss    0.395908
mae           0.754896
_rmse         0.978516
dtype: float64
Performing stepwise search to minimize aic
 ARIMA(20,1,0)(0,0,0)[0] intercept   : AIC=-30047.933, Time=26.72 sec
 ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=-2584.548, Time=0.26 sec
 ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=-15998.099, Time=0.17 sec
 ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=-10467.897, Time=0.69 sec
 ARIMA(0,1,0)(0,0,0)[0]             : AIC=-2586.547, Time=0.16 sec
 ARIMA(19,1,0)(0,0,0)[0] intercept   : AIC=-30020.511, Time=4.97 sec
 ARIMA(21,1,0)(0,0,0)[0] intercept   : AIC=-30813.112, Time=14.24 sec
 ARIMA(22,1,0)(0,0,0)[0] intercept   : AIC=-30987.793, Time=17.92 sec
 ARIMA(23,1,0)(0,0,0)[0] intercept   : AIC=-30986.050, Time=33.65 sec
 ARIMA(22,1,1)(0,0,0)[0] intercept   : AIC=-29025.957, Time=65.28 sec
 ARIMA(21,1,1)(0,0,0)[0] intercept   : AIC=-28110.755, Time=56.19 sec
 ARIMA(23,1,1)(0,0,0)[0] intercept   : AIC=-27887.095, Time=70.07 sec
 ARIMA(22,1,0)(0,0,0)[0]             : AIC=-30989.725, Time=5.32 sec
 ARIMA(21,1,0)(0,0,0)[0]             : AIC=-30815.036, Time=4.29 sec
 ARIMA(23,1,0)(0,0,0)[0]             : AIC=-30988.033, Time=13.09 sec
 ARIMA(22,1,1)(0,0,0)[0]             : AIC=-29445.799, Time=40.43 sec
 ARIMA(21,1,1)(0,0,0)[0]             : AIC=-27890.561, Time=31.20 sec
 ARIMA(23,1,1)(0,0,0)[0]             : AIC=-30201.318, Time=38.79 sec

Best model:  ARIMA(22,1,0)(0,0,0)[0]          
Total fit time: 423.464 seconds
RMSE: 1.1789677577462956
MAE: 0.9223833228704643
R: 0.733685937314567
MAPE: 0.10368214499197707
Correlao Linear: 0.8596011206046437
index
2018-12-23    1.883934
2018-12-24    0.437126
2018-12-25   -1.952691
2018-12-26   -1.458319
2018-12-27   -0.444618
                ...   
2021-12-18    0.930120
2021-12-19    1.403894
2021-12-20    1.777933
2021-12-21    2.170435
2021-12-22    2.361117
Freq: D, Length: 1096, dtype: float64
count    1096.000000
mean        0.018732
std         1.179357
min        -4.037848
25%        -0.716835
50%         0.058722
75%         0.749663
max         4.345004
dtype: float64
Shapiro-Wilk Test p-value: 0.05693588596549243
Interpretao: No podemos rejeitar a hiptese nula de que os resduos seguem uma distribuio normal (p > 0.05).
Kolmogorov-Smirnov Test p-value: 0.04807192335002386
Interpretao: Rejeitamos a hiptese nula de que os resduos seguem uma distribuio normal (p <= 0.05).
Shapiro-Francia Test p-value: 0.038207457043847586
Interpretao: Rejeitamos a hiptese nula de que os resduos seguem uma distribuio normal (p <= 0.05).
Durbin-Watson Statistic: 1.7413498376597665
Interpretao: No h evidncia de autocorrelao nos resduos (Durbin-Watson dentro do intervalo 1.5-2.5).
Breusch-Pagan Test p-value: 5.574188969784834e-09
Interpretao: Rejeitamos a hiptese nula de homocedasticidade (varincia constante dos resduos) (p <= 0.05).
